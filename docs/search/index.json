[{"content":"Redis 多机服务 主从同步(复制） 主从同步是Redis高可用服务的基石，其将主要存储数据的服务器成为主服务器(master)，把对主服务器进行复制的服务器成为从服务器(slave)。 并且从节点还可以是其他服务器的主节点，并且拥有属于自己的从节点 通过主从模式来进行读写的分离，主服务器进行写操作，然后将数据同步给从服务器，让从服务器来进行读操作，通过这种模式来分摊主服务器的压力。\nRedis的复制功能主要分为同步(sync)与命令传播(command propagate) 两个操作\n同步 同步操作用于将从服务器的数据库状态更新至主服务器当前的数据库状态。\n在从服务器对主服务器进行复制之前，需要先将从服务器的数据库状态更新至主服务器的服务器状态\n命令传播 命令传播操作用于在主服务器的数据库状态发生变化(执行写命令)，导致主从服务器的数据库状态不一致时，让主从服务器的数据库重新回到一致状态。 当客户端对主服务器进行写操作后，此时主从服务器的数据库状态就会不一致。\n为了能够再次让主从服务器的数据库状态恢复一致，此时主服务器会将同一命令发送给从服务器，当从服务器执行完改命令时，数据库状态再次恢复一致。 优缺点 优点\n 性能方面：可以实现读写的分离，由主服务器来进行写操作，并将写的结果同步至从服务器，由从服务器来进行读操作，这样就能将压力分摊到各个服务器上 高可用：当主服务器宕机之后，可以通过故障转移机制将从节点提升为主节点，快速的进行服务器的宕机恢复。 防止数据丢失：当主服务器的磁盘损坏或者数据丢失后，因为从服务器还保留相关的数据，不至于导致数据全部丢失  缺点\n 由于主从同步需要人工管理，主节点崩溃后需要人工进行从节点的提升才能恢复Redis的正常使用  从上面可以看到，主从同步并没有一个自动的管理机制，当出现主服务器宕机的情况，需要人工干预来进行恢复，但是如果主从服务器数量庞大，又或是因为高并发导致的大量崩溃，这时需要的时间和难度都是非常大的，于是Redis中又引入了哨兵模式(Sentinel) 来作为解决方案，将管理由人工转向哨兵，使得Redis具有自动容灾恢复的能力\n哨兵 哨兵是Redis高可用性的解决方案，通过一个或者多个哨兵组成的哨兵系统，可以监控任意多个主服务器以及它们的从服务器。当某个被监视的主服务器进入下线状态时，哨兵就会自动将下线主服务器的某个从服务器升级为新的主服务器，然后由新的主服务器继续处理命令请求\n总结下来就是哨兵模式可以用来监控主从同步服务器节点，并在主从服务器出现问题的时候实现自动容灾恢复 下线判断与选举 由于一个主服务器可能会同时被多个哨兵进行同时进行监控，所以当一个哨兵主观的将其判定为下线时，为了确保这个主服务器真的下线了，它会向同样监视这一主服务器的其他哨兵进行询问，看看它们是否也认为该服务器下线了，当积累到一定数量的下线判断时，此时就会客观认为主服务器下线，开始进行故障转移。\n但是故障转移只能由一个哨兵来进行，所以此时所有监控该服务器的哨兵会进行协商，选举出一个领头哨兵来进行故障转移。\n每一个哨兵都会向其他哨兵发送一个带有自己运行ID的命令，如果接收到该命令的哨兵还没有进行投票，就会将该ID设置为它的头领ID，并返回一个确认恢复。通过这种方法每一个哨兵都可以直到有多少个人为其投票，并选出一个票数最高的作为头领哨兵\n故障转移 故障转移分为以下三个步骤\n 在已下线主服务器的从服务器中挑选一个出来作为新的主服务器 在已下线主服务器的从服务器改为复制新的主服务器 将已下线主服务器设置为新的主服务器的从服务器，当这个旧的主服务器重新上线时，他就会成为新的主服务器的从服务器   集群 集群(Cluster)是Redis多机运行中最完美的方案 ，它的出现甚至可以让我们抛弃掉主从同步和哨兵来实现Redis多机的运行。\n集群是无代理模式去中心化的运行模式，客户端发送的绝大多数命令会直接交给相关节点执行，大部分情况下请求命令不需要转发，或者仅仅只需要转发一次就能完成请求和响应。所以集群中的单个节点的性能与单机Redis服务器的性能非常接近，并且通过水平拓展能够使得性能进行翻倍，所以集群的性能非常的高 由于主从同步只能有一个主节点，而集群可以拥有无数个主从节点，有着更强大的平行拓展能力。 所以在理论情况下，如果水平拓展一倍的主节点，相当于请求处理的性能也提高了一倍，也就是说通过平行拓展N倍的主从节点，就会比单机服务来说性能提升了N倍。\n握手 每个节点其实就是运行在集群模式下的Redis服务器，而这些节点在一开始时都是互相独立的，它们都处于一个只包含自己的集群中，要组建一个真正可以工作的集群，我们就必须要将各个独立的节点通过握手的方式连接起来。 分片 集群通过分片的方式来保存数据库中的键值对。 集群的整个数据库被分为个16384个槽，并且将一个或者多个槽指派给某个节点，让这个节点来负责管理这个槽中的数据以及相关命令，通过这种方法就能很好的进行压力的分摊。\n节点之间会互相转递指派槽的信息 对于发送来的命令，会通过其所在的槽来分配至对应的节点，如果分配错误，也会通过转向操作来转交给至正确的节点 ","date":"2022-05-26T15:07:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E5%A4%9A%E6%9C%BA%E6%9C%8D%E5%8A%A1/","title":"Redis 多机服务"},{"content":"Redis 事务 MySQL 事务 ：ACID、并发带来的问题、事务的隔离级别、事务的实现 在之前的MySQL系列博客中我已经讲过了一些事务的内容，但是Redis与传统的关系型数据库不同，因此下面我会在讲解Redis事务的同时与SQL数据库的事务进行比较。\n为了能帮助大家更好的理解，首先给出Redis事务的所有接口，并结合案例来讲解其具体使用方法\n   命令 作用     MUTLI 标记一个事务块的开始。   EXEC 执行所有事务块内的命令   DISCARD 取消事务，放弃执行事务块内的所有命令   WATCH 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断   UNWATCH 取消 WATCH 命令对所有 key 的监视    事务的实现 Redis的事务与传统的SQL事务不同，它的本质是一组命令的集合， 一个事务中的所有命令都会被序列化，在事务执行过程的中，会按照顺序执行它的事务主要存在以下三个阶段\n 事务开始(multi) 命令入队 事务执行(exec)  下面就结合一个具体案例，来讲解一下它的实现原理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  127.0.0.1:6379\u0026gt; MULTI\t# 开始事务 OK 127.0.0.1:6379\u0026gt; SET CITY1 \u0026#34;beijing\u0026#34;\t# 插入三个城市 QUEUED 127.0.0.1:6379\u0026gt; SET CITY2 \u0026#34;shanghai\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; SET CITY3 \u0026#34;shenzhen\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; GET CITY2\t# 获取城市的名字 QUEUED 127.0.0.1:6379\u0026gt; GET CITY3 QUEUED 127.0.0.1:6379\u0026gt; EXEC\t# 执行事务 1) OK 2) OK 3) OK 4) \u0026#34;shanghai\u0026#34; 5) \u0026#34;shenzhen\u0026#34;    事务开始\n 当我们执行MULTI命令时即代表着事务的开启，此时会将客户端从非事务状态切换到事务状态，通过为客户端状态中的flags加上REDIS_MULTI标识实现\n1 2  # 打开事务标识 client.flags |= REDIS_MULTI    命令入队\n 当我们切换至事务状态后，Redis服务器会根据我们命令来决定执行命令还是将命令放入队列中\n 如果客户端发送的命令是事务相关即EXEC、DISCARD、WATCH、UNWATCH、MULTI等，服务器会立刻执行命令 如果客户端发送的是上面以外的命令，这时候服务器就会将命令放入事务队列中，并向客户端返回QUEUED，告知客户端命令入队  如下图  事务队列\n 在客户端的事务状态中维护者一个事务队列，以及队列长度的计数器\n1 2 3 4 5 6 7  typedef struct multiState { multiCmd* commands;\t//事务队列，FIFO \tint count;\t//命令计数器 \t} multiState   事务队列其实就是multiCmd类型的数组，其中每一个multiCmd节点都包含着每条命令的具体信息，如指向具体实现的命令指针、命令的参数、命令的数量\n1 2 3 4 5 6 7 8 9  typedef struct multiCmd { robj** argv;\t//参数 \tint argc;\t//参数的数量 \tstruct redisCommand* cmd;\t//指向具体实现的命令指针 \t} multiCmd   假设此时客户端执行以下命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  redis\u0026gt; MULTI OK redis\u0026gt; SET \u0026#34;name\u0026#34; \u0026#34;Practical Common Lisp\u0026#34; QUEUED redis\u0026gt; GET \u0026#34;name\u0026#34; QUEUED redis\u0026gt; SET \u0026#34;author\u0026#34; \u0026#34;Peter Seibel\u0026#34; QUEUED redis\u0026gt; GET \u0026#34;author\u0026#34; QUEUED   此时底层的事务队列如下  执行事务\n 当客户端向服务器发送EXEC命令时，服务器会立即遍历客户端的事务队列，按照FIFO(先进先出)的顺序执行队列中的所有命令，执行完毕后将命令所得的结果全部返回给客户端 讲完了原理，下面就来讲讲Redis的ACID与传统SQL的有什么不同\nACID  原子性：原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。\n 在Redis中，单条命令能够保证原子性，但是事务并不能保证原子性。下面我分别以编译、运行两个阶段的异常举例，来验证这个结论\n编译时异常\n首先我们来验证编译时异常是否能够保证原子性，我们故意产生语法错误，来验证编译异常时事务是否能够执行\n1 2 3 4 5 6 7 8 9 10 11 12  127.0.0.1:6379\u0026gt; MULTI\t#开启事务 OK\t127.0.0.1:6379\u0026gt; SET CITY1 \u0026#34;beijing\u0026#34;\tQUEUED 127.0.0.1:6379\u0026gt; SET CITY2 \u0026#34;shanghai\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; SET CITY3\t# 故意不给value,使其语法错误 (error) ERR wrong number of arguments for \u0026#39;set\u0026#39; command\t# 编译报错 127.0.0.1:6379\u0026gt; EXEC\t# 执行事务 (error) EXECABORT Transaction discarded because of previous errors.\t# 事务中存在错误命令，执行失败 127.0.0.1:6379\u0026gt; KEYS *\t# 所有命令都没有执行 (empty array)   从上面可以看到，在编译时异常时Redis是能够保证原子性的。\n运行时异常\n接着来看看运行时异常，我们故意对一个字符串使用计数操作，看看报错后事务是否能够执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  127.0.0.1:6379\u0026gt; MULTI\t# 开启事务 OK 127.0.0.1:6379\u0026gt; SET key1 \u0026#34;HELLO\u0026#34; # 插入一个字符串 QUEUED 127.0.0.1:6379\u0026gt; INCRBY key1 10 # 对这个字符串+10,必定执行失败 QUEUED 127.0.0.1:6379\u0026gt; SET key2 \u0026#34;WORLD\u0026#34; # 其他命令 QUEUED 127.0.0.1:6379\u0026gt; GET key2 QUEUED 127.0.0.1:6379\u0026gt; EXEC\t#执行事务 1) OK 2) (error) ERR value is not an integer or out of range\t# 运行错误 3) OK 4) \u0026#34;WORLD\u0026#34; 127.0.0.1:6379\u0026gt; GET key1\t# 其他命令执行成功 \u0026#34;HELLO\u0026#34; 127.0.0.1:6379\u0026gt; GET key2\t# 其他命令执行成功 \u0026#34;WORLD\u0026#34;   从上面我们可以看到，即使事务中有一条命令在执行期间出现了错误，整个事务也会继续执行下去，并且之前执行的命令也不会有任何影响。总结一下两种情况\n 编译时异常（代码有问题、命令有错）：事务中所有的命令都不会被执行！ 运行时错误（命令存在语法性错误）：其他命令可以正常执行的，错误命令抛出异常！  这也就是Redis事务与传统SQL数据库事务最大的区别，即Redis不支持事务回滚机制(rollback) 在官方文档中作者是这样描述的，不支持事务回滚的原因是因为这种复杂的功能和Redis追求的简单高效不相符。并且这种运行时错误通常由编程错误产生，通常只会出现在开发环境中，而并不会在生产环境中发生，就没有必要为Redis开发事务回滚功能\n 基于以上几点，我们得出结论，Redis的事务不能保证原子性 一致性：一致性即事务操作前与操作后的状态始终一致\n  隔离性：隔离性指的是即使数据库中有多个事务并发执行，各个事务之间也不会互相影响。\n 由于Redis使用单线程来执行事务以及事务队列中的命令，并且在执行事务的期间不会对事务进行终端，因此Redis的事务总是以串行的方式运行的，因此也不存在隔离级别这个概念 持久性：持久性指的是事务一旦提交，其结果就是永久性的。\n 从上面也可以看出，除了隔离性以及原子性以外，其余部分都与传统SQL数据库区别不大。\nWATCH乐观锁 并发编程中常见的锁机制：乐观锁、悲观锁、CAS、自旋锁、互斥锁、读写锁 如果不了解乐观锁及其实现原理的小伙伴可以看看我的往期博客，在这里就不再重复，我就简单的说明一下\n 悲观锁做事比较悲观，它始终认为共享资源在我们使用的时候会被其他线程修改，容易导致线程安全的问题，因此在访问共享数据之前就要先加锁，阻塞其他线程的访问 乐观锁则于悲观锁相反，它则比较乐观。它始终认为多线程同时修改共享资源的概率较低，所以乐观锁会直接对共享资源进行修改，但是在更新修改结果之前它会验证这段时间有没有其他线程对资源进行修改，如果没有则提交更新，如果有的话则放弃本次操作。  WATCH命令就是一个乐观锁，当它会监视任意数量的key，当执行事务时，如果这些key中有任何一个被修改，服务器都会拒绝执行事务，并向客户端返回事务执行失败的空回复\n下面就结合具体场景来演示一下\n假设此时小明的账户中有1000元，小王的账户有500元，此时小明想转250元给小王 1 2 3 4 5 6 7 8 9 10  127.0.0.1:6379\u0026gt; SET xiaoming 1000 OK 127.0.0.1:6379\u0026gt; SET xiaowang 500 OK 127.0.0.1:6379\u0026gt; MULTI OK 127.0.0.1:6379\u0026gt; DECRBY xiaoming 250\t# 转账 QUEUED 127.0.0.1:6379\u0026gt; INCRBY xiaowang 250 QUEUED   但是在转账的途中，正好小明花呗的自动还款时间到了，扣费900元，并先他一步提交\n1 2 3  # 另外一个线程中，抢先扣费 127.0.0.1:6379\u0026gt; DECRBY xiaoming 900 (integer) 100   当我们再次执行事务的时候，按道理来说金额不够就应该转账失败，但是此时小明的余额却变成了负数，这就出现了问题，用户可以无限的进行套现，这也就是我们通常所说的事务并发执行的问题。\n1 2 3  127.0.0.1:6379\u0026gt; EXEC 1) (integer) -150 2) (integer) 750   为了保证安全，通常我们会使用WATCH当作乐观锁操作，对key进行监控，当另一个线程修改被监控的key时，就会让事务失败。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  127.0.0.1:6379\u0026gt; WATCH xiaoming\t# 监控小明的账户 OK 127.0.0.1:6379\u0026gt; MULTI\t# 开启事务，转账 OK 127.0.0.1:6379\u0026gt; DECRBY xiaoming 250 QUEUED 127.0.0.1:6379\u0026gt; INCRBY xiaowang 250 # 另一个线程修改小明余额 127.0.0.1:6379\u0026gt; DECRBY xiaoming 900 (integer) 100 # 继续运行事务 127.0.0.1:6379\u0026gt; EXEC (nil)\t# key被修改，事务执行失败 127.0.0.1:6379\u0026gt; GET xiaoming\t# 可以看到，事务并没有执行 \u0026#34;100\u0026#34;   通过这种方法，就能够确保我们并发执行事务的安全，当我们确认当前操作不会导致恶劣影响的时候，就可以通过UNWATCH取消监控，然后WATCH来获取修改后的新余额来继续监控、执行事务。\n当你看到这里的时候，是不是感觉似曾相识？没错，这就是之前博客中我提到的CAS以及版本号机制，也是乐观锁的常见实现方法\n在Redis中，服务器通过一个字典来标记所有正在监控key的客户端 当某一个key被修改时，就会将所有监控它的客户端的REDIS_DIRTY_CAS标识打开，来标记数据已经被修改，当客户端执行EXEC命令时如果发现标识被修改，则说明此时可能会存在安全问题，于是拒绝执行事务 总结一下就是，我们通过CAS机制判断REDIS_DIRTY_CAS是否被打开来决定事务的执行，并通过WATCH实现版本号机制以及服务器对客户端的统一管理\n总结  Redis单条命令保证原子性，但是事务不能保证原子性 Redis事务中没有隔离级别的概念 Redis事务的本质就是一组命令的集合，命令通过事务队列以FIFO的方式顺序执行 WATCH命令即乐观锁，服务器通过字典将所有监控客户端与被监控key进行关联，并通过REDIS_DIRTY_CAS来判断key是否被修改，从而决定是否执行事务  ","date":"2022-05-26T15:06:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E4%BA%8B%E5%8A%A1/","title":"Redis 事务"},{"content":"Redis 缓存 缓存一致性 对于缓存和数据库的更新操作，主要分为以下两种\n 先删除缓存，再更新数据库 先更新数据库，再删除缓存   首先可能会带来疑惑的点是，为什么这里是删除缓存而不是更新缓存？\n 按照常理来说，更新的效率通常都会比删除高，因为我们在删除了缓存后当有读操作到来时，当其查询缓存不存在时，就会去查询数据库，并将读取到的值写入到缓存中，这样的效率明显比更新低。\n但是我们还需要考虑一个问题，即缓存的使用率问题。如果在短时间内对数据库进行了10000次更新操作，那么缓存也必定会进行10000次的更新操作，那这个缓存它真的有用到那么多次吗？如果它仅仅是一个冷门数据，可能在这期间内只进行了仅仅几次的查询操作，那我们的这些更新操作不是会显得很多余吗？\n所以，我们才会去使用删除。因为在我们删除缓存后，只有在其真正使用到这个数据的时候，才会将其写入缓存，因此我们就不用每次都对缓存进行更新操作，从而保证效率。\n先删除缓存，再更新数据库 对于这种情况，能够保证缓存的一致性吗？\n答案是否定的，例如下面这种场景：  线程A写入数据，此时先删除缓存 线程B读取数据，查询缓存不存在，直接去查询数据库 线程B将查询到的旧值写入至缓存中 线程A将新数据更新至数据库中  对于上述这种情况，线程B在线程A更新数据库之前就提前读取了数据库，从而读取到了旧值，而后线程B将读取到的旧值再次写入缓存中，就出现了缓存不一致的情况。\n 那么这个问题如何解决呢？\n 这时候就需要引入延时双删的机制\n延时双删 为了避免在更新数据库的时候，其他的线程读取到了数据库中的旧值并将其写入缓存这种情况，我们会在数据库更新完后等待一段时间，再次删除缓存，来保证下一个到来的线程能够将正确的缓存更新回去。 流程如下\n 线程A写入数据，此时先删除缓存 线程B读取数据，查询缓存不存在，直接去查询数据库 线程B将查询到的旧值写入至缓存中 线程A将新数据更新至数据库中，休眠一段时间 线程A将缓存再次删除，来确保缓存的一致性 其他线程查询数据库，将正确的值更新至缓存中  那么，为了保证我们能够将错误的缓存删除，所以我们的sleep时间只需要大于线程读写缓存的时间即可\n先更新数据库，再删除缓存 那么如果我们先更新数据库，再更新缓存呢？ 对于这种操作，缓存不一致的情况就更加明显了。由于磁盘I/O速度慢，在更新数据库、删除缓存这段操作之前，其他线程读取到的都是原本缓存中的旧值。甚至可能会由于缓存删除失败(如缓存服务当前不可用的情况)从而导致严重的缓存不一致问题。\n那么如何解决这个问题呢？可以使用以下几种方法\n修改缓存过期时间 这是解决这个问题最简单的方法，同时也是治标不治本的方法。\n我们可以将缓存过期时间变短，使其每隔一段时间就会去数据库中加载数据，对于更新不频繁的数据来说，就可以很好的解决不一致的问题，但若是更新特别频繁的热点数据，这个方法则失去了作用。\n由于这个方法的适用面小，且实时性和一致性不高，所以我们通常都会选择使用消息队列来解决这个问题。\n消息队列 我们可以引入一个消息队列来解决这个问题，在更新数据库后，我们往消息队列中写入数据，等到消费者从消息队列中取出数据时，再将缓存删除。借助消息队列的消息重试机制来保证我们一定能够成功删除缓存，从而确保缓存的一致性。 但是这种方法也存在几个问题\n 引入消息队列后可能会因为消息的处理导致一定程度的延迟，从而引起短期内的消息不一致 引入消息队列后导致问题整体复杂化  所以我们只有在对实时性和一致性要求不高的情况下才会选择这种做法\n缓存淘汰策略 redis中缓存的数据是有过期时间的，当缓存数据失效时，redis会删除过期数据以节省内存，那redis是怎样怎样的策略来删除过期数据的呢？\n过期键删除策略 过期删除策略通常有以下三种\n 定时删除：在为键设置过期时间的同时创建一个定时器，当过期时间到来时就会触发定时器中的处理函数，立即执行过期键的删除操作 定期删除：每隔一段时间就对数据库进行一次检查，删除其中的过期键。检查的数据库数量及删除的过期键数量由算法决定 惰性删除：不会主动去删除过期键。每次获取键时都会判断获取的键是否过期，如果过期则删除，没过期则返回  其中前两种为主动删除策略，最后一种为被动删除策略。下面就来谈谈这三种策略的优缺点以及Redis中究竟使用的哪一种\n定时删除 定时删除策略对于内存来说十分友好，通过定时器能够保证过期键能够在第一时间被删除，而不会一直占用内存。\n但是同样的，它对CPU时间非常不友好。在过期键比较多的时候，维护大量的定时器会给CPU带来巨大的压力，即使过期键少的时候，它也会将宝贵的CPU时间用在维护定时器，以及删除和当前任务无关的过期键上，对服务器的响应时间与吞吐量造成了一定的影响。\n惰性删除 从开始的描述可以看出，惰性删除对于CPU时间来说是最为友好的，因为我们只会在取出键的时候才会对其进行删除操作，这也就保证了我们不会在执行其他任务的时候又背地里去删除无关的过期键，合理的利用了CPU时间。\n但是！！！也正是因为这个原因，使得它对内存极度不友好。如果一个键已经过期，而只要我们不去获取这个键，就不会触发过期检查，那也就意味着他会一直占用这一块内存而不释放。\n这意味着什么呢？如果我们有非常多的过期键，而这些过期键又恰好因为版本迭代、项目组交替，在后续版本中并没有对其进行访问，那么它可能永远也不会被删除。我们可以将这种情况当成内存泄漏中的一种，对于Redis这种内存数据库来说，这种情况造成的后果十分严重\n定期删除 定期删除策略其实是上述两种策略的折中选择。\n定期删除策略相对于定时删除策略来说，由于其每隔一段时间才进行一次删除操作，通过限制了删除操作的时常和频率，大大减少了删除操作对CPU时间的影响。\n相比于惰性删除，并且由于定期删除过期键，有效地减少了过期键带来的空间浪费。即兼顾了CPU，又避免了内存浪费，是两者的折中选择。\n但是上述这些优点的前提，就是我们必须要确定一个合理的删除操作的时长和频率。\n 如果删除操作过于频繁，则又退化成了定时删除策略，浪费了大量的CPU时间 如果删除操作执行过少，则又会像惰性删除一样，出现大量的内存浪费问题。  Redis的选择 下面给出三种的效率对比\n CPU：惰性删除 \u0026gt; 定期删除 \u0026gt; 定时删除 内存利用率：定时删除 \u0026gt; 定期删除 \u0026gt; 惰性删除\n  定时删除占用太多CPU时间，影响服务器的吞吐量和性能，但是很好的避免了内存浪费 惰性删除浪费太多内存，有内存泄漏的风险，但是却保证了CPU的效率 定期删除属于前两种的折中，既保证了CPU时间的合理利用，又避免了内存的浪费  为了能够在合理利用CPU时间与避免浪费内存空间之间取得平衡，Redis同时使用了惰性删除和定期删除。\n这样的搭配虽然保证了Redis强大的吞吐量以及响应速度，但是却存在因为没有定时删除机制，所以存在着内存浪费问题。\n由于Redis中通常存储的数据量十分庞大，这就导致了定期删除每次只能抽取其中的一部分进行删除，倘若有一部分过期键一直没有被抽取到，并且我们也一直没有访问它来触发惰性删除，这个过期键就会一直存在内存中，如果不进行处理，就可能导致内存耗尽。\n为了解决这个问题，Redis又引入了内存淘汰机制\n内存淘汰机制 当Redis的内存占用过高时，如果内存不足以容纳新写入的数据，就会通过某种机制来删除一部分键，来减少当前占用的内存，这就是内存淘汰机制。\n当前Redis提供了8种内存淘汰策略，除却之前的6种，还有两种Redis4.0后新增的LFU模式：volatile-lfu以及allkeys-lfu\n   名称 作用     volatile-lru 在已设置过期时间的key中，挑选最近最少使用的key淘汰   volatile-lfu 在已设置过期时间的key中，挑选最不经常使用的key淘汰   volatile-ttl 在已设置过期时间的key中，挑选将要过期的key淘汰   volatile-random 在已设置过期时间的key中，随机挑选key淘汰   allkeys-lru 在所有key中，挑选最近最少使用的key淘汰   allkeys-lfu 在所有key中， 挑选最不经常使用的key淘汰   allkeys-random 在所有key中，随机挑选key淘汰   no-eviction 当内存不足以写入新数据时，写入操作会报错，并且不会淘汰数据(不常用)    乍一看策略很多很难记，其实总共就是四种不同的淘汰策略，以及两种key的选择范围\n选择范围\n allkeys：淘汰的范围为所有的key volatile：淘汰的范围为已设置过期时间的key  淘汰策略\n LRU：Least recently used，即淘汰最近最少使用的key LFU：Least Frequently Used，即淘汰最不经常使用的key TTL：Time To Live，即淘汰生命时间最短，即将要过期的key Random：随机淘汰  其中LRU和LFU较为常用，如果有想了解其算法原理的，可以看看我的往期博客 高级数据结构与算法 | LRU缓存机制（Least Recently Used） 高级数据结构与算法 | LFU缓存机制（Least Frequently Used）\n缓存常见问题 缓存雪崩 缓存雪崩指的是在短时间内，有大量缓存的键同时过期，由于缓存过期，导致此时所有的请求就直接查询数据库，而数据库很难抵挡这样巨大的压力，严重情况下就会导致数据库被大流量打死，直接宕机。\n下面是正常的查询流程以及缓存雪崩后的查询流程 缓存雪崩的解决方法有以下几种\n 随机化过期时间，为了避免缓存同时过期，在设置缓存时在原有时间上添加随机时间，使失效时间分散开来 加锁排队，加锁排队可以起到缓冲的作用，防止大量请求同时操作数据库，但是也正因为如此也减少了吞吐量，导致响应时间变慢，用户体验变差。 设置二级缓存，即加入一个本地缓存作为备案，当Redis缓存失效后就暂时使用本地缓存进行代替，避免直接访问数据库。 设置热点数据永不过期，有更新操作时直接更新缓存即可  缓存击穿 缓存击穿与缓存雪崩很像，不过一个是针对大量缓存一个是针对热点缓存。\n缓存击穿即当某个热点缓存突然失效，而正好对其有着大量的请求，此时这些请求就会直接向数据库进行查询，导致数据库面临巨大的压力\n缓存击穿的解决方法有以下几种\n 设置热点数据永不过期，有更新操作时直接更新缓存即可 加锁排队，通过加锁来减少同一时间的访问量，缓解压力  缓存穿透 缓存穿透是指查询的数据在缓存中和数据库中都不存在，此时请求就会直接绕过缓存抵达数据库，导致数据库压力过大。(由于主键通常都是从1开始自增，此时大量查询负数或者特别大的数据就会导致缓存穿透)。 出于容错考虑，由于这些数据在数据库中不存在，所以不会将结果保存到缓存中。而又因为缓存中没有这些数据，所以每次请求都会绕过缓存，直接向数据库查询，这就是缓存穿透。 缓存穿透的解决方法有以下几种\n 参数校验，对于那些不合法的请求就直接返回空结果，不进行查询 布隆过滤器，可以根据布隆过滤器来判断数据在不在数据库，虽然布隆过滤器查询存在不一定准确，但是如果布隆过滤器中查不到，则一定说明不存在，就不会进入数据库查询 缓存空结果，将每次查询的结果进行缓存，即使查询不到的也缓存一个空结果，当有非法请求时就直接返回空结果  缓存预热 与上面三种不同，缓存预热并不是一个需要解决的问题，而是一种优化的策略，通过这种策略能够更快的响应用户的查询。\n缓存预热指的是在启动系统的时候，提前将查询的结果预存到缓存中，这样用户查询时就可以直接从缓存中读取，减少了用户的等待时间 缓存预热的实现方法有以下三种\n 把需要缓存的函数写入到系统的构造函数中，这样系统就会在启动的时候自动的加载数据并缓存数据 把需要缓存的函数挂载到前端页面或者后端的接口上，手动触发缓存预热 设置定时任务，定时自动进行缓存预热  ","date":"2022-05-26T15:05:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E7%BC%93%E5%AD%98/","title":"Redis 缓存"},{"content":"Redis 持久化策略 什么是持久化 由于内存具有易失性，无法进行断电存储，所以在重启之后数据就会丢失，但是硬盘具有永久存储的特性，所以持久化就是将数据从内存中保存到硬盘的过程，目的就是为了防止数据的丢失。 同时持久化也是Redis比起Memcached的优势，Memcached并不支持持久化\nRedis的持久化分为以下三种\n RDB（Redis DataBase）持久化 AOF（Append Only File）持久化 混合持久化  RDB持久化 RDB持久化其实就是以快照的方式进行持久化的存储。 对于一个Redis服务器来说，它的所有非空数据库以及数据库中的所有键值对就是当前数据库的状态。所以只需要将数据库的状态保存在硬盘当中，即使服务器停机或者断电，只要硬盘中存储的状态还在，就可以通过它来还原数据库原来的状态。\n为了保证文件的安全以及容量更小，RDB持续化所生成的RDB文件是一个经过压缩的二进制文件，通过这个文件就可以还原数据库的状态。\nSAVA与BGSAVA RDB持久化根据执行持久化的对象不同又分为SAVA和BGSAVA两种方式\nSAVA即让Redis服务进程来执行持久化，所以直到RDB持久化结束之前，Redis服务进程会一直处于阻塞状态，无法处理任何命令。\nBGSAVE则会通过fork()来创建一个子进程，然后让子进程来接管RDB持久化，而父进程继续处理命令请求\n由于SAVA的会导致主进程的阻塞，所以使用时基本不会考虑，所以通常我们都会默认使用BGSAVA来进行，下面我指的也都是BGSAVA\nRDB持久化的优缺点 优点：\n RDB文件是经过压缩的二进制文件，占用内存更小更紧凑，适合作为备份文件 RDB容灾恢复更有用，因为其更加紧凑，可以更快的传输到远程服务器进行数据恢复。 RDB可以提高Redis的运行速度，因为使用BGSAVA持久化时会fork出子进程进行持久化的I/O操作，主进程不会受到干扰。 比起AOF格式的文件，RDB文件这种直接恢复状态的重启更快 缺点： 由于RDB是以快照形式进行保存的，并且快照之间存在一定的时间间隔，如果Redis服务被终止，则会导致丢失一段时间的数据 RDB的BGSAVA需要fork()出子进程来进行持久化，但是如果CPU性能不佳且数据量很大的时候，fork()的时间就会增加，导致Redis可能会停止服务一段时间。  AOF持久化 AOF持久化其实就是保存Redis服务器所执行的命令来保存数据库的状态，将命令追加到AOF文件的末尾（Append Only File），AOF的核心其实就是将所有执行过的命令重新执行，来恢复状态\n什么意思呢？例如我们执行了几条命令，此时AOF持久化会将这些命令以请求协议格式追加到AOF文件末尾 当服务器启动时，就会读取AOF文件中的所有命令，将其在服务器上重新执行一次，来恢复服务器的状态。 AOF重写 随着服务器存储的数据越来越多，此时AOF保存的命令也越来越多，文件的体积就会变得非常大，这样就可能导致对Redis服务器以及宿主机造成影响，并且随着文件的增大，使用AOF来进行数据还原需要的时间也就更多。\n并且还存在一个问题，就是命令中存在着大量冗余和多余的命令。\n1 2 3 4 5 6  127.0.0.1:6379\u0026gt;lpushlist252134(integer)5127.0.0.1:6379\u0026gt;lpoplist2\u0026#34;4\u0026#34;127.0.0.1:6379\u0026gt;rpoplist2\u0026#34;5\u0026#34;  例如上面这些命令，我存储了5 2 1 3 4五个数据，之后我又删除了4和5，所以最终剩下的只有1 2 3。 但是如果将所有的命令都保存进去，在恢复状态的时候又会重新模拟一次中间的删除流程，这些步骤是不需要的，我们只需要最终的结果。\n所以AOF引入了重写的机制，即只保存能够获取最终结果的命令 重写的流程很简单，就是去直接读取当前数据库中的键值状态，然后构造出对应的命令来进行保存 例如上面那个，就直接进行一次lpush 1 2 3，就可以直接省去了中间的操作。\n并且和RDB的BGSAVA一样，为了不阻塞主进程，所有的重写操作都会通过创建子进程来进行，并且由于子进程创建时会通过写时拷贝机制带有服务器数据的副本，所以也不需要对数据进行加锁就可以保证安全，提高了效率 此时子进程进行AOF的重写，父进程则继续处理接受的请求。 但是这时又引入了一个问题，如果父进程接受了新的命令，这些命令可能就会对数据库的状态进行修改，这样就会导致重写后的AOF文件所保存的状态和当前的数据库状态不一致。\n为了解决这个问题，服务器新增了一个AOF重写缓冲区，将两个AOF的过程给分割开 服务器流程\n 执行客户端发送来的新命令 将执行后的写命令追加到AOF缓冲区中 将执行后的写命令追加到AOF重写缓冲区中  这样AOF缓冲区的数据会被定期写入和同步到AOF文件中，不会影响整个AOF的流程。 而在执行重写后所执行的命令也都会保存到AOF重写缓冲区中。\n所以在子进程重写结束后，其会通过信号的方式来通知父进程，此时服务器就会进行以下的操作来完成重写的AOF文件的更换\n 将当前AOF重写缓冲区中的命令保存到一个新的AOF文件中，此时的新AOF文件的状态与当前数据库保存一致，并且比起旧的AOF更加简洁 此时对新的AOF文件进行改名，原子性的覆盖掉原先的AOF文件，完成AOF重写文件与旧文件的更替  AOF持久化的优缺点 优点\n AOF持久化保存的数据更加完整，其设定了三种保存策略：每次操作保存、每秒钟保存、跟随系统的持久化策略保存，其中每秒保存一次为AOF的默认策略。通过这种方法，使得即使发生了意外情况，最多也只会丢失1秒的数据，而不像RDB会丢失一段时间的数据 AOF如其名，采用了命令追加的方式写入到文件中，所以不会出现文件损坏的问题 AOF持久化将命令以协议格式写入文件，非常容易理解和解析，即使不小心使用flushall进行删库，并且状态被保存了下来。也可以通过删除AOF文件中的flushall命令来消除那次操作。（RDB快照则没办法避免）  缺点\n 在数据量相同的时候，AOF文件要大于RDB文件 在Redis负载较高的时候，RDB的性能比AOF更好 RDB使用快照的形式来持久化整个Redis数据，而AOF是将每次执行的命令追加到AOF文件中，所以RDB比AOF更健壮  混合持久化 混合持久化是在Redis4.0之后新增的一种方式，混合持久化结合了RDB和AOF的优点，在写入文件的时候，会先把当前的数据以RDB的形式写入文件的开头，再将后续的操作命令以AOF的格式写入文件中，这样既能保证Redis重启时的速度（RDB快照状态恢复），又能减少数据丢失的风险（AOF丢失时间短）\n混合持久化的优缺点 优点\n 结合了RDB和AOF的优点，开头为RDB格式的数据，可以快速启动（快照），并且之后为AOF格式，减少了大量数据丢失的风险 缺点 AOF文件可读性变差，因为AOF文件前面增加了RDB格式的内容 兼容性差，由于混合持久化是在4.0之后才引入的，如果开启之后则混合持久化AOF文件就不能使用在之前的版本  ","date":"2022-05-26T15:04:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E6%8C%81%E4%B9%85%E5%8C%96%E7%AD%96%E7%95%A5/","title":"Redis 持久化策略"},{"content":"数据类型 基础数据类型 String 字符串类型（SDS）即简单动态字符串，它是以键值对key-value的形式进行存储的，根据 key 来存储和获取value值\n依据不同情况，字符串在底层会使用 int 、 raw 或者 embstr 三种不同的编码格式\n 如果数据为可以使用long类型来保存的整数，则使用int 如果数据为可以使用long double类型来保存的浮点数，则使用embstr或者raw 如果数据为字符串，或者长度过大没办法用long来表示的整数，以及长度过大无法用long double表示的浮点数，则使用embstr或者raw。当数据小于39字节时，使用embstr，当大于39字节时使用raw   基本用法 1 2 3 4 5 6  127.0.0.1:6379\u0026gt;sethelloworld//设置key-valueOK127.0.0.1:6379\u0026gt;gethello//根据key获取value\u0026#34;world\u0026#34;127.0.0.1:6379\u0026gt;strlenhello//计算value长度(integer)5  使用场景  存放用户（登录）信息； 存放文章详情和列表信息； 存放和累计网页的统计信息。  Hash 字典类型 (Hash) 又被成为散列类型或者是哈希表类型，它是将⼀个键值 (key) 和⼀个特殊的“哈希表”关联起来。 哈希类型的底层数据结构可以是压缩列表（ZipList)或者字典（Dict）\n 当哈希对象的所有键值对的键和值的字符串长度都小于64字节，并且保存的键值对数量小于512个时，使用压缩列表 如果不满足上述条件中的任意一个，都会使用字典   基本用法 1 2 3 4 5 6  127.0.0.1:6379\u0026gt;hsethash1nameleeage20//设置key-value的映射(integer)2127.0.0.1:6379\u0026gt;hgethash1name//获取key为name的value\u0026#34;lee\u0026#34;127.0.0.1:6379\u0026gt;hgethash1age//获取key为age的value\u0026#34;20\u0026#34;  使用场景  存储用户信息或者某个物品的信息，无需序列化，直接建立映射  List 列表类型 (List) 是⼀个使用线性结构存储的结构，它的元素插入会按照先后顺序存储到链表结构中。 列表类型的底层数据结构可以是压缩列表（ZipList)或者链表（LinkedList）\n 当列表对象的所有字符串元素长度都小于64字节，并且保存的元素数量小于512个时，使用压缩列表 如果不满足上述条件中的任意一个，都会使用链表   基本用法 1 2 3 4 5 6  127.0.0.1:6379\u0026gt;lpushlist112345//依次头插12345，此时数据为54321(integer)5127.0.0.1:6379\u0026gt;rpoplist1//尾删\u0026#34;1\u0026#34;127.0.0.1:6379\u0026gt;lpoplist1//头删\u0026#34;5\u0026#34;  使用场景  消息队列：列表类型可以使用 rpush 实现先进先出的功能，同时又可以使用 lpop 轻松的弹出（查询并删除）第⼀个元素，所以列表类型可以用来实现消息队列； 文章列表：对于博客站点来说，当用户和文章都越来越多时，为了加快程序的响应速度，我们可以把用户自己的文章存入到 List 中，因为 List 是有序的结构，所以这样不仅可以完美的实现分页功能，而且加速了程序的响应速度。  Set 集合类型 (Set) 是⼀个无序并唯⼀的键值集合。\n集合类型的底层数据结构可以是整数集合（IntSet)或者字典（Dict）\n 当集合对象的所有元素都是整数值，并且保存的元素数量小于512个时，使用整数集合 如果不满足上述条件中的任意一个，都会使用字典   基本用法 1 2 3 4 5 6 7  127.0.0.1:6379\u0026gt;saddtestSetv1v2v3v4v2v4v1(integer)4127.0.0.1:6379\u0026gt;smemberstestSet//去重且无序1)\u0026#34;v2\u0026#34;2)\u0026#34;v1\u0026#34;3)\u0026#34;v4\u0026#34;4)\u0026#34;v3\u0026#34;  使用场景  微博关注我的人和我关注的人都适合用集合存储，可以保证人员不会重复； 中奖人信息也适合用集合类型存储，这样可以保证⼀个人不会重复中奖。  Zset 有序集合类型 (SortedSet) 相比于集合类型多了⼀个排序属性 score（分值），所以对于有序集合ZSet 来说，每个存储元素相当于有两个值组成的，⼀个是有序结合的元素值，⼀个是分值。有序集合的存储元素值也是不能重复的，但分值是可以重复的。\n有序集合类型的底层数据结构可以是压缩列表（ZipList)或者跳表（SkipList ）\n 当有序集合对象的所有元素成员的长度都小于64字节，并且保存的元素数量小于128个时，使用压缩列表 如果不满足上述条件中的任意一个，都会使用跳表（这里的跳表是结合字典的） 这里不是直接使用跳表，而是搭配字典一起使用 之所以这样设置是因为考虑到如果直接使用跳跃表，如果需要查找成员的分值时只能通过遍历来进行查找，而这样的效率是O(logN) 而字典虽然建立映射后可以O(1)的查找到分值，但是哈希只能通过key值进行查找，并不支持范围查询。 所以将两者进行结合，使用字典建立起元素与分值的映射，使用字典来进行成员分数的查找，而使用跳跃表来进行范围型操作，这样就很好的解决了这个问题。  1 2 3 4 5  typedef struct zset { zskiplist *zsl; dict *dict; } zset;   基本用法 1 2 3 4 5 6 7  127.0.0.1:6379\u0026gt;zaddzset13v18v22v36v4#插入时以分值-值的形式插入(integer)4127.0.0.1:6379\u0026gt;zrangezset10-1#查找结果按照升序排序1)\u0026#34;v3\u0026#34;2)\u0026#34;v1\u0026#34;3)\u0026#34;v4\u0026#34;4)\u0026#34;v2\u0026#34;  使用场景  学生成绩排名； 粉丝列表，根据关注的先后时间排序。  特殊数据类型 Geospatial(地理空间) 在使用一些小程序的时候，里面通常都会通过定位使用者的位置，来显示附近的人、外卖距离、剩余路径等功能，在Redis3.2中也引入了推算地理信息的数据结构，即Geospatial介绍 把某个具体的位置信息（经度，纬度，名称）添加到指定的key中，数据将会用一个sorted set存储，以便稍后能使用 GEORADIUS和 GEORADIUSBYMEMBER命令来根据半径来查询位置信息。\n这个命令(指GEOADD)的参数使用标准的x,y形式，所以经度（longitude）必须放在纬度（latitude）之前，对于可被索引的坐标位置是有一定限制条件的：非常靠近极点的位置是不能被索引的， 在EPSG:900913 / EPSG:3785 / OSGEO:41001指定如下：\n 有效的经度是-180度到180度 有效的纬度是-85.05112878度到85.05112878度  上面是官方文档的描述，其实Geo并没有我们想象中的复杂，它的本质就是一个Zset，通过将坐标以Geohash的方式进行处理，将经度纬度错位后形成一个52位整数，所以我们同样能够使用Zset提供的接口来操作Geo。\n用法    命令 作用     GEOADD key 经度 纬度 地点名称 将指定的地理空间位置（经度、纬度、名称）添加到指定的key中   GEODIST key 地点1 地点2 返回两个给定位置之间的距离   GEOPOS key 地点 从key里返回所有给定位置元素的位置（经度和纬度）   GEOHASH key 地点 返回一个或多个位置元素的 Geohash 表示   GEORADIUS key 经度 纬度 半径 以给定的经纬度为中心， 找出某一半径内的元素   GEORADIUSBYMEMBER key 地点 半径 找出位于指定范围内的元素，中心点是由给定的位置元素决定   Zset的接口同样适用于Geo，因此需要删除、查询全部时就可以使用Zset的接口。     下面示范一下这些命令的使用方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  # 为了方便示范，下面加入一些城市的地理信息——GEOADD  127.0.0.1:6379\u0026gt; GEOADD CHINA 108.94683 34.29296 \u0026#34;xian\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 116.405285 39.904989 \u0026#34;beijing\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 121.472644 31.231706 \u0026#34;shanghai\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 113.280637 23.125178 \u0026#34;guangzhou\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 114.085947 22.547 \u0026#34;shenzhen\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 110.33119 20.031971 \u0026#34;hainan\u0026#34; (integer) 1 # 获取北京和上海的距离——GEODIST  127.0.0.1:6379\u0026gt; GEODIST CHINA beijing shanghai km \u0026#34;1067.5980\u0026#34; # 获取西安的坐标——GEOPOS  127.0.0.1:6379\u0026gt; GEOPOS CHINA xian 1) 1) \u0026#34;108.94683212041854858\u0026#34; 2) \u0026#34;34.29296115814533863\u0026#34; # 以经度120 纬度35位置为中心，获取半径1000千米内的城市——GEORADIUS  127.0.0.1:6379\u0026gt; GEORADIUS CHINA 120 35 1000 km 1) \u0026#34;beijing\u0026#34; 2) \u0026#34;shanghai\u0026#34; # 获取在广州半径500千米内的城市——GEORADIUSBYMEMBER  127.0.0.1:6379\u0026gt; GEORADIUSBYMEMBER CHINA guangzhou 500 km 1) \u0026#34;shenzhen\u0026#34; 2) \u0026#34;guangzhou\u0026#34; 3) \u0026#34;hainan\u0026#34; # 将广州和深圳的坐标转换为11为的GEO哈希值——GEOHASH 127.0.0.1:6379\u0026gt; GEOHASH CHINA guangzhou shenzhen 1) \u0026#34;ws0e9cb3yj0\u0026#34; 2) \u0026#34;ws10k0dcg10\u0026#34;   Hyperloglog(基数统计) 在我们为网站统计访问量、日活量时，由于我们统计的是用户数量而非访问次数，因此即使一个用户多次访问也只会统计一次，这种不重复的数据通常被称为基数。\n在传统的做法中，我们通常会采用set来保存用户的ID来进行计数，因为其本身存在着去重的功能，但是由于我们所需要的是对用户进行计数，如果通过将所有用户的ID保存的方法来完成，当用户量大的时候就会对内存产生巨大的压力，并且效率也大大降低。\n为了解决这个问题，Redis在2.8.9版本添加了HyperLogLog结构。\n介绍 Redis HyperLogLog是用来做基数统计的算法，HyperLogLog的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。在Redis 里面，每个HyperLogLog键只需要花费12KB内存，就可以计算接近2^64个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。\n但是，由于HyperLogLog使用的是概率算法，通过存储元素的hash值的第一个1的位置，来计算元素数量，所以HyperLogLog 不会存储元素本身，在数据量大的时候也可能会存在一定的误差。但是在基数统计这一方面，它的效果是其他结构无法比拟的。\n用法    命令 作用     PFADD key value 添加指定的值到Hyperloglog中   PFCONUT key 返回给定Hyperloglog的基数估算值   PFMERGE destkey sourcekey 将目标Hyperloglog合并到源Hyperloglog中    使用示范\n1 2 3 4 5 6 7 8 9 10 11 12 13  127.0.0.1:6379\u0026gt; PFADD NUMS1 1 2\t3 4\t#向NUMS1插入1-4 (integer) 1 127.0.0.1:6379\u0026gt; PFADD NUMS1 1\t#数据已存在，不再插入 (integer) 0 127.0.0.1:6379\u0026gt; PFCOUNT NUMS1\t#查看当前基数数量 (integer) 4 127.0.0.1:6379\u0026gt; PFADD NUMS2 3 4 5 6\t#向NUMS2插入3-6 (integer) 1 127.0.0.1:6379\u0026gt; PFMERGE NUMS1 NUMS2\t#将NUMS2合并到NUMS1中 OK 127.0.0.1:6379\u0026gt; PFCOUNT NUMS1\t#此时NUMS1中记录了1-6,六个元素 (integer) 6\t  Bitmap(位图) 介绍 位图其实就是哈希的变形，他通过哈希映射来处理数据，位图本身并不存储数据，而是存储标记。通过一个比特位，即0/1来标记一个数据的两种状态位图通常情况下用在数据量庞大，且数据不重复的情景下标记某个数据的两种状态。我们可以使用位图来记录当前用户的登陆情况、或者实现打卡、签到等功能。\n用法    命令 作用     GETBIT key offset value(0/1) 设置Bitmap中偏移量为offset的位置的值   SETBIT key offset value 返回Bitmap中偏移量为offset的位置的值   BITCOUNT key 计算位图中有多少个1    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  127.0.0.1:6379\u0026gt; SETBIT TEST 1 1\t#将位图中第1，3，5位设置为1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT TEST 3 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT TEST 5 1 (integer) 0 127.0.0.1:6379\u0026gt; GETBIT TEST 1 #查看位图中1，2，3位的值 (integer) 1 127.0.0.1:6379\u0026gt; GETBIT TEST 2 (integer) 0 127.0.0.1:6379\u0026gt; GETBIT TEST 3 (integer) 1 127.0.0.1:6379\u0026gt; BITCOUNT TET\t#统计位图中1的数量，由于我们只设置了1，3，5位，因此为3 (integer) 3   ","date":"2022-05-26T15:03:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","title":"Redis 数据类型"},{"content":"数据结构 SDS 结构分析 由于C字符串存在大量问题，所以在Redis中，并没有使用C风格字符串，而是自己构建了一个简单动态字符串即SDS（simple dynamic string）\n1 2 3 4 5 6 7 8  struct sdshdr { // buf 中已占用空间的长度  int len; // buf 中剩余可用空间的长度  int free; // 数据空间  char buf[]; };   为解决C字符串缓冲区溢出问题以及长度计算问题，SDS中引入了len来统计当前已使用空间长度，free来计算剩余的空间长度\nC字符串的主要缺陷就是因为它没有记录自己的长度，而如果在需要了解长度时，就只能通过O（N）的效率进行一次遍历 并且因为C字符串没有统计剩余空间的字段，也没有容量字段，所以很容易就会因为strcat等函数造成缓冲区的溢出，为弥补这一缺陷，redis在sds中增加了free字段 通过标记剩余空间，当对SDS进行插入操作时，就会提前判断当前剩余空间是否足够，如果不足则会先进行空间的拓展，再进行插入，这样就解决了缓冲区溢出的问题\n内存策略 由于Redis作为一个高效的内存数据库，用于速度要求严苛，插入删除频繁 的场景，为了提高内存分配的效率，防止大量使用内存重分配而调用系统函数导致的性能损失问题（用户态和内核态的切换），Redis主要依靠空间预分配和惰性空间释放来解决这个问题\n空间预分配 为减少空间分配的次数，当需要进行空间拓展时，不仅仅会为SDS分配修改所必须要的空间，并且会为SDS预分配额外的未使用空间。\n预分配未使用空间的策略如下\n 当SDS修改后的长度小于1MB时，将会预分配大小和当前len一样的空间(free = len)，也就是使空间增长一倍，来减少因为初始时申请大空间导致的连续分配问题 当SDS修改后的长度大于等于1MB时，每次分配都会分配1MB的空间，防止空间的浪费。  惰性空间释放 当我们对SDS进行删除操作时，并不会立即回收删除后空余的空间，而是将空余空间以free字段记录下来，以备后面使用。 这样做的目的在于防止因为空间缩短后因为再度插入导致的空间拓展问题。 并且如果有需求需要真正释放空间，Redis也提供了对应的API，所以不必担心会因为惰性的空间释放而导致的内存浪费问题。\n总结 比起 C 字符串， SDS 具有以下优点：\n 常数复杂度获取字符串长度。（len字段） 杜绝缓冲区溢出。（free字段） 减少修改字符串长度时所需的内存重分配次数。（空间预分配，惰性空间释放） 二进制安全。（以二进制形式处理） 兼容部分 C 字符串函数。（底层基于C字符串，以空字符结尾）  链表 结构分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  typedef struct listNode { // 前置节点  struct listNode *prev; // 后置节点  struct listNode *next; // 节点的值  void *value; } listNode; /* * 双端链表迭代器 */ typedef struct listIter { // 当前迭代到的节点  listNode *next; // 迭代的方向  int direction; } listIter; /* * 双端链表结构 */ typedef struct list { // 表头节点  listNode *head; // 表尾节点  listNode *tail; // 节点值复制函数  void *(*dup)(void *ptr); // 节点值释放函数  void (*free)(void *ptr); // 节点值对比函数  int (*match)(void *ptr, void *key); // 链表所包含的节点数量  unsigned long len; } list;   从上面的结构可以看出，Redis的链表是一个带头尾的双端无环链表，并且通过len字段记录了链表节点的长度 同时为了实现多态与泛型，链表中还提供了dup，free，match属性来设置相关的函数，使得链表支持不同类型的值的存储\n总结  链表被广泛用于实现 Redis 的各种功能， 比如列表键， 发布与订阅， 慢查询， 监视器， 等等。 每个链表节点由一个 listNode 结构来表示， 每个节点都有一个指向前置节点和后置节点的指针， 所以 Redis 的链表实现是双端链表。 每个链表使用一个 list 结构来表示， 这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。 因为链表表头节点的前置节点和表尾节点的后置节点都指向 NULL ， 所以 Redis 的链表实现是无环链表。 通过为链表设置不同的类型特定函数， Redis 的链表可以用于保存各种不同类型的值。  字典 结构分析 Redis的字典底层采用了哈希表来进行实现。\n首先看看字典底层哈希表的结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  typedef struct dictht { // 哈希表数组  dictEntry **table; // 哈希表大小  unsigned long size; // 哈希表大小掩码，用于计算索引值  // 总是等于 size - 1  unsigned long sizemask; // 该哈希表已有节点的数量  unsigned long used; } dictht;   哈希表中记录了当前的总长度，已有节点，以及当前索引大小（用于哈希函数来计算节点位置)\n为解决哈希冲突，Redis字典采用了链地址法来构造了哈希桶的结构，也就是哈希数组中的每个元素都是一个链表。 下面来看看哈希节点的结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  typedef struct dictEntry { // 键  void *key; // 值  union { void *val; uint64_t u64; int64_t s64; } v; // 指向下个哈希表节点，形成链表  struct dictEntry *next; } dictEntry;   可以看到，为保证键值对适用于多重类型，key值使用的时void的形式，而value使用了64位有符号整型和64位无符号整型，void指针的一个联合体，每个节点使用next来链接成一个链表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  typedef struct dictType { // 计算哈希值的函数  unsigned int (*hashFunction)(const void *key); // 复制键的函数  void *(*keyDup)(void *privdata, const void *key); // 复制值的函数  void *(*valDup)(void *privdata, const void *obj); // 对比键的函数  int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数  void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数  void (*valDestructor)(void *privdata, void *obj); } dictType;   为保证字典具有多态及泛型，dictType中提供了如哈希函数以及K-V的各种操作函数，使得字典适用于多重情景\nrehash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  /* * 字典 */ typedef struct dict { // 类型特定函数  dictType *type; // 私有数据  void *privdata; // 哈希表  dictht ht[2]; // rehash 索引  // 当 rehash 不在进行时，值为 -1  int rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 目前正在运行的安全迭代器的数量  int iterators; /* number of iterators currently running */ } dict;   从字典的结构中，我们可以看到里面同时存放了两个哈希表，以及一个rehashidx属性。 这就牵扯到了字典的核心之一，rehash。\nRedis作为一个插入频繁且对效率要求高的数据库，当插入的数据过多时，就会因为哈希表中的负载因子过高而导致查询或者插入的效率降低，此时就需要通过rehash来进行重新扩容并重新映射。 但是如果只是用一个哈希表，映射时就会导致数据库暂时不可用，作为一个使用频繁的数据库，短期的停机几乎是不可容许的问题，所以Redis设计时采用了双哈希的结构，并采用了渐进式rehash的方法来解决这个问题。 rehash的步骤如下\n 为ht[1]的哈希表分配空间 将ht[0]中的键值对重新映射到ht[1]上 当ht[0]的数据迁移完成，此时ht[0]为一个空表，此时释放ht[0]，并让ht[1]成为新的ht[0]，再为ht[1]创建一个新的空白哈希表，为下一次的rehash做准备   渐进式rehash 由于数据库中可能存在大量的数据，而rehash的时候又过长，为了避免因为rehash造成的服务器停机，rehash的过程并不是一次完成的，而是一个多次的，渐进式的过程。 在渐进式rehash的时候，由于数据不断的进行迁移，无法确定数据处于哪一个表上， 此时如果进行插入、删除、查找的操作时就会在两个表上进行，如果在一个表中没找到对应数据，就会到另一个表中继续查找。\n并且如果此时新插入节点，都会统一的防止在新表ht[1]中，防止对ht[0]的rehash造成干扰，保证ht[0]节点的只减少不增加\n总结  字典被广泛用于实现 Redis 的各种功能， 其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现， 每个字典带有两个哈希表， 一个用于平时使用， 另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突， 被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时， 程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面， 并且这个 rehash 过程并不是一次性地完成的， 而是渐进式地完成的。  跳表 跳表是一个较为少见的数据结构，如果不了解的可以看看我之前的博客 看了这篇博客，还敢说你不懂跳表吗？ 由于跳表的实现简单且性能可与平衡树相媲美，对于大量插入删除的数据库来说，跳表只需要进行简单的链表插入和索引的选拔，而不像平衡树一样需要进行整体平衡的维持。并且由于在范围查找上的效率远远强于平衡树，所以Redis底层选取跳表来作为有序集合的底层之一。\n结构分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  typedef struct zskiplistNode { // 成员对象  robj *obj; // 分值  double score; // 后退指针  struct zskiplistNode *backward; // 层  struct zskiplistLevel { // 前进指针  struct zskiplistNode *forward; // 跨度  unsigned int span; } level[]; } zskiplistNode;   跳跃表的查询从最顶层出发，通过前进指针来往后查找，通过比较节点的分数来判断当前节点是否与索引匹配，如果查找不到则进入下层继续查找，并记录下跨越的层数span来进行排位。 同时为了处理特殊情况，还准备了一个后退指针来进行从表尾到表头的遍历，但是与前进不同，后退指针并不存在跳跃，而是只能一个一个向后查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  /* * 跳跃表 */ typedef struct zskiplist { // 表头节点和表尾节点  struct zskiplistNode *header, *tail; // 表中节点的数量  unsigned long length; // 表中层数最大的节点的层数  int level; } zskiplist;   跳跃表通过保存表头和表尾节点，来快速访问表头和表尾。并且保存了节点的数量来实现O(1)的长度计算 为了避免因为层数过高导致的大量空间损失，Redis跳跃表的节点高度最高位32层。\n总结  跳跃表是有序集合的底层实现之一， 除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成， 其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度）， 而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中， 多个节点可以包含相同的分值， 但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序， 当分值相同时， 节点按照成员对象的大小进行排序。  整数集合 整数集合时集合键的底层实现之一，当集合中的元素全部都是整数值的时候，并且集合中元素不多时，Redis就会使用整数集合来作为集合键的底层结构。整数集合具有去重且排序的特性\n结构分析 1 2 3 4 5 6 7 8 9 10 11  typedef struct intset { // 编码方式  uint32_t encoding; // 集合包含的元素数量  uint32_t length; // 保存元素的数组  int8_t contents[]; } intset;   在上面的结构中，虽然content数组的类型是一个8bit的整型，但是数据真正存储的方式并不是这个类型，而是根据encoding来决定具体的类型，8bit只是作为一个基本单位来进行使用。\n例如此时encoding设置为INTSET_ENC_INT16时，数组存储的格式就有每个元素16bit 如果encoding设置为INTSET_ENC_INT64时，数组存储的格式就有每个元素64bit 升级 升级的流程\n 根据插入的类型来决定集合升级的类型，拓展数组的整体空间并为新节点分配空间 对集合中所有数据类型进行升级，在保持有序的前提下将所有升级后的元素移动到合适的位置上 将新节点插入到集合的对应位置  新元素的插入位置 由于会引起升级的元素的类型都必顶比数组中的所有数据都大，所以也就决定了其要么比所有数据都大，要么比所有数据都小（负数），所以插入位置只能是首部和尾部\n 当新元素比所有数据都大时在尾部插入 当新元素比所有数据都小时在首部插入  如果插入一个32位的数据，则引起全体升级\n分配底层空间\n整体升级并挪动位置\n插入元素\n降级 在整数列表中升级是一个不可逆的过程，即使将所有高类型的数据删除了，也不会进行降级。 理由是防止因为降级后再次升级带来的大量数据挪动的问题，在保证了效率的同时，也带来了一定程度上的空间浪费（非必要时尽量不要升级） 升级带来的好处\n 灵活 ： 由于C语言是一个静态类型语言，为了避免出现类型错误通常会将不同类型分开，例如如果要存储16bit、32bit、64bit的整型就需要3个数组，但是使用整数集合就可以通过升级的策略来进行元素类型的自适应，就可以任意的将各种类型的整数插入进去，而不必担心类型错误 节约内存：与上面类似，如果我们只有一个64bit的数组，而里面存储了不少16bit、32bit的元素，则会造成空间的大量浪费。而使用整数集合则可以同时保存多个类型，只需要确保升级的操作只在必要时进行，就不会造成空间的浪费  总结  整数集合是集合键的底层实现之一。 整数集合的底层实现为数组， 这个数组以有序、无重复的方式保存集合元素， 在有需要时， 程序会根据新添加元素的类型， 改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性， 并且尽可能地节约了内存。 整数集合只支持升级操作， 不支持降级操作。  压缩列表 压缩列表（ziplist）是列表键和哈希键的底层实现之一。\n当一个列表键只包含少量列表项， 并且每个列表项要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做列表键的底层实现。主要核心就是为了节约空间\n结构分析 例如这张图，可以看出当前包含三个节点，总空间为0x50(十进制80），到尾部的偏移量为0x3c(十进制60)，节点数量为0x3(十进制3)\n每个压缩列表节点可以由一个整数或者一个字节数组组成\n整数的类型可以是以下六种之一：\n 4位的介于0-12的无符号整数 1字节的有符号整数 3字节的有符号整数 int16_t int32_t int64_t  字节数组可以是以下三种之一：\n 长度小于等于63（2^6 -1)字节的字节数组 长度小于等于16383（2^14 -1)字节的字节数组 长度小于等于4294967295（2^32 -1)字节的字节数组  而压缩列表节点又有三个属性组成，分别是previous_entry_length，encoding，content。\nprevious_entry_length 这个属性记录了压缩列表前一个节点的长度，该属性根据前一个节点的大小不同可以是1个字节或者5个字节。\n 如果前一个节点的长度小于254个字节，那么previous_entry_length的大小为1个字节，即前一个节点的长度可以使用1个字节表示 如果前一个节点的长度大于等于254个字节，那么previous_entry_length的大小为5个字节，第一个字节会被设置为0xFE(十进制的254），之后的四个字节则用于保存前一个节点的长度。  小于254字节时的表示 大于等于254字节时的表示 为什么要这样设计呢？ 由于压缩列表中的数据以一种不规则的方式进行紧邻，无法通过后退指针来找到上一个元素，而通过保存上一个节点的长度，用当前的地址减去这个长度，就可以很容易的获取到了上一个节点的位置，通过一个一个节点向前回溯，来达到从表尾往表头遍历的操作\nencoding encoding通过以下规则来记录content的类型\n 一字节、两字节或者五字节长， 值的最高位为 00 、 01 或者 10 的是字节数组编码： 这种编码表示节点的 content 属性保存着字节数组， 数组的长度由编码除去最高两位之后的其他位记录； 一字节长， 值的最高位以 11 开头的是整数编码： 这种编码表示节点的 content 属性保存着整数值， 整数值的类型和长度由编码除去最高两位之后的其他位记录；   content content属性负责保存节点的值，值的具体类型由上一个字段encoding来决定。\n例如存储字节数组，00表示类型为字节数组，01011表示长度为11 存储整数值，表示存储的为整数，类型为int16_t 连锁更新 当添加或删除节点时，可能就会因为previous_entry_length的变化导致发生连锁的更新操作。\n假设e1的previous_entry_length只有1个字节，而新插入的节点大小超过了254字节，此时由于e1 的previous_entry_length无法该长度，就会将previous_entry_length的长度更新为5字节。 但是问题来了，假设e1原本的大小为252字节，当previous_entry_length更新后它的大小则超过了254，此时又会引发对e2的更新。 顺着这个思路，一直更新下去 同理，删除也会引发连锁的更新 从上图可以看出来，在最坏情况下，会从插入位置一直连锁更新到末尾，即执行了N次空间重分配， 而每次空间重分配的最坏复杂度为 O(N) ， 所以连锁更新的最坏复杂度为 O(N^2) 。\n即使存在这种情况，但是并不影响我们使用压缩列表\n 压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点， 连锁更新才有可能被引发， 这种情况就和连中彩票一样，很少见 即使出现连锁更新， 但只要被更新的节点数量不多， 就不会对性能造成任何影响： 比如说， 对三五个节点进行连锁更新是绝对不会影响性能的；  总结  压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表， 或者从压缩列表中删除节点， 可能会引发连锁更新操作， 但这种操作出现的几率并不高。  ","date":"2022-05-26T15:02:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"Redis 数据结构"},{"content":"Redis 基本概念 什么是 Redis? Redis 是一个使用 ANSI C 编写的开源、支持网络、基于内存、分布式、可选持久性的键值对存储数据库。它可以作为内存数据库、缓存和消息中间件，其中缓存是它最主要的使用场景。\n什么是缓存？ 缓存概念 缓存是⼀个高速数据交换的存储器，使用它可以快速的访问和操作数据。\n举个通俗的例子。 小明经营着一家饭店，在刚开张的时候由于名气不足，客源少，生意并不是很忙，平时没事的时候就闲着，有客人来了再进厨房安排做菜。随着饭店的日益发展，此时的饭店已经不同往日，有着大量的稳定客源，并且在某些节假日的时候甚至爆满。按照以前的做法，那肯定是行不通了，在用餐高峰期的时候因为备餐慢导致了客户的长时间等待，使得饭店的屡遭投诉。 为解决这一问题，小明想到了一个办法，可以在空闲的时候，提前将热门的菜做完后放入保温柜，等用餐高峰期时再拿出来加热后就可以直接上菜，就规避了短时间内大量客源而导致的备餐慢的问题，通过这一方法，即使在高峰期，也能很好的应对。\n这就是缓存的本质，将热点资源（高频读、低频写）提前放入离用户最近、访问速度更快的地方，以提高访问速度。\n缓存 VS 数据库 相比于数据库而言，缓存的操作性能更高\n 缓存⼀般都是通过 key-value 查询数据，因为不像数据库⼀样还有查询的条件等因素，所以查询的性能⼀般会比数据库高； 缓存的数据是存储在内存中的，而数据库的数据是存储在磁盘中的，因为内存的操作性能远远大于磁盘，因此缓存的查询效率会高很多； 缓存更容易做分布式部署（当⼀台服务器变成多台相连的服务器集群），而数据库⼀般比较难实现分布式部署，因此缓存的负载和性能更容易平行扩展和增加。  本地缓存 VS 分布式缓存 根据缓存是否与应用进程属于同一进程（单机与多机)，又分为本地缓存和分布式缓存\n本地缓存 本地缓存也叫做单机缓存，即将服务部署到一台服务器上，所以本地缓存只适用于当前系统 举个例子，这个就如同每个学校的校规，根据学校的宗旨以教学理念不同，每个学校的校规都不一样，对于A学校的学生来说，B学校的校规毫无意义，也就是说每个学校的校规只适用与那个学校。\n所以本地缓存只适用于当前系统\n优缺点\n 访问速度快，但无法进行大数据存储 集群的数据更新问题 数据随应用进程的重启而丢失  分布式缓存 分布式缓存也叫做多机缓存，即将服务部署到多台服务器上，并且通过负载分发将用户的请求按照⼀定的规则分发到不同服务器。 而分布式缓存就如同教育局定下来的教学规范，无论是任何学校都必须遵守这个规范。\n所以分布式缓存适用与所有的系统。\n优缺点\n 支持大数据量存储，不受应用进程重启影响 数据集中存储，保证数据一致性 数据读写分离，高性能，高可用 数据跨网络传输，性能低于本地缓存  Memcached VS Redis 在市面上流行的分布式缓存中间件有两种，分别是Redis和Memcached，我们该如何对他们进行一个选择呢？\n  存储方式\n  Memcached把所有数据存在内存当中，数据大小不能超过内存大小，并且断电后数据会丢失。（不支持持久化，导致容灾能力弱）\n  Redis有部分存储在硬盘中，保证了数据的持久性。（持久化策略）\n    数据类型\n  Memcached对数据类型的支持较为简单，有时需要将数据拿到客户端来进行类似的修改再set回去，增加了网络IO的次数和数据体积\n  Redis具有复杂的数据类型，并且这些复杂类型的操作和get/set一样高效\n    存储值大小\n  Redis最大可以达到512mb\n  Memcached最大只有1mb\n    性能\n  Redis使用单核，在存储小数据时Redis有着明显的优势\n  Memcached使用多核，虽然在存储小数据的时候性能不及Redis，但是在存储大数据的时候Memcached要远远强于Redis\n    虽然从上面的结论以及当前流行程度来看，Redis都遥遥领先，但是在某些场景下，Memcached的作用也会高于Redis(例如海量数据查询），所以还需要根据具体使用场景来进行选择\n  适用场景\n  Redis除了作为NoSQL数据库使用外，还能用做消息队列、数据堆栈和数据缓存等；\n  Memcached适合于缓存SQL语句、数据集、用户临时性数据、延迟查询数据和session等。\n    ","date":"2022-05-26T15:01:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"Redis 基本概念"},{"content":"一致性哈希 分布式存储 如果我们需要存储QQ号与个人信息，建立起\u0026lt;QQ, 个人信息\u0026gt;的KV模型。\n假设QQ有10亿用户，并且每个用户的个人信息占据了100M，如果要存储这些数据，所需要的空间就得(100亿* 100M) = 10WT，这么庞大的数据是不可能在单机环境下存储的，只能采用分布式的方法，用多个机器进行存储，但是即使使用多机，这些数据也至少要10w台机器(假设每台服务器存1T)才能存储。\n并且我们还需要考虑，如何将这10w台机器与我们的数据建立起映射关系呢？ 换句话来说就是，我们如何确定哪些数据应该放在哪个机器呢？这时就需要用到哈希算法。\n简单哈希 我们可以采用除留余数法来完成一个映射，key值为qq号，余数为机器数量，得到的结果就是应该存储的机器的编号。这样我们将数据放入指定机器中，使用时再根据机器号进入对应的机器进行增删查改即可。\n1  机器号 = hash(QQ号) % 机器数量   但是这个方法存在着一个致命的缺陷，随着用户量不断增多或者用户信息增加，10w台机器就会不够用，此时就需要将机器扩容至15w台。 当进行扩容后，由于机器数量发生变化，数据的映射关系也会变化，我们就需要进行rehash来将数据重新映射到正确的位置上。\n但是问题来了，这10w台机器的数据如果需要进行重新映射，花费的时间几乎是不可想象的，我们不可能说为了迁移数据而让服务器宕机数月之久，所以这种方法是不可能行得通的。\n一致性哈希 为了弥补上一种方法的，就引入了一致性哈希算法。\n上面一种方法的主要缺陷就是由于扩容后rehash带来的数据大量迁移问题。 为了解决上述问题，一致性哈希将哈希构造成一个0~2^32-1的环形结构，并将余数从原来的机器数量修改值为整型最大值(也可以是比这个更大的)。因为这个数据足够大，所以不需要考虑因为机器数增加导致的rehash问题。\n1  机器号 = hash(QQ号) % 2^32   我们将环中的某一区间去映射到某台服务器，让这台服务器负责这个区间的管理，这样就能让这10w台服务器来切分这个闭环结构 当我们要查询某个数据的时候，根据哈希函数算出的映射位置来找到包含该位置的那个区间所对应的服务器，然后在那个服务器中进行操作即可 如果原先的服务器不够用了，此时增加5w个服务器，也不需要像之前一样对所有机器的数据进行迁移，我们只需要迁移负载重的机器即可 例如此时NodeC中存储了25000-50000的数据，此时往其中增加一个新服务器NodeE，让其负责映射闭环中25000-37500的数据。 此时我们需要做的就是将NodeC中25000~37500的那一部分重新迁移到NodeE上，并改变两个服务器的映射范围，就完成了数据的迁移。从这里我们可以看到，一致性哈希将服务器数据的整体迁移变成了高负载服务器的部分迁移，大大提高了效率以及稳定性。\n总结： 一致性哈希就是一个大范围的闭环，由于除数过大，我们也不需要因为由于除数增加导致全体rehash。并且映射关系变味了数据区间——机器，如果要增加机器，就只需要改变映射范围，并将区间中的小部分数据进行迁移，大大的提高了效率。\n虚拟节点 但是，一致性哈希也存在缺陷，就是在节点过少的时候可能会因为节点分布不均匀而导致数据倾斜问题。例如当前只有两个服务器 此时就会出现这种情况，部分节点数据过少，而部分节点数据过多，此时的数据大量集中在NodeA上，数据大量倾斜。\n既然节点较少，那就可以考虑在不增加服务器的基础上多增加几个节点，所以为了解决这问题，一致性哈希又引入了虚拟节点。对每个服务节点进行多次哈希映射，每个映射的位置都会放置该服务节点，成为虚拟节点。例如上图，就分别将NodeA和NodeB分成了三个虚拟节点。我们不需要改变数据定位的算法，只需要将虚拟节点与服务节点进行映射，将定位到虚拟节点NodeX #1、#2、#3的节点再定位回服务节点即可。\n通过这种方法就保证了即使服务节点少，也能做到相对均匀的数据分布\n","date":"2022-05-24T17:20:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/","title":"一致性哈希"},{"content":"一致性模型 什么是一致性模型? 在分布式系统中，C（一致性） 和 A（可用性）始终存在矛盾。若想保证可用性，就必须通过复制、分片等方式冗余存储。而一旦进行复制，又来带多副本数据一致性的问题——一个副本的数据更新之后，其他副本必须要保持同步，否则数据不一致就可能导致业务出现问题。\n因此，每次更新数据对所有副本进行修改的时间以及方式决定了复制代价的大小。全局同步与性能实际上是矛盾的，而为了提高性能，往往会采用放宽一致性要求的方法。因此，我们需要用一致性模型来理解和推理在分布式系统中数据复制需要考虑的问题和基本假设。\n 那么什么是一致性模型呢？\n 一致性模型本质上是进程与数据存储的约定：如果进程遵循某些规则，那么进程对数据的读写操作都是可预期的。\n下图即为 Jepsen 概括的常见的一致性模型：\n 不可用（Unavailable）：粉色代表网络分区后完全不可用。当出现网络隔离等问题的时候，为了保证数据的一致性，不提供服务。熟悉 CAP 理论的同学应该清楚，这就是典型的 CP 系统了。 严格可用 （Sticky Available）：黄色代表严格可用。即使一些节点出现问题，在一些还没出现故障的节点，仍然保证可用，但需要保证 client 的操作是一致的。 完全可用（Highly Available）：蓝色代表完全可用。就是网络全挂掉，在没有出现问题的节点上面，仍然可用。  一致性模型主要可以分为两类：能够保证所有进程对数据的读写顺序都保持一致的一致性模型称为强一致性模型，而不能保证的一致性模型称为弱一致性模型。\n强一致性模型 强一致性包含线性一致性和顺序一致性，其中前者对安全性的约束更强，也是分布式系统中能保证的最好的一致性。\n线性一致性（Linearizable Consistency） 线性一致性是最严格的且可实现的单对象单操作一致性模型。在这种模型下，写入的值在调用和完成之间的某个时间点可以被其他节点读取出来。且所有节点读到数据都是原子的，即不会读到数据转换的过程和中间未完成的状态。\n要想达到线性一致，需要满足以下条件：\n 任何一次读都能读取到某个数据最近的一次写的数据。 所有进程看到的操作顺序都跟全局时钟下的顺序一致。 所有进程都按照全局时钟的时间戳来区分事件的先后，那么必然所有进程看到的数据读写操作顺序一定是一样的  我们发现，这个要求十分苛刻，难以在现实中实现。因为各种物理限制使分布式数据不可能一瞬间去同步这种变化。（通信是必然有延迟的，一旦有延迟，时钟的同步就没法做到一致。）\n顺序一致性（Sequential Consistency） 由于线性一致的代价高昂，因此人们想到，既然全局时钟导致严格一致性很难实现，那么我们能否放弃了全局时钟的约束，改为分布式逻辑时钟实现呢？\nLamport 在 1979 年就提出的顺序一致性正是基于上述原理。顺序一致性中所有的进程以相同的顺序看到所有的修改。读操作未必能及时得到此前其他进程对同一数据的写更新，但是每个进程读到的该数据的不同值的顺序是一致的。\n其需要满足以下条件：\n 任何一次读写操作都是按照某种特定的顺序。 所有进程看到的读写操作顺序都保持一致。  我们发现他们都能够保证所有进程对数据的读写顺序保持一致。那么它与线性一致性有什么不同呢？\n尽管顺序一致性通过逻辑时钟保证所有进程保持一致的读写操作顺序，但这些读写操作的顺序跟实际上发生的顺序并不一定一致。而线性一致性是严格保证跟实际发生的顺序一致的。\n我们以下图为例：\n 图 a 满足了顺序一致性，但未满足线性一致性。从全局时钟来看，p2 的 read(x,0) 在 p1 的 write(x,4) 之后，但是 p1 却读出了旧的数据。因此不满足线性一致性。但是两个进程各自的读写顺序却是合理的，进程之间也没有产生冲突，因此从这两个进程的视角来看，执行流程是这样的 write(y,2)、read(x,0)、 write(x,4)、read(y,2)。此时满足顺序一致性。 图 b 满足线性一致性。因为每个读操作都读到了该变量的最新写的结果，同时两个进程看到的操作顺序与全局时钟的顺序一样。 图 c 不满足顺序一致性。因为从进程 P1 的角度看，它对变量 y 的读操作返回了结果 0。那么就是说，P1 进程的对变量 y 的读操作在 P2 进程对变量 y 的写操作之前，x 变量也如此。此时两个进程存在冲突，因此这个顺序不满足顺序一致性。  弱一致性模型 弱一致性是指系统在数据成功写入之后，不承诺立即可以读到最新写入的值，也不会具体承诺多久读到，但是会尽可能保证在某个时间级别之后，可以让数据达到一致性状态。其中包含因果一致性和最终一致性、客户端一致性。\n因果一致性（Causal Consistency）  什么是因果关系呢？\n 如果事件 B 是由事件 A 引起的或者受事件 A 的影响，那么这两个事件就具有因果关系。\n因果一致性是一种弱化的顺序一致性模型，它仅要求有因果关系的操作顺序是一致的，没有因果关系的操作顺序是随机的。\n因果一致性的条件包括：\n 所有进程必须以相同的顺序看到具有因果关系的读写操作。 不同进程可以以不同的顺序看到并发的读写操作。   我们如何确定是否具有因果关系呢？如何传播这些因果关系呢?\n 即通过逻辑时钟来保证两个写入是有因果关系的。而实现这个逻辑时钟的一种主要方式就是向量时钟。向量时钟算法利用了向量这种数据结构，将全局各个进程的逻辑时间戳广播给所有进程，每个进程发送事件时都会将当前进程已知的所有进程时间写入到一个向量中，而后进行传播。\n最终一致性（Eventual Consistency） 最终一致性是更加弱化的一致性模型，它被表述为副本之间的数据复制完全是异步的，如果数据停止修改，那么副本之间最终会完全一致。而这个最终可能是数毫秒到数天，乃至数月，甚至是“永远”。\n对于最终一致性，我们主要关注以下两点：\n 最终是多久。通常来说，实际运行的系统需要能够保证提供一个有下限的时间范围。 并发冲突如何解决。一段时间内可能数据可能多次更新，到底以哪个数据为准？通常采用最后写入成功或向量时钟等策略。  因为在数据写入与读取完全不考虑别的约束条件，因此最终一致性具有最高的并发度，经常被应用于对性能要求高的场景中。\n客户端一致性（Client-centric Consistency）  在最终一致性的模型中，如果客户端在数据不同步的时间窗口内访问不同的副本的同一个数据，会出现读取同一个数据却得到不同的值的情况。为了解决这个问题，有人提出了以客户端为中心的一致性模型。\n 客户端一致性是站在一个客户端的角度来观察系统的一致性。其保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证。\n分布式数据库中，一个节点很可能同时连接到多个副本中，复制的延迟性会造成它从不同副本读取数据是不一致的。而客户端一致性就是为了定义并解决这个问题而存在的，这其中包含了写跟随读、管道随机访问存储两大类别。\n写跟随读（Writes-Follow-Reads Consistency） 写跟随读的另一个名字是回话因果（session causal）。可以看到它与因果一致的区别是，它只针对一个客户端。 即对于一个客户端，如果一次读取到了写入的值 V1，那么这次读取之后写入了 V2。从其他节点看，写入顺序一定是 V1、V2。\n管道随机访问存储（PRAM，Pipeline Random Access Memory） 管道随机访问存储的名字来源于共享内存访问模型。其对于单个进程的写操作都被观察到是顺序的，但不同的进程写会观察到不同的顺序。\n其可拆解为以下三种一致性：\n 单调读一致性（Monotonic-read Consistency）：它强调一个值被读取出来，那么后续任何读取都会读到该值，或该值之后的值，而不会读取到旧值。 单调写一致性（Monotonic-write Consistency）：如果从一个节点写入两个值，它们的执行顺序是 V1、V2。那么从任何节点观察它们的执行顺序都应该是 V1、V2。 读你所写一致性（Read-your-writes Consistency）：一个节点写入数据后，在该节点或其他节点上是一定能读取到这个数据的。  能够同时满足以上三种一致性的即为满足 PRAM。\n PRAM、因果一致、线性一致到底有什么区别呢？\n  图 a 满足了顺序一致性与因果一致性。图上的进程都满足相同的顺序与因果关系，因此满足顺序一致性与因果一致性。 图 b 满足了因果一致性，但未满足顺序一致性。对于进程 p3 和 p4 其看到的 p1 和 p2 的执行顺序不一致，因此不满足顺序一致性。但是由于 p1 与 p2 的写入没有任何因果关系，所以此时满足因果一致性。 图 c 满足了 PRAM，但未满足因果一致性。由于 p2 的 r(x,4) 依赖于 p1 的 w(x,4)，此时两者存在因果关系。然而对于进程 p3 和 p4 而言，其看到的 p1 和 p2 执行顺序不同，因此此时并不满足因果一致性。但此时我们再来分析它们的观察顺序，此时 p4 观察的到顺序是 w(x.7)、w(x,2)、w(x,4)。而 p3 观察到的是 w(x,2)、w(x,4)、w(x,7)。尽管此时它们对不同进程写操作观察的顺序不同，但是对于同一个进程的写操作观察顺序是一致的，因此其满足 PRAM 一致性。  下图即为上面讨论的几种一致性模型的关系：\n事务隔离性 在一开始那张一致性模型图中，其实是有两个分支的，一个对应的就是数据库里面的隔离性（Isolation），另一个其实对应的是分布式系统的一致性（Consistency）。\n事务隔离是描述并行事务之间的行为，而一致性是描述非并行事务之间的行为。其实广义的事务隔离应该是经典隔离理论与一致性模型的一种混合。\n潜在问题 如下即数据库实现中遇到的各种各样的 isolation 问题。\n P0 Dirty Write（脏写）：一个事务修改了另一个尚未提交的事务已经修改的值。 P1 Dirty Read（脏读）：一个事务读取到了另一个执行到一半的事务中修改的值。 P2 Non-Repeatable Read（不可重复读）：一个事务读取过程中读到了另一个事务更新后的结果。 P3 Phantom（幻读）：某一事务 A 先挑选出了符合一定条件的数据，之后另一个事务 B 修改了符合该条件的数据，此时 A 再进行的操作都是基于旧的数据，从而产生不一致。 P4 Lost Update（丢失更新）：更新被另一个事务覆盖。 P4C Cursor Lost Update（游标丢失更新）：与 Lost Update 类似，只是发生于 cursor 的操作过程之中。 A5A Read Skew（读倾斜）：由于事务的交叉导致读取到了不一致的数据。 A5B Write Skew（写倾斜）：两个事务同时读取到了一致的数据，然后分别进行了满足条件的修改，但最终结果破坏了一致性。  隔离级别 对于分布式数据库来说，原始的隔离级别并没有舍弃，而是引入了一致性模型后，扩宽数据库隔离级别的内涵。其中共有如下数种隔离级别：\n Read Uncommitted（读未提交）：事务执行过程中能够读到未提交的修改。 Read Committed（读已提交）：事务执行过程中能够读到已提交的修改。 Monotonic Atomic View（单调原子视图）：在 Read Committed 的基础上加上了原子性的约束，观测到其他事务的修改时会观察到完整的修改。 Cursor Stability（稳定游标）：使用 cursor 读取某个数据时，这个不能被其他事务修改直至 cursor 释放或事务结束。 Snapshot Isolation（快照隔离级别）：即使其他事务修改了数据，重复读取都会读到一样的数据。 Repeatable Read（可重复读）：每个事务在独立、一致的 snapshot 上进行操作，直至提交后其他事务才可见。 Serializable（串行化）：事务按照一定的次序顺序执行。  对应的可能发生的问题如下\n    P0 P1 P4C P4 P2 P3 A5A A5B     Read Uncommitted NP P P P P P P P   Read Committed NP NP P P P P P P   Cursor Stability NP NP NP SP SP P P SP   Repeatable Read NP NP NP NP NP P NP NP   Snapshot NP NP NP NP NP SP NP P   Serializable NP NP NP NP NP NP NP NP     P（Possible）：会发生。 SP（Sometimes Possible）：有时候可能发生。 NP（Not Possible）：不可能发生。  ","date":"2022-05-24T16:40:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/","title":"一致性模型"},{"content":"一致性协议：Gossip Gossip协议（Gossip Protocol）又称Epidemic协议（Epidemic Protocol），是基于谣言传播方式的节点或者进程之间信息交换的协议，在分布式系统中被广泛使用，比如我们可以使用Gossip协议来确保网络中所有节点的数据一致。\n 说到社交网络，就不得不提著名的六度分隔理论。1967年，哈佛大学的心理学教授Stanley Milgram想要描绘一个连结人与社区的人际连系网。做过一次连锁信实验，结果发现了“六度分隔”现象。简单地说：“你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过六个人你就能够认识任何一个陌生人。\n数学解释该理论：若每个人平均认识260人，其六度就是260↑6 =1,188,137,600,000。消除一些节点重复，那也几乎覆盖了整个地球人口若干多多倍，这也是Gossip协议的雏形。\n Gossip基于六度分隔理论，每个节点像谣言传播一样，随机的将信息传播到其他节点上，不断重复这个过程，直到将信息传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致。\nGossip主要有以下三个功能：\n 直接邮寄（Direct Mail） 反熵（Anti-entropy） 谣言传播（Rumor mongering）  直接邮寄 直接发送需要更新的数据到其他节点，当数据发送失败时，将数据缓存到队列中，然后进行重传。\n从上面可以看出，这种方法实现简单切数据同步及时，但是可能会因为重试的缓存队列满了而丢数据，从而无法实现最终一致性。\n 那么我们如何实现最终一致性呢？\n 这时候就需要借助到了Gossip协议中的反熵。\n反熵 熵指混乱程度，反熵就是消除不同节点间数据的差异，提升节点间数据的相似度。\n反熵的过程如下：\n 集群中的节点每隔一段时间就随机选取某个其他节点 互相交换数据来消除两个节点之间的差异 实现数据的最终一致性  反熵主要通过三种方式进行：\n  推（Push）\n 节点A将数据（key,value,version）推送给节点B，节点B将A中比自己新的数据更新过来。     拉（Pull）\n 节点A仅将数据（key,version）推送给 B，B将本地比A新的数据（key, value, version）推送给A，A更新本地数据。     推拉（Push/Pull）\n 同时执行上述两个步骤，同时修复两个节点的数据。      从上面的三种反熵方式可以看出，反熵是需要节点两两交换和比对自己所有的数据，这样来看的话，通讯成本是很高的，而在实际场景下这种频繁的交换会大大影响性能。\n那有没有办法来减少反熵的次数呢？\n 我们可以通过引入如校验和、奇偶校验、CRC校验和格雷码校验等机制来降低需要对比的数据量和通讯信息。\n 执行反熵时，相关节点都是已知的，且节点数量不能太多。如果节点动态变化或节点数过多，反熵就不合适。\n那在这种场景下，有没有办法来解决动态、多节点的最终一致性呢？\n 答案是有的，那这时候就要用到Gossip协议中的谣言传播。\n谣言传播 谣言传播，就像是一个谣言的产生流程一样，每个人都会向自己身边的人传播，知道谣言散布各地。\n在分布式系统中，当一个节点有了新数据后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新数据，直到所有节点都存储了该数据，可以理解为之前讲的反熵中的推的方式。\n从上面可以看出，谣言传播仍然具有以下缺点：\n 时间随机：所有节点达到一致性是一个随机性的概率。可以使用闭环反熵修复。 消息冗余：同一节点会多次接收同一消息，增加了消息处理的压力，每一次通信都会对网络带宽、CPU等资源造成负载，进而影响达到最终一致性的时间。 拜占庭问题：如果有恶意节点出现，那么其他节点也会出问题。所以需要先修复故障节点。  闭环反熵 对于谣言传播，所有节点达到一致性是一个随机性的概率，其达到最终一致性的时间并不可控，这并不满足我们的期望，我们更希望能在一个确定的时间范围内实现数据副本的最终一致性，因此Gossip协议又引入了闭环反熵。\n它按照一定顺序来修复节点的数据差异，先随机选择一个节点，顺着这个节点往下循环修复。每个节点都会对比自身与下一个节点，将本节点存在而下个节点不存在的缺失数据发送给下一个节点来进行修复，如下图：\n与上面的反熵不同，闭环反熵不再是一个节点不断随机选择另一个节点，来修复副本上的熵，而是设计了一个闭环的流程，一次修复所有节点的副本数据不一致。通过这种方法我们就能够将最终一致性的时间范围明确下来，使其可控。\n总结 上面说了那么多缺点，下面也来讲讲Gossip的几个优点\n 拓展性：网络可以允许节点的动态增加和减少，新增加的节点的状态最终会与其他节点一致。 容错：网络中任何节点的宕机和重启都不会影响 Gossip 消息的传播，Gossip 协议具有天然的分布式系统容错特性。 去中心化：Gossip协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。 实现简单：Gossip 协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了 logN。 高性能：Gossip 协议的过程极其简单，实现起来几乎没有太多复杂性。  Gossip的三种功能其实都是为了实现反熵，第一种用消息队列，第二种用推拉消息，第三种用散播谣言，下面给出三个功能的使用场景\n   功能 应用场景     直接邮寄 实际场景，直接邮寄一定要实现，性能损耗最低。通过发送更新数据或缓存重传就能修复数据的不一致。   反熵 在存储组件中，节点都是已知的，采用反熵修复数据副本的不一致。   谣言传播 集群节点变化时，或节点较多时，采用谣言传播方式，来同步更新多节点的数据，来实现最终一致性。    ","date":"2022-05-24T16:30:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEgossip/","title":"一致性协议：Gossip"},{"content":"一致性协议：ZAB ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。 在ZooKeeper中，主要依赖ZAB协议来实现分布式数据一致性，基于该协议，ZooKeeper实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。\nZAB协议包括了两种基本的模式，分别是崩溃恢复和消息广播。\n消息广播 为了保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致，ZAB协议中引入了消息广播模式。\n在上面我们提到了，ZooKeeper集群中只有Leader服务器能够执行写操作，为了保证集群的数据一致性，我们需要将Leader节点更新的数据同步到Follower与Observer服务器中，所以当Leader服务器接收到客户端发送的写请求后，会自动生成对应的提案并发起一轮消息广播。\n消息广播的执行流程如下：\n 接受到客户端发送的事务请求，Leader服务器为其生成对应的事务提议。 Leader为每一个Follower和Observer都准备了一个FIFO的队列，并把提议发送到队列上。 当Follower接收到事务提议后，都会先将其以事务日志的形式写入本地磁盘中，然后再写入成功后反馈给Leader服务器一个ACK。 当Leader接收到半数以上Follower节点的ACK，它就会认为大部分节点都同意议题，准备开始提交。 Leader向所有节点发送提交事务的Commit请求，完成事务。  为了防止因为网络等原因导致的Follower、Observer节点处理请求的顺序不同而导致的数据不一致问题，保证消息广播过程中消息接收与发送的顺序性，消息广播中引入了FIFO队列和事务ID来解决这个问题。\n 在消息广播的过程中，Leader服务器会为每一个Follower、Observer服务器都各自分配一个单独的队列，然后将需要广播的事务提议放到这些队列中，并根据FIFO策略进行消息发送。由于ZAB由于协议是通过TCP协议来进行网络通信的，这样不仅保证了消息的发送顺序性，也保证了接受顺序性。 在广播事务提议之前，Leader服务器会先给这个提议分配一个全局单调递增的唯一事务ID（ZXID）。为了保证每一个消息严格的因果关系，必须将每一个事务提议按照其ZXID的先后顺序来进行排序与处理。  如果你了解过二阶段提交（2PC）协议，你会发现其实消息广播的过程实际上就是一个简化版本的二阶段提交过程，他将二阶段提交中的中断逻辑删除，Leader服务器不需要等待集群中的全部Follower服务器都响应反馈，只需要得到过半Follower的ACK就开始执行事务的提交。这种简化版的2PC虽然提高了效率，但是无法处理Leader服务器崩溃退出而导致的数据不一致问题，因此ZooKeeper中又添加了崩溃恢复模式来解决这个问题。\n崩溃恢复 当Leader服务器出现崩溃退出或机器重启，亦或是集群中不存在半数以上的服务器与Leader服务器保持正常通信时，在重新开始新的一轮原子广播事务操作之前，此时所有节点都会使用崩溃恢复协议来使彼此达到一个一致的状态。\n 崩溃恢复过程需要确保那些已经在Leader服务器上提交的事务最终被所有的事务提交。\n 假设一个事务中Leader服务器（server2）上被提交了，并且已经得到了过半Follower服务器的ACK反馈，但是在它将Commit消息发送给所有的Follower机器之前，Leader服务器就挂掉了，如下图：\n从上图可以看到，部分的节点收到了commit请求并进行了提交，而有一部分Leader还没来得及发送就已经崩溃了。针对这种情况，崩溃恢复必须要确保该事务最终能够在所有的服务器上都被提交成功，否则将会出现数据不一致的情况。所以在重新选举的时候，必定会选取ZXID最大的节点来确保其保留了最新的事件。\n 崩溃恢复过程需要确保丢弃那些只在Leader服务器上被提出的事务。\n 如果Leader服务器在提交了一个事务之后，还没来得及广播发送commit就已经崩溃推出了，从而导致集群中的其他服务器都没有收到这个事务提议。当原先的Leader节点故障恢复后，再次以Follower的角色加入集群后，此时就因为只有它完成了事务提交，而产生了数据不一致的情况，如下图：\n针对这种情况，我们需要让server2在故障恢复后能够丢弃这些只在它这个节点上提出的事务，来确保数据一致。\n为了能够满足上述的两个要求，所以ZooKeeper让Leader选举算法保证新选举出来的Leader服务器拥有集群中所有机器最高的事务编号（ZXID最大），那么这就肯定能够保证新选举出来的Leader一定具有所有已经提交的提案，此时新的Leader就会将事务日志中尚未提交的消息同步到各个服务器中。\n","date":"2022-05-24T16:20:23+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEzab/","title":"一致性协议：ZAB"},{"content":"一致性协议：Bully Bully 是最常用的一种领导选举算法，它使用节点 ID 的大小来选举新领导者。在所有活跃的节点中，选取节点 ID 最大或者最小的节点为主节点。\n核心算法  每个节点都会获得分配给它的唯一 ID。在选举期间，ID 最大的节点成为领导者。因为 ID 最大的节点“逼迫”其他节点接受它成为领导者，它也被称为君主制领导人选举：类似于各国王室中的领导人继承顺位，由顺位最高的皇室成员来继承皇位。如果某个节点意识到系统中没有领导者，则开始选举，或者先前的领导者已经停止响应请求。\n 算法核心如下：\n 集群中每个活着的节点查找比自己 ID 大的节点，如果不存在则向其他节点发送 Victory 消息，表明自己为领导节点。 如果存在比自己 ID 大的节点，则向这些节点发送 Election 消息，并等待响应。 如果在给定的时间内，没有收到这些节点回复的消息，则自己成为领导节点，并向比自己 ID 小的节点发送 Victory 消息。 节点收到比自己 ID 小的节点发送的 Election 消息，则回复 Alive 消息。  上图举例说明了 Bully 领导者选举算法，其中：\n 节点3 注意到先前的领导者 6 已经崩溃，并且通过向比自己 ID 更大的节点发送选举消息来开始新的选举。 4 和 5 以 Alive 响应，因为它们的 ID 比 3 更大。 3 通知在这一轮中作出响应的最大 ID 节点是5。 5 被选为新领导人，它广播选举信息，通知排名较低的节点选举结果。  这种算法的一个明显问题是：它违反了“安全性”原则（即一次最多只能选出一位领导人）。在存在网络分区的情况下，在节点被分成两个或多个独立工作的子集的情况下，每个子集都会选举其领导者。（脑裂）\n该算法的另一个问题是：**它对 ID较大的节点有强烈的偏好，但是如果它们不稳定，会严重威胁选举的稳定性，并可能导致不稳定节点永久性地连任。**即不稳定的高排名节点提出自己作为领导者，不久之后失败，但是在新一轮选举中又赢得选举，然后再次失败，选举过程就会如此重复而不能结束。这种情况，可以通过监控节点的存活性指标，并在选举期间根据这些指标来评价节点的活性，从而解决该问题。\n算法改进 Bully 算法虽然经典，但由于其相对简单，在实际应用中往往不能得到良好的效果。因此在分布式数据库中，我们会看到如下所述的多种演进版本来解决真实环境中的一些问题，但需要注意的是，其核心依然是经典的 Bully 算法。\n故障转移节点列表 **我们可以使用多个备选节点作为在发生领导节点崩溃后的故障转移目标，从而缩短重选时间。**每个当选的领导者都提供一个故障转移节点列表。当集群中的节点检测到领导者异常时，它通过向该领导节点提供的候选列表中排名最高的候选人发送信息，开始新一轮选举。如果其中一位候选人当选，它就会成为新的领导人，而无须经历完整的选举。\n如果已经检测到领导者故障的进程本身是列表中排名最高的进程，它可以立即通知其他节点自己就是新的领导者。\n上图显示了采用这种优化方式的过程，其中：\n 6 是具有指定候选列表 {5，4} 的领导者，它崩溃退出，3 注意到该故障，并与列表中具有最高等级的备选节点5 联系； 5 响应 3，表示它是 Alive 的，从而防止 3 与备选列表中的其他节点联系； 5 通知其他节点它是新的领导者。  如果备选列表中，第一个节点是活跃的，我们在选举期间需要的步骤就会更少。\n节点分角色 另一种算法试图通过将节点分成候选和普通两个子集来降低消息数量，其中只有一个候选节点可以最终成为领导者。普通节点联系候选节点、从它们之中选择优先级最高的节点作为领导者，然后将选举结果通知其余节点。\n为了解决并发选举的问题，该算法引入了一个随机的启动延迟，从而使不同节点产生了不同的启动时间，最终导致其中一个节点在其他节点之前发起了选举。该延迟时间通常大于消息在节点间往返时间。具有较高优先级的节点具有较低的延迟，较低优先级节点延迟往往很大。\n上图显示了选举过程的步骤，其中：\n 节点 4 来自普通的集合，它发现了崩溃的领导者 6，于是通过联系候选集合中的所有剩余节点来开始新一轮选举； 候选节点响应并告知 4 它们仍然活着； 4 通知所有节点新的领导者是 2。  该算法减小了领导选举中参与节点的数量，从而加快了在大型集群中该算法收敛的速度。\n邀请算法 **邀请算法允许节点“邀请”其他进程加入它们的组，而不是进行组间优先级排序。**该算法允许定义多个领导者，从而形成每个组都有其自己的领导者的局面。每个节点开始时都是一个新组的领导者，其中唯一的成员就是该节点本身。\n组领导者联系不属于它们组内的其他节点，并邀请它们加入该组。如果受邀节点本身是领导者，则合并两个组；否则，受邀节点回复它所在组的组长 ID，允许两个组长直接取得联系并合并组，这样大大减少了合并的操作步骤。\n上图显示了邀请算法的执行步骤，其中：\n 四个节点形成四个独立组，每个节点都是所在组的领导，1 邀请 2 加入其组，3 邀请 4 加入其组； 2 加入节点 1的组，并且 4 加入节点3的组，1 为第一组组长，联系人另一组组长 3，剩余组成员（在本例中为 4个）获知了新的组长 1； 合并两个组，并且 1 成为扩展组的领导者。  由于组被合并，不管是发起合并的组长成为新的领导，还是另一个组长成为新的领导。为了将合并组所需的消息数量保持在最小，一般选择具有较大 ID 的组长的领导者成为新组的领导者，这样，只有来自较小 ID 组的节点需要更新领导者。\n与所讨论的其他算法类似，该算法采用“分而治之”的方法来收敛领导选举。邀请算法允许创建节点组并合并它们，而不必从头开始触发新的选举，这样就减少了完成选举所需的消息数量。\n环形算法 **在环形算法中，系统中的所有节点形成环，并且每个节点都知道该环形拓扑结构，了解其前后邻居。**当节点检测到领导者失败时，它开始新的选举，选举消息在整个环中转发，方式为：每个节点联系它的后继节点（环中离它最近的下一节点）。如果该节点不可用，则跳过该节点，并尝试联系环中其后的节点，直到最终它们中的一个有回应。\n节点联系它们的兄弟节点，收集所有活跃的节点从而形成可用的节点集。在将该节点集传递到下一个节点之前，该节点将自己添加到集合中。\n该算法通过完全遍历该环来进行。当消息返回到开始选举的节点时，从活跃集合中选择排名最高的节点作为领导者。\n如上图所示，你可以看到这样一个遍历的例子：\n 先前的领导 6 失败了，环中每个节点都从自己的角度保存了一份当前环的拓扑结构； 以 3 为例，说明查找新领导的流程，3 通过开始遍历来发起选举轮次，在每一步中，节点都按照既定路线进行遍历操作，5 不能到 6，所以跳过，直接到 1； 由于 5 是具有最高等级的节点，3 发起另一轮消息，分发关于新领导者的信息。  该算法的一个优化方法是每个节点只发布它认为排名最高的节点，而不是一组活跃的节点，以节省空间：因为 Max 最大值函数是遵循交换率的，也就是知道一个最大值就足够了。当算法返回到已经开始选举的节点时，最后就得到了 ID 最大的节点。\n另外由于环可以被拆分为两个或更多个部分，每个部分就会选举自己的领导者，这种算法也不具备“安全性”。 如前所述，要使具有领导的系统正常运行，我们需要知道当前领导的状态。因此，为了系统整体的稳定性，领导者必须保证是一直活跃的，并且能够履行其职责。\n脑裂的解决方案 在上文讨论的所有算法都容易出现脑裂的问题，即最终可能会在独立的两个子网中出现两个领导者，而这两个领导并不知道对方的存在。\n为了避免脑裂问题，我们一般需要引入法定人数来选举领导。比如 Elasticsearch 选举集群领导，就使用 Bully 算法结合最小法定人数来解决脑裂问题。\n如上图所示，目前有 2 个网络、5 个节点，假定最小法定人数是3。A 目前作为集群的领导，A、B 在一个网络，C、D 和 E 在另外一个网络，两个网络被连接在一起。\n当这个连接失败后，A、B 还能连接彼此，但与 C、D 和 E 失去了联系。同样， C、D 和 E 也能知道彼此，但无法连接到 A 和B。\n此时，C、D 和 E 无法连接原有的领导 A。同时它们三个满足最小法定人数3，故开始进行新一轮的选举。假设 C 被选举为新的领导，这三个节点就可以正常进行工作了。\n而在另外一个网络中，虽然 A 是曾经的领导，但是这个网络内节点数量是 2，小于最小法定人数。故 A 会主动放弃其领导角色，从而导致该网络中的节点被标记为不可用，从而拒绝提供服务。这样就有效地避免了脑裂带来的问题。\n","date":"2022-05-24T16:20:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEbully/","title":"一致性协议：Bully"},{"content":"一致性协议：Raft 由于Paxos算法相对来说较为复杂且难以理解，因此在后来又出现了一种用于替代Paxos的算法——Raft\nRaft 算法是一种简单易懂的共识算法，所谓共识，就是多个节点对某个事情达成一致的看法，即使是在部分节点故障、网络延时、网络分割的情况下。它依靠状态机和主从同步的方式，在各个节点之间实现数据的一致性。\nRaft算法的核心主要为以下两部分，下面将进行具体讲解\n 主节点选举（Leader Election） 数据同步（Log Replication）  状态机 在Raft中节点中存在三种状态，状态之间可以相互进行转换\n  Leader（主节点）\n  Follower（从节点）\n  Candidate（竞选节点）\n  同时，每个节点上会存放一个倒计时器（Election Timeout），时间随机在150ms到300ms之间。Leader节点会周期性的向所有Follower发送一个心跳包（Heartbeat），收到心跳包的节点会将其计时器清零后重新计时，如果在倒计时结束前没有收到Leader的心跳包，此时Follower就会变为Candidate，开始进入竞选状态。\n具体的状态转移如下图\n执行流程 主节点选举  介绍了状态机后，下面就来看看主节点的选举流程\n 第一步，在一开始时，由于没有Leader，所以此时所有节点的身份都是Follower，每一个节点上都有着自己的计数器，当计时器达到了超时时间后，该节点就会转换为Candidate，开始选举过程。\n第二步，成为Candidate的节点首先会给自己投一张票，然后向集群中的其他所有节点发起投票请求。\n第三步，收到投票请求并且还未投票的Follower节点会向发起者回复投票反馈，如果收到了超过半数的回复，此时Candidate选举成功，状态转变为Leader。\n第四步，Leader节点会立刻向其他节点发出通告，告知其他节点它已成功选举成Leader，收到通知的节点会转换为Follower。并且Leader会周期性的发送心跳包给所有Follower来表明它还存活，当Follower接收到心跳包时，就会重置计时器。\n一旦Leader节点挂掉，它就无法发出心跳包来重置Follower的倒计时器，那么当Follower的超时时间到达后，其就会转换成Candidate节点，再次重复以上过程。\n上面介绍的是单节点的选举，倘若同时有多个Follower同时倒计时结束后成为Candidate，同时开始选举，并且在选举过程中所获得的票数相同，此时就陷入了僵局，谁都无法成为Leader。\n在重新选举时，由于每个节点设置的超时时间都是随机的，因此下一次同时出现多个Candidate并获得同样票数导致选举失败的情况的概率则非常低。\n数据同步 Raft中数据同步的方式与之前写过的2PC（二阶段提交协议）有一些相似，也是分为投票和提交两个阶段，具体流程如下。\n第一步，客户端将修改传入Leader中（此时修改并没有提交，只是写入日志中）。\n第二步，Leader将修改复制到集群内的所有Follower节点上，如果复制失败，会不断进行重试直至成功。\n第三步，Follow节点们成功接收到复制的数据后，会反馈结果给Leader节点，如果Leader节点接收到超过半数的Follower反馈，则表明复制成功，于是提交自己的数据，并且通知客户端数据提交成功。\n第四步，提交成功后，Leader会通知所有的Follower让它们也提交修改，此时所有节点的值达成一致，完成数据同步流程。\n为了便于理解，可以结合下面的Raft原理动画一同学习。\nRaft原理动画\n","date":"2022-05-24T15:59:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEraft/","title":"一致性协议：Raft"},{"content":"一致性协议：Paxos 拜占庭将军问题  拜占庭位于如今的土耳其的伊斯坦布尔，是东罗马帝国的首都。由于当时拜占庭罗马帝国国土辽阔，为了达到防御目的，每个军队都分隔很远，将军与将军之间只能靠信差传消息。在战争的时候，拜占庭军队内所有将军和副官必须达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，在军队内有可能存有叛徒和敌军的间谍，叛徒可以任意行动以达到以下目标：欺骗某些将军采取进攻行动；促成一个不是所有将军都同意的决定，如当将军们不希望进攻时促成进攻行动；或者迷惑某些将军，使他们无法做出决定。如果叛徒达到了这些目的之一，则任何攻击行动的结果都是注定要失败的，只有完全达成一致的努力才能获得胜利。\n这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，拜占庭问题就此形成 。\n 拜占庭将军问题的核心在于在缺少可信任的中央节点和可信任的通道的情况下，分布在不同地方的各个节点应如何达成共识。我们可以将这个问题延伸到分布式计算领域中。\n在分布式计算领域中，试图在异步系统和不可靠的通道上达到一致性状态是不可能的，因此在对一致性的研究过程中，都往往假设信道是可靠的，而事实上，大多数系统都是部署在同一个局域网中，因此消息被篡改的情况非常罕见；另一方面，由于机器硬件和网络原因导致的消息不完整问题，也仅仅只需要一套简单的校验算法即可避免。\n那么当我们假设拜占庭问题不存在（所有消息都是完整的，没有被篡改），那么这种情况下需要什么算法来保证一致性呢？拜占庭将军问题的提出者Lamport提出了一种非拜占庭将军问题的一致性解决方案——Paxos算法\nPaxos 问题描述 与拜占庭将军问题一样，Lamport同样使用故事的方式来提出Paxos问题\n 希腊岛屿Paxon上的议员在议会大厅中表决通过法律，并通过服务员传递纸条的方式交流信息，每个议员会将通过的法律记录在自己的账目上。问题在于执法者和服务员都不可靠，他们随时会因为各种事情离开议会大厅，并随时可能有新的议员进入议会大厅进行法律表决，使用何种方式能够使得这个表决过程正常进行，且通过的法律不发生矛盾。\n 不难看出故事中的议会大厅就是分布式系统，议员对应节点或进程，服务员传递纸条的过程就是消息传递的过程，法律即是我们需要保证一致性的值。议员和服务员的进出对应着节点/网络的失效和加入，议员的账目对应节点中的持久化存储设备。上面表决过程的正常进行可以表述为进展需求：当大部分议员在议会大厅呆了足够长时间，且期间没有议员进入或者退出，那么提出的法案应该被通过并被记录在每个议员的账目上。\nPaxos算法需要解决的问题就是如何在上述分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。\n为了能够使得表决过程正常进行，且通过的法律不发生矛盾，那么我们需要保证以下几点\n 在这些被提出的提案中，最后只能有一个被选定 如果没有提案被提出，那么就不会有被选定的提案 当一个提案被选定后，节点们应该可以获取被选定的提案信息  在Paxos算法中，主要有以下三种角色，并且一个节点可能同时担任几种角色\n 提议者（Proposer）：负责提出提议； 接受者（Acceptor）：对每个提议进行投票； 告知者（Learner）：被告知投票的结果，不参与投票过程。  同时，假设不同参与者之间通过收发消息来进行通信，那么我们需要具备以下条件\n 每个参与者可能会因为出错而导致停止、重启等情况， 虽然消息在传输过程中可能会出现不可预知的延迟、重复、丢失等情况，但是消息不会被损坏或篡改（即不存在拜占庭问题）  执行过程 首先我们规定一个提议包含两个字段：[n, v]，其中 n 为序号（具有唯一性），v 为提议值。\nPaxos执行过程主要分为两个阶段，与之前讲过的2PC（二阶段提交协议）类似。\n Prepare阶段 Accept阶段  如下图\nPrepare阶段  首先，每个Proposer都会向所有的Acceptor发送一个Prepare请求。 当Acceptor接收到一个Prepare请求，提议内容为[n1,v1]，由于之前还未接受过Prepare请求，那么它会返回一个[no previous]的Prepare响应，并且设置当前接受的提议为[n1,v1]，同时保证以后不会再接收序号小于n1的提议。 如果Acceptor再次收到一个Prepare请求，提议内容为[n2,v2]，此时会有两种情况。  如果n2 \u0026lt; n1，此时Acceptor就会直接抛弃该请求 如果n2 \u0026gt;= n1，此时Acceptor就会发送一个[n1,v1]的Prepare响应，设置当前接受到的提议为[n2,v2]，同时保证与上一步逻辑相同，保证以后不会再接受序号小于n2的提议。    Accept阶段  当一个Proposer接收到超过一半的Acceptor的Prepare响应时，此时它就会发送一个针对[n,v]提案的Accept请求给Acceptor。 如果一个Acceptor收到一个编号为n的提案的Accept请求，此时有两种情况。  如果该Acceptor没有对编号大于n的Prepare请求做出过响应，它就会接受该提案，并发送Learn提议给Learner 如果该Acceptor接受过编号大于n的Prepare请求，那么它就会拒绝、不回应或回复error。（如果一个Proposer没有收到过半的回应，那他就会重新进入第一阶段，递增提案号后重新提出Prepare请求）    在上述过程中，每一个Proposer都有可能会产生多个提案。但只要每个Proposer都遵循如上述算法运行，就一定能保证算法执行的正确性。\nLearner获取提案 在Accept阶段之后Acceptor选定提案后，根据具体的应用场景不同，Learner主要采用以下三种方案来学习选定的提案\n   方案 优点 缺点     Acceptor直接将提议发给所有的Learner Learner能够快速获取被选定的提议 通信次数过多，每个Acceptor都要和Learn产生通信（M * N）   Acceptor接受提议后将提议发给主Learner，主Learner再通知其他Learner 通信次数减少（M + N - 1） 单点问题（主Learner出现故障就会崩溃）   Acceptor将提议发一个Learner集合，Learener集合再发给其他Learener 解决了单点问题，集合中Learner个数越多就越可靠 网络通信复杂度变高    当Learner们发现有大多数的Acceptor接受了某一个提议，那么该提议的提议值则就是Paxos最终选择出来的结果。\n活锁问题 在Paxos算法实际运作的时候还存在这样一种极端的情况——当有两个Proposer依次提出了一系列编号递增的提案，此时就会导致陷入死循环，无法完成第二阶段，也就是无法选定一个提案，如以下场景。\n Proposer P1发出编号为n1的Prepare请求，收到过半响应，完成了阶段一的流程。 同时，Proposer P2发出编号为n2的Prepare请求（n2 \u0026gt; n1），也收到了过半的响应，完成了阶段一的流程，并且Acceptor承诺不再接受编号小于n2的提案。 P1进入第二阶段时，由于Acceptor不接受小于n2的提案，所以P1重新回到第一阶段，递增提案号为n3后重新发出Prepare请求 紧接着，P2进入第二阶段，由于Acceptor不接受小于n3的提案，此时它也重新回到第一阶段，递增提案号后重新发出Prepare请求 于是P1、P2陷入了死循环，谁都无法完成阶段二，这也就导致了没有value能被选定。  为了保证Paxos算法的可持续性，以避免陷入上述提到的死循环，就必须选择一个主Proposer，并规定只有主Proposer才能够提出提案。这样一来，只要主Proposer和过半的Acceptor能够正常进行网络通信，那么肯定会有一个提案被批准（第二阶段的accept），则可以解决死循环导致的活锁问题。\n","date":"2022-05-24T15:58:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEpaxos/","title":"一致性协议：Paxos"},{"content":"分布式锁 随着互联网技术的不断发展、数据量的大幅增加、业务逻辑的复杂化导致传统的集中式系统已经无法应用于当前的业务场景，因此分布式系统被应用在越来越多的地方，但是在分布式系统中，由于网络、机器（如网络延迟、分区，机器宕机）等情况导致场景更加复杂，充满了不可靠的情况。为了保证一致性，在这种情况下我们就需要用到分布式锁。\n 那么分布式锁需要具备哪些条件呢？\n  获取、释放锁的性能要高 判断锁的获取操作必须要是原子的（防止同一个锁被多个节点获取） 网络或者机器出现问题导致无法继续工作时，必须要释放锁（防止死锁） 可重入的，一个线程可以多次获取同一把锁（防止死锁） 阻塞锁（依据业务需求）  但是目前并没有能够满足上面所有要求的完美结局方案，对于分布式锁，我们通常使用以下三种方法来实现\n 数据库 Redis(缓存) Zookeeper  数据库 唯一索引 我们可以利用数据库中的唯一索引来实现。由于唯一索引能够保证记录只被插入一次，因此我们可以利用其判断当前是否处于锁定状态。所以当想要获取锁的时候，就向数据库中插入一条记录，而释放锁的时候就删除这条记录即可。\n但是该方法存在以下问题\n 锁没有失效时间，如果解锁失败的话其他进程无法再获得该锁（死锁） 非阻塞锁，插入失败就直接报错，没有办法进入队列重试 不可重入，同一线程在没有释放锁之前无法重复获得该锁  对于数据库来说我们还可以选择使用排他锁、乐观锁等方法来实现分布式锁，但是由于这些方法对原表有侵入、占用数据库连接等情况，一般情况下都不做考虑，因此这里也就不详细描述。\nRedis SETNX、EXPIRE 我们可以利用setnx(set if not exist)命令来实现锁。只有在缓存中不存在Key的时候才会set并返回true，而Key已存在的时候就直接返回false。同时为了防止获取锁失败而导致的死锁情况，我们可以利用expire命令对这个key设置一个超时时间。\n为了防止我们setnx成功之后线程发生异常中断导致我们来不及设置expire而导致死锁，我们通常会使用以下命令来设置\n1 2 3 4 5 6 7 8  SETkeyrandom_valueNXEX30000/* EX seconds – 设置键key的过期时间，单位时秒 PX milliseconds – 设置键key的过期时间，单位时毫秒 NX – 只有键key不存在的时候才会设置key的值 XX – 只有键key存在的时候才会设置key的值 */  该命令仅在Key不存在（NX选项）时才插入，并且设置到期时间为30000毫秒（PX选项），value设置为随机值，该值在所有客户端和所有锁定请求中必须唯一（防止被他人误删）。\n当我们想要释放锁时，为了保证安全（防止误删除另一个客户端创建的锁），仅当密钥存在且存储在密钥上的值恰好是期望的值时，才删除该密钥，下面是以lua脚本完成的删除逻辑\n1 2 3 4 5  if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end   这种方法虽然实现起来非常简单，但是其存在着单点问题，它加锁时只作用于一个Redis节点上，如果该节点出现故障故障，即使使用哨兵来保证高可用，也会出现锁丢失的情况，如下场景\n 在Redis的Master节点拿到锁，此时锁还没有同步到Slave节点 此时Master发生故障，哨兵主导进行故障转移，Slave节点升级为Master节点 由于锁没来得及同步，因此导致锁丢失  考虑到这种情况，Redis作者antirez基于分布式环境下提出了一种更高级的分布式锁的实现方式：Redlock。\nRedLock算法 Redlock 是 Redis 的作者 antirez 给出的集群模式的 Redis 分布式锁，它基于 N 个完全独立（不存在主从复制或者其他集群协调机制）的 Redis节点（通常情况下 N 设置成 5，为了资源的合理利用通常为奇数）。\n算法的流程如下\n 获取当前时间 客户端依次尝试从5个(奇数)相互独立的Redis实例中使用相同的key和具有唯一性的value获取锁 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从**大多数（N / 2 + 1）**实例上获取了锁，才认为获取锁成功； 如果获取了锁，则重新计算有效期时间，即原有效时间减去获取锁消耗的时间 如果客户端获取锁失败，则释放所有实例上的锁  虽然RedLock算法比上面的单点Redis锁更可靠，但是由于分布式的复杂性，实现起来的条件也更加的苛刻。\n 由于必须获取**（N / 2 + 1）**个节点上的锁，所以可能会出现锁冲突的情况（即每个人都获取了一些锁，但是没有人获取一半以上的锁）。针对这个问题，其借鉴了Raft算法的思路，即产生冲突后为每个节点设置一个随机开始时间，在时间到后重新尝试获取锁，但是这也导致了获取锁的成本增加。 如果5个节点有2个宕机，锁的可用性会极大降低，因为必须等待这两个宕机节点的结果超时才能返回。另外只剩3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，加锁难度也加大了。 如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。  介于以上情况，我们还可以选择更加可靠的方法，即Zookeeper实现的分布式锁。\nZookeeper Zookeeper是一个为分布式应用提供一致性服务的软件，它内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名。\n由于Zookeeper同样没有实现锁API，所以我们利用其数据节点来表示锁，数据节点分为以下三种类型\n 永久节点：节点创建后永久存在，不会因为会话的消失而消失 临时节点：与永久节点相反，当客户端结束会话后立即删除 顺序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。  实现原理 除了上面介绍的节点外，我们还需要用到Watcher（监视器）来注册对节点状态的监听\n Watcher：注册一个该节点的监视器，当节点状态发生变化时，Watcher就会触发，此时Zookeeper将会向客户端发送一条通知（Watcher只能被触发一次）  根据上述特性，我们就可以使用临时顺序节点与Watcher来实现分布式锁\n 创建一个锁目录/lock 当需要获取锁时，就在lock目录下创建临时顺序节点 获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则到lock目录下注册一个子节点变更的Watcher监听，获得子节点的变更通知后重复此步骤直至获得锁 执行完业务逻辑后，主动删除自己的节点来释放锁。此时会触发其他节点的Watcher，让其他人获取锁。   那如果出现网络中断或者机器宕机，锁还能释放吗？\n 这里需要注意的是，我们使用的是临时节点，所以当客户端因为某种原因无法继续工作时，就会导致会话的中断，临时节点就会被Zookeeprer自动删除。这也就是Zookeeper相较于Redis更加可靠的原因。\n羊群效应 上面这个实现方法，大体上能够满足一般的分布式集群竞争锁的需求，并且能够保证一定的性能。但是随着机器规模的扩大，其效率会越来越低。\n为什么呢？我们思考一下锁的释放流程\n 在我们获取锁失败后，会注册一个对lock目录的Watcher监控，当有节点变更消息时，就会通知给所有注册了的机器。然而这个通知除了使序号最小的节点获取锁外，对其他的节点没有产生任何实际作用。\n 性能瓶颈的原因就是上面这个问题，大量的Watcher通知和子节点列表获取两个操作重复运行，并且绝大多数运行结果都是判断出自己并非是序号最小的节点，从而继续等待下一次的通知，浪费了大量的资源。\n如果集群规模较大，不仅会对Zookeeper服务器造成巨大的性能影响和网络冲击，更严重的时候甚至会因为多个节点对应的客户端同时释放锁导致大量的节点消失，从而短时间内向剩余客户端发送大量的事件通知——这就是所谓的羊群效应。\n改进方法 羊群效应出现根源在于其没有找到事件的真正关注点，对于分布式锁的竞争过程来说，它的核心逻辑就是判断自己是否是所有子节点中序号最小的。那么问题就简单了，我们只需要关注比自己序号小的那一个相关节点的变更情况就可以了，而不再需要关注全局的子列表变更情况。\n于是，改进后的获取流程如下\n 创建一个锁目录/lock 当需要获取锁时，就在lock目录下创建临时顺序节点 获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则到Watcher自己的次小节点（防止羊群效应） 执行完业务逻辑后，主动删除自己的节点来释放锁，此时会触发顺序的下一个节点的Watcher  总结    方案 优点 缺点 应用场景     数据库 直接使用数据库，操作简单 分布式系统的性能瓶颈大部分都在数据库，而使用数据库锁加大了负担 业务逻辑简单，对性能要求不高   Redis 性能较高，且实现起来方便 锁超时机制不可靠，当线程获取锁时，可能因为处理时间过长导致锁超时失效 追求高性能，允许偶发的锁失效问题   ZooKeeper 不依赖超时时间释放锁，可靠性高 由于频繁的创建和删除节点，性能比不上Redis锁 系统要求高可靠性    ","date":"2022-05-24T15:54:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","title":"分布式锁"},{"content":"分布式ID ID是数据的唯一标识，传统的做法是使用数据库的自增ID，但是随着业务规模的不断发展，数据量将越来越大，于是需要进行分库分表，而分表后，每个表中的ID都会按自己的节奏进行自增，很有可能出现ID冲突的情况。这时就需要一个单独的机制来负责生成一个全局唯一的ID。这个ID也可以叫做分布式ID。\n对于一个合格的分布式ID，它应该满足以下几项条件\n 全局唯一 高可用 高性能 趋势递增 方便接入  下面就介绍几种常见的分布式ID生成方案\n数据库 自增ID 我们可以利用数据库的auto_increment自增ID来生成分布式ID，只需要一个数据库实例即可完成。\n建表如下\n1 2 3 4 5 6 7 8  CREATEDATABASE`SEQID`;CREATETABLESEQID.SEQUENCE_ID(idbigint(20)unsignedNOTNULLauto_increment,stubchar(10)NOTNULLdefault\u0026#39;\u0026#39;,PRIMARYKEY(id),UNIQUEKEYstub(stub))ENGINE=MyISAM;  我们通过往该表中插入数据，并获取到自增的主键id。为了考虑到并发的安全 ，我们可以通过事务来完成这个操作。\n1 2 3 4  begin;insertintoSEQUENCE_ID(value)VALUES(\u0026#39;values\u0026#39;);selectlast_insert_id();commit;  我们可以看出，这种依赖单点的方法虽然实现起来简单，但是这种依赖单点的方法并不可靠，因为Mysql并不能很好的支持高并发，当请求ID量特别大的时候就会因为单点的宕机而影响到整个业务系统。\n多主模式 既然单点不可靠，那么我们可以考虑使用集群的方式来解决这个问题。考虑到单个主节点可能会因为宕机而没有将数据同步到从节点，可能会导致ID重复的情况，因此我们可以考虑使用多主集群。\n为了防止多个主节点生成重复的ID，我们通常会通过控制初始值和步长来避免这个问题。\n例如在有两个主节点的情况下，我们就可以通过这种方式来错开ID。\n1 2 3 4 5 6 7  -- 节点1 set@@auto_increment_offset=1;-- 起始值 set@@auto_increment_increment=2;-- 步长 -- 节点2 set@@auto_increment_offset=2;-- 起始值 set@@auto_increment_increment=2;-- 步长   但是，这种方法的可拓展性不强，倘若我们要往集群中增加新的主节点时，我们就需要重新去设置步长，并且还需要根据前两个节点的自增ID的大小来考虑我们新节点的起始值，而这些操作只能人工来进行。如果在修改步长的时候出现了重复的ID，此时就还需要进行停机修改。\n号段模式 前面两种方法的局限性在于其每次获取ID时都需要直接访问数据库，效率较低，如果能够一次获取大量的ID，并将其缓存在本地，那样就可以大大的提升ID获取的效率，这也是号段模式的核心思想。号段模式每次从数据库中批量的获取一段自增ID，即取出一个范围的ID交给号段服务维护。例如（1，2000]即代表着2000个ID。我们的业务系统只需要到号段服务中申请ID，不需要每次都去请求数据库，直到所有的ID都用完后才会去申请下一个号段。\n数据库表修改如下\n1 2 3 4 5 6 7 8  CREATETABLEid_generator(idint(10)NOTNULL,max_idbigint(20)NOTNULLCOMMENT\u0026#39;当前最大id\u0026#39;,stepint(20)NOTNULLCOMMENT\u0026#39;号段的步长\u0026#39;,biz_typeint(20)NOTNULLCOMMENT\u0026#39;业务类型\u0026#39;,versionint(20)NOTNULLCOMMENT\u0026#39;版本号\u0026#39;,PRIMARYKEY(`id`))  号段服务不再强依赖数据库，即使数据库不可用，号段服务也可以继续工作直到申请的ID全部使用完。但是如果此时号段服务重启，就会导致剩余的ID丢失。\n为了保证号段服务的高可用，我们同样需要建立一个集群，在请求方从号段服务获取ID时，就会随机的选取一个节点来获取，而这种并发场景下我们同样需要考虑到并发安全的问题，因此我们上面的表中也提供了一个版本号的字段version，我们可以使用乐观锁来进行并发的控制。\n1  updateid_generatorsetcurrent_max_id=#{newMaxId},version=version+1whereversion=#{version}  我们可以使用上面的SQL来获取新号段，当update更新成功就说明号段获取成功了。\n雪花算法 雪花算法（Snowflake） 是twitter开源的一个分布式ID的生成算法，它的核心思想是：生成一个long类型的ID，一个long大小8字节，一个字节8个比特位，因此它使用64个比特位来确定一个分布式ID。其中41bit代表时间戳，10bit标识一台机器，剩下12bit则用来标识每个id 第1个bit位为符号位，因为生成的id通常为正数所以固定为0 2~42位为时间戳部分，其精确至毫秒。同时为了更加合理的利用，其并不会存储当前的时间，而是使用时间戳的差值（当前时间 - 固定的开始时间），这样就可以保证ID从更小的起点开始生成。 43~52位为工作机器的id，这里的计算会更加灵活，可以根据机房数量、机器数量来自行均衡，保证能够利用到更多的机器。 53~64位为序列编号，在同一毫秒中的同一台机器上我们可以生成4096个ID  考虑到不同的业务场景以及各个公司的特性，大多数公司并不会去直接使用snowflake，而是会对其进行改造，让其更贴合自身的使用场景。如百度的uid-generator、美团的Leaf、滴滴的TinyId等。\n下面是github上开源的一个java实现的snowflake\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101  /** * twitter的snowflake算法 -- java实现 * github链接：https://github.com/beyondfengyu/SnowFlake * @author beyond * @date 2016/11/26 */ public class SnowFlake { /** * 起始的时间戳 */ private final static long START_STMP = 1480166465631L; /** * 每一部分占用的位数 */ private final static long SEQUENCE_BIT = 12; //序列号占用的位数  private final static long MACHINE_BIT = 5; //机器标识占用的位数  private final static long DATACENTER_BIT = 5;//数据中心占用的位数  /** * 每一部分的最大值 */ private final static long MAX_DATACENTER_NUM = -1L ^ (-1L \u0026lt;\u0026lt; DATACENTER_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L \u0026lt;\u0026lt; MACHINE_BIT); private final static long MAX_SEQUENCE = -1L ^ (-1L \u0026lt;\u0026lt; SEQUENCE_BIT); /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT; private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT; private long datacenterId; //数据中心  private long machineId; //机器标识  private long sequence = 0L; //序列号  private long lastStmp = -1L;//上一次时间戳  public SnowFlake(long datacenterId, long machineId) { if (datacenterId \u0026gt; MAX_DATACENTER_NUM || datacenterId \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;datacenterId can\u0026#39;t be greater than MAX_DATACENTER_NUM or less than 0\u0026#34;); } if (machineId \u0026gt; MAX_MACHINE_NUM || machineId \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;machineId can\u0026#39;t be greater than MAX_MACHINE_NUM or less than 0\u0026#34;); } this.datacenterId = datacenterId; this.machineId = machineId; } /** * 产生下一个ID * * @return */ public synchronized long nextId() { long currStmp = getNewstmp(); if (currStmp \u0026lt; lastStmp) { throw new RuntimeException(\u0026#34;Clock moved backwards. Refusing to generate id\u0026#34;); } if (currStmp == lastStmp) { //相同毫秒内，序列号自增  sequence = (sequence + 1) \u0026amp; MAX_SEQUENCE; //同一毫秒的序列数已经达到最大  if (sequence == 0L) { currStmp = getNextMill(); } } else { //不同毫秒内，序列号置为0  sequence = 0L; } lastStmp = currStmp; return (currStmp - START_STMP) \u0026lt;\u0026lt; TIMESTMP_LEFT //时间戳部分  | datacenterId \u0026lt;\u0026lt; DATACENTER_LEFT //数据中心部分  | machineId \u0026lt;\u0026lt; MACHINE_LEFT //机器标识部分  | sequence; //序列号部分  } private long getNextMill() { long mill = getNewstmp(); while (mill \u0026lt;= lastStmp) { mill = getNewstmp(); } return mill; } private long getNewstmp() { return System.currentTimeMillis(); } public static void main(String[] args) { SnowFlake snowFlake = new SnowFlake(2, 3); for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; 12); i++) { System.out.println(snowFlake.nextId()); } } }   Redis 我们可以利用Redis中的incr命令来原子的获取自增ID\n1 2 3 4  127.0.0.1:6379\u0026gt;setseq_id1//初始化自增IDOK127.0.0.1:6379\u0026gt;incrseq_id//自增并返回结果(integer)2  使用Redis实现起来特别简单且高效，但是我们还需要考虑到持久化时带来的一些问题\n RDB持久化：由于其是定期保存一次数据库的快照，为了保证效率他也存在着一定的时间间隔。倘若在我们刚保存一次快照后，连续获取了几次ID，而此时还没来得及做下一次持久化就宕机了。当我们通过持久化重启Redis后，这段时间生成的ID就会被重复使用。 AOF持久化：AOF相当于是逻辑日志，其会通过保存我们执行过的命令来进行持久化。它并不像RDB出现一段时间的数据丢失而导致的ID重复的情况，但是在它恢复的过程中需要重新执行保存的命令，因此随着ID数量的增多，它重启恢复数据的时间也会越来越慢。  总结     优点 缺点     数据库自增ID 实现简单，ID单调自增，数值类型查询效率高 单点问题，在高并发时可能会有宕机的风险   数据库多主集群 解决了单点问题，一定程度上提高了稳定性 可拓展性不强，随着业务规模的不断扩大， 集群也会随之增加，但是新主节点的加入较为麻烦，需要人工操作   号段模式 不强依赖数据库，提高了效率 当为了保证高可用而使用多主集群时，仍然需要去修改起始值和步长   雪花算法 1.可以根据业务特性自由分配比特位，较为灵活。2.不依赖第三方系统，独立部署 强依赖机器时钟，如果出现时钟回拨则会导致系统不可用   Redis 实现起来简单且高效 持久化恢复存在问题，如RDB重复ID，AOF速度慢    ","date":"2022-05-24T15:50:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8Fid/","title":"分布式ID"},{"content":"分布式事务 分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于分布式系统的不同节点之上。\n简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性\n2PC（二阶段提交协议） 在分布式系统中，每一个机器节点虽然都能够明确地知道自己在进行事务操作过程中的结果是成功或失败，但是却无法直接获取到其他分布式节点的操作结果。\n为了保证事务处理的ACID特性，就需要引入一个称为协调者的组件来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点则被称为参与者。协调者负责调度参与者的行为，最终决定这些参与者是否要把事务真正进行提交，基于这个思想，衍生出了二阶段提交和三阶段提交两种协议。\n2PC是Two-Phase Commit的缩写，即二阶段提交，是为了使基于分布式系统架构下的所有节点在进行事务处理过程中能够保持原子性和一致性而设计的一种算法。目前绝大多数关系型数据库都是使用二阶段提交协议来完成分布式事务处理的。\n执行流程 顾名思义，二阶段提交协议就是将事务的提交过程分成了两个阶段来进行处理，在第一阶段的主要内容就是进行投票，来表明是否有继续执行事务的必要。\n阶段一：提交事务请求（投票阶段）\n 事务询问：协调者向所有参与者发出事务询问，询问是否可以执行事务操作，并等待各参与者的响应。 执行事务：各个参与者节点执行事务操作，并将undo和redo信息记入事务日志中。 各参与者向协调者反馈事务询问的响应：如果参与者成功执行事务，则反馈YES响应，表示事务执行成功。如果参与者执行事务失败，则反馈NO响应，表示事务执行失败。  在第二阶段中，会根据第一阶段参与者的反馈来决定是否能够提交事务，要么全都成功，要么全都失败。\n阶段二：执行事务提交（执行阶段）\n 执行事务提交  发送提交请求：协调者向所有参与者发起提交请求。 事务提交：参与者在收到提交请求后，会正式执行事务提交操作。 反馈事务提交结果：参与者在完成事务提交之后向协调者发送ACK消息。 完成事务：协调者接收到所有参与者的ACK后完成事务。   中断事务  发送回滚请求：协调者向所有参与者发起回滚请求。 事务回滚：参与者在收到回滚请求后，利用阶段一记录的undo信息来执行事务回滚操作。 反馈事务回滚结果：参与者在完成事务回滚之后向协调者发送ACK消息。 中断事务：协调者接收到所有参与者的ACK后完成事务中断。    优缺点  优点  原理简单 实现方便   缺点  同步阻塞：在执行过程中，所有参与该事务操作的逻辑都会处于阻塞状态，也就是说各个参与者在等待其他参与者响应的过程中将无法执行其他操作。 单点问题：在上述过程中，协调者起到了核心的调度作用。一旦协调者出现了问题，那么整个提交流程将无法运转，甚至如果在二阶段的提交流程中出现了问题，将导致其他参与者都处于锁定事务资源的状态中，无法完成事务。 数据不一致：倘若在第二阶段的提交过程中，协调者向参与者发送提交请求，而由于网络原因或者是协调者本身的原因，导致只有部分参与者收到了提交请求，此时就导致了只有接收到请求的参与者进行了事务提交，而产生数据不一致的问题。 过于保守：二阶段提交协议没有设计较为完整的容错机制，任意一个节点的失败都会导致整个事务的失败    二阶段提交协议存在着上述几种缺陷，因此研究者在二阶段协议的基础上进行了改进\n3PC（三阶段提交协议） 3PC是Three-Phase Commit的缩写，即三阶段提交，是2PC的改进版，其将二阶段提交协议的提交事务请求过程一分为二，形成了由CanCommit、PreCommit、DoCommint三个阶段组成的事务处理协议，其协议设计如下图所示 执行流程 阶段一：CanCommit\n 事务询问 各参与者向协调者反馈事务询问的响应  阶段二：PreCommit\n 执行事务预提交  发送预提交请求 事务预提交 各参与者向协调者反馈事务执行的结果   中断事务  发送中断请求 中断事务    阶段三：DoCommit\n 执行提交  发送提交请求 事务提交 反馈事务提交结果 完成事务   中断事务  发送中断请求 事务回滚 反馈事务回滚结果 中断事务    需要注意的是，在阶段三中可能会出现以下两种问题\n 协调者出现问题 协调者和参与者之间的网络出现故障  无论出现上述那种问题，最终都会导致参与者无法及时的接收到来自协调者的DoCommit或是Abort请求，针对这种异常情况，参与者都会在等待超时后继续进行事务提交。\n优缺点  优点：相较于二阶段提交协议，降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致。 缺点：三阶段提交协议在去除阻塞的同时也引入了新的问题，那就是参与者在收到预提交的消息时，如果出现了网络分区的情况，协调者与参与者无法进行正常的网络通信，但是参与者依旧会进行事务的提交，从而导致数据的不一致。  本地消息表（异步确保） 本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。\n 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。 之后将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。  ","date":"2022-05-24T15:48:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","title":"分布式事务"},{"content":"分布式时钟 逻辑时钟与物理时钟 对于分布式系统来说，时钟分为逻辑时钟与物理时钟两种。物理时钟对应的是我们真实世界的时间，一般由操作系统提供，而逻辑时钟则一般被实现为一个单调递增的计数器。\n 为什么在分布式系统中不直接使用物理时钟，而是使用逻辑时钟呢？\n 对于分布式系统而言，即使我们可以使用一些工具去同步集群内的时间，但是使所有节点的时间保持一致这个目标依旧是很难达成的。而如果我们使用不同节点产生的不一致的物理时间来进行一致性计算，就会导致结果出现很大的偏差，因此分布式系统就通过另外的方法来记录事件的顺序关系，也就是上面提到的逻辑时间。\n如何实现逻辑时钟? Lamport timestamps  Leslie Lamport 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为 Lamport 时间戳（Lamport timestamps）。\n 分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport 时间戳原理如下图：\n 每个事件对应一个 Lamport 时间戳，初始值为0 如果事件在节点内发生，时间戳加1 如果事件属于发送事件，时间戳加1并在消息中带上该时间戳 如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1  假设有事件 a、b，C(a)、C(b )分别表示事件 a、b 对应的 Lamport 时间戳，如果 a 发生在b 之前，记作 a⇒b，则有C(a)\u0026lt;C(b)，例如图1中有 C1→B1，那么 C(C1)\u0026lt;C(B1)。通过该定义，事件集中 Lamport 时间戳不等的事件可进行比较，我们获得事件的偏序关系(partial order)。注意：如果C(a)\u0026lt;C(b)，并不能说明a⇒b，也就是说C(a)\u0026lt;C(b是a⇒b的必要不充分条件\n如果 C(a) = C(b)，那a、b事件的顺序又是怎样的？值得注意的是当 C(a) = C(b)的时候，它们肯定没有因果关系，所以它们之间的先后顺序其实并不会影响结果，我们这里只需要给出一种确定的方式来定义它们之间的先后就能得到全序关系。注意：Lamport逻辑时钟只保证因果关系（偏序）的正确性，不保证绝对时序的正确性。\n通过以上定义，我们可以对所有事件排序，获得事件的全序关系。以上图例子，我们可以进行排序： C1 ⇒ B1 ⇒ B2 ⇒ A1 ⇒ B3 ⇒ A2 ⇒ C2 ⇒ B4 ⇒ C3 ⇒ A3 ⇒ B5 ⇒ C4 ⇒ C5 ⇒ A4\n观察上面的全序关系你可以发现，从时间轴来看 B5 是早于 A3 发生的，但是在全序关系里面我们根据上面的定义给出的却是 A3 早于 B5，可以发现 Lamport 逻辑时钟是一个正确的算法，即有因果关系的事件时序不会错，但并不是一个公平的算法，即没有因果关系的事件时序不一定符合实际情况。\nVector clock 因此 Vector clock 就在 Lamport timestamps 的基础上加以改进，它通过 vector 结构记录了本节点与其他节点的 Lamport timestamps。其原理如下图\n从上图可以看出，Vector clock 与 Lamport timestamps 的规则几乎一模一样。\n假设有事件 a、b 分别在节点 P、Q 上发生，Vector clock 分别为 Ta、Tb，如果 Tb[Q] \u0026gt; Ta[Q] 并且 Tb[P] \u0026gt;= Ta[P]，则a发生于b之前，记作a⇒b。到目前为止还和 Lamport timestamps 差别不大，那 Vector clock 怎么判别同时发生关系呢？\n如果 Tb[Q] \u0026gt; Ta[Q] 并且 Tb[P] \u0026lt; Ta[P]，则认为 a、b 同时发生，记作 a \u0026lt;=\u0026gt; b。例如图 2 中节点 B 上的第4 个事件 (A:2，B:4，C:1) 与节点 C 上的第 2 个事件 (B:3，C:2) 没有因果关系、属于同时发生事件。\nVersion vector 基于 Vector clock 我们可以获得任意两个事件的顺序关系，或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突。这时，就诞生了 Version vector。\n分布式系统中为了保证可用性，数据往往存在多个副本，而这多个副本又可能会被同时更新，这就会导致副本之间产生数据不一致的情况。Version vector 的目的就是为了发现这些数据冲突，其实现与 Vector clock类似，下图则是其具体运作流程。\n client端写入数据，该请求被Sx处理并创建相应的 vector ([Sx, 1])，记为数据 D1 第 2 次请求也被 Sx 处理，数据修改为 D2，vector 修改为 ([Sx, 2]) 第 3、第 4 次请求分别被 Sy、Sz 处理，client 端先读取到 D2，然后 D3、D4 被写入 Sy、Sz 第 5 次更新时 client 端读取到 D2、D3和D4 3个数据版本，通过类似 Vector clock 判断同时发生关系的方法可判断 D3、D4 存在数据冲突，最终通过一定方法解决数据冲突并写入 D5  Vector clock只用于发现数据冲突，不能解决数据冲突。解决数据冲突因场景而异，具体方法有以最后更新为准(last write win)，或将冲突的数据交给client由client端决定如何处理，或通过quorum决议事先避免数据冲突的情况发生。\n 由于记录了所有数据在所有节点上的逻辑时钟信息，Vector clock 和 Version vector 在实际应用中可能面临的一个问题是 vector 过大，用于数据管理的元数据甚至大于数据本身。\n 解决该问题的方法是使用 server id 取代 client id 创建 vector (因为 server 的数量相对 client 稳定)，或设定最大的 size、如果超过该 size 值则淘汰最旧的 vector 信息。\n","date":"2022-05-24T15:45:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%B6%E9%92%9F/","title":"分布式时钟"},{"content":"分布式基础理论：CAP与BASE CAP 一个分布式系统不可能同时满足一致性（Consistency）、可用性（Availability）、分区容错性（Partition Tolerance）这三个基本需求，最多只能满足其中的两项，不可能三者兼顾。 一致性：在分布式系统中的所有数据副本，在同一时刻是否一致（等同于所有节点访问同一份最新的数据副本） 可用性：分布式系统在面对各种异常时可以提供正常服务的能力（非故障的节点在有限的时间内返回合理的响应） 分区容错性：分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。  如下图\n 为什么三者不可兼顾呢？\n 我们首先就需要了解以下网络分区的概念。\n在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而就导致了整个系统的网络环境被切分成了若干个孤立的区域。\n节点被划分为多个区域后，每个区域内部可以通信，但是区域之间无法通信。\n对于一个分布式系统而言，我们的组件必然要被部署到不同的节点上，也必然会出现子网络。我们无法保证网络始终可靠，那么网络分区则是一个必定会产生的异常情况。\n当发生网络分区的时候，如果我们要继续提供服务，那么分区容错性也就是我们必然需要面对和解决的问题，因此分区容错性P是必定要满足的。 那么一致性C和可用性A可以兼顾吗？\n 答案必定是否定的，为什么呢？倘若分布式系统中出现了网络分区的情况，此时某一个节点在进行写操作，为了保证一致性，那么就必须要禁止其他节点的读写操作以防止数据冲突，而此时就导致其他的节点无法正常工作，即与可用性发生冲突。而如果让其他节点都正常进行读写操作的话，那就无法保证数据的一致，影响了数据的一致性，因此，我们只能满足可用性A和一致性C二者其一用一句话总结就是，CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性A或者一致性C。 因此在上面我给出的图中，CA是不可能的选项，在实际场景中，我们会根据具体的需求来选择CP和AP。\nBASE BASE理论是基本可用（Basically Available） 、软状态（Soft-state）和最终一致性（Eventually Consistent） 三个短语的缩写。其是对CAP中的一致性和可用性进行一个权衡的结果。\n 基本可用（Basically Available）：基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性（响应时间上的损失、系统功能上的损失）。但是，这绝不等价于系统不可用。 软状态（Soft-state）：允许系统中的数据存在中间状态（CAP理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性（Eventually Consistent）：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。  BASE理论的核心思想就是我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。什么意思呢？其实就是要牺牲数据的一致性（不保证强一致性，只保证最终一致性）来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。\n总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID特性使相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。\n","date":"2022-05-24T15:43:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BAcap%E4%B8%8Ebase/","title":"分布式基础理论：CAP与BASE"},{"content":"状态一致性 什么是状态一致性   有状态的流处理，内部每个算子任务都可以有自己的状态。\n  对于流处理器内部（没有接入sink）来说，所谓的状态一致性，其实就是我们所说的计算结果要保证准确，一条数据不应该丢失，也不应该重复计算。\n  在遇到故障时可以恢复状态，恢复以后的重新计算，结果应该也是完全正常的。\n  状态一致性种类   最多一次（At-Most-Once）\n  任务发生故障时最简单的措施就是既不恢复丢失的状态，也不重放丢失的事件，所以至多一次是最简单的一种情况。\n  它保证了每个事件至多被处理一次。\n    至少一次（At-Least-Once）\n 对于大多数现实应用而言，用户的期望是不丢事件，这类保障被称为至少一次。 它意味着所有事件最终都会处理，虽然有些可能会处理多次。    精确一次（Exactly-Once）\n 精确一次是最严格，最难实现的一类保障。 它不但能够保证事件没有丢失，而且每个事件对于内部状态的更新都只有一次。 Flink利用Checkpoints机制来保证精确一次语义。    端到端（end-to-end）状态一致性 端到端的保障指的是在整个数据处理管道上结果都是正确的。在每个组件都提供自身的保障情况下，整个处理管道上端到端的保障会受制于保障最弱的那个组件。\n 那么端到端的精确一次在各部分又是如何实现的呢？\n  内部：Checkpoints机制，在发生故障的时候能够恢复各个环节的数据。 Source：可设置数据读取的偏移量，当发生故障的时候重置偏移量到故障之前的位置。 Sink：从故障恢复时，数据不会重复写入外部系统。  其中前两种在上文已经介绍过了，下面就介绍一下Sink如何提供端到端的精确一次性保障。\nSink端到端状态一致性的保证 应用若是想提供端到端的精确一次性保障，就需要一些特殊的Sink连接器，根据情况不同，这些连接器可以使用两种技术来实现精确一次保障：\n  幂等性写（idempotent write）\n- 幂等操作的含义就是可以多次执行，但是只会引起一次改变。 - 例如我们将相同的键值对插入一个哈希结构中就是一个幂等操作， 因为由于该键值对已存在后，无论插入多少次都不会改变结果。 - 由于可以在不改变结果的前提下多次执行，因此幂等性写操作在一定程度上减轻Flink检查点机制所带来的重复结果的影响\n  事务性写（transactional write）\n 事务性写其实就是原子性写，即只有在上次成功的检查点之前计算的结果才会被写入外部Sink系统。 事务性写虽然不会像幂等性写那样出现重放过程中的不一致现象，但是会增加一定延迟，因为结果只有在检查点完成后才对外可见。 实现思想：构建的事务对应着Checkpoints，待Checkpoints真正完成的时候，才把所有对应的结果写入Sink系统中。 实现方式：  预写日志（Write Ahead Log，WAL） 两阶段提交（Two Phase Commit，2PC）      预写日志 把结果数据先当成状态保存，然后在收到Checkpoints完成的通知时，一次性写入Sink系统。 简单易于实现，由于数据提前在状态后端做了缓存，所以无论什么Sink系统都能用这种方式一批搞定。 但同时它也存在问题，写入数据时出现故障则会导致一部分数据成功一部分失败。 DataStream API提供了一个模板类GenericWriteAheadSink，来实现这种事务性Sink。  两阶段提交 对于每个Checkpoints，Sink任务会启动一个事务，并将接下来所有接收的数据添加到事务里。 然后将这些数据写入外部 Sink，但不提交它们，这时只是“预提交”。 当它收到Checkpoints完成的通知时，它才正式提交事务，实现结果的真正写入。 这种方式真正实现了精确一次，它需要一个提供事务支持的外部Sink系统，Flink提供了TwoPhaseCommitSinkFunction接口。 对外部Sink系统的要求  外部Sink系统必须提供事务支持，或者Sink任务必须能够模拟外部系统上的事务。 在Checkpoints的隔离期间里，必须能够开启一个事务并接受数据写入。 在收到Checkpoints完成的通知之前，事务必须是“等待提交”的状态。在故障恢复的情况下，这可能需要一些时间。如果这个时候 Sink系统关闭事务（例如超时了），那么未提交的数据就会丢失。 Sink任务必须能够在进程失败后恢复事务。 提交事务必须是幂等操作。    Flink+Kafka端到端状态一致性的保证   内部：利用Checkpoints机制把状态保存，当发生故障的时候可以恢复状态，从而保证内部的状态一致性。\n  source 端：Kafka Consumer作为Source，可以将偏移量保存下来，当发生故障时可以从发生故障前的偏移量重新消费数据，从而保证一致性。\n  sink端：Kafka Producer作为Sink，采用两阶段提交Sink，需要实现一个TwoPhaseCOmmitSinkFunction。\n  ","date":"2022-05-24T14:40:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/","title":"Flink 状态一致性"},{"content":"Flink 容错机制 Checkpoints（检查点） Flink中基于异步轻量级的分布式快照技术提供了Checkpoints容错机制，Checkpoints可以将同一时间点作业/算子的状态数据全局统一快照处理，包括前面提到的算子状态和键值分区状态。当发生了故障后，Flink会将所有任务的状态恢复至最后一次Checkpoint中的状态，并从那里重新开始执行。\n 那么Checkpoints的生成策略是什么样的呢？它会在什么时候进行快照的生成呢？\n 其实就是在所有任务都处理完同一个输入数据流的时候，这时就会对当前全部任务的状态进行一个拷贝，生成Checkpoints。\n为了方便理解，这里先简单的用一个朴素算法来解释这一生成过程（Flink的Checkpoints算法实际要更加复杂，在下面会详细讲解）\n 暂停接受所有输入流。 等待已经流入系统的数据被完全处理，即所有任务已经处理完所有的输入数据。 将所有任务的状态拷贝到远程持久化，生成Checkpoints。在所有任务完成自己的拷贝工作后，Checkpoints生成完毕。 恢复所有数据流的接收  恢复流程 为了方便进行实例的讲解，假设当前有一个Source任务，负责从一个递增的数字流（1、2、3、4……）中读取数据，读取到的数据会分为奇数流和偶数流，求和算子的两个任务会分别对它们进行求和。在当前任务中，数据源算子的任务会将输入流的当前偏移量存为状态，求和算子的任务会将当前和存为状态。\n如上图，在当前生成的Checkpoints中保存的输入偏移为5，偶数求和为6，奇数求和为9。\n假设在下一轮计算中，任务sum_odd计算出现了问题，任务sum_odd的时候产生了问题，导致结果出现错误。由于出现问题，为了防止从头开始重复计算，此时会通过Checkpoints来进行快照的恢复。\nCheckpoints恢复应用需要以下三个步骤\n 重启整个应用 利用最新的检查点重置任务状态 恢复所有任务的处理   第一步我们需要先重启整个应用，恢复到最原始的状态。   紧接着从检查点的快照信息中读取出输入源的偏移量以及算子计算的结果，进行状态的恢复   状态恢复完成后，继续Checkpoints恢复的位置开始继续处理。  从检查点恢复后，它的内部状态会和生成检查点的时候完全一致，并且会紧接着重新处理那些从之前检查点完成开始，到发生系统故障之间已经处理过的数据。虽然这意味着Flink会重复处理部分消息，但上述机制仍然可以实现精确一次的状态一致性，因为所有的算子都会恢复到那些数据处理之前的时间点。\n但这个机制仍然面临一些问题，因为Checkpoints和恢复机制仅能重置应用内部的状态，而应用所使用的Sink可能在恢复期间将结果向下游系统（如事件日志系统、文件系统或数据库）重复发送多次。为了解决这个问题，对于某些存储系统，Flink提供的Sink函数支持精确一次输出 （在检查点完成后才会把写出的记录正式提交）。另一种方法则是适用于大多数存储系统的幂等更新。\n生成策略 Flink中的Checkpoints是基于Chandy-Lamport分布式快照算法 实现的，该算法不会暂停整个应用，而是会将生成Checkpoints的过程和处理过程分离，这样在部分任务持久化状态的过程中，其他任务还可以继续执行。\n在介绍生成策略之前，首先需要介绍一下Checkpoints barrier（屏障） 这一种特殊记录。\n如上图，与水位线相同，Flink会在Source中间隔性地生成barrier，通过barrier把一条流上的数据划分到不同的Checkpoints中，在barrier之前到来的数据导致的状态更改，都会被包含在当前所属的Checkpoints中；而基于barrier之后的数据导致的所有更改，就会被包含在之后的Checkpoints中。\n 假设当前有两个Source任务，各自消费一个递增的数字流（1、2、3、4……），读取到的数据会分为奇数流和偶数流，求和算子的两个任务会分别对它们进行求和，并将结果值更新至下游Sink。   此时JobManager向每一个Source任务发送一个新的Checkpoints编号，以此启动Checkpoints生成流程。   在Source任务收到消息后，会暂停发出记录，紧接着利用状态后端生成本地状态的Checkpoints，并把barrier连同编号广播给所有传出的数据流分区。 状态后端在状态存入Checkpoints后通知Source任务，并向JobManager发送确认消息。 在所有barrier发出后，Source将恢复正常工作。   Source任务会广播barrier至所有与之相连的任务，确保这些任务能从它们的每个输入都收到一个barrier 在等待过程中，对于barrier未到达的分区，数据会继续正常处理。而barrier已经到达的分区，它们新到来的记录会被缓冲起来，不能处理。这个等待所有barrier到来的过程被称为barrier对齐   任务中收齐全部输入分区发送的barrier后，就会通知状态后端开始生成Checkpoints，同时继续把Checkpoints barrier广播转发到下游相连的任务。   任务在发出所有的Checkpoints barrier后就会开始处理缓冲的记录。等到所有缓冲记录处理完后，任务就会继续处理Source。   Sink任务在收到分隔符后会依次进行barrier对齐，然后将自身状态写入Checkpoints，最终向JobManager发送确认信息。 JobManager在接收到所有任务返回的Checkpoints确认信息后，就说明此次Checkpoints生成结束。  Savepoints（保存点）  由于Cheakpoints是周期性自动生成的，但有些时候我们需要手动的去进行镜像保存功能，于是Flink同时还为我们提供了Savepoints来完成这个功能，Savepoints不仅可以做到故障恢复，还可以用于手动备份、版本迁移、暂停或重启应用等。 Savepoints是Checkpoints的一种特殊实现，底层也是使用Checkpoint机制，因此Savepoints可以认为是具有一些额外元数据的Checkpoints。 Savepoints的生成和清理都无法由Flink自动进行，因此都需要用户自己来显式触发。  ","date":"2022-05-24T14:38:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/","title":"Flink 容错机制"},{"content":"Flink 状态管理 通常意义上，函数里所有需要任务去维护并用来计算结果的数据都属于任务的状态，可以把状态想象成任务的业务逻辑所需要访问的本地或实例变量。\n如上图，任务首先会接受一些输入数据。在处理这些数据的过程中，任务对其状态进行读取或更新，并根据状态的输入数据计算结果。我们以一个持续计算接收到多少条记录的简单任务为例。当任务收到一个新的记录后，首先会访问状态获取当前统计的记录数目，然后把数目增加并更新状态，最后将更新后的状态数目发送出去。\nFlink会负责进行状态的管理，包括状态一致性、故障处理以及高效存取相关的问题都由Flink负责搞定，这样开发人员就可以专注于自己的应用逻辑。\n在Flink中，状态都是和特定operator（算子）相关联，为了让Flink的Runtime（运行）层知道算子有哪些状态，算子需要自己对其进行注册。根据作用域的不同，状态可以分为以下两类\n operator state（算子状态） keyed state（键值分区状态）  算子状态 算子状态的作用域是某个算子任务，这意味着所有在同一个并行任务之内的记录都能访问到相同的状态**（每一个并行的子任务都共享一个状态）。算子状态不能通过其他任务访问，无论该任务是否来自相同算子（相同算子的不同任务之间也不能访问）**。\nFlink为算子状态提供了三种数据结构\n 列表状态（list state）：将状态表示为一组数据的列表。（每一个并行的子任务共享一个状态） 联合列表状态（union list state）：同样将状态表示为数据的列表，但在进行故障恢复或者从某个保存点（savepoint）启动应用的时候，状态恢复的方式和普通的列表状态有所不同。（把之前的每一个状态广播到对应的每一个算子中） 广播状态（broadcast state）：专门为那些需要保证算子的每个任务状态都相同的场景而设计。（把同一个状态广播给所有算子子任务）  键值分区状态 键值分区状态会按照算子输入记录所定义的键值来进行维护或访问。Flink为每个键值都维护了一个状态实例，该实例总是位于那个处理对应键值记录的算子任务上。当任务在处理一个记录时，会自动把状态的访问范围限制为当前记录的键值，因此所有键值相同的记录都能访问到一样的状态。\nFlink为键值分区状态提供以下几种数据结构\n 单值状态（value state）：每个键对应存储一个任意类型的值。 列表状态（list state）：每个键对应存储一个值的列表。 映射状态（map state）：每个键对应存储一个键值映射。 聚合状态（Reducing state \u0026amp; Aggregating State）：每个键对应存储一个用于聚合操作的列表  状态后端（State Backends） 有状态算子的任务通常会对每一条到来的记录读写状态，因此高效的状态访问对于记录处理的低延迟而言至关重要。为了保证快速访问状态，每个并行任务都会把状态维护在本地。至于状态具体的存储、访问和维护，则是由一个名为状态后端的可拔插（pluggable） 组件来决定。状态后端主要负责两件事情：本地状态管理和将状态以检查点的形式写入远程存储。\n目前，Flink提供了三种状态后端，状态后端的选择会影响有状态应用的鲁棒性及性能。\n  MemoryStateBackend\n MemoryStateBackend将状态以常规对象的方式存储在TaskManager进程的JVM堆，并在生成Checkpoints时会将状态发送至JobManager并保存到它的堆内存中。 如果状态过大，则可能导致JVM上的任务由于OutOfMemoryError而终止，并且可能由于堆中放置了过多常驻内存的对象而引发垃圾回收停顿问题。 由于内存具有易失性，所以一旦JobManager出现故障就会导致状态丢失，因此MemoryStateBackend通常用于开发和调试。 内存访问速度快，延迟低，但容错性也低。    FsStateBackend\n 与MemoryStateBackend一样将本地状态存储在TaskManager进程的JVM堆里，不同的是将Checkpoints存到了远程持久化文件系统（FileSystem）中。 受到TaskManager内存大小的限制，并且也可能导致垃圾回收停顿问题。 FsStateBackend既让本地访问享有内存的速度，又可以支持故障容错。    RocksDBStateBackend\n  RocksDBStateBackend会将全部状态序列化后存到本地RocksDB实例中\n  由于磁盘I/O以及序列化/反序列化对象的性能开销，相较于内存中维护状态而言， 读写性能会偏低。\n  RocksDB的支持并不直接包含在Flink中，需要额外引入依赖\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-statebackend-rocksdb_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;       有状态算子的扩缩容 流式应用的一项基本需求是根据输入数据到达速率的变化调整算子的并行度。对于无状态的算子扩缩容很容易，但是对于有状态算子来说，这就变的复杂了很多。因为我们需要把状态重新分组，分配到与之前数量不等的并行任务上。\n针对不同类型状态的算子，Flink提供了四种扩缩容模式\n 键值分区状态 算子列表状态 算子联合列表状态 算子广播状态  键值分区状态 带有键值分区状态的算子在扩缩容时会根据新的任务数量对键值重新分区，但为了降低状态在不同任务之间迁移的必要成本，Flink不会对单独的键值实施再分配，而是会把所有键值分为不同的键值组（Key group）。每个键值组都包含了部分键值，Flink以此为单位把键值分配给不同任务。\n算子列表状态 带有算子列表状态的算子在扩缩容时会对列表中的条目进行重新分配。理论上，所有并行算子任务的列表条目会被统一收集起来，随后均匀分配到更少或更多的任务之上。如果列表条目的数量小于算子新设置的并行度，部分任务在启动时的状态就可能为空。\n算子联合列表状态 带有算子联合列表状态的算子会在扩缩容时把状态列表的全部条目广播到全部任务上，随后由任务自己决定哪些条目应该保留，哪些应该丢弃。\n算子广播状态 带有算子广播状态的算子在扩缩容时会把状态拷贝到全部新任务上，这样做的原因是广播状态能确保所有任务的状态相同。在缩容的情况下，由于状态经过复制不会丢失，我们可以简单的停掉多出的任务。\n","date":"2022-05-24T14:35:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/","title":"Flink 状态管理"},{"content":"Flink 流处理 Dataflow编程 顾名思义，Dataflow程序描述了数据如何在不同操作之间流动。Dataflow程序通常表现为有向无环图（DAG），图中顶点称为算子（Operator），表示计算。而边表示数据依赖关系。\n算子是Dataflow程序的基本功能单元，他们从输入获取数据，对其进行计算，然后产生数据并发往输出以供后续处理。而所有Flink程序都由三部分算子组成。\n Source（数据源）：负责获取输入数据。 Transformation（数据处理）：对数据进行处理加工，通常对应着多个算子。 Sink（数据汇）：负责输出数据。  执行图 类似上图的Dataflow图被称为逻辑图，因为它们表达了高层视角下的计算逻辑。为了执行Dataflow程序，需要将逻辑图转化为物理Dataflow图（执行图），后者会指定程序的执行细节。\n在Flink中，执行图按层级顺序分为以下四层\n StreamingGraph  是根据用户通过Stream API编写的代码生成的初始流程图，用于表示程序的拓扑结构。   JobGraph  StreamGraph经过优化后生成了JobGraph，提交给JobManager的数据结构。主要的优化为将多个符合条件的节点链接在一起作为一个节点（任务链Operator Chains）后放在一个作业中执行，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。   ExecutionGraph  JobManager根据JobGraph生成ExecutionGraph，ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。   物理执行图  JobManager根据ExecutionGraph对任务进行调度后，在各个TaskManager上部署作业后形成的“图”，并不是一个具体的数据结构。    并行度 Flink程序的执行具有并行、分布式的特性。\n在执行过程中，一个Stream包含一个或多个分区（partition），而每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务中不同的线程、不同的物理机或不同的容器中彼此互不依赖地执行。\n一个特定算子的子任务的个数被称之为并行度（paralelism）。一般情况下一个流程序的并行度可以认为就是其所有算子中最大的并行度，一个程序中不同的算子可以具有不同的并行度。\n数据传输策略 Stream在算子之间传输数据的形式可以是one-to-one（forwarding）的模式也可以是Redistributing的模式，具体是哪一种需要取决于算子的种类。\n One-to-one  Stream维护着分区以及元素的顺序（比如在Source和map operator之间），那意味着map算子的子任务看到的元素的个数以及顺序跟Source算子的子任务生产的元素的个数、顺序相同，map、filter、flatmap等算子都是one-to-one的对应关系。 类似于Spark中的窄依赖。   Redistributing  Stream的分区会发生改变（map()跟keyBy/window之间或者keyBy/windows跟Sink之间）。每一个算子的子任务依据所选择的Transformation发送数据到不同的目标任务。 例如keyBy()基于hashCode重分区、broadcast和rebalance会随机重新分区，这些算子都会引起redistribute过程，而redistribute过程就类似于Spark中的shuffle过程。 类似于Spark中的宽依赖。    任务链 Flink 采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过本地转发（local forward） 的方式进行连接。\n相同并行度的 one-to-one 操作 （两个条件缺一不可），Flink 这样相连的算子链接在一起形成一个 task，原来的算子成为里面的 subtask。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。\n时间语义 对于流式数据处理，最大的特点就是数据上具有时间的属性特征，Flink根据时间产生的位置不同，将时间区分为如下三种时间概念\n 事件时间（Event Time）：数据流事件实际发生的时间。 接入时间（Ingestion Time）：数据进入Flink系统的时间。 处理时间（Processing Time）：当前流处理算子所在机器上的本地时钟时间。  Flink中默认使用的是处理时间，但是在大多数情况下都会使用事件时间（即实际事件的发生点，也符合事件发生进而分析的逻辑），一般只有在Event Time无法使用的情况下才会使用接入时间和处理时间，因此我们可以通过调用执行环境的setStreamTimeCharacteristic方法来指定时间语义\n1 2 3 4 5  //创建执行环境\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置指定的时间语义，如下面的设置为EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);    处理时间与事件时间的选择\n 在大部分场景由于我们需要依据事件发生的顺序来进行逻辑处理，因此都会使用事件时间。但是在一些特殊场景下，考虑到事件数据数据乱序到达以及延迟到达等问题，为了保证实时性和低延迟，处理时间就会派上用场。\n例如下面几种场景：\n 更重视处理速度而非准确性的应用。 需要周期性实时报告结果而无论其准确性（如实时监控仪表盘）。  因此，对比处理时间和事件时间得出结论：\n 处理时间提供了低延迟，但是它的结果依赖处理速度，因此具有不确定性。 事件时间则与之相反，能够保证结果的准确性，并允许你处理延迟甚至无序的事件。  水位线（Watermarks） 在理想状态下，事件数据都是按照事件产生的时间顺序传输至Flink系统中。但事实上，由于网络或者分布式系统等外部因素的影响下，事件数据往往不能及时传输，导致系统的不稳定而造成数据乱序到达或者延迟到达等情况。\n一旦出现这种问题，如果我们严格按照Event Time来决定窗口的运行，我们既不能保证属于该窗口的数据已经全部到达，也不能无休止的等待延迟到达的数据，因此我们需要一种机制来控制数据处理的进度，这就是水位线（Watermarks）机制。\n水位线是一个全局的进度指标，它能够衡量数据处理进度==（表达数据到达的完整性）**，保证事件数据全部到达Flink系统，即使数据乱序或者延迟到达，也能够像预期一样计算出正确和连续的结果。\n 那么它是如何做到的呢？\n  Flink会使用最新的事件时间减去固定时间间隔作为水位线，该时间时间为用户外部配置的支持最大延迟到达的时间长度。 当一个算子接收到一个时间为T的水位线，就可以认为不会再收到任何时间戳小于或等于T的事件了（迟到事件或异常事件） 水位线其实就相当于一个提示算子的信号，当水位线时间戳大于时间窗口的结束时间，且窗口中含有事件数据时，此时算子就会认为某个特定时间区间的时间戳已经全部到齐，立即开始触发窗口计算或对接收的数据进行排序。  从上面我们可以看出，水位线其实就是在结果的准确性和延迟之间做出取舍，它虽然保证了低延迟，但是伴随而来的却是低可信度。倘若我们要保证后续的延迟事件不丢失，就必须额外增加一些代码来处理他们，但是如果采用这种保守的机制，虽然可信度低高了，但是延迟又会继续增加，因此延迟和可信无法做到两全其美，需要我们依据具体场景来自己平衡。\n","date":"2022-05-24T14:29:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E6%B5%81%E5%A4%84%E7%90%86/","title":"Flink 流处理"},{"content":"Flink 架构 架构体系 在Flink整个软件架构体系中，遵循了分层的架构设计理念，在降低系统耦合度的同时也为上层用户构建Flink应用提供了丰富且友好的借口。从上图可以看出Flink的架构体系基本上可以分为以下三层\n API \u0026amp; Libraries层 Runtime核心层 物理部署层  API \u0026amp; Libraries层 作为分布式数据处理框架，Flink同时提供了支持流计算和批计算的借口，同时在此基础之上抽象出不同的应用类型的组件库，如基于流处理的CEP（复杂事件处理库）、SQL\u0026amp;Table库和基于批处理的FlinkML（机器学习库）、Gelly（图处理库）等。\nAPI层包括构建流计算应用的DataStream API和批计算应用的DataSet API，两者都提供给用户丰富的数据处理高级API，例如Map、FlatMap操作等，同时也提供比较低级的Process Function API，用户可以直接操作状态和时间等底层数据。\nRuntime核心层 该层主要负责对上层不同接口提供基础服务，也是Flink分布式计算框架的核心实现层，支持分布式Stream作业的执行、JobGraph到ExecutionGraph的映射转换、任务调度等。\n物理部署层 该层主要涉及Flink的部署模式，目前Flink支持多种部署模式：本地、集群（Standalone/YARN）、云（GCE/EC2）、Kubenetes。Flink能够通过该层能够支持不同的部署，用户可以根据需要选择使用对应的部署模式\n运行时组件 Flink系统主要由以下四个组件组成\n JobManager（任务管理器） TaskManager（作业管理器） ResourceManger（资源管理器） Dispatcher（分发器）  Flink本身是用Java和Scala实现的，因此所有组件都基于JVM（Java虚拟机） 运行。\nJobManager Flink遵循Master-Slave（主从）架构设计原则，JobManager为Master节点，TaskManager为Slave节点，并且所有组件之间的通信都借助Akka，包括任务的状态以及CheckPoint（检查点）触发等信息。\n 作为主进程（Master Process），JobManager控制着单个应用程序的执行，也就是每个应用都由一个不同的JobManager管理。 JobManager可以接受需要执行的应用，该应用会包含一个所谓的Job Graph（任务图），即逻辑Dataflow Graph（数据流图），以及一个打包了全部所需类、库以及其他资源的JAR文件。 JobManager将JobGraph转化为名为Execution Graph（执行图）的物理Dataflow Graph，其中包含了所有可以并发实行的任务。 JobManager会从ResourceManager申请执行任务的必要资源——TaskManager slot，一旦它收到了足够数量的TaskManager slot，它就会将Execution Graph中的任务分发给TaskManager来执行。在执行过程中，JobManager还要负责所有需要集中协调的操作，如创建CheakPoint等。  TaskManager  TaskManager是Flink的工作进程（Worker Process），在Flink的搭建过程中要启动多个TaskManager。每个TaskManager提供一定数量的slot（处理槽），slot的数量限制了TaskManager可执行的任务数。 TaskManager在启动之后会向ResourceManager注册它的slot，当接收到ResourceManager的指示时，TaskManager会向JobManager提供一个或者多个slot。之后JobManager就可以向slot中分配任务来执行。 在执行过程中，运行同一应用的不同任务的TaskManager之间会产生数据交换。  ResourceManger  Flink为不同的环境和资源提供者（如YARN、Kubernetes、Stand-alone）提供了不同的ResourceManger。 ResourceManger负责管理Flink的处理资源单元——TaskManager Slot。 当JobManager申请TaskManager slot时，ResourceManger会指示一个拥有空闲slot的TaskManager将其slot提供给JobManager。如果ResourceManger的slot数无法满足JobManager的请求，则ResourceManger可以与资源提供者通信，让他们提供额外的容器来启动更多的TaskManager进程。同时，ResourceManger还负责终止空闲进程的TaskManager以释放计算资源。  Dispatcher   Dispatcher在会跨多个作业运行，它提供了一个REST接口来让我们提交需要执行的应用，一旦某个应用提交执行，则Dispatcher会启动一个JobManager并将应用转交给它。\n  REST接口意味着Dispatcher这一集群的HTTP入口可以受到防火墙的保护。\n  Dispatcher同时还会启动一个Web UI，用来展示和监控有关作业执行的信息。\n  Dispatcher并不是必需的组件，某些应用提交执行的方式可能用不到Dispatcher。\n   ","date":"2022-05-24T14:23:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E6%9E%B6%E6%9E%84/","title":"Flink 架构"},{"content":"Flink介绍 概述 Apache Flink是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。\nApache Flink功能强大，支持开发和运行多种不同种类的应用程序。它的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。Flink不仅可以运行在包括 YARN、 Mesos、Kubernetes在内的多种资源管理框架上，还支持在裸机集群上独立部署。在启用高可用选项的情况下，它不存在单点失效问题。事实证明，Flink已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。世界各地有很多要求严苛的流处理应用都运行在Flink之上。\n接下来，我们来介绍一下Flink中的几个重要概念。\n批与流   批处理的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。\n  流处理的特点是无界、实时, 无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。\n  在Spark的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。而在Flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。\n 无界流：有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流：有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。  Flink 擅长处理无界和有界数据集，精确的时间控制和状态化使得Flink的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。\n部署应用到任何地方 Apache Flink是一个分布式系统，它需要计算资源来执行应用程序。Flink集成了所有常见的集群资源管理器，例如Hadoop YARN、 Apache Mesos和Kubernetes，但同时也可以作为独立集群运行。\nFlink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。\n部署Flink应用程序时，Flink会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。\n利用内存性能 有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。\n分层AP Flink 根据抽象程度分层，提供了三种不同的 API。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。\n ProcessFunction：可以处理一或两条输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的某一时刻触发回调函数。因此，你可以利用 ProcessFunction 实现许多有状态的事件驱动应用所需要的基于单个事件的复杂业务逻辑。 DataStream API：为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如map()、reduce()、aggregate() 等函数。你可以通过扩展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。 SQL \u0026amp; Table API：Flink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API和SQL借助了 Apache Calcite来进行查询的解析，校验以及优化。它们可以与DataStream和DataSet API无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。Flink 的关系型 API 旨在简化数据分析、数据流水线和 ETL 应用的定义。  特点 Apache Flink是一个集合众多具有竞争力特性于一身的第三代流处理引擎，它的以下特点使得它能够在同类系统中脱颖而出。\n 同时支持高吞吐、低延迟、高性能。  Flink是目前开源社区中唯一一套集高吞吐、低延迟、高性能三者于一身的分布式流式处理框架。像Apache Spark也只能兼顾高吞吐和高性能特性，主要因为在Spark Streaming流式计算中无法做到低延迟保障；而流式计算框架Apache Storm只能支持低延迟和高性能特性，但是无法满足高吞吐的要求。   同时支持事件时间和处理时间语义。  在流式计算领域中，窗口计算的地位举足轻重，但目前大多数框架窗口计算采用的都是处理时间，也就是事件传输到计算框架处理时系统主机的当前时间。Flink能够支持基于事件时间语义进行窗口计算，也就是使用事件产生的时间，这种基于事件驱动的机制使得事件即使乱序到达，流系统也能够计算出精确的结果，保证了事件原本的时序性。   支持有状态计算，并提供精确一次的状态一致性保障。  所谓状态就是在流式计算过程中将算子的中间结果数据保存着内存或者文件系统中，等下一个事件进入算子后可以从之前的状态中获取中间结果中计算当前的结果，从而不须每次都基于全部的原始数据来统计结果，这种方式极大地提升了系统的性能，并降低了数据计算过程的资源消耗。   基于轻量级分布式快照实现的容错机制。  Flink能够分布式运行在上千个节点上，将一个大型计算任务的流程拆解成小的计算过程，然后将Task分布到并行节点上进行处理。在任务执行过程中，能够自动发现事件处理过程中的错误而导致的数据不一致问题，在这种情况下，通过基于分布式快照技术的Checkpoints，将执行过程中的状态信息进行持久化存储，一旦任务出现异常终止，Flink就能够从Checkpoints中进行任务的自动恢复，以确保数据中处理过程中的一致性。   保证了高可用，动态扩展，实现7 * 24小时全天候运行。  支持高可用性配置（无单点失效），和Kubernetes、YARN、Apache Mesos紧密集成，快速故障恢复，动态扩缩容作业等。基于上述特点，它可以7 X 24小时运行流式应用，几乎无须停机。当需要动态更新或者快速恢复时，Flink通过Savepoints技术将任务执行的快照保存在存储介质上，当任务重启的时候可以直接从事先保存的Savepoints恢复原有的计算状态，使得任务继续按照停机之前的状态运行。   支持高度灵活的窗口操作。  Flink将窗口划分为基于Time、Count、Session，以及Data-driven等类型的窗口操作，窗口可以用灵活的触发条件定制化来达到对复杂流传输模式的支持，用户可以定义不同的窗口触发机制来满足不同的需求。    应用场景 在实际生产的过程中，大量数据在不断地产生，例如金融交易数据、互联网订单数据、GPS定位数据、传感器信号、移动终端产生的数据、通信信号数据等，以及我们熟悉的网络流量监控、服务器产生的日志数据，这些数据最大的共同点就是实时从不同的数据源中产生，然后再传输到下游的分析系统。\n针对这些数据类型主要包括以下场景，Flink对这些场景都有非常好的支持。\n  实时智能推荐\n 利用Flink流计算帮助用户构建更加实时的智能推荐系统，对用户行为指标进行实时计算，对模型进行实时更新，对用户指标进行实时预测，并将预测的信息推送给Web/App端，帮助用户获取想要的商品信息，另一方面也帮助企业提高销售额，创造更大的商业价值。    复杂事件处理\n 例如工业领域的复杂事件处理，这些业务类型的数据量非常大，且对数据的时效性要求较高。我们可以使用Flink提供的CEP（复杂事件处理）进行事件模式的抽取，同时应用Flink的SQL进行事件数据的转换，在流式系统中构建实时规则引擎。    实时欺诈检测\n 在金融领域的业务中，常常出现各种类型的欺诈行为。运用Flink流式计算技术能够在毫秒内就完成对欺诈判断行为指标的计算，然后实时对交易流水进行规则判断或者模型预测，这样一旦检测出交易中存在欺诈嫌疑，则直接对交易进行实时拦截，避免因为处理不及时而导致的经济损失    实时数仓与ETL\n 结合离线数仓，通过利用流计算等诸多优势和SQL灵活的加工能力，对流式数据进行实时清洗、归并、结构化处理，为离线数仓进行补充和优化。另一方面结合实时数据ETL处理能力，利用有状态流式计算技术，可以尽可能降低企业由于在离线数据计算过程中调度逻辑的复杂度，高效快速地处理企业需要的统计结果，帮助企业更好的应用实时数据所分析出来的结果。    流数据分析\n 实时计算各类数据指标，并利用实时结果及时调整在线系统相关策略，在各类投放、无线智能推送领域有大量的应用。流式计算技术将数据分析场景实时化，帮助企业做到实时化分析Web应用或者App应用的各种指标。    实时报表分析\n 实时报表分析说近年来很多公司采用的报表统计方案之一，其中最主要的应用便是实时大屏展示。利用流式计算实时得出的结果直接被推送到前段应用，实时显示出重要的指标变换，最典型的案例就是淘宝的双十一实时战报。    Flink VS Spark Streaming   数据模型\n  Flink基本数据模型是数据流，以及事件序列。\n  Spark采用RDD模型，Spark Streaming的DStream实际上也就是一组组小批\n数据RDD的集合。\n    运行时架构\n  Flink是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节\n点进行处理。\n  Spark是批计算，将DAG划分为不同的Stage，一个完成后才可以计算下一个。\n    ","date":"2022-05-24T14:22:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"Flink 基本概念"},{"content":"DBImpl模块 Open 数据库 Open 操作主要用于创建新的 LevelDB 数据库或打开一个已存在的数据库。Open 操作的主要函数共需传递 3 个参数：两个输入参数 options 与 dbname，一个输出参数 dbptr。\n首先我们来看看它的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DB::Open(const Options\u0026amp; options, const std::string\u0026amp; dbname, DB** dbptr) { *dbptr = nullptr; //初始化dbimpl  DBImpl* impl = new DBImpl(options, dbname); impl-\u0026gt;mutex_.Lock(); VersionEdit edit; //尝试恢复之前已经存在的数据库文件中的数据  bool save_manifest = false; Status s = impl-\u0026gt;Recover(\u0026amp;edit, \u0026amp;save_manifest); //判断Memtable是否为空  if (s.ok() \u0026amp;\u0026amp; impl-\u0026gt;mem_ == nullptr) { //创建新的Log和MemTable  uint64_t new_log_number = impl-\u0026gt;versions_-\u0026gt;NewFileNumber(); WritableFile* lfile; s = options.env-\u0026gt;NewWritableFile(LogFileName(dbname, new_log_number), \u0026amp;lfile); if (s.ok()) { edit.SetLogNumber(new_log_number); impl-\u0026gt;logfile_ = lfile; impl-\u0026gt;logfile_number_ = new_log_number; impl-\u0026gt;log_ = new log::Writer(lfile); impl-\u0026gt;mem_ = new MemTable(impl-\u0026gt;internal_comparator_); impl-\u0026gt;mem_-\u0026gt;Ref(); } } //判断是否需要保存Manifest文件  if (s.ok() \u0026amp;\u0026amp; save_manifest) { edit.SetPrevLogNumber(0); edit.SetLogNumber(impl-\u0026gt;logfile_number_); //生成新的版本  s = impl-\u0026gt;versions_-\u0026gt;LogAndApply(\u0026amp;edit, \u0026amp;impl-\u0026gt;mutex_); } if (s.ok()) { //请理无用的文件  impl-\u0026gt;RemoveObsoleteFiles(); //尝试进行Compaction  impl-\u0026gt;MaybeScheduleCompaction(); } impl-\u0026gt;mutex_.Unlock(); if (s.ok()) { assert(impl-\u0026gt;mem_ != nullptr); *dbptr = impl; } else { delete impl; } return s; }   具体的实现流程如下图所示：\n 初始化一个 DBImpl 的对象 impl，将相关的参数选项 options 与数据库名称 dbname 作为构造函数的参数。 调用 DBImpl 对象的 Recover 函数，尝试恢复之前存在的数据库文件数据。 进行 Recover 操作后，判断 impl 对象中的 MemTable 对象指针 mem_ 是否为空，如果为空，则进入第 4 步，不为空则进入第 5 步。 创建新的 Log 文件以及对应的 MemTable 对象。这一步主要分别实例化 log::Writer 和 MemTable 两个对象，并赋值给 impl 中对应的成员变量，后续通过 impl 中的成员变量操作 Log 文件和 MemTable。 判断是否需要保存 Manifest 相关信息，如果需要，则保存相关信息。 判断前面步骤是否都成功了，如果成功，则调用 DeleteObsoleteFiles 函数对一些过时文件进行删除，且调用 MaybeScheduleCompaction 函数尝试进行数据文件的 Compaction 操作。  Get Get 主要用于从 LevelDB 中获取对应的键-值对数据，它是单个数据读取的主要接口。Get 的主要参数为数据读参数选项 options、键 key，以及一个用于返回数据值的 string 类型指针 value。其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Get(const ReadOptions\u0026amp; options, const Slice\u0026amp; key, std::string* value) { Status s; MutexLock l(\u0026amp;mutex_); SequenceNumber snapshot; //获取序列号并赋值给snapshot  if (options.snapshot != nullptr) { snapshot = static_cast\u0026lt;const SnapshotImpl*\u0026gt;(options.snapshot)-\u0026gt;sequence_number(); } else { snapshot = versions_-\u0026gt;LastSequence(); } MemTable* mem = mem_; MemTable* imm = imm_; Version* current = versions_-\u0026gt;current(); mem-\u0026gt;Ref(); if (imm != nullptr) imm-\u0026gt;Ref(); current-\u0026gt;Ref(); bool have_stat_update = false; Version::GetStats stats; { mutex_.Unlock(); //首先查找memtable  LookupKey lkey(key, snapshot); if (mem-\u0026gt;Get(lkey, value, \u0026amp;s)) { //如果查找不到，接着查找immutable  } else if (imm != nullptr \u0026amp;\u0026amp; imm-\u0026gt;Get(lkey, value, \u0026amp;s)) { //如果还是没找到，则继续查找SSTable  } else { s = current-\u0026gt;Get(options, lkey, value, \u0026amp;stats); have_stat_update = true; } mutex_.Lock(); } if (have_stat_update \u0026amp;\u0026amp; current-\u0026gt;UpdateStats(stats)) { MaybeScheduleCompaction(); } mem-\u0026gt;Unref(); if (imm != nullptr) imm-\u0026gt;Unref(); current-\u0026gt;Unref(); return s; }   具体的实现流程如下图所示：\nGet 在查询读取数据时，依次从 MemTable、Immutable MemTable 以及当前保存的 SSTable 文件中进行查找。如果在 MemTabel 中找到，立即返回对应的数值，如果没有找到，再从 Immutable MemTable 中查找。而如果Immutable MemTable 中还是没有找到，则会从持久化的文件 SSTable 中查找，直到找出该键对应的数值为止。\n SequenceNumber 有什么用呢？\n其主要作用是对 DB 的整个存储空间进行时间刻度上的序列备份，即要从 DB 中获取某一个数据，不仅需要其对应的键 key，而且需要其对应的时间序列号。对数据库进行写操作会改变序列号，每进行一次写操作，则序列号加 1。\n Put、Delete、Write Put 主要有3个参数：写操作参数 opt、操作数据的 key 与操作数据新值 value。其代码如下：\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Put(const WriteOptions\u0026amp; o, const Slice\u0026amp; key, const Slice\u0026amp; val) { return DB::Put(o, key, val); } Status DB::Put(const WriteOptions\u0026amp; opt, const Slice\u0026amp; key, const Slice\u0026amp; value) { WriteBatch batch; batch.Put(key, value); return Write(opt, \u0026amp;batch); }   从上面的代码可以看出， Put 其实也是将单条数据的操作变更为一个批量操作，然后调用 Write 进行实现。\nDelete 不会直接删除数据，而是在对应位置插入一个 key 的删除标志，然后在后续的 Compaction 过程中才最终去除这条 key-value 记录。其代码如下:\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Delete(const WriteOptions\u0026amp; options, const Slice\u0026amp; key) { return DB::Delete(options, key); } Status DB::Delete(const WriteOptions\u0026amp; opt, const Slice\u0026amp; key) { WriteBatch batch; batch.Delete(key); return Write(opt, \u0026amp;batch); }   从上面的代码可以看出 Delete 的本质其实也是一个 Write 操作。\n在介绍 Write 之前，首先介绍其封装的消息结构 Writer 与任务队列 writes_。\nWriter 用于保存基本信息，如批量操作 batch、状态信息 status、是否同步 sync、是否完成 done 以及用于多线程操作的条件变量cv 。\n1 2 3 4 5 6 7 8 9 10 11 12  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  struct DBImpl::Writer { explicit Writer(port::Mutex* mu) : batch(nullptr), sync(false), done(false), cv(mu) {} Status status; //状态  WriteBatch* batch; //批量写入对象  bool sync;//表示是否已经同步了  bool done;//表示是否已经处理完成  port::CondVar cv;//这个是条件变量 };   接着看看任务队列 writers_，该队列对象中的元素节点为 Writer 对象指针。可见 writes_ 与写操作的缓存空间有关，批量操作请求均存储在这个队列中，按顺序执行，已完成的出队，而未执行的则在这个队列中处于等待状态。\n1 2  // https://weread.qq.com/web/reader/9f932e70727ac58e9f9d8cck636320102206364d3f0ffdc std::deque\u0026lt;Writer*\u0026gt; writers_ GUARDED_BY(mutex_);   Write 主要有两个参数：WriteOptions 对象与 WriteBatch 对象。WriteOptions 主要包含一些关于写操作的参数选项，而WriteBatch对象，相当于一个缓冲区，用于定义、保存一系列的批量操作。其代码实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Write(const WriteOptions\u0026amp; options, WriteBatch* updates) { //实例化一个Writer对象b并插入writers_队列中等待执行  Writer w(\u0026amp;mutex_); w.batch = updates; w.sync = options.sync; w.done = false; MutexLock l(\u0026amp;mutex_); writers_.push_back(\u0026amp;w); while (!w.done \u0026amp;\u0026amp; \u0026amp;w != writers_.front()) { w.cv.Wait(); } if (w.done) { return w.status; } Status status = MakeRoomForWrite(updates == nullptr); uint64_t last_sequence = versions_-\u0026gt;LastSequence(); Writer* last_writer = \u0026amp;w; if (status.ok() \u0026amp;\u0026amp; updates != nullptr) { //合并写入操作  WriteBatch* write_batch = BuildBatchGroup(\u0026amp;last_writer); WriteBatchInternal::SetSequence(write_batch, last_sequence + 1); last_sequence += WriteBatchInternal::Count(write_batch); { mutex_.Unlock(); //将更新写入日志文件中，并且将日志文件写入磁盘中  status = log_-\u0026gt;AddRecord(WriteBatchInternal::Contents(write_batch)); bool sync_error = false; if (status.ok() \u0026amp;\u0026amp; options.sync) { status = logfile_-\u0026gt;Sync(); if (!status.ok()) { sync_error = true; } } //将更新写入Memtable中  if (status.ok()) { status = WriteBatchInternal::InsertInto(write_batch, mem_); } mutex_.Lock(); if (sync_error) { RecordBackgroundError(status); } } if (write_batch == tmp_batch_) tmp_batch_-\u0026gt;Clear(); versions_-\u0026gt;SetLastSequence(last_sequence); } //由于和并写入操作一次可能会处理多个writer_队列中的元素，因此将所有已经处理的元素状态进行变更，并且发送signal信号  while (true) { Writer* ready = writers_.front(); writers_.pop_front(); if (ready != \u0026amp;w) { ready-\u0026gt;status = status; ready-\u0026gt;done = true; ready-\u0026gt;cv.Signal(); } if (ready == last_writer) break; } //通知writers_队列中的第一个元素，发送signal信号  if (!writers_.empty()) { writers_.front()-\u0026gt;cv.Signal(); } return status; }   具体的实现流程如下图所示：\n  实例化一个 Writer 对象，并将其插入所示的 writers_ 队列中。\n  通过 Writer 中的条件变量 cv 调用 wait 方法将该线程挂起，等待其他线程发送 signal 信号，并且等待队列前面的 Writer 操作全部执行完毕：\n 如果线程收到了 signal 信号：则解除阻塞。 如果线程没有收到了 signal 信号：说明队列前面仍有其他的 Writer 操作，那么该线程会再次调用 wait 方法实现阻塞，从而保证了 Writer 操作按照队列生成次序执行。    当轮到本线程操作时，首先通过 MakeRoomForWrite 函数进行内存空间分配。\n  当获取到需要的内存后，根据一系列的批量操作，对 Log 文件以及 MemTable 分别进行更新。\n  依据批量操作的数目更新 SequenceNumber。\n  通过 Writer 中的条件变量 cv 发送 signal 信号，以通知处于等待状态的其他线程开始执行。\n  ","date":"2022-05-23T23:48:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-dbimpl%E6%A8%A1%E5%9D%97/","title":"LevelDB DBImpl模块"},{"content":"Compaction模块 LevelDB 中的 Level 代表层级，有 0～6 共 7 个层级，每个层级都由一定数量的 SSTable 文件组成。其中，高层级文件是由低层级的一个文件与高层级中与该文件有键重叠的所有文件使用归并排序算法生成，该过程称为Compaction。\nLevelDB 通过 Compaction 将冷数据逐层下移，并且在 Compaction 过程中重复写入的键只会保留一个最终值，已经删除的键不再写入，因此可以减少磁盘空间占用。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  // https://github.com/google/leveldb/blob/master/db/version_set.h  class Compaction { public: ~Compaction(); int level() const { return level_; } VersionEdit* edit() { return \u0026amp;edit_; } int num_input_files(int which) const { return inputs_[which].size(); } FileMetaData* input(int which, int i) const { return inputs_[which][i]; } uint64_t MaxOutputFileSize() const { return max_output_file_size_; } bool IsTrivialMove() const; void AddInputDeletions(VersionEdit* edit); bool IsBaseLevelForKey(const Slice\u0026amp; user_key); bool ShouldStopBefore(const Slice\u0026amp; internal_key); void ReleaseInputs(); private: friend class Version; friend class VersionSet; Compaction(const Options* options, int level); int level_; uint64_t max_output_file_size_; Version* input_version_; VersionEdit edit_; std::vector\u0026lt;FileMetaData*\u0026gt; inputs_[2]; // The two sets of inputs  std::vector\u0026lt;FileMetaData*\u0026gt; grandparents_; size_t grandparent_index_; // Index in grandparent_starts_  bool seen_key_; // Some output key has been seen  int64_t overlapped_bytes_; // Bytes of overlap between current output  // and grandparent files  size_t level_ptrs_[config::kNumLevels]; };   在 LevelDB 中，Compaction 共有两种，分别叫 Minor Compaction 和 Major Compaction。\n  Minor Compaction：将 Immtable dump 到 SStable 。\n  Major Compaction：Level 之间的 SSTable Compaction。\n  这两类compaction负责在不同的场景下进行不同的数据整理。\nMinor Compaction 定义 Minor Compaction 非常简单，其本质就是将一个内存数据库（Memtable）中的所有数据持久化到一个磁盘文件中（SSTable）。整体流程如下图：\n触发时机 在 LSM 树的实现中，会先将数据写入 MemTable，当 MemTable 大小超过 options_.write_buffer_size （默认4M）时，需要将其作为 SSTable 写入磁盘，此时就会采取 Minor Compaction。\n核心要点  每次 Minor Compaction 结束后，都会生成一个新的 SSTable 文件，也意味着 Leveldb 的版本状态发生了变化，会进行一个版本的更替。 Minor Compaction 是一个时效性要求非常高的过程，要求其在尽可能短的时间内完成，否则就会堵塞正常的写入操作，因此 Minor Compaction 的优先级高于 Major Compaction。当进行 Minor Compaction 的时候有 Major Compaction正在进行，则会首先暂停 Major Compaction。  Major Compaction 定义 Major Compaction 是将不同层级的 SSTable 文件进行合并。\n如下图，可以看出其比 Minor Compaction 复杂的多。\n触发时机 那么什么时候，会触发 LevelDB 进行 Major Compaction 呢？总的来说为以下三个条件：\n 当 0 层文件数超过预定的上限（默认为 4 个）。 当 Level i层文件的总大小超过 10 ^ i MB。 当某个文件无效读取的次数过多。  这也就引出了 Size Compaction 与 Seek Compaction 两种判断策略。LevelDB先按 Size Compaction 判断是否需要进行 Compaction ，如果 Size Compaction 不满足则通过 Seek Compaction 继续判断，如果仍不满足，则表明暂时不需要进行 Compaction。\nSize Compaction size_compaction 通过判断 Level 0 中的文件个数（Level 0 会被频繁访问）或者 Level 1 ～ Level 5 的文件总大小来计算得出需要进行 Compaction 的 Level。\n该赋值逻辑位于 VersionSet 中的 Finalize，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // https://github.com/google/leveldb/blob/master/db/version_set.cc  void VersionSet::Finalize(Version* v) { int best_level = -1; double best_score = -1; for (int level = 0; level \u0026lt; config::kNumLevels - 1; level++) { double score; //对于Level 0来说，其分数为文件个数 / 4  if (level == 0) { score = v-\u0026gt;files_[level].size() / static_cast\u0026lt;double\u0026gt;(config::kL0_CompactionTrigger); } else { //对于Level 1~5的分数由该层所有文件总大小除以每层允许的最大大小决定  const uint64_t level_bytes = TotalFileSize(v-\u0026gt;files_[level]); score = static_cast\u0026lt;double\u0026gt;(level_bytes) / MaxBytesForLevel(options_, level); } //选取最高分的一个level  if (score \u0026gt; best_score) { best_level = level; best_score = score; } } //更新level与score  v-\u0026gt;compaction_level_ = best_level; v-\u0026gt;compaction_score_ = best_score; }   compaction_score_ 的赋值逻辑如下：\n Level 0：将当前 Level 0 包含的文件个数除以 4 并赋值给 compaction_score_ ，如果 Level 0 的文件个数大于等于 4，则此时 compaction_score_ 会大于等于 1。 Level 1～Level 5：通过该层文件的总大小除以该层文件允许的最大大小并赋值给 compaction_score_。  每次当版本中的 compaction_score_ 大于等于 1 时，则需要进行一次 Compaction 操作。\nSeek Compaction Seek compaction 主要通过记录某个 SSTable 的 Seek 次数，当其无效读取次数到达阈值（allowed_seeks）之后，将会记录下它的 level，参与下一次压缩。\n阈值 allowed_seeks 是每个文件允许的最大无效读取次数，该值的计算代码在 VersionSet::Build 中的 Apply，代码如下：\n1 2  f-\u0026gt;allowed_seeks = static_cast\u0026lt;int\u0026gt;((f-\u0026gt;file_size / 16384U)); if (f-\u0026gt;allowed_seeks \u0026lt; 100) f-\u0026gt;allowed_seeks = 100;   allowed_seeks 计算逻辑为文件大小除以 16384 后取值。但如果计算得到的值小于 100，则将其设置为 100。\n 为什么是除以16384呢？LevelDB作者给出了这样的解释：\n 硬盘中的一次查找操作耗费10ms。 硬盘读取速度为100MB/s，因此读取或者写入1MB数据需要10ms。 执行Compaction操作时，1MB的数据需要25MB数据的I/O，因为从当前层级读取1MB后，相应地需要从下一个层级读取10MB～12MB（因为每一层的最大大小为前一层的10倍，并且考虑到边界重叠的情况，因此执行Compaction操作时需要读取下一层的10MB～12MB数据），然后执行归并排序后写入的10MB～12MB的数据到下一个层级，因此读取加写入最大需要25MB数据的I/O。 因此25次查找（约耗费250ms）约略等于一次执行Compaction操作时处理1MB数据的时间（1MB的当前层读取加10MB～12MB的下一层读取，再加10MB～12MB的下一层写入，约为25MB的数据读取和写入总量，因此也是消耗250ms）。那么一次查找的数据量约略等于处理Compaction操作时的40KB数据（1MB除以25）。进一步保守处理，取16384（16K）这个值，即当查找次数超出allowed_seeks时，执行一次Compaction操作是一个更加合理的选择。   file_to_compact_level_ 的赋值逻辑位于 Version 的 UpdateStats，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // https://github.com/google/leveldb/blob/master/db/version_set.cc  bool Version::UpdateStats(const GetStats\u0026amp; stats) { //此时的f为无效查找的文件  FileMetaData* f = stats.seek_file; if (f != nullptr) { //当前查找无效，allowed_seeks-1  f-\u0026gt;allowed_seeks--; //如果此时allowed_seeks小于等于0，则说明此时到达阈值，则将当前层级记录下来，待进行Compaction  if (f-\u0026gt;allowed_seeks \u0026lt;= 0 \u0026amp;\u0026amp; file_to_compact_ == nullptr) { file_to_compact_ = f; file_to_compact_level_ = stats.seek_file_level; return true; } } return false; }   假设进行无效查找的文件为f（FileMetaData结构），先将 f 的 allowed_seeks 次数减 1，此时判断如果allowed_seeks 变量已经小于等于 0 且 Version 中的 file_to_compact_ 成员变量为空，则将 f 赋值给file_to_compact_ ，并且将 f 所属层级赋值给 file_to_compact_level_ 变量。\nManual Compaction Manual Compaction 是指人工触发的 Compaction，由外部接口调用产生。\n实际其内部触发调用的接口是 DBImpl 中的 CompactRange，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  void DBImpl::CompactRange(const Slice* begin, const Slice* end) { int max_level_with_files = 1; { MutexLock l(\u0026amp;mutex_); Version* base = versions_-\u0026gt;current(); for (int level = 1; level \u0026lt; config::kNumLevels; level++) { if (base-\u0026gt;OverlapInLevel(level, begin, end)) { max_level_with_files = level; } } } //略过所有没有重叠的文件  TEST_CompactMemTable(); //一层层压缩存在重叠的文件  for (int level = 0; level \u0026lt; max_level_with_files; level++) { TEST_CompactRange(level, begin, end); } }   在 Manual Compaction 中会指定的 begin 和 end，它将会一个层层的分次的 Compact 所有 Level 中与 begin 和 end 有重叠（overlap）的 SSTable 文件。\n文件选取 每次进行 Compaction 时，首先决定在哪个层级进行该次操作，假设为Level n，接着选取 Level n 层参与的文件，然后选取 Level n+1 层需要参与的文件，最后对选中的文件使用归并排序生成一个新文件。\nPickCompaction 决定层级 n 以及选取 Level n 层参与文件的方法为 VersionSet 中的 PickCompaction。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Compaction* VersionSet::PickCompaction() { Compaction* c; int level; //根据current_判断是size_compaction还是seek_compaction  const bool size_compaction = (current_-\u0026gt;compaction_score_ \u0026gt;= 1); const bool seek_compaction = (current_-\u0026gt;file_to_compact_ != nullptr); /* 如果根据size_compaction触发，则根据每个层级的compact_pointer_选取本次Compaction的 level n层文件（记录每个层级下一次开始进行Compaction操作时需要从哪个键开始。） */ if (size_compaction) { //获取当前层级  level = current_-\u0026gt;compaction_level_; assert(level \u0026gt;= 0); assert(level + 1 \u0026lt; config::kNumLevels); c = new Compaction(options_, level); //选出第一个在compact_pointer_之后的文件  for (size_t i = 0; i \u0026lt; current_-\u0026gt;files_[level].size(); i++) { FileMetaData* f = current_-\u0026gt;files_[level][i]; if (compact_pointer_[level].empty() || icmp_.Compare(f-\u0026gt;largest.Encode(), compact_pointer_[level]) \u0026gt; 0) { c-\u0026gt;inputs_[0].push_back(f); break; } } //如果通过compact_pointer_没有选取到文件（Compaction已遍历本层），则选取本层第一个文件  if (c-\u0026gt;inputs_[0].empty()) { // Wrap-around to the beginning of the key space  c-\u0026gt;inputs_[0].push_back(current_-\u0026gt;files_[level][0]); } } else if (seek_compaction) { //如果根据seek_compaction触发，则直接将无效查找次数超限的文件选取为本次的Level n层文件  level = current_-\u0026gt;file_to_compact_level_; c = new Compaction(options_, level); c-\u0026gt;inputs_[0].push_back(current_-\u0026gt;file_to_compact_); } else { return nullptr; } c-\u0026gt;input_version_ = current_; c-\u0026gt;input_version_-\u0026gt;Ref(); //对level 0特殊处理  if (level == 0) { InternalKey smallest, largest; //取出Level 0中参与本次Compaction操作的文件的最小键和最大键  GetRange(c-\u0026gt;inputs_[0], \u0026amp;smallest, \u0026amp;largest); //根据最小键和最大键对比Level 0中的所有文件，如果存在文件与[Lkey,Hkey]有重叠，则扩大最小键和最大键范围，并继续查找。  current_-\u0026gt;GetOverlappingInputs(0, \u0026amp;smallest, \u0026amp;largest, \u0026amp;c-\u0026gt;inputs_[0]); assert(!c-\u0026gt;inputs_[0].empty()); } //调用SetupOtherInputs选取level n+1层需要参与的文件  SetupOtherInputs(c); return c; }   执行逻辑如下：\n 根据 current_ 判断是 size_compaction 还是 seek_compaction：  根据 size_compaction 触发：则根据每个层级的 compact_pointer_ 选取本次 Compaction 的 level n 层文件（记录每个层级下一次开始进行 Compaction 时需要从哪个键开始）。 根据 seek_compaction 触发：则直接将无效查找次数超限的文件选取为本次的 Level n 层文件。   对 level 0 特殊处理：  取出 Level 0 中参与本次 Compaction 的文件的最小键和最大键，假设其范围为 [Lkey,Hkey]。 根据最小键和最大键对比 Level 0 中的所有文件，如果存在文件与 [Lkey,Hkey] 有重叠，则扩大最小键和最大键范围，并继续查找。   调用 SetupOtherInputs 选取 level n+1 层需要参与的文件。   为何Level 0中需要扩展有键重叠的文件呢？\n举例说明，假设Level 0有4个文件：f1、f2、f3、f4，每个文件的键范围分别为[c,e]，[a,f]，[a,b]，[i,z]。\n通过第一步选取的inputs_ [0]文件是f1，f1中的键范围和f2有重叠，则扩大最小键和最大键范围到[a,f]，此时发现f3的键范围也和f2有重叠，因此最终inputs_[0]中的文件包括f1、f2、f3三个文件。\n假设有这样一种情况，我们首先写了d这个键，在f2中的序列号为10，然后删除了d，删除操作在f1中的序列号为100，假设Compaction操作时只是选取了f1，则下次查找d这个键时先从Level 0选取，会读取到f2中序列号为10的值（实际上该键已经删除），此时会出现错误。\n SetupOtherInputs PickCompaction 会选定进行 Compaction 操作的层级 n 以及 Level n 层的参与文件，之后会调用SetupOtherInputs 进行 Level n+1 层文件的选取，SetupOtherInputs 的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/db/version_set.cc  void VersionSet::SetupOtherInputs(Compaction* c) { const int level = c-\u0026gt;level(); InternalKey smallest, largest; AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level], \u0026amp;c-\u0026gt;inputs_[0]); //获取input_[0]所有文件的最大键和最小键  GetRange(c-\u0026gt;inputs_[0], \u0026amp;smallest, \u0026amp;largest); //根据input_[0]中的最大键/最小键查找level n+1层的文件，并分别赋值到input_[1]中  current_-\u0026gt;GetOverlappingInputs(level + 1, \u0026amp;smallest, \u0026amp;largest, \u0026amp;c-\u0026gt;inputs_[1]); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level + 1], \u0026amp;c-\u0026gt;inputs_[1]); InternalKey all_start, all_limit; //继续获取input_[0]和[1]的所有文件的最大键和最小键  GetRange2(c-\u0026gt;inputs_[0], c-\u0026gt;inputs_[1], \u0026amp;all_start, \u0026amp;all_limit); //在不扩大level n+1层的前提下，尝试扩大level n层的文件，并且扩大后的文件总大小不超过50M  if (!c-\u0026gt;inputs_[1].empty()) { std::vector\u0026lt;FileMetaData*\u0026gt; expanded0; current_-\u0026gt;GetOverlappingInputs(level, \u0026amp;all_start, \u0026amp;all_limit, \u0026amp;expanded0); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level], \u0026amp;expanded0); const int64_t inputs0_size = TotalFileSize(c-\u0026gt;inputs_[0]); const int64_t inputs1_size = TotalFileSize(c-\u0026gt;inputs_[1]); const int64_t expanded0_size = TotalFileSize(expanded0); if (expanded0.size() \u0026gt; c-\u0026gt;inputs_[0].size() \u0026amp;\u0026amp; inputs1_size + expanded0_size \u0026lt; ExpandedCompactionByteSizeLimit(options_)) { InternalKey new_start, new_limit; GetRange(expanded0, \u0026amp;new_start, \u0026amp;new_limit); std::vector\u0026lt;FileMetaData*\u0026gt; expanded1; current_-\u0026gt;GetOverlappingInputs(level + 1, \u0026amp;new_start, \u0026amp;new_limit, \u0026amp;expanded1); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level + 1], \u0026amp;expanded1); if (expanded1.size() == c-\u0026gt;inputs_[1].size()) { Log(options_-\u0026gt;info_log, \u0026#34;Expanding@%d %d+%d (%ld+%ld bytes) to %d+%d (%ld+%ld bytes)\\n\u0026#34;, level, int(c-\u0026gt;inputs_[0].size()), int(c-\u0026gt;inputs_[1].size()), long(inputs0_size), long(inputs1_size), int(expanded0.size()), int(expanded1.size()), long(expanded0_size), long(inputs1_size)); smallest = new_start; largest = new_limit; c-\u0026gt;inputs_[0] = expanded0; c-\u0026gt;inputs_[1] = expanded1; GetRange2(c-\u0026gt;inputs_[0], c-\u0026gt;inputs_[1], \u0026amp;all_start, \u0026amp;all_limit); } } } if (level + 2 \u0026lt; config::kNumLevels) { current_-\u0026gt;GetOverlappingInputs(level + 2, \u0026amp;all_start, \u0026amp;all_limit, \u0026amp;c-\u0026gt;grandparents_); } //将本次Compaction的最大键保存到compact_pointer_中，下次Compaction时根据该值选取level n层文件  compact_pointer_[level] = largest.Encode().ToString(); c-\u0026gt;edit_.SetCompactPointer(level, largest); }   当 Level n 层和 Level n+1 层的文件都已经选定，LevelDB 的实现中有一个优化点，即判断是否可以在不扩大 Level n+1 层文件个数的情况下，将 Level n 层的文件个数扩大，优化逻辑如下：\n1. inputs_ [1] 选取完毕之后，首先计算 inputs_ [0] 和 inputs_ [1] 所有文件的最大/最小键范围，然后通过该范围重新去 Level n 层计算 inputs_ [0]，此时有可能选取到新的文件进入 inputs_ [0]。\r2. 通过新的 inputs_ [0] 的键范围重新选取 inputs_ [1] 中的文件，如果 inputs_ [1] 中的文件个数不变并且扩大范围后所有文件的总大小不超过50MB，则使用新的 inputs_ [0] 进行本次 Compaction ，否则继续使用原来的inputs_ [0]。50 MB 的限制是防止执行一次 Compaction 导致大量的 I/O 操作，从而影响系统性能。\r3. 如果扩大 Level n 层的文件个数之后导致 Level n+1 层的文件个数也进行了扩大，则不能进行此次优化。因为Level 1到 Level 6 的所有文件键范围不能有重叠，如果继续执行该优化，会导致 Compaction 之后 Level n+1 层的文件有键重叠的情况产生。\r 整体流程 其代码实现在 DBImpl 中的 DoCompactionWork，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::DoCompactionWork(CompactionState* compact) { //...  //计算最小序列号，如果Compaction中存在重复写入或者删除的键，则根据序列号判断是否需要删除  //如果没有快照，则将当前版本的last_sequence赋值为最小的序列号  if (snapshots_.empty()) { compact-\u0026gt;smallest_snapshot = versions_-\u0026gt;LastSequence(); } else { //如果有则根据最老的快照获取序列号  compact-\u0026gt;smallest_snapshot = snapshots_.oldest()-\u0026gt;sequence_number(); } //获取一个归并排序迭代器，每次选取最小的键写入文件  Iterator* input = versions_-\u0026gt;MakeInputIterator(compact-\u0026gt;compaction); //...  input-\u0026gt;SeekToFirst(); //...  //遍历迭代器  while (input-\u0026gt;Valid() \u0026amp;\u0026amp; !shutting_down_.load(std::memory_order_acquire)) { //...  //用于标记一个文件是否需要删除  bool drop = false; //...  //如果需要写入新文件，则写入到SSTable中  if (!drop) { if (compact-\u0026gt;builder == nullptr) { status = OpenCompactionOutputFile(compact); if (!status.ok()) { break; } } if (compact-\u0026gt;builder-\u0026gt;NumEntries() == 0) { compact-\u0026gt;current_output()-\u0026gt;smallest.DecodeFrom(key); } compact-\u0026gt;current_output()-\u0026gt;largest.DecodeFrom(key); compact-\u0026gt;builder-\u0026gt;Add(key, input-\u0026gt;value()); if (compact-\u0026gt;builder-\u0026gt;FileSize() \u0026gt;= compact-\u0026gt;compaction-\u0026gt;MaxOutputFileSize()) { status = FinishCompactionOutputFile(compact, input); if (!status.ok()) { break; } } } //继续查找归并排序中下一个最小的键  input-\u0026gt;Next(); } //...  //生成一个SSTable文件并刷新到磁盘  if (status.ok() \u0026amp;\u0026amp; compact-\u0026gt;builder != nullptr) { status = FinishCompactionOutputFile(compact, input); } //...  //调用VersionSet中的LogAndApply生成新的版本  if (status.ok()) { status = InstallCompactionResults(compact); } //... }   DoCompactionWork 的执行步骤如下：\n 计算一个本次 Compaction 的最小序列号值，如果有快照，则取最老的快照的序列号，如果没有快照，则选取当前版本 current_ 的序列号。因为快照存在时需要有一个一致性的读取视图，因此如果一个键的序列号比该值大，则该键不能够删除。 生成一个归并排序的迭代器，该迭代器会遍历 inputs_ [0] 和 inputs_ [1] 中的所有文件，每次选取一个最小的键写入新生成的文件。 选取键之后，判断该键是否可以删除，两种情况下可以删除一个键：  第一种情况为重复写入一个键，因为新键的序列号更大，因此之前被覆盖的键可以删除（当然被删除键的序列号需要小于第一步中计算得到的最小序列号）。 第二种情况为删除了一个键（实际上也是写入该键，不过被标记为删除操作），并且更高层级没有该键，则该键可以彻底删除（前提也是该键的序列号需要小于第 1 步中计算得到的最小序列号），即不需要写入新生成的 SSTable。   如果该键不需要删除，则将其写入新生成的 SSTable，并且当一个 SSTable 大小大于 2 MB 时，将该文件刷新到磁盘并且重新打开一个新的 SSTable。 执行 VersionSet 的 LogAndApply，生成一个新的版本并挂载到 VersionSet 中，并且将新版本赋值为当前版本。  垃圾回收 随着 Compaction 操作的进行，会有新文件生成，生成新文件之后可以进行旧文件清理。每次当一个 MemTable生成 SSTable 并刷新到磁盘之后，该 MemTable 对应的日志也可以进行删除。\nLevelDB 中负责清理文件的是 RemoveObsoleteFiles，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  void DBImpl::RemoveObsoleteFiles() { mutex_.AssertHeld(); if (!bg_error_.ok()) { return; } //将所有正在Compaction和versionset中的版本文件放入live集合  std::set\u0026lt;uint64_t\u0026gt; live = pending_outputs_; versions_-\u0026gt;AddLiveFiles(\u0026amp;live); //将所有数据目录下的文件放入filenames数组中  std::vector\u0026lt;std::string\u0026gt; filenames; env_-\u0026gt;GetChildren(dbname_, \u0026amp;filenames); // Ignoring errors on purpose  uint64_t number; FileType type; std::vector\u0026lt;std::string\u0026gt; files_to_delete; //保存可以删除的文件  //遍历所有文件，判断是否可以删除  for (std::string\u0026amp; filename : filenames) { //解析出每个文件的序列号和类型  if (ParseFileName(filename, \u0026amp;number, \u0026amp;type)) { //keep标记文件是否需要保留  bool keep = true; switch (type) { //删除序列号小于versionset的log_number，且不等于prev_log_number的日志  case kLogFile: keep = ((number \u0026gt;= versions_-\u0026gt;LogNumber()) || (number == versions_-\u0026gt;PrevLogNumber())); break; //删除版本较低的Manifest  case kDescriptorFile: keep = (number \u0026gt;= versions_-\u0026gt;ManifestFileNumber()); break; //删除没有参与Compaction的且不在versionset中的sstable  case kTableFile: keep = (live.find(number) != live.end()); break; //临时文件  case kTempFile: keep = (live.find(number) != live.end()); break; case kCurrentFile: case kDBLockFile: case kInfoLogFile: keep = true; break; } //将keep为false的放入files_to_delete，后续删除。  if (!keep) { files_to_delete.push_back(std::move(filename)); //如果是sstable文件则从缓存中删除  if (type == kTableFile) { table_cache_-\u0026gt;Evict(number); } Log(options_.info_log, \u0026#34;Delete type=%d #%lld\\n\u0026#34;, static_cast\u0026lt;int\u0026gt;(type), static_cast\u0026lt;unsigned long long\u0026gt;(number)); } } } //删除所有files_to_delete中的文件  mutex_.Unlock(); for (const std::string\u0026amp; filename : files_to_delete) { env_-\u0026gt;RemoveFile(dbname_ + \u0026#34;/\u0026#34; + filename); } mutex_.Lock(); }   执行逻辑如下：\n 将正在进行 Compaction 操作的 SSTable 文件和 VersionSet 的所有版本中的 SSTable 文件放入 live 集合中。 通过 filename 找到对应的文件，解析出文件序列号和文件类型。 遍历所有文件，判断是否需要删除，如果需要删除则将 keep 标记为 false，并放入 files_to_delete 中。 遍历 files_to_delete 数组，调用 RemoveObsoleteFiles 删除文件。  ","date":"2022-05-23T23:47:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-compaction%E6%A8%A1%E5%9D%97/","title":"LevelDB Compaction模块"},{"content":"版本管理  为什么 LevelDB 需要版本的概念呢？\n 针对共享的资源，有三种方式：\n 悲观锁：这是最简单的处理方式。加锁保护，读写互斥。效率低。 乐观锁：它假设多用户并发的事物在处理时不会彼此互相影响，各事务能够在不产生锁的的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。 果其他事务有更新的话，正在提交的事务会进行回滚；这样做不会有锁竞争更不会产生死锁， 但如果数据竞争的概率较高，效率也会受影响 。 MVCC：MVCC 是一个数据库常用的概念。Multi Version Concurrency Control 多版本并发控制。每一个执行操作的用户，看到的都是数据库特定时刻的的快照 （Snapshot）， Writer 的任何未完成的修改都不会被其他的用户所看到；当对数据进行更新的时候并是不直接覆盖，而是先进行标记，然后在其他地方添加新的数据（这些变更存储在 VersionEdit），从而形成一个新版本，此时再来读取的 Reader 看到的就是最新的版本了。所以这种处理策略是维护了多个版本的数据的，但只有一个是最新的（VersionSet 中维护着全局最新的 Seqnum）。  LevelDB 通过 Version 以及 VersionSet 来管理元信息，用 Manifest 来保存元信息。\nManifest Manifest 文件专用于记录版本信息。LevelDB 采用了增量式的存储方式，记录每一个版本相较于一个版本的变化情况。\n 变化情况大致包括：\n（1）新增了哪些 SSTable 文件；\n（2）删除了哪些 SSTable 文件（由于Compaction导致）；\n（3）最新的 Journal 日志文件标号等；\n 一个 Manifest 文件中，包含了多条 Session Record。其中第一条 Session Record 记载了当时 LevelDB 的全量版本信息，其余若干条 Session Record 仅记录每次更迭的变化情况。具体结构如下图：\nLevelDB 启动时会先到数据目录寻找一个名为 CURRENT 的文件，该文件中会保存 Manifest 的文件名称，通过读取 Manifest 记录的 Session Record，从初始状态开始不断地应用这些版本改动，即可使得系统的版本信息恢复到最近一次使用的状态。\nVersion Version 表示当前的一个版本，该结构中会保存每个层级拥有的文件信息以及指向前一个和后一个版本的指针等。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85  class Version { public: struct GetStats { FileMetaData* seek_file; int seek_file_level; }; void AddIterators(const ReadOptions\u0026amp;, std::vector\u0026lt;Iterator*\u0026gt;* iters); Status Get(const ReadOptions\u0026amp;, const LookupKey\u0026amp; key, std::string* val, GetStats* stats); bool UpdateStats(const GetStats\u0026amp; stats); bool RecordReadSample(Slice key); void Ref(); void Unref(); void GetOverlappingInputs( int level, const InternalKey* begin, // nullptr means before all keys  const InternalKey* end, // nullptr means after all keys  std::vector\u0026lt;FileMetaData*\u0026gt;* inputs); bool OverlapInLevel(int level, const Slice* smallest_user_key, const Slice* largest_user_key); int PickLevelForMemTableOutput(const Slice\u0026amp; smallest_user_key, const Slice\u0026amp; largest_user_key); int NumFiles(int level) const { return files_[level].size(); } std::string DebugString() const; private: friend class Compaction; friend class VersionSet; class LevelFileNumIterator; explicit Version(VersionSet* vset) : vset_(vset), next_(this), prev_(this), refs_(0), file_to_compact_(nullptr), file_to_compact_level_(-1), compaction_score_(-1), compaction_level_(-1) {} Version(const Version\u0026amp;) = delete; Version\u0026amp; operator=(const Version\u0026amp;) = delete; ~Version(); Iterator* NewConcatenatingIterator(const ReadOptions\u0026amp;, int level) const; void ForEachOverlapping(Slice user_key, Slice internal_key, void* arg, bool (*func)(void*, int, FileMetaData*)); VersionSet* vset_; // VersionSet to which this Version belongs  Version* next_; // Next version in linked list  Version* prev_; // Previous version in linked list  int refs_; // Number of live refs to this version  std::vector\u0026lt;FileMetaData*\u0026gt; files_[config::kNumLevels]; FileMetaData* file_to_compact_; int file_to_compact_level_; double compaction_score_; int compaction_level_; }; struct FileMetaData { FileMetaData() : refs(0), allowed_seeks(1 \u0026lt;\u0026lt; 30), file_size(0) {} int refs;\t// 引用计数  int allowed_seeks; // 用于seek compaction  uint64_t number;\t// 唯一标识一个sstable  uint64_t file_size; // 文件大小  InternalKey smallest; // 最小key  InternalKey largest; // 最大key };    成员变量：  GetStats：键查找时用来保存中间状态的一个结构。 vset_：该版本属于的版本集合。 next_：指向后一个版本的指针。 prev_：指向前一个版本的指针。 refs_：该版本的引用计数。 files_：每个层级所包含的 SSTable 文件，每一个文件以一个 FileMetaData 结构表示。 file_to_compact_：下次需要进行 Compaction 操作的文件。 file_to_compact_level_：下次需要进行 Compaction 操作的文件所属的层级。 compaction_score_：如果 compaction_score_ 大于 1，说明需要进行一次 Compaction 操作。 compaction_level_：表明需要进行 Compaction 操作的层级。    VersionEdit **VersionEdit 是一个版本的中间状态，会保存一次 Compaction 操作后增加的删除文件信息以及其他一些元数据。**当数据库正常/非正常关闭，重新打开时，只需要按顺序把 Manifest 文件中的 VersionEdit 执行一遍，就可以把数据恢复到宕机前的最新版本。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  // https://github.com/google/leveldb/blob/master/db/version_edit.h  class VersionEdit { public: VersionEdit() { Clear(); } ~VersionEdit() = default; void Clear(); void SetComparatorName(const Slice\u0026amp; name) { has_comparator_ = true; comparator_ = name.ToString(); } void SetLogNumber(uint64_t num) { has_log_number_ = true; log_number_ = num; } void SetPrevLogNumber(uint64_t num) { has_prev_log_number_ = true; prev_log_number_ = num; } void SetNextFile(uint64_t num) { has_next_file_number_ = true; next_file_number_ = num; } void SetLastSequence(SequenceNumber seq) { has_last_sequence_ = true; last_sequence_ = seq; } void SetCompactPointer(int level, const InternalKey\u0026amp; key) { compact_pointers_.push_back(std::make_pair(level, key)); } void AddFile(int level, uint64_t file, uint64_t file_size, const InternalKey\u0026amp; smallest, const InternalKey\u0026amp; largest) { FileMetaData f; f.number = file; f.file_size = file_size; f.smallest = smallest; f.largest = largest; new_files_.push_back(std::make_pair(level, f)); } void RemoveFile(int level, uint64_t file) { deleted_files_.insert(std::make_pair(level, file)); } void EncodeTo(std::string* dst) const; Status DecodeFrom(const Slice\u0026amp; src); std::string DebugString() const; private: friend class VersionSet; typedef std::set\u0026lt;std::pair\u0026lt;int, uint64_t\u0026gt;\u0026gt; DeletedFileSet; std::string comparator_; uint64_t log_number_;\t//已经弃用  uint64_t prev_log_number_; uint64_t next_file_number_; SequenceNumber last_sequence_; bool has_comparator_; bool has_log_number_; bool has_prev_log_number_; bool has_next_file_number_; bool has_last_sequence_; std::vector\u0026lt;std::pair\u0026lt;int, InternalKey\u0026gt;\u0026gt; compact_pointers_; DeletedFileSet deleted_files_; std::vector\u0026lt;std::pair\u0026lt;int, FileMetaData\u0026gt;\u0026gt; new_files_; };    成员变量：  comparator_：比较器名称。 log_number_：日志文件序号。 **next_file_number_：**下一个文件序列号。 last_sequence_：下一个写入序列号。 has_xxxxx_：has_comparator_ ，has_log_number_ ，has_prev_log_number_ ，has_last_sequence_ ，has_next_file_number_，布尔型变量，表明相应的成员变量是否已经设置。 compact_pointers_：该变量用来指示 LevelDB 中每个层级下一次进行 Compaction 操作时需要从哪个键开始。 **deleted_files_：**记录每个层级执行 Compaction 操作之后删除掉的文件。 new_files_：记录每个层级执行 Compaction 操作之后新增的文件。新增文件记录为一个个FileMetaData 结构体。    EncodeTo EncodeTo 会将 VersionEdit 各个成员变量的信息编码为一个字符串，编码时会先给每个成员变量定义一个 Tag，Tag的枚举值如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  enum Tag { kComparator = 1,\t//比较器  kLogNumber = 2,\t//日志文件序列号  kNextFileNumber = 3,\t//下一个文件序列号  kLastSequence = 4,\t//下一个写入序列号  kCompactPointer = 5,\t//CompactPointer类型  kDeletedFile = 6,\t//删除的文件  kNewFile = 7,\t//增加的文件  // 8 曾经用于大Value的引用，现以弃用  kPrevLogNumber = 9\t//前一个日志文件序列号 };   接下来看看 EncodeTo 的实现逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  void VersionEdit::EncodeTo(std::string* dst) const { //如果为true，则先将kComparator Tag编码，然后将比较器名称编码写入。  if (has_comparator_) { PutVarint32(dst, kComparator); PutLengthPrefixedSlice(dst, comparator_); } //与上面类似，就不再重复  if (has_log_number_) { PutVarint32(dst, kLogNumber); PutVarint64(dst, log_number_); } if (has_prev_log_number_) { PutVarint32(dst, kPrevLogNumber); PutVarint64(dst, prev_log_number_); } if (has_next_file_number_) { PutVarint32(dst, kNextFileNumber); PutVarint64(dst, next_file_number_); } if (has_last_sequence_) { PutVarint32(dst, kLastSequence); PutVarint64(dst, last_sequence_); } //依次将compact_pointers_中的level和Key编码  for (size_t i = 0; i \u0026lt; compact_pointers_.size(); i++) { PutVarint32(dst, kCompactPointer); PutVarint32(dst, compact_pointers_[i].first); // level  PutLengthPrefixedSlice(dst, compact_pointers_[i].second.Encode()); } //依次将deleted_file_中的level和file number编码  for (const auto\u0026amp; deleted_file_kvp : deleted_files_) { PutVarint32(dst, kDeletedFile); PutVarint32(dst, deleted_file_kvp.first); // level  PutVarint64(dst, deleted_file_kvp.second); // file number  } //依次将new_files_中的level和FileMetaData进行编码  for (size_t i = 0; i \u0026lt; new_files_.size(); i++) { const FileMetaData\u0026amp; f = new_files_[i].second; PutVarint32(dst, kNewFile); PutVarint32(dst, new_files_[i].first); // level  //编码FileMetaData  PutVarint64(dst, f.number); PutVarint64(dst, f.file_size); PutLengthPrefixedSlice(dst, f.smallest.Encode()); PutLengthPrefixedSlice(dst, f.largest.Encode()); } }   EncodeTo 函数会依次以 Tag 开头，将比较器名称、日志序列号、上一个日志序列号、下一个文件序列号、最后一个序列号、CompactPointers、每个层级删除的文件以及增加的文件信息保存到一个字符串中。\nDecodeFrom 解码逻辑与上面的类似，根据不同的 tag 解析对应的成员变量，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  Status VersionEdit::DecodeFrom(const Slice\u0026amp; src) { Clear(); Slice input = src; const char* msg = nullptr; uint32_t tag; // Temporary storage for parsing  int level; uint64_t number; FileMetaData f; Slice str; InternalKey key; while (msg == nullptr \u0026amp;\u0026amp; GetVarint32(\u0026amp;input, \u0026amp;tag)) { //根据不同的tag执行对应的解码逻辑  switch (tag) { case kComparator: if (GetLengthPrefixedSlice(\u0026amp;input, \u0026amp;str)) { comparator_ = str.ToString(); has_comparator_ = true; } else { msg = \u0026#34;comparator name\u0026#34;; } break; case kLogNumber: if (GetVarint64(\u0026amp;input, \u0026amp;log_number_)) { has_log_number_ = true; } else { msg = \u0026#34;log number\u0026#34;; } break; case kPrevLogNumber: if (GetVarint64(\u0026amp;input, \u0026amp;prev_log_number_)) { has_prev_log_number_ = true; } else { msg = \u0026#34;previous log number\u0026#34;; } break; case kNextFileNumber: if (GetVarint64(\u0026amp;input, \u0026amp;next_file_number_)) { has_next_file_number_ = true; } else { msg = \u0026#34;next file number\u0026#34;; } break; case kLastSequence: if (GetVarint64(\u0026amp;input, \u0026amp;last_sequence_)) { has_last_sequence_ = true; } else { msg = \u0026#34;last sequence number\u0026#34;; } break; case kCompactPointer: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;key)) { compact_pointers_.push_back(std::make_pair(level, key)); } else { msg = \u0026#34;compaction pointer\u0026#34;; } break; case kDeletedFile: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;number)) { deleted_files_.insert(std::make_pair(level, number)); } else { msg = \u0026#34;deleted file\u0026#34;; } break; case kNewFile: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;f.number) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;f.file_size) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;f.smallest) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;f.largest)) { new_files_.push_back(std::make_pair(level, f)); } else { msg = \u0026#34;new-file entry\u0026#34;; } break; default: msg = \u0026#34;unknown tag\u0026#34;; break; } } if (msg == nullptr \u0026amp;\u0026amp; !input.empty()) { msg = \u0026#34;invalid tag\u0026#34;; } Status result; if (msg != nullptr) { result = Status::Corruption(\u0026#34;VersionEdit\u0026#34;, msg); } return result; }   VersionSet LevelDB 为了支持 MVCC，引入了 Version 和 VersionEdit 的概念，那么如何来有效的管理这些 Version 呢？于是引出了 VersionSet，VersionSet 是一个双向链表，而且整个 DB 只会有一个 VersionSet。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107  class VersionSet { public: VersionSet(const std::string\u0026amp; dbname, const Options* options, TableCache* table_cache, const InternalKeyComparator*); VersionSet(const VersionSet\u0026amp;) = delete; VersionSet\u0026amp; operator=(const VersionSet\u0026amp;) = delete; ~VersionSet(); Status LogAndApply(VersionEdit* edit, port::Mutex* mu) EXCLUSIVE_LOCKS_REQUIRED(mu); Status Recover(bool* save_manifest); Version* current() const { return current_; } uint64_t ManifestFileNumber() const { return manifest_file_number_; } uint64_t NewFileNumber() { return next_file_number_++; } void ReuseFileNumber(uint64_t file_number) { if (next_file_number_ == file_number + 1) { next_file_number_ = file_number; } } int NumLevelFiles(int level) const; int64_t NumLevelBytes(int level) const; uint64_t LastSequence() const { return last_sequence_; } void SetLastSequence(uint64_t s) { assert(s \u0026gt;= last_sequence_); last_sequence_ = s; } void MarkFileNumberUsed(uint64_t number); uint64_t LogNumber() const { return log_number_; } uint64_t PrevLogNumber() const { return prev_log_number_; } Compaction* PickCompaction(); Compaction* CompactRange(int level, const InternalKey* begin, const InternalKey* end); int64_t MaxNextLevelOverlappingBytes(); Iterator* MakeInputIterator(Compaction* c); bool NeedsCompaction() const { Version* v = current_; return (v-\u0026gt;compaction_score_ \u0026gt;= 1) || (v-\u0026gt;file_to_compact_ != nullptr); } void AddLiveFiles(std::set\u0026lt;uint64_t\u0026gt;* live); uint64_t ApproximateOffsetOf(Version* v, const InternalKey\u0026amp; key); struct LevelSummaryStorage { char buffer[100]; }; const char* LevelSummary(LevelSummaryStorage* scratch) const; private: class Builder; friend class Compaction; friend class Version; bool ReuseManifest(const std::string\u0026amp; dscname, const std::string\u0026amp; dscbase); void Finalize(Version* v); void GetRange(const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs, InternalKey* smallest, InternalKey* largest); void GetRange2(const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs1, const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs2, InternalKey* smallest, InternalKey* largest); void SetupOtherInputs(Compaction* c); Status WriteSnapshot(log::Writer* log); void AppendVersion(Version* v); Env* const env_; const std::string dbname_; const Options* const options_; TableCache* const table_cache_; const InternalKeyComparator icmp_; uint64_t next_file_number_; uint64_t manifest_file_number_; uint64_t last_sequence_; uint64_t log_number_; uint64_t prev_log_number_; // 0 or backing store for memtable being compacted  WritableFile* descriptor_file_; log::Writer* descriptor_log_; Version dummy_versions_; // Head of circular doubly-linked list of versions.  Version* current_; // == dummy_versions_.prev_  std::string compact_pointer_[config::kNumLevels]; };    关键成员变量：  **next_file_number_**下一个文件序列号。 manifest_file_number_：Manifest 文件的文件序列号。 last_sequence_：当前最大的写入序列号。 log_number_：Log文件的文件序列号。 current_：当前的最新版本。 compact_pointer_：记录每个层级下一次开始进行 Compaction 操作时需要从哪个键开始。    下面就介绍一下其中最核心的 LogAndApply 和 Recover 两个函数。\nLogAndApply 当每次进行完 Compaction 操作后，需要调用并执行 VersionSet 中的 LogAndApply 写入版本变化后并生成一个新的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) { //根据版本变化进行处理  if (edit-\u0026gt;has_log_number_) { assert(edit-\u0026gt;log_number_ \u0026gt;= log_number_); assert(edit-\u0026gt;log_number_ \u0026lt; next_file_number_); } else { edit-\u0026gt;SetLogNumber(log_number_); } if (!edit-\u0026gt;has_prev_log_number_) { edit-\u0026gt;SetPrevLogNumber(prev_log_number_); } edit-\u0026gt;SetNextFile(next_file_number_); edit-\u0026gt;SetLastSequence(last_sequence_); //生成新的版本，其为上一个Version+VersionEdit  Version* v = new Version(this); { Builder builder(this, current_); builder.Apply(edit); builder.SaveTo(v); } //计算这个新version的compact score和compact level，算出最应该被Compact的level  Finalize(v); std::string new_manifest_file; Status s; if (descriptor_log_ == nullptr) { //这里没有必要进行unlock操作，因为只有在第一次调用，也就是打开数据库的时候才会走到这个路径里面来  assert(descriptor_file_ == nullptr); new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_); edit-\u0026gt;SetNextFile(next_file_number_); s = env_-\u0026gt;NewWritableFile(new_manifest_file, \u0026amp;descriptor_file_); //此时重新开启一个新的MANIFEST文件，并将当前状态作为base状态写入快照  if (s.ok()) { descriptor_log_ = new log::Writer(descriptor_file_); //写入当前快照  s = WriteSnapshot(descriptor_log_); } } //在MANIFEST写入磁盘时解锁（此时没必要加锁，由后台线程完成）  { mu-\u0026gt;Unlock(); 后将其写入磁盘 if (s.ok()) { std::string record; edit-\u0026gt;EncodeTo(\u0026amp;record); //将版本变化写入MANIFEST  s = descriptor_log_-\u0026gt;AddRecord(record); if (s.ok()) { //将MANIFEST写入磁盘文件  s = descriptor_file_-\u0026gt;Sync(); } if (!s.ok()) { Log(options_-\u0026gt;info_log, \u0026#34;MANIFEST write: %s\\n\u0026#34;, s.ToString().c_str()); } } //当产生新的Manifest时更新current  if (s.ok() \u0026amp;\u0026amp; !new_manifest_file.empty()) { s = SetCurrentFile(env_, dbname_, manifest_file_number_); } mu-\u0026gt;Lock(); } // 将新生成的版本挂在到VersionSet，并且将当前版本（current_）设置为新生成的版本。  if (s.ok()) { AppendVersion(v); log_number_ = edit-\u0026gt;log_number_; prev_log_number_ = edit-\u0026gt;prev_log_number_; } else { delete v; if (!new_manifest_file.empty()) { delete descriptor_log_; delete descriptor_file_; descriptor_log_ = nullptr; descriptor_file_ = nullptr; env_-\u0026gt;RemoveFile(new_manifest_file); } } return s; }   执行逻辑如下:\n 将当前的版本根据版本变化（VersionEdit）进行处理，然后生成一个新的版本。 如果是第一次调用，则创建一个新的 Manifest 文件，并将当前状态作为 base 写入。 调用 Finalize 算出下一次需要 Compaction 的 level。 将版本变化写入 Manifest，把 Manifest 写入磁盘。 将新生成的版本挂载到 VersionSet 中，并且将 current_ 设置为新生成的版本。  Recover Recover 会根据 Manifest 文件中记录的每次版本变化（调用 VersionEdit 的 DecodeFrom 方法）逐次回放生成一个最新的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Status VersionSet::Recover(bool* save_manifest) { struct LogReporter : public log::Reader::Reporter { Status* status; void Corruption(size_t bytes, const Status\u0026amp; s) override { if (this-\u0026gt;status-\u0026gt;ok()) *this-\u0026gt;status = s; } }; //读取CURRENT文件，找到Manifest文件  std::string current; Status s = ReadFileToString(env_, CurrentFileName(dbname_), \u0026amp;current); if (!s.ok()) { return s; } if (current.empty() || current[current.size() - 1] != \u0026#39;\\n\u0026#39;) { return Status::Corruption(\u0026#34;CURRENT file does not end with newline\u0026#34;); } current.resize(current.size() - 1); std::string dscname = dbname_ + \u0026#34;/\u0026#34; + current; SequentialFile* file; s = env_-\u0026gt;NewSequentialFile(dscname, \u0026amp;file); if (!s.ok()) { if (s.IsNotFound()) { return Status::Corruption(\u0026#34;CURRENT points to a non-existent file\u0026#34;, s.ToString()); } return s; } bool have_log_number = false; bool have_prev_log_number = false; bool have_next_file = false; bool have_last_sequence = false; uint64_t next_file = 0; uint64_t last_sequence = 0; uint64_t log_number = 0; uint64_t prev_log_number = 0; Builder builder(this, current_); int read_records = 0; { LogReporter reporter; reporter.status = \u0026amp;s; log::Reader reader(file, \u0026amp;reporter, true /*checksum*/, 0 /*initial_offset*/); Slice record; std::string scratch; //读取versionedit并获取变更  while (reader.ReadRecord(\u0026amp;record, \u0026amp;scratch) \u0026amp;\u0026amp; s.ok()) { ++read_records; VersionEdit edit; s = edit.DecodeFrom(record); if (s.ok()) { if (edit.has_comparator_ \u0026amp;\u0026amp; edit.comparator_ != icmp_.user_comparator()-\u0026gt;Name()) { s = Status::InvalidArgument( edit.comparator_ + \u0026#34; does not match existing comparator \u0026#34;, icmp_.user_comparator()-\u0026gt;Name()); } } if (s.ok()) { builder.Apply(\u0026amp;edit); } if (edit.has_log_number_) { log_number = edit.log_number_; have_log_number = true; } if (edit.has_prev_log_number_) { prev_log_number = edit.prev_log_number_; have_prev_log_number = true; } if (edit.has_next_file_number_) { next_file = edit.next_file_number_; have_next_file = true; } if (edit.has_last_sequence_) { last_sequence = edit.last_sequence_; have_last_sequence = true; } } } delete file; file = nullptr; if (s.ok()) { if (!have_next_file) { s = Status::Corruption(\u0026#34;no meta-nextfile entry in descriptor\u0026#34;); } else if (!have_log_number) { s = Status::Corruption(\u0026#34;no meta-lognumber entry in descriptor\u0026#34;); } else if (!have_last_sequence) { s = Status::Corruption(\u0026#34;no last-sequence-number entry in descriptor\u0026#34;); } if (!have_prev_log_number) { prev_log_number = 0; } MarkFileNumberUsed(prev_log_number); MarkFileNumberUsed(log_number); } if (s.ok()) { //生成最新版本  Version* v = new Version(this); builder.SaveTo(v); Finalize(v); //加入versionset并设置current指针  AppendVersion(v); manifest_file_number_ = next_file; next_file_number_ = next_file + 1; last_sequence_ = last_sequence; log_number_ = log_number; prev_log_number_ = prev_log_number; //判断是否能够复用已有MANIFEST文件  if (ReuseManifest(dscname, current)) { } else { *save_manifest = true; } } else { std::string error = s.ToString(); Log(options_-\u0026gt;info_log, \u0026#34;Error recovering version set with %d records: %s\u0026#34;, read_records, error.c_str()); } return s; }   执行流程如下：\n 通过 current_ 获取到 Manifest，读取 Manifest 获取 base 状态。 读取 Manifest 文件中的 VersionEdit，并执行变更。 生成最终的版本，并将其加入 VersionSet，更新 current_。 判断是否能够复用已有 MANIFEST 文件。  ","date":"2022-05-23T23:43:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/","title":"LevelDB 版本管理"},{"content":"WAL日志模块 当向 LevelDB 写入数据时，只需要将数据写入内存中的 MemTable，而由于内存是易失性存储，因此 LevelDB 需要一个额外的持久化文件：预写日志（Write-Ahead Log，WAL），又称重做日志。这是一个追加修改、顺序写入磁盘的文件。当宕机或者程序崩溃时 WAL 能够保证写入成功的数据不会丢失。将 MemTable 成功写入 SSTable 后，相应的预写日志就可以删除了。\n结构 Log文件以块为基本单位，一条记录可能全部写到一个块上，也可能跨几个块。记录的格式如下图所示：\n首先我们来看看 Log 中的数据格式，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // https://github.com/google/leveldb/blob/master/db/log_format.h  namespace log { enum RecordType { // Zero is reserved for preallocated files  kZeroType = 0, kFullType = 1, // For fragments  kFirstType = 2, kMiddleType = 3, kLastType = 4 }; static const int kMaxRecordType = kLastType; static const int kBlockSize = 32768; // Header is checksum (4 bytes), length (2 bytes), type (1 byte).  static const int kHeaderSize = 4 + 2 + 1; } // namespace log   结合上面的代码和图片，我们可以看到每一个块大小为 32768 字节，并且每一个块由头部和正文组成。头部由 4 字节校验，2 字节的长度与 1 字节的类型构成，即每一个块的开始 7 字节属于头部。头部中的类型字段有如下 4 种：\n kZeroType：为预分配的文件保留。 kFullType：表示一条记录完整地写到了一个块上。 kFirstType：表示该条记录的第一部分。 kMiddleType：表示该条记录的中间部分。 kLastType：表示该条记录的最后一部分。  通过记录结构可以推测出 Log 文件的读取流程，即首先根据头部的长度字段确定需要读取多少字节，然后根据头部类型字段确定该条记录是否已经完整读取，如果没有完整读取，继续按该流程进行，直到读取到记录的最后一部分，其头部类型为 kLastType。\n读写流程 写入 Log 的读取主要由 Writer 中的 AddRecord 实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  // https://github.com/google/leveldb/blob/master/db/log_writer.h  class Writer { public: explicit Writer(WritableFile* dest); Writer(WritableFile* dest, uint64_t dest_length); Writer(const Writer\u0026amp;) = delete; Writer\u0026amp; operator=(const Writer\u0026amp;) = delete; ~Writer(); Status AddRecord(const Slice\u0026amp; slice); private: Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length); WritableFile* dest_; int block_offset_; // Current offset in block  uint32_t type_crc_[kMaxRecordType + 1]; }; // https://github.com/google/leveldb/blob/master/db/log_writer.cc  Status Writer::AddRecord(const Slice\u0026amp; slice) { const char* ptr = slice.data(); size_t left = slice.size(); Status s; //begin表明本条记录是第一次写入，即当前块中第一条记录  bool begin = true; do { //当前块剩余空间，用于判断头部能否完整写入  const int leftover = kBlockSize - block_offset_; assert(leftover \u0026gt;= 0); if (leftover \u0026lt; kHeaderSize) { //如果块剩余空间小于七个字节且不等于0，说明当前无法完整写入数据，此时填充\\x00，从下一个块写入  if (leftover \u0026gt; 0) { static_assert(kHeaderSize == 7, \u0026#34;\u0026#34;); dest_-\u0026gt;Append(Slice(\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34;, leftover)); } //此时块正好写满，将block_offset_置为0，表明开始写入新的块  block_offset_ = 0; } assert(kBlockSize - block_offset_ - kHeaderSize \u0026gt;= 0); //计算块剩余空间  const size_t avail = kBlockSize - block_offset_ - kHeaderSize; //计算当前块能够写入的数据大小（块剩余空间和记录剩余内容中最小的）  const size_t fragment_length = (left \u0026lt; avail) ? left : avail; RecordType type; //end表明该记录是否已经完整写入，即最后一条记录  const bool end = (left == fragment_length); //根据begin与end来确定记录类型  if (begin \u0026amp;\u0026amp; end) { //记录为第一条且同时又是最后一条，说明当前是完整的记录，状态为kFullType  type = kFullType; } else if (begin) { //记录为第一条，状态为kFirstType  type = kFirstType; } else if (end) { //记录为最后一条，标记状态为kLastType  type = kLastType; } else { //记录不为第一条，也并非最后一条，则说明是中间状态，标记为kMiddleType  type = kMiddleType; } //将数据按照格式写入，并刷新到磁盘文件中  s = EmitPhysicalRecord(type, ptr, fragment_length); ptr += fragment_length; left -= fragment_length; begin = false; } while (s.ok() \u0026amp;\u0026amp; left \u0026gt; 0); //循环至数据完全写入或者写入失败时才停止  return s; }   写入流程如下：\n 判断头部能否完整写入，如果不能则将剩余空间用 \\x00 填充，接着从新的块开始写入。 根据 begin 和 end 判断记录类型。 将数据按照格式写入，并刷新到磁盘文件中。 循环至数据完全写入或者写入失败后停止，将结果返回。  读取 Log 的读取主要由 Reader 中的 ReadRecord 实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148  // https://github.com/google/leveldb/blob/master/db/log_reader.h  class Reader { public: // Interface for reporting errors.  class Reporter { public: virtual ~Reporter(); virtual void Corruption(size_t bytes, const Status\u0026amp; status) = 0; }; Reader(SequentialFile* file, Reporter* reporter, bool checksum, uint64_t initial_offset); Reader(const Reader\u0026amp;) = delete; Reader\u0026amp; operator=(const Reader\u0026amp;) = delete; ~Reader(); bool ReadRecord(Slice* record, std::string* scratch); uint64_t LastRecordOffset(); private: enum { kEof = kMaxRecordType + 1, kBadRecord = kMaxRecordType + 2 }; }; // https://github.com/google/leveldb/blob/master/db/log_reader.cc  bool Reader::ReadRecord(Slice* record, std::string* scratch) { if (last_record_offset_ \u0026lt; initial_offset_) { if (!SkipToInitialBlock()) { return false; } } scratch-\u0026gt;clear(); record-\u0026gt;clear(); bool in_fragmented_record = false; uint64_t prospective_record_offset = 0; Slice fragment; while (true) { //ReadPhysicalRecord读取log文件并将记录保存到fragment，同时返回记录的类型  const unsigned int record_type = ReadPhysicalRecord(\u0026amp;fragment); uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size(); if (resyncing_) { if (record_type == kMiddleType) { continue; } else if (record_type == kLastType) { resyncing_ = false; continue; } else { resyncing_ = false; } } //根据记录的类型来判断是否需要将当前记录附加到scratch后并继续读取  switch (record_type) { //类型为kFullType则说明当前是完整的记录，直接赋值给record后返回  case kFullType: if (in_fragmented_record) { if (!scratch-\u0026gt;empty()) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;partial record without end(1)\u0026#34;); } } prospective_record_offset = physical_record_offset; scratch-\u0026gt;clear(); *record = fragment; last_record_offset_ = prospective_record_offset; return true; //类型为kFirstType则说明当前是第一部分，先将记录复制到scratch后继续读取  case kFirstType: if (in_fragmented_record) { if (!scratch-\u0026gt;empty()) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;partial record without end(2)\u0026#34;); } } prospective_record_offset = physical_record_offset; scratch-\u0026gt;assign(fragment.data(), fragment.size()); in_fragmented_record = true; break; //类型为kMiddleType则说明当前是中间部分，先将记录追加到scratch后继续读取  case kMiddleType: //初始读取到的类型为kMiddleType或者kLastType，则需要忽略并且继续偏移  if (!in_fragmented_record) { ReportCorruption(fragment.size(), \u0026#34;missing start of fragmented record(1)\u0026#34;); } else { scratch-\u0026gt;append(fragment.data(), fragment.size()); } break; //类型为kLastType则说明当前为最后，继续追加到scratch，并将scratch赋值给record并返回  case kLastType: if (!in_fragmented_record) { ReportCorruption(fragment.size(), \u0026#34;missing start of fragmented record(2)\u0026#34;); } else { scratch-\u0026gt;append(fragment.data(), fragment.size()); *record = Slice(*scratch); last_record_offset_ = prospective_record_offset; return true; } break; //如果状态为kEof、kBadRecord时说明日志损坏，此时清空scratch并返回false  case kEof: if (in_fragmented_record) { scratch-\u0026gt;clear(); } return false; case kBadRecord: if (in_fragmented_record) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;error in middle of record\u0026#34;); in_fragmented_record = false; scratch-\u0026gt;clear(); } break; //未定义的类型，输出日志，剩余同上处理  default: { char buf[40]; std::snprintf(buf, sizeof(buf), \u0026#34;unknown record type %u\u0026#34;, record_type); ReportCorruption( (fragment.size() + (in_fragmented_record ? scratch-\u0026gt;size() : 0)), buf); in_fragmented_record = false; scratch-\u0026gt;clear(); break; } } } return false; }   执行流程如下：\n ReadRecord 读取一条记录到 fragment 变量中，并且返回该条记录的类型。 根据记录的类型来判断是否需要将当前记录附加到 scratch 后并继续读取：  kFullType：当前是完整的记录，直接赋值给 record 后返回。 kFirstType：当前是第一部分，先将记录覆盖到 scratch 后继续读取。 kMiddleType：当前是中间部分，先将记录追加到 scratch 后继续读取。 kLastType：当前为最后部分，继续追加到 scratch，并将完整的 scratch 赋值给 record 后返回。 其它/异常：清空 scratch 并返回 false，如果是未定义类型需要输出日志。    这里还有一个需要注意的细节，由于读取 Log 文件时可以从指定偏移量开始，所以如果初始读取到的类型为 kMiddleType 或者 kLastType，则需要忽略并且继续偏移，直到碰见第一个 kFirstType。\n崩溃恢复 当打开一个 LevelDB 的数据文件时，需先检验是否进行崩溃恢复，如果需要，则会从 Log 文件生成一个MemTable，其实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::RecoverLogFile(uint64_t log_number, bool last_log, bool* save_manifest, VersionEdit* edit, SequenceNumber* max_sequence) { struct LogReporter : public log::Reader::Reporter { Env* env; Logger* info_log; const char* fname; Status* status; // null if options_.paranoid_checks==false  void Corruption(size_t bytes, const Status\u0026amp; s) override { Log(info_log, \u0026#34;%s%s: dropping %d bytes; %s\u0026#34;, (this-\u0026gt;status == nullptr ? \u0026#34;(ignoring error) \u0026#34; : \u0026#34;\u0026#34;), fname, static_cast\u0026lt;int\u0026gt;(bytes), s.ToString().c_str()); if (this-\u0026gt;status != nullptr \u0026amp;\u0026amp; this-\u0026gt;status-\u0026gt;ok()) *this-\u0026gt;status = s; } }; mutex_.AssertHeld(); //打开log文件  std::string fname = LogFileName(dbname_, log_number); SequentialFile* file; Status status = env_-\u0026gt;NewSequentialFile(fname, \u0026amp;file); if (!status.ok()) { MaybeIgnoreError(\u0026amp;status); return status; } //创建log reader.  LogReporter reporter; reporter.env = env_; reporter.info_log = options_.info_log; reporter.fname = fname.c_str(); reporter.status = (options_.paranoid_checks ? \u0026amp;status : nullptr); log::Reader reader(file, \u0026amp;reporter, true /*checksum*/, 0 /*initial_offset*/); Log(options_.info_log, \u0026#34;Recovering log #%llu\u0026#34;, (unsigned long long)log_number); //读取所有的records并写入一个memtable  std::string scratch; Slice record; WriteBatch batch; int compactions = 0; MemTable* mem = nullptr; //循环读取日志文件  while (reader.ReadRecord(\u0026amp;record, \u0026amp;scratch) \u0026amp;\u0026amp; status.ok()) { if (record.size() \u0026lt; 12) { reporter.Corruption(record.size(), Status::Corruption(\u0026#34;log record too small\u0026#34;)); continue; } WriteBatchInternal::SetContents(\u0026amp;batch, record); if (mem == nullptr) { mem = new MemTable(internal_comparator_); mem-\u0026gt;Ref(); } //将records写入memtable  status = WriteBatchInternal::InsertInto(\u0026amp;batch, mem); MaybeIgnoreError(\u0026amp;status); if (!status.ok()) { break; } const SequenceNumber last_seq = WriteBatchInternal::Sequence(\u0026amp;batch) + WriteBatchInternal::Count(\u0026amp;batch) - 1; if (last_seq \u0026gt; *max_sequence) { *max_sequence = last_seq; } //如果memtable大于阈值，则将其转换成sstable(默认4MB)\t if (mem-\u0026gt;ApproximateMemoryUsage() \u0026gt; options_.write_buffer_size) { compactions++; *save_manifest = true; status = WriteLevel0Table(mem, edit, nullptr); mem-\u0026gt;Unref(); mem = nullptr; if (!status.ok()) { break; } } } delete file; //判断是否应该继续重复使用最后一个日志文件  if (status.ok() \u0026amp;\u0026amp; options_.reuse_logs \u0026amp;\u0026amp; last_log \u0026amp;\u0026amp; compactions == 0) { assert(logfile_ == nullptr); assert(log_ == nullptr); assert(mem_ == nullptr); uint64_t lfile_size; if (env_-\u0026gt;GetFileSize(fname, \u0026amp;lfile_size).ok() \u0026amp;\u0026amp; env_-\u0026gt;NewAppendableFile(fname, \u0026amp;logfile_).ok()) { Log(options_.info_log, \u0026#34;Reusing old log %s \\n\u0026#34;, fname.c_str()); log_ = new log::Writer(logfile_, lfile_size); logfile_number_ = log_number; if (mem != nullptr) { mem_ = mem; mem = nullptr; } else { mem_ = new MemTable(internal_comparator_); mem_-\u0026gt;Ref(); } } } if (mem != nullptr) { if (status.ok()) { *save_manifest = true; status = WriteLevel0Table(mem, edit, nullptr); } mem-\u0026gt;Unref(); } return status; }   具体的逻辑如下：\n 打开 log，创建 log reader 开始读取数据。 循环读取日志文件，并将其写入 MemTable 中。 如果 MemTable 过大，则将其转换为 SSTable。  ","date":"2022-05-23T23:42:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-wal%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97/","title":"LevelDB WAL日志模块"},{"content":"SSTable模块 SSTable（Sorted Strings Table，有序字符串表），在各种存储引擎中得到了广泛的使用，包括 LevelDB、HBase、Cassandra 等。SSTable 会根据 Key 进行排序后保存一系列的 K-V 对，这种方式不仅方便进行范围查找，而且便于对 K-V 对进行更加有效的压缩。\nSSTable Format SSTable 文件由一个个块组成，块中可以保存数据、数据索引、元数据或者元数据索引。整体的文件格式如下图：\n如上图，SSTable 文件整体分为 4 个部分：\n Data Block（数据区域）：保存具体的键-值对数据。 Meta Block（元数据区域）：保存元数据，例如布隆过滤器。 Index Block（索引区域）：分为数据索引和元数据索引。  数据索引：数据索引块中的键为前一个数据块的最后一个键（即一个数据块中最大的键，因为键是有序排列保存的）与后一个数据块的第一个键（即一个数据块中的最小键）的最短分隔符。 元数据索引：元数据索引块可指示如何查找该布隆过滤器的数据。   File Footer（尾部）：总大小为48个字节。  BlockHandle BlockHandle在SSTable中是经常使用的一个结构，其定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  // https://github.com/google/leveldb/blob/master/table/format.h  class BlockHandle { public: // Maximum encoding length of a BlockHandle  enum { kMaxEncodedLength = 10 + 10 }; BlockHandle(); // The offset of the block in the file.  uint64_t offset() const { return offset_; } void set_offset(uint64_t offset) { offset_ = offset; } // The size of the stored block  uint64_t size() const { return size_; } void set_size(uint64_t size) { size_ = size; } void EncodeTo(std::string* dst) const; Status DecodeFrom(Slice* input); private: uint64_t offset_; uint64_t size_; };   BlockHandler 本质就是封装了 offset 和 size，用于定位某些区域。\nBlock Format SSTable 中一个块默认大小为 4 KB，由 4 部分组成：\n 键-值对数据：即我们保存到 LevelDB 中的多组键-值对。 重启点数据：最后 4 字节为重启点的个数，前边部分为多个重启点，每个重启点实际保存的是偏移量。并且每个重启点固定占据 4 字节的空间。 压缩类型：在 LevelDB 的 SSTable 中有两种压缩类型：  kNoCompression：没有压缩。 kSnappyCompression：Snappy压缩。   校验数据：4 字节的 CRC 校验字段。  Block读写流程 生成Block 块生成主要在 BlockBuilder 中实现，下面先看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // https://github.com/google/leveldb/blob/master/table/block_builder.h  class BlockBuilder { public: explicit BlockBuilder(const Options* options); BlockBuilder(const BlockBuilder\u0026amp;) = delete; BlockBuilder\u0026amp; operator=(const BlockBuilder\u0026amp;) = delete; void Reset(); void Add(const Slice\u0026amp; key, const Slice\u0026amp; value); Slice Finish(); size_t CurrentSizeEstimate() const; bool empty() const { return buffer_.empty(); } private: const Options* options_; std::string buffer_; // Destination buffer  std::vector\u0026lt;uint32_t\u0026gt; restarts_; // Restart points  int counter_; // Number of entries emitted since restart  bool finished_; // Has Finish() been called?  std::string last_key_; };    成员变量  options_：在 BlockBuilder 类构造函数中传入，表示一些配置选项。 buffer_：块的内容，所有的键-值对都保存到 buffer_ 中。 restarts_：每次开启新的重启点后，会将当前 buffer_ 的数据长度保存到 restarts_ 中，当前 buffer_ 中的数据长度即为每个重启点的偏移量。 counter_：开启新的重启点之后加入的键-值对数量，默认保存 16 个键-值对，之后会开启一个新的重启点。 finished_：指明是否已经调用了 Finish 方法，BlockBuilder 中的 Add 方法会将数据保存到各个成员变量中，而 Finish 方法会依据成员变量的值生成一个块。 last_key_：上一个保存的键，当加入新键时，用来计算和上一个键的共同前缀部分。    介绍完了结构，下面来看具体的生成方法。当需要保存一个键-值对时，需要调用 BlockBuilder 类中的 Add 方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  // https://github.com/google/leveldb/blob/master/table/block_builder.cc  void BlockBuilder::Add(const Slice\u0026amp; key, const Slice\u0026amp; value) { //保存上一个加入的key  Slice last_key_piece(last_key_); assert(!finished_); assert(counter_ \u0026lt;= options_-\u0026gt;block_restart_interval); assert(buffer_.empty() // No values yet?  || options_-\u0026gt;comparator-\u0026gt;Compare(key, last_key_piece) \u0026gt; 0); size_t shared = 0; //判断counter_是否大于block_restart_interval  if (counter_ \u0026lt; options_-\u0026gt;block_restart_interval) { const size_t min_length = std::min(last_key_piece.size(), key.size()); //计算相同前缀的长度  while ((shared \u0026lt; min_length) \u0026amp;\u0026amp; (last_key_piece[shared] == key[shared])) { shared++; } } else { //如果键-值对数量超过block_restart_interval，则开启新的重启点，清空计数器  restarts_.push_back(buffer_.size()); counter_ = 0; } const size_t non_shared = key.size() - shared; //将共同前缀长度、非共享部分长度、值长度追加到buffer_中  PutVarint32(\u0026amp;buffer_, shared); PutVarint32(\u0026amp;buffer_, non_shared); PutVarint32(\u0026amp;buffer_, value.size()); //将key的非共享数据追加到buffer_中_  buffer_.append(key.data() + shared, non_shared); //将Value数据追加到buffer_中  buffer_.append(value.data(), value.size()); //更新状态  last_key_.resize(shared); last_key_.append(key.data() + shared, non_shared); assert(Slice(last_key_) == key); counter_++; }   执行流程如下：\n1. 判断 counter_ 是否大于 block_restart_interval，如果大于，则开启新的重启点，清空计数器并保存 buffer_ 中数据长度的值（该值即每个重启点的偏移量）压到 restarts_ 数组中。\r2. 如果 counter_ 未超出配置的每个重启点可以保存的键-值对数值，则计算当前键和上一次保存键的共同前缀，然后将键-值对按格式保存到 buffer_ 中。\r3. 更新状态，将 last_key_ 置为当前保存的 key，并且将 counter_ 加 1。\r 从上面的代码中可以看出，Add 中将所有的键-值对按格式保存到成员变量 buffer_ 中。实际生成 Block 的其实是 Finish 。代码如下：\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/table/block_builder.cc  Slice BlockBuilder::Finish() { //将重启点偏移量写入buffer_中  for (size_t i = 0; i \u0026lt; restarts_.size(); i++) { PutFixed32(\u0026amp;buffer_, restarts_[i]); } PutFixed32(\u0026amp;buffer_, restarts_.size()); finished_ = true; return Slice(buffer_); }   Finish  首先将所有重启点偏移量的值依次以 4 字节大小追加到 buffer_ 字符串，最后将重启点个数继续以 4 字节大小追加到 buffer_ 后部，此时返回的结果就是一个完整的 Block。\n读取Block 读取 Block 由 Block 类实现，其主要依靠 NewIterator 生成一个 Block 迭代器，再借助迭代器的 Seek 来查找对应的 Key。首先来看看 Block 的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // https://github.com/google/leveldb/blob/master/table/block.h  class Block { public: // Initialize the block with the specified contents.  explicit Block(const BlockContents\u0026amp; contents); Block(const Block\u0026amp;) = delete; Block\u0026amp; operator=(const Block\u0026amp;) = delete; ~Block(); size_t size() const { return size_; } Iterator* NewIterator(const Comparator* comparator); private: class Iter; uint32_t NumRestarts() const; const char* data_; size_t size_; uint32_t restart_offset_; // Offset in data_ of restart array  bool owned_; // Block owns data_[] };   读取一个块通过在 NewIterator 中生成一个迭代器来实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/table/block.cc  Iterator* Block::NewIterator(const Comparator* comparator) { if (size_ \u0026lt; sizeof(uint32_t)) { return NewErrorIterator(Status::Corruption(\u0026#34;bad block contents\u0026#34;)); } const uint32_t num_restarts = NumRestarts(); if (num_restarts == 0) { return NewEmptyIterator(); } else { return new Iter(comparator, data_, restart_offset_, num_restarts); } }   这里的逻辑比较简单，就不多作介绍了。\n核心的查找逻辑主要是迭代器中的 Seek 下面直接看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/table/block.cc  void Seek(const Slice\u0026amp; target) override { uint32_t left = 0; uint32_t right = num_restarts_ - 1; int current_key_compare = 0; if (Valid()) { current_key_compare = Compare(key_, target); if (current_key_compare \u0026lt; 0) { left = restart_index_; } else if (current_key_compare \u0026gt; 0) { right = restart_index_; } else { return; } } //通过重启点进行二分查找  while (left \u0026lt; right) { uint32_t mid = (left + right + 1) / 2; uint32_t region_offset = GetRestartPoint(mid); uint32_t shared, non_shared, value_length; const char* key_ptr = DecodeEntry(data_ + region_offset, data_ + restarts_, \u0026amp;shared, \u0026amp;non_shared, \u0026amp;value_length); if (key_ptr == nullptr || (shared != 0)) { CorruptionError(); return; } Slice mid_key(key_ptr, non_shared); //如果key小于target，则将left置为mid  if (Compare(mid_key, target) \u0026lt; 0) { left = mid; //如果key大于等于target，则将right置为mid-1  } else { right = mid - 1; } } assert(current_key_compare == 0 || Valid()); //在块中线性查找，依次遍历每一个K-V对，将key与target对比，直到找到第一个大于等于target的后返  bool skip_seek = left == restart_index_ \u0026amp;\u0026amp; current_key_compare \u0026lt; 0; if (!skip_seek) { SeekToRestartPoint(left); } while (true) { if (!ParseNextKey()) { return; } if (Compare(key_, target) \u0026gt;= 0) { return; } } }   查找逻辑如下：\n1. 对重启点数组进行二分查找，找到可能包含数据的重启点。\r2. 在块中线性查找，依次遍历每一个 K-V 对，将 key 与 target 对比。\r3. 找到第一个 key 大于等于 target 的后将该 K-V 对存储后返回。\r SSTable读写 生成SSTable SSTable 的生成主要在 TableBuilder 中实现，下面先看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  // https://github.com/google/leveldb/blob/master/include/leveldb/table_builder.h  class LEVELDB_EXPORT TableBuilder { public: TableBuilder(const Options\u0026amp; options, WritableFile* file); TableBuilder(const TableBuilder\u0026amp;) = delete; TableBuilder\u0026amp; operator=(const TableBuilder\u0026amp;) = delete; ~TableBuilder(); Status ChangeOptions(const Options\u0026amp; options); void Add(const Slice\u0026amp; key, const Slice\u0026amp; value); void Flush(); Status status() const; Status Finish(); void Abandon(); uint64_t NumEntries() const; uint64_t FileSize() const; private: bool ok() const { return status().ok(); } void WriteBlock(BlockBuilder* block, BlockHandle* handle); void WriteRawBlock(const Slice\u0026amp; data, CompressionType, BlockHandle* handle); struct Rep; Rep* rep_; };   在介绍该 TableBuilder 的核心逻辑之前，首先我们要看看里面的一个结构体 Rep。其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  struct TableBuilder::Rep { Rep(const Options\u0026amp; opt, WritableFile* f) : options(opt), index_block_options(opt), file(f), offset(0), data_block(\u0026amp;options), index_block(\u0026amp;index_block_options), num_entries(0), closed(false), filter_block(opt.filter_policy == nullptr ? nullptr : new FilterBlockBuilder(opt.filter_policy)), pending_index_entry(false) { index_block_options.block_restart_interval = 1; } Options options; Options index_block_options; WritableFile* file; uint64_t offset; Status status; BlockBuilder data_block; BlockBuilder index_block; std::string last_key; int64_t num_entries; bool closed; // Either Finish() or Abandon() has been called.  FilterBlockBuilder* filter_block; bool pending_index_entry; BlockHandle pending_handle; // Handle to add to index block  std::string compressed_output; };   我们需要注意的关键变量如下：\n file：SSTable 生成的文件。 data_block：用于生成 SSTable 的数据区域。 index_block：用于生成 SSTable 的索引区域。 pending_index_entry：决定是否需要写数据索引。 **pending_handle：**写数据索引的方法。SSTable中每次完整写入一个块后需要生成该块的索引，索引中的键是当前块最大键与即将插入的键的最短分隔符，例如一个块中最大键为 abceg，即将插入的键为 abcqddh，则二者之间的最小分隔符为 abcf。  了解完结构后，接着就看看生成 SSTable 的核心函数 Add 和 Finish。\n首先来看 Add，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  void TableBuilder::Add(const Slice\u0026amp; key, const Slice\u0026amp; value) { Rep* r = rep_; assert(!r-\u0026gt;closed); if (!ok()) return; if (r-\u0026gt;num_entries \u0026gt; 0) { assert(r-\u0026gt;options.comparator-\u0026gt;Compare(key, Slice(r-\u0026gt;last_key)) \u0026gt; 0); } //判断是否需要写入数据索引块中  if (r-\u0026gt;pending_index_entry) { assert(r-\u0026gt;data_block.empty()); //找到最短分隔符，即大于等于上一个块最大的键，小于下一个块最小的键  r-\u0026gt;options.comparator-\u0026gt;FindShortestSeparator(\u0026amp;r-\u0026gt;last_key, key); std::string handle_encoding; r-\u0026gt;pending_handle.EncodeTo(\u0026amp;handle_encoding); //在数据索引块中写入key和BlockHandle  r-\u0026gt;index_block.Add(r-\u0026gt;last_key, Slice(handle_encoding)); r-\u0026gt;pending_index_entry = false; } //写入元数据块中  if (r-\u0026gt;filter_block != nullptr) { r-\u0026gt;filter_block-\u0026gt;AddKey(key); } //修改last_key为当前要插入的key  r-\u0026gt;last_key.assign(key.data(), key.size()); r-\u0026gt;num_entries++; //写入数据块中  r-\u0026gt;data_block.Add(key, value); //判断数据块大小是否大于配置的块大小，如果大于则调用Flush写入SSTable文件并刷新到硬盘  const size_t estimated_block_size = r-\u0026gt;data_block.CurrentSizeEstimate(); if (estimated_block_size \u0026gt;= r-\u0026gt;options.block_size) { Flush(); } }   Add 主要就是调用生成数据块与数据索引块的方法 BlockBuilder::Add 以及生成元数据块的方法 FilterBlockBuilder::Add 依次将键值对加入数据索引块、元数据块以及数据块。\n可以看到，最后会判断数据块大小是否大于配置的块大小，如果大于则调用 Flush 写入 SSTable 文件并刷新到硬盘中。我们接着来看看 Flush 的执行逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  void TableBuilder::Flush() { Rep* r = rep_; assert(!r-\u0026gt;closed); if (!ok()) return; if (r-\u0026gt;data_block.empty()) return; assert(!r-\u0026gt;pending_index_entry); //写入数据块  WriteBlock(\u0026amp;r-\u0026gt;data_block, \u0026amp;r-\u0026gt;pending_handle); if (ok()) { //将pending_index_entry修改为true，表明下一次将写入数据索引块。  r-\u0026gt;pending_index_entry = true; //将文件刷新到磁盘\t r-\u0026gt;status = r-\u0026gt;file-\u0026gt;Flush(); } if (r-\u0026gt;filter_block != nullptr) { r-\u0026gt;filter_block-\u0026gt;StartBlock(r-\u0026gt;offset); } }   执行逻辑如下：\n1. 将数据写入数据块中。\r2. 将 pending_index_entry 修改为 true，表明下一次调用 `Add` 时将写入数据索引块。\r3. 将文件刷新到磁盘中。\r 介绍完了 Add，下面来看看 Finish。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  Status TableBuilder::Finish() { Rep* r = rep_; //写入SSTable文件并刷新到硬盘中  Flush(); assert(!r-\u0026gt;closed); r-\u0026gt;closed = true; BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle; //写入元数据块  if (ok() \u0026amp;\u0026amp; r-\u0026gt;filter_block != nullptr) { WriteRawBlock(r-\u0026gt;filter_block-\u0026gt;Finish(), kNoCompression, \u0026amp;filter_block_handle); } //写入元数据块索引  if (ok()) { BlockBuilder meta_index_block(\u0026amp;r-\u0026gt;options); if (r-\u0026gt;filter_block != nullptr) { std::string key = \u0026#34;filter.\u0026#34;; key.append(r-\u0026gt;options.filter_policy-\u0026gt;Name()); std::string handle_encoding; filter_block_handle.EncodeTo(\u0026amp;handle_encoding); meta_index_block.Add(key, handle_encoding); } WriteBlock(\u0026amp;meta_index_block, \u0026amp;metaindex_block_handle); } //写入数据块索引  if (ok()) { if (r-\u0026gt;pending_index_entry) { r-\u0026gt;options.comparator-\u0026gt;FindShortSuccessor(\u0026amp;r-\u0026gt;last_key); std::string handle_encoding; r-\u0026gt;pending_handle.EncodeTo(\u0026amp;handle_encoding); r-\u0026gt;index_block.Add(r-\u0026gt;last_key, Slice(handle_encoding)); r-\u0026gt;pending_index_entry = false; } WriteBlock(\u0026amp;r-\u0026gt;index_block, \u0026amp;index_block_handle); } //写入尾部  if (ok()) { Footer footer; footer.set_metaindex_handle(metaindex_block_handle); footer.set_index_handle(index_block_handle); std::string footer_encoding; footer.EncodeTo(\u0026amp;footer_encoding); r-\u0026gt;status = r-\u0026gt;file-\u0026gt;Append(footer_encoding); if (r-\u0026gt;status.ok()) { r-\u0026gt;offset += footer_encoding.size(); } } return r-\u0026gt;status; }   Finish 会按照我们一开始给出的 SSTable 的格式，将数据分别写入数据块、数据块索引、元数据块、元数据块索引、尾部。最后生成 SSTable。\n读取SSTable 介绍完了写入，我们再来看看读取。SSTable 读取的逻辑与 Block 类似，都是借助于迭代器实现的。主要实现代码在 Table 类中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // https://github.com/google/leveldb/blob/master/include/leveldb/table.h  class LEVELDB_EXPORT Table { public: static Status Open(const Options\u0026amp; options, RandomAccessFile* file, uint64_t file_size, Table** table); Table(const Table\u0026amp;) = delete; Table\u0026amp; operator=(const Table\u0026amp;) = delete; ~Table(); Iterator* NewIterator(const ReadOptions\u0026amp;) const; uint64_t ApproximateOffsetOf(const Slice\u0026amp; key) const; private: friend class TableCache; struct Rep; static Iterator* BlockReader(void*, const ReadOptions\u0026amp;, const Slice\u0026amp;); explicit Table(Rep* rep) : rep_(rep) {} Status InternalGet(const ReadOptions\u0026amp;, const Slice\u0026amp; key, void* arg, void (*handle_result)(void* arg, const Slice\u0026amp; k, const Slice\u0026amp; v)); void ReadMeta(const Footer\u0026amp; footer); void ReadFilter(const Slice\u0026amp; filter_handle_value); Rep* const rep_; };   这里我们主要关注生成迭代器的 NewIterator 和 第二层生成迭代器的 BlockReader。\n首先看 NewIterator 的代码：\n1 2 3 4 5 6 7  // https://github.com/google/leveldb/blob/master/table/table.cc  Iterator* Table::NewIterator(const ReadOptions\u0026amp; options) const { return NewTwoLevelIterator( rep_-\u0026gt;index_block-\u0026gt;NewIterator(rep_-\u0026gt;options.comparator), \u0026amp;Table::BlockReader, const_cast\u0026lt;Table*\u0026gt;(this), options); }   这里返回了一个双层迭代器 NewTwoLevelIterator。第一层为数据索引块的迭代器，即 rep_-\u0026gt;index_block-\u0026gt;NewIterator。通过第一层的数据索引块迭代器查找一个键应该属于的块，然后通过第二层迭代器去读取这个块并查找该键。\n接着看生成第二层块迭代器的 BlockReader：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  // https://github.com/google/leveldb/blob/master/table/table.cc  Iterator* Table::BlockReader(void* arg, const ReadOptions\u0026amp; options, const Slice\u0026amp; index_value) { Table* table = reinterpret_cast\u0026lt;Table*\u0026gt;(arg); Cache* block_cache = table-\u0026gt;rep_-\u0026gt;options.block_cache; Block* block = nullptr; Cache::Handle* cache_handle = nullptr; BlockHandle handle; Slice input = index_value; Status s = handle.DecodeFrom(\u0026amp;input); if (s.ok()) { //contents保存一个块的内容  BlockContents contents; if (block_cache != nullptr) { char cache_key_buffer[16]; EncodeFixed64(cache_key_buffer, table-\u0026gt;rep_-\u0026gt;cache_id); EncodeFixed64(cache_key_buffer + 8, handle.offset()); Slice key(cache_key_buffer, sizeof(cache_key_buffer)); cache_handle = block_cache-\u0026gt;Lookup(key); if (cache_handle != nullptr) { block = reinterpret_cast\u0026lt;Block*\u0026gt;(block_cache-\u0026gt;Value(cache_handle)); } else { //通过BlockHandle存储的偏移量和大小读取一个块的数据到contents  s = ReadBlock(table-\u0026gt;rep_-\u0026gt;file, options, handle, \u0026amp;contents); if (s.ok()) { //根据contents生成一个Block结构  block = new Block(contents); if (contents.cachable \u0026amp;\u0026amp; options.fill_cache) { cache_handle = block_cache-\u0026gt;Insert(key, block, block-\u0026gt;size(), \u0026amp;DeleteCachedBlock); } } } } else { s = ReadBlock(table-\u0026gt;rep_-\u0026gt;file, options, handle, \u0026amp;contents); if (s.ok()) { block = new Block(contents); } } } Iterator* iter; if (block != nullptr) { //生成块迭代器，通过该迭代器读取数据  iter = block-\u0026gt;NewIterator(table-\u0026gt;rep_-\u0026gt;options.comparator); if (cache_handle == nullptr) { iter-\u0026gt;RegisterCleanup(\u0026amp;DeleteBlock, block, nullptr); } else { iter-\u0026gt;RegisterCleanup(\u0026amp;ReleaseBlock, block_cache, cache_handle); } } else { iter = NewErrorIterator(s); } return iter; }   SSTable 的读取先通过第一层迭代器（即数据索引）获取到一个键需要查找的块位置，读取该块的内容并且构造第二层迭代器遍历该块，通过两层迭代器即可在 SSTable 中进行键的查找。\n布隆过滤器 如果在 LevelDB 中查找某个不存在的键，必须先检查内存表 MemTable，然后逐层查找，为了优化这种读取，LevelDB 中会使用布隆过滤器。当布隆过滤器判定键不存在时，可以直接返回，无须继续查找。\n这里就不过多介绍原理，如果想了解可以看看我的往期博客 海量数据处理（一） ：位图与布隆过滤器的概念以及实现\n实现 布隆过滤器继承了 LevelDB 中抽象出的过滤器纯虚类 FilterPolicy，其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/include/leveldb/filter_policy.h  class LEVELDB_EXPORT FilterPolicy { public: virtual ~FilterPolicy(); virtual const char* Name() const = 0; virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const = 0; virtual bool KeyMayMatch(const Slice\u0026amp; key, const Slice\u0026amp; filter) const = 0; };   FilterPolicy 定义了几个接口：\n Name：返回过滤器名称。 CreateFilter：将一个字符串加入过滤器中。 KeyMayMatch：通过过滤器内容判断一个元素是否存在。  接下来我们看看 BloomFilterPolicy 是如何实现这些接口的，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  // https://github.com/google/leveldb/blob/master/util/bloom.cc  class BloomFilterPolicy : public FilterPolicy { public: explicit BloomFilterPolicy(int bits_per_key) : bits_per_key_(bits_per_key) { k_ = static_cast\u0026lt;size_t\u0026gt;(bits_per_key * 0.69); // 0.69 =~ ln(2)  if (k_ \u0026lt; 1) k_ = 1; if (k_ \u0026gt; 30) k_ = 30; } const char* Name() const override { return \u0026#34;leveldb.BuiltinBloomFilter2\u0026#34;; } void CreateFilter(const Slice* keys, int n, std::string* dst) const override { size_t bits = n * bits_per_key_; if (bits \u0026lt; 64) bits = 64; size_t bytes = (bits + 7) / 8; bits = bytes * 8; const size_t init_size = dst-\u0026gt;size(); dst-\u0026gt;resize(init_size + bytes, 0); dst-\u0026gt;push_back(static_cast\u0026lt;char\u0026gt;(k_)); // Remember # of probes in filter  char* array = \u0026amp;(*dst)[init_size]; //依次处理每一个Key  for (int i = 0; i \u0026lt; n; i++) { uint32_t h = BloomHash(keys[i]); //通过位运算获取delta  const uint32_t delta = (h \u0026gt;\u0026gt; 17) | (h \u0026lt;\u0026lt; 15); // Rotate right 17 bits  //计算哈希值，对每一个key计算k次哈希值（这里采用对每次计算出的的哈希值增加delta来模拟）  for (size_t j = 0; j \u0026lt; k_; j++) { const uint32_t bitpos = h % bits; array[bitpos / 8] |= (1 \u0026lt;\u0026lt; (bitpos % 8)); h += delta; } } } bool KeyMayMatch(const Slice\u0026amp; key, const Slice\u0026amp; bloom_filter) const override { const size_t len = bloom_filter.size(); if (len \u0026lt; 2) return false; const char* array = bloom_filter.data(); const size_t bits = (len - 1) * 8; const size_t k = array[len - 1]; if (k \u0026gt; 30) { return true; } //计算出key的哈希  uint32_t h = BloomHash(key); const uint32_t delta = (h \u0026gt;\u0026gt; 17) | (h \u0026lt;\u0026lt; 15); // Rotate right 17 bits  //判断对应位置是否全为1，如果有任何一个为0说明数据不可能存在布隆过滤器中，返回false，否则true  for (size_t j = 0; j \u0026lt; k; j++) { const uint32_t bitpos = h % bits; if ((array[bitpos / 8] \u0026amp; (1 \u0026lt;\u0026lt; (bitpos % 8))) == 0) return false; h += delta; } return true; } private: size_t bits_per_key_; size_t k_; };   具体逻辑都标识在了注释中，就不多说了，这里提一下这里用到的一个哈希小技巧：\n 为了避免哈希冲突，大部分布隆过滤器中都会采用多种哈希算法来计算。LevelDB 为了简化规则，使用位运算 计算出 delta，对每轮计算出的哈希值累加上 delta，模拟多轮哈希计算。  应用 LevelDB 中具体使用布隆过滤器时又封装了两个类，分别为 FilterBlockBuilder 和 FilterBlockReader。\nFilterBlockBuilder 的主要功能是通过调用 BloomFilterPolicy 的 CreateFilter 方法生成布隆过滤器，并且将布隆过滤器的内容写入 SSTable 的元数据块。其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // https://github.com/google/leveldb/blob/master/table/filter_block.h  class FilterBlockBuilder { public: explicit FilterBlockBuilder(const FilterPolicy*); FilterBlockBuilder(const FilterBlockBuilder\u0026amp;) = delete; FilterBlockBuilder\u0026amp; operator=(const FilterBlockBuilder\u0026amp;) = delete; void StartBlock(uint64_t block_offset); void AddKey(const Slice\u0026amp; key); Slice Finish(); private: void GenerateFilter(); const FilterPolicy* policy_; std::string keys_; // Flattened key contents  std::vector\u0026lt;size_t\u0026gt; start_; // Starting index in keys_ of each key  std::string result_; // Filter data computed so far  std::vector\u0026lt;Slice\u0026gt; tmp_keys_; // policy_-\u0026gt;CreateFilter() argument  std::vector\u0026lt;uint32_t\u0026gt; filter_offsets_; };    成员变量  policy_：实现了 FilterPolicy 接口的类，在布隆过滤器中为 BloomFilterPolicy。 keys_：生成布隆过滤器的键。 start_：数组类型，保存 keys_ 参数中每一个键的开始索引。 result_：保存生成的布隆过滤器内容。 tmp_keys_：生成布隆过滤器时，会通过 keys_ 和 start_ 拆分出每一个键，将拆分出的每一个键保存到 tmp_keys_ 数组中。 filter_offsets_：过滤器偏移量，即每一个过滤器在元数据块中的偏移量。    写入的核心逻辑主要在 AddKey 和 Finish 中，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // https://github.com/google/leveldb/blob/master/table/filter_block.cc  void FilterBlockBuilder::AddKey(const Slice\u0026amp; key) { Slice k = key; //记录key的索引位置  start_.push_back(keys_.size()); //将key追加到该字符串中  keys_.append(k.data(), k.size()); } Slice FilterBlockBuilder::Finish() { //写入内容  if (!start_.empty()) { GenerateFilter(); } //写入偏移量  const uint32_t array_offset = result_.size(); for (size_t i = 0; i \u0026lt; filter_offsets_.size(); i++) { PutFixed32(\u0026amp;result_, filter_offsets_[i]); } //写入总大小  PutFixed32(\u0026amp;result_, array_offset); //写入基数  result_.push_back(kFilterBaseLg); // Save encoding parameter in result  return Slice(result_); }   首先调用 AddKey 记录 Key 的索引位置，并将 Key 追加到 Keys_中。接着，调用 Finish 生成一个元数据块，并分别写入布隆过滤器的内容、偏移量、内容总大小和基数。\nFilterBlockReader 用于查找一个元素是否在一个布隆过滤器中，其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/table/filter_block.h  class FilterBlockReader { public: // REQUIRES: \u0026#34;contents\u0026#34; and *policy must stay live while *this is live.  FilterBlockReader(const FilterPolicy* policy, const Slice\u0026amp; contents); bool KeyMayMatch(uint64_t block_offset, const Slice\u0026amp; key); private: const FilterPolicy* policy_; const char* data_; // Pointer to filter data (at block-start)  const char* offset_; // Pointer to beginning of offset array (at block-end)  size_t num_; // Number of entries in offset array  size_t base_lg_; // Encoding parameter (see kFilterBaseLg in .cc file) };    成员变量  policy_：实现了 FilterPolicy 接口的类，在布隆过滤器中为 BloomFilterPolicy。 data_：指向元数据块的开始位置。 offset_：指向元数据块中过滤器偏移量的开始位置。 num_：过滤器偏移量的个数。 base_lg_：过滤器基数。    FilterBlockReader 中的 KeyMayMatch 根据数据块偏移量找到对应的过滤器内容，然后调用 BloomFilterPolicy 中的 KeyMayMatch 方法判断一个元素是否在该过滤器之中。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/table/filter_block.cc  bool FilterBlockReader::KeyMayMatch(uint64_t block_offset, const Slice\u0026amp; key) { uint64_t index = block_offset \u0026gt;\u0026gt; base_lg_; if (index \u0026lt; num_) { uint32_t start = DecodeFixed32(offset_ + index * 4); uint32_t limit = DecodeFixed32(offset_ + index * 4 + 4); if (start \u0026lt;= limit \u0026amp;\u0026amp; limit \u0026lt;= static_cast\u0026lt;size_t\u0026gt;(offset_ - data_)) { Slice filter = Slice(data_ + start, limit - start); return policy_-\u0026gt;KeyMayMatch(key, filter); } else if (start == limit) { // Empty filters do not match any keys  return false; } }   LRU Cache 我们希望经常使用的 SSTable 内容尽量保存在内存中，但如果磁盘中的 SSTable 文件的总大小大于服务器内存大小，或者需要控制 LevelDB 的内存总占用量时，就需要使用 LRU（least recentlyused）Cache 来管理内存。LRU 是一种缓存置换策略，根据该策略不仅可以管理内存的占用量，还可以将热数据尽量保存到内存中，以加快读取速度。\n内存是有限并且昂贵的资源，因此 LevelDB 通过 LRU 策略管理读取到内存的数据。LRU 基于这样一种假设：如果一个资源最近没有或者很少被使用到，那么将来也会很少甚至不被使用。因此如果内存不足，需要淘汰数据时，可以根据 LRU 策略来执行。\n这里就不过多介绍原理，如果想了解可以看看我的往期博客 高级数据结构与算法 | LRU缓存机制（Least Recently Used）\n结构 为了方便拓展其他的缓存置换算法（目前仅实现了 LRU），LevelDB 抽象出了一个 Cache 纯虚类，结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  // https://github.com/google/leveldb/blob/master/include/leveldb/cache.h  class LEVELDB_EXPORT Cache { public: Cache() = default; Cache(const Cache\u0026amp;) = delete; Cache\u0026amp; operator=(const Cache\u0026amp;) = delete; virtual ~Cache(); struct Handle {}; virtual Handle* Insert(const Slice\u0026amp; key, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)) = 0; virtual Handle* Lookup(const Slice\u0026amp; key) = 0; virtual void Release(Handle* handle) = 0; virtual void* Value(Handle* handle) = 0; virtual void Erase(const Slice\u0026amp; key) = 0; virtual uint64_t NewId() = 0; virtual void Prune() {} virtual size_t TotalCharge() const = 0; private: void LRU_Remove(Handle* e); void LRU_Append(Handle* e); void Unref(Handle* e); struct Rep; Rep* rep_; };   LevelDB 中 LRU 由 LRUCache 实现，并且 LRU 节点由 LRUHandle 实现。首先我们来看看 LRUHandle：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/util/cache.cc  struct LRUHandle { void* value; void (*deleter)(const Slice\u0026amp;, void* value); LRUHandle* next_hash; LRUHandle* next;\tLRUHandle* prev; size_t charge; // TODO(opt): Only allow uint32_t?  size_t key_length; bool in_cache; // Whether entry is in the cache.  uint32_t refs; // References, including cache reference, if present.  uint32_t hash; // Hash of key(); used for fast sharding and comparisons  char key_data[1]; // Beginning of key  Slice key() const { assert(next != this); return Slice(key_data, key_length); } };    成员变量  value：具体的值，指针类型。 deleter：自定义回收节点的回调函数。 next_hash：用于 hashtable 冲突时，下一个节点。 next：代表 LRU 中双向链表中下一个节点。 prev：代表 LRU 中双向链表中上一个节点。 charge：记录当前 value 所占用的内存大小，用于后面超出容量后需要进行 lru。 key_length：数据 key 的长度。 in_cache：表示是否在缓存中。 refs：引用计数，因为当前节点可能会被多个组件使用，不能简单的删除。 hash：记录当前 key 的 hash 值。    这个节点设计的非常巧妙，既可以用来当做 LRU 节点，也可以用来当作 hashtable 的节点。\n接着我们来看看 LRUCache 的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  // https://github.com/google/leveldb/blob/master/util/cache.cc  class LRUCache { public: LRUCache(); ~LRUCache(); void SetCapacity(size_t capacity) { capacity_ = capacity; } Cache::Handle* Insert(const Slice\u0026amp; key, uint32_t hash, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)); Cache::Handle* Lookup(const Slice\u0026amp; key, uint32_t hash); void Release(Cache::Handle* handle); void Erase(const Slice\u0026amp; key, uint32_t hash); void Prune(); size_t TotalCharge() const { MutexLock l(\u0026amp;mutex_); return usage_; } private: void LRU_Remove(LRUHandle* e); void LRU_Append(LRUHandle* list, LRUHandle* e); void Ref(LRUHandle* e); void Unref(LRUHandle* e); bool FinishErase(LRUHandle* e) EXCLUSIVE_LOCKS_REQUIRED(mutex_); size_t capacity_; mutable port::Mutex mutex_; size_t usage_ GUARDED_BY(mutex_); LRUHandle lru_ GUARDED_BY(mutex_); LRUHandle in_use_ GUARDED_BY(mutex_); HandleTable table_ GUARDED_BY(mutex_); };   实现 LRU Cache 的实现有如下两处细节需要注意：\n 减小锁的粒度：LRUCache 的并发操作不安全，因此操作时需要加锁。为了减小锁的粒度，LevelDB 中通过哈希将键分为 16 个段，可以理解为有 16 个相同的 LRU Cache 结构，每次进行 Cache 操作时需要先去查找键属于的段。 缓存淘汰：每个LRU Cache结构中有两个成员变量：lru_ 和 in_use_ 。lru_ 双向链表中的节点是可以进行淘汰的，而 in_use_ 双向链表中的节点表示正在使用，因此不可以进行淘汰。  减小锁的粒度 LevelDB 为了减小锁的粒度，封装了一个 ShardedLRUCache，其中有一个大小为 16（默认值） 的 shard_ 成员，shard_ 成员的每个元素均为一个 LRUCache 结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  // ShardedLRUCache class ShardedLRUCache : public Cache { private: LRUCache shard_[kNumShards]; port::Mutex id_mutex_; uint64_t last_id_; static inline uint32_t HashSlice(const Slice\u0026amp; s) { return Hash(s.data(), s.size(), 0); } static uint32_t Shard(uint32_t hash) { return hash \u0026gt;\u0026gt; (32 - kNumShardBits); } public: explicit ShardedLRUCache(size_t capacity) : last_id_(0) { const size_t per_shard = (capacity + (kNumShards - 1)) / kNumShards; for (int s = 0; s \u0026lt; kNumShards; s++) { shard_[s].SetCapacity(per_shard); } } ~ShardedLRUCache() override {} Handle* Insert(const Slice\u0026amp; key, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)) override { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } Handle* Lookup(const Slice\u0026amp; key) override { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } void Release(Handle* handle) override { LRUHandle* h = reinterpret_cast\u0026lt;LRUHandle*\u0026gt;(handle); shard_[Shard(h-\u0026gt;hash)].Release(handle); } void Erase(const Slice\u0026amp; key) override { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } void* Value(Handle* handle) override { return reinterpret_cast\u0026lt;LRUHandle*\u0026gt;(handle)-\u0026gt;value; } uint64_t NewId() override { MutexLock l(\u0026amp;id_mutex_); return ++(last_id_); } void Prune() override { for (int s = 0; s \u0026lt; kNumShards; s++) { shard_[s].Prune(); } } size_t TotalCharge() const override { size_t total = 0; for (int s = 0; s \u0026lt; kNumShards; s++) { total += shard_[s].TotalCharge(); } return total; } };   为了能使哈希值刚好能够映射 shard_ 数组，其通过位操作再次对哈希值进行处理，如下代码：\n1 2 3  // https://github.com/google/leveldb/blob/master/util/cache.cc  static uint32_t Shard(uint32_t hash) { return hash \u0026gt;\u0026gt; (32 - kNumShardBits); }   其将哈希值右移 28 位，只取最高的 4 位，这样就能够保证处理过的哈希值刚好小于 16。\n缓存淘汰 每个LRU Cache结构中有两个成员变量：lru_ 和 in_use_ 。lru_ 双向链表中的节点是可以进行淘汰的，而 in_use_ 双向链表中的节点表示正在使用，因此不可以进行淘汰。如果内存超出限制需要淘汰一个节点时，LevelDB 会将 lru_ 链表中的节点逐个淘汰。\n那我们如何决定节点放置的规则呢？其采用如下规则：如果一个缓存节点的 in_cache 为 true，并且 refs 等于 1，则放置到 lru_ 中；如果 in_cache 为 true，并且 refs 大于等于 2，则放置到 in_use_ 中。\n 对于 in_cache，其会在以下情况变为 false：  删除该节点后。 调用 LRUCache 的析构函数时，会将所有节点的 in_cache 置为 false。 插入一个节点时，如果已经存在一个键值相同的节点，则旧节点的 in_cache 会置为 false。   对于 refs，其变化规则如下:  每次调用 Ref 函数，会将 refs 变量加 1，调用 Unref 函数，会将 refs 变量减 1。 插入一个节点时，该节点会放到 in_use_ 链表中，并且初始的引用计数为 2，不再使用该节点时将引用计数减 1，如果此时节点也不再被其他地方引用，那么引用计数为 1，将其放到 lru_ 链表中。 查找一个节点时，如果查找成功，则调用 Ref，将该节点的引用计数加 1，如果引用计数大于等于 2，会将节点放到 in_use_ 链表中，同理，不再使用该节点时将引用计数减 1，如果此时节点也不再被其他地方引用，那么引用计数为 1，将其放到 lru_ 链表中。    Ref 和 Unref 代码如下：\n1 2 3 4 5 6 7 8 9  // https://github.com/google/leveldb/blob/master/util/cache.cc  void LRUCache::Ref(LRUHandle* e) { if (e-\u0026gt;refs == 1 \u0026amp;\u0026amp; e-\u0026gt;in_cache) { // If on lru_ list, move to in_use_ list.  LRU_Remove(e); LRU_Append(\u0026amp;in_use_, e); } e-\u0026gt;refs++; }   如果缓存节点在 lru_ 链表中（refs 为 1，in_cache 为 true），则首先从 lru_ 链表中删除该节点，然后将节点放到 in_use_ 链表中。最后将节点的 refs 变量加 1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/util/cache.cc  void LRUCache::Unref(LRUHandle* e) { assert(e-\u0026gt;refs \u0026gt; 0); e-\u0026gt;refs--; if (e-\u0026gt;refs == 0) { // Deallocate.  assert(!e-\u0026gt;in_cache); (*e-\u0026gt;deleter)(e-\u0026gt;key(), e-\u0026gt;value); free(e); } else if (e-\u0026gt;in_cache \u0026amp;\u0026amp; e-\u0026gt;refs == 1) { // No longer in use; move to lru_ list.  LRU_Remove(e); LRU_Append(\u0026amp;lru_, e); } }   Unref 首先将节点的 refs 变量减 1，然后判断如果 refs 已经等于 0，则删除并释放该节点，否则，如果 refs 变量等于 1 并且 in_cache 为 true，则将该节点从 in_use_ 链表中删除并且移动到 lru_ 链表中。如果内存超出限制需要淘汰一个节点时，LevelDB 会将 lru_ 链表中的节点逐个淘汰。\n应用 LevelDB 中 Cache 缓存的主要是 SSTable，即缓存节点的键为 8 字节的文件序号，值为一个包含了 SSTable 实例的结构。其主要逻辑由 TableCache 实现，首先我们先来看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  // https://github.com/google/leveldb/blob/master/db/table_cache.h  class TableCache { public: TableCache(const std::string\u0026amp; dbname, const Options\u0026amp; options, int entries); ~TableCache(); Iterator* NewIterator(const ReadOptions\u0026amp; options, uint64_t file_number, uint64_t file_size, Table** tableptr = nullptr); Status Get(const ReadOptions\u0026amp; options, uint64_t file_number, uint64_t file_size, const Slice\u0026amp; k, void* arg, void (*handle_result)(void*, const Slice\u0026amp;, const Slice\u0026amp;)); void Evict(uint64_t file_number); private: Status FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle**); Env* const env_; const std::string dbname_; const Options\u0026amp; options_; Cache* cache_; };     成员变量\n env_：用于读取 SSTable 文件。 dbname_：SSTable 名字。 options_：Cache 参数配置。 cache_：缓存基类句柄。    这里的核心逻辑 FindTable 方法会使用文件序号（file_number）作为键，并且在 cache_ 中查找是否存在该键，如果不存在，则需要打开一个 SSTable，经过处理之后作为值插入 cache_ 中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  Status TableCache::FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle** handle) { Status s; char buf[sizeof(file_number)]; EncodeFixed64(buf, file_number); Slice key(buf, sizeof(buf)); //在缓存中查找key是否存在  *handle = cache_-\u0026gt;Lookup(key); //如果不存在，则打开一个SSTable文件  if (*handle == nullptr) { //生成fname和RandomAccessFile实例  std::string fname = TableFileName(dbname_, file_number); RandomAccessFile* file = nullptr; Table* table = nullptr; s = env_-\u0026gt;NewRandomAccessFile(fname, \u0026amp;file); if (!s.ok()) { std::string old_fname = SSTTableFileName(dbname_, file_number); if (env_-\u0026gt;NewRandomAccessFile(old_fname, \u0026amp;file).ok()) { s = Status::OK(); } } //打开SSTable文件并且生成一个Table实例，并保存到table变量中。  if (s.ok()) { s = Table::Open(options_, file, file_size, \u0026amp;table); } if (!s.ok()) { assert(table == nullptr); delete file; } else { TableAndFile* tf = new TableAndFile; tf-\u0026gt;file = file; tf-\u0026gt;table = table; //以文件序号为key，TableAndFile为Value，插入缓存中。  *handle = cache_-\u0026gt;Insert(key, tf, 1, \u0026amp;DeleteEntry); } } return s; }   SSTable 在 Cache 中缓存时的键为文件序列号，值为一个 TableAndFile 实例，该实例中包括两个成员变量，分别为 file 和 table，查找时通过保存在 table 变量中的 Table 实例迭代器进行查找。\n","date":"2022-05-23T23:41:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-sstable%E6%A8%A1%E5%9D%97/","title":"LevelDB SSTable模块"},{"content":"MemTable模块 MemTable 在 LevelDB 中，MemTable 是底层数据结构 SkipList 的封装。\n结构 首先我们来看看它的结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  // https://github.com/google/leveldb/blob/master/db/memtable.h  class MemTable { public: explicit MemTable(const InternalKeyComparator\u0026amp; comparator); MemTable(const MemTable\u0026amp;) = delete; MemTable\u0026amp; operator=(const MemTable\u0026amp;) = delete; void Ref() { ++refs_; } void Unref() { --refs_; assert(refs_ \u0026gt;= 0); if (refs_ \u0026lt;= 0) { delete this; } } size_t ApproximateMemoryUsage(); Iterator* NewIterator(); void Add(SequenceNumber seq, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value); bool Get(const LookupKey\u0026amp; key, std::string* value, Status* s); private: friend class MemTableIterator; friend class MemTableBackwardIterator; struct KeyComparator { const InternalKeyComparator comparator; explicit KeyComparator(const InternalKeyComparator\u0026amp; c) : comparator(c) {} int operator()(const char* a, const char* b) const; }; typedef SkipList\u0026lt;const char*, KeyComparator\u0026gt; Table; ~MemTable(); // Private since only Unref() should be used to delete it  KeyComparator comparator_; int refs_; Arena arena_; Table table_; };   其组成如下：\n 成员变量  comparator_：比较器，用于决定 key 的顺序。 refs__：引用计数器，当计数为 0 时释放资源。 arena_：内存池，负责管理内存。 table_：底层存储的 SkipList。   成员函数  Ref ：引用计数增加。 Unref：引用计数减少。 ApproximateMemoryUsage：统计内存使用量。 NewIterator：返回首部迭代器。 Add：插入数据。 Get：查找数据。    接着我们来看看最为核心的插入与查找。\n插入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // https://github.com/google/leveldb/blob/master/db/memtable.cc  void MemTable::Add(SequenceNumber s, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value) { //计算需要的内存大小  size_t key_size = key.size(); size_t val_size = value.size(); size_t internal_key_size = key_size + 8; const size_t encoded_len = VarintLength(internal_key_size) + internal_key_size + VarintLength(val_size) + val_size; //分配内存，并按照 len key sequencelValueType value_len value顺序将数据写入缓冲区  char* buf = arena_.Allocate(encoded_len); char* p = EncodeVarint32(buf, internal_key_size); std::memcpy(p, key.data(), key_size); p += key_size; EncodeFixed64(p, (s \u0026lt;\u0026lt; 8) | type); p += 8; p = EncodeVarint32(p, val_size); std::memcpy(p, value.data(), val_size); assert(p + val_size == buf + encoded_len); //调用底层SkipList的Insert将数据插入  table_.Insert(buf) }   插入的逻辑主要分为三步：\n 计算需要的内存大小。 分配内存，并按照 len key sequencelValueType value_len value顺序将数据写入缓冲区。 调用底层SkipList的Insert将数据插入。  查找 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  // https://github.com/google/leveldb/blob/master/db/memtable.cc  bool MemTable::Get(const LookupKey\u0026amp; key, std::string* value, Status* s) { Slice memkey = key.memtable_key(); //获取迭代器  Table::Iterator iter(\u0026amp;table_);\t//通过迭代器的Seek查找数据  iter.Seek(memkey.data()); if (iter.Valid()) { // entry format is:  // klength varint32  // userkey char[klength]  // tag uint64  // vlength varint32  // value char[vlength]  const char* entry = iter.key(); uint32_t key_length; const char* key_ptr = GetVarint32Ptr(entry, entry + 5, \u0026amp;key_length); //判断查找是否成功  if (comparator_.comparator.user_comparator()-\u0026gt;Compare( Slice(key_ptr, key_length - 8), key.user_key()) == 0) { const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8); //判断查找到的类型  switch (static_cast\u0026lt;ValueType\u0026gt;(tag \u0026amp; 0xff)) { //如果数据存在，则将数据保存后返回true  case kTypeValue: { Slice v = GetLengthPrefixedSlice(key_ptr + key_length); value-\u0026gt;assign(v.data(), v.size()); return true; } //如果数据删除，则将状态标记为未找到并返回true  case kTypeDeletion: *s = Status::NotFound(Slice()); return true; } } } //没有找到则返回false  return false; }   查找流程如下：\n 获取迭代器，通过 Seek 查找数据。 判断查找结果：  查找成功，数据存在，保存数据后返回 true。 查找成功，数据已删除（曾经存在），标记状态为 NotFound 后返回 true。 查找失败，数据不存在，返回 false。    了解完了 MemTable，接下来再看看它底层使用的 SkipList 是如何实现的。\nSkipList SkipList 是一个多层有序链表结构，通过在每个节点中保存多个指向其他节点的指针，将有序链表平均的复杂度O（N） 降低到 O（logN）。SkipList 因具有实现简单、性能优良等特点得到了广泛应用，例如 Redis 中的 ZSet，以及 LevelDB 的 MemTable。\n具体信息可以看看我之前写的博客 高级数据结构与算法 | 跳跃表（Skip List）。这里就不多作介绍，直接看代码。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; class SkipList { private: struct Node; public: explicit SkipList(Comparator cmp, Arena* arena); SkipList(const SkipList\u0026amp;) = delete; SkipList\u0026amp; operator=(const SkipList\u0026amp;) = delete; void Insert(const Key\u0026amp; key); bool Contains(const Key\u0026amp; key) const; class Iterator { public: explicit Iterator(const SkipList* list); bool Valid() const; const Key\u0026amp; key() const; void Next(); void Prev(); void Seek(const Key\u0026amp; target); void SeekToFirst(); void SeekToLast(); private: const SkipList* list_; Node* node_; }; private: enum { kMaxHeight = 12 }; inline int GetMaxHeight() const { return max_height_.load(std::memory_order_relaxed); } Node* NewNode(const Key\u0026amp; key, int height); int RandomHeight(); bool Equal(const Key\u0026amp; a, const Key\u0026amp; b) const { return (compare_(a, b) == 0); } bool KeyIsAfterNode(const Key\u0026amp; key, Node* n) const; Node* FindGreaterOrEqual(const Key\u0026amp; key, Node** prev) const; Node* FindLessThan(const Key\u0026amp; key) const; Node* FindLast() const; Comparator const compare_; Arena* const arena_; Node* const head_; std::atomic\u0026lt;int\u0026gt; max_height_; Random rnd_; };   这里也是只介绍几个重要的函数——晋升、插入、查找。\n晋升 在 LevelDB 中，每次插入节点的层高由 RandomHeight 决定，比起 Redis 的 1/2 来说，LevelDB 中的晋升逻辑更加复杂，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; int SkipList\u0026lt;Key, Comparator\u0026gt;::RandomHeight() { static const unsigned int kBranching = 4; int height = 1; while (height \u0026lt; kMaxHeight \u0026amp;\u0026amp; ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u0026gt; 0); assert(height \u0026lt;= kMaxHeight); return height; }   上述代码中 rnd_.Next() 的作用是生成一个随机数，将该随机数对 4 取余，如果余数等于 0 并且层高小于规定的最大层高 12，则将层高加 1。因为对 4 取余数结果只有 0、1、2、3 这 4 种可能，因此可以推导得出每个节点层高为 1 的概率是 3/4，层高为 2 的概率是 1/4。依此类推，层高为 3 的概率是 3/16（ 1/4 × 3/4 ），层高为4的概率是3/64（ 1/4 × 1/4 × 3/4 ），即层级越高，概率越小。\n查找 SkipList 的查找主要是借助迭代器的 Seek 来实现的，而在 Seek 中又调用了 FindGreaterOrEqual，下面看看它们的实现逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; inline void SkipList\u0026lt;Key, Comparator\u0026gt;::Iterator::Seek(const Key\u0026amp; target) { node_ = list_-\u0026gt;FindGreaterOrEqual(target, nullptr); } SkipList\u0026lt;Key, Comparator\u0026gt;::FindGreaterOrEqual(const Key\u0026amp; key, Node** prev) const { //从顶层开始查询  Node* x = head_; int level = GetMaxHeight() - 1; while (true) { Node* next = x-\u0026gt;Next(level); //如果当前节点的值小于要查询的值，则在该层继续查找  if (KeyIsAfterNode(key, next)) { x = next; } else { //如果大于等于，则说明不可能在该层，前往下一层查找。  if (prev != nullptr) prev[level] = x; //如果查询到底就直接返回，此时有两种情况1.查询成功，返回底层结果 2.查询失败，返回对应最底层位置  if (level == 0) { return next; } else { // Switch to next list  level--; } } } }   查找逻辑与常规 SkipList 实现一样：\n 从顶层开始查询。 对比当前阶段的值是否小于查询的值：  小于：沿着当前层继续查找。 大于等于：前往下一层查找。   判断当前层数是否到底，没到就继续往下走。 到底了返回数据，如果返回的数据与 key 相同则说明查询成功，否则失败。  插入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; void SkipList\u0026lt;Key, Comparator\u0026gt;::Insert(const Key\u0026amp; key) { //记录每一层级查找到的位置  Node* prev[kMaxHeight]; //查找适合插入的位置  Node* x = FindGreaterOrEqual(key, prev); assert(x == nullptr || !Equal(key, x-\u0026gt;key)); //获取本次插入的最高层数  int height = RandomHeight(); //如果本次插入的最高层数大于目前最高层数，则将多出的几层指向新插入节点  if (height \u0026gt; GetMaxHeight()) { for (int i = GetMaxHeight(); i \u0026lt; height; i++) { prev[i] = head_; } max_height_.store(height, std::memory_order_relaxed); } x = NewNode(key, height); //将需要更新的节点依次更新  for (int i = 0; i \u0026lt; height; i++) { x-\u0026gt;NoBarrier_SetNext(i, prev[i]-\u0026gt;NoBarrier_Next(i)); prev[i]-\u0026gt;SetNext(i, x); } }   具体逻辑如下：\n 首先用一个数组存储每一层所查找到的位置。 使用 FindGreaterOrEqual 获取适合插入的位置。 通过 RandomHeight 获取本次插入的最高层数：  如果本次插入的最高层数大于目前最高层数，则将多出的几层指向新插入节点。 如果小于等于，则无需更新。   遍历 prev，将需要更新的节点依次更新。  MemTable写入SSTable 在 LSM 树的实现中，会先将数据写入 MemTable，当 MemTable 大于配置的阈值时，将其作为 SSTable 写入磁盘。\n这里我们只看核心逻辑，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { //...  //生成一个MemTable迭代器  Iterator* iter = mem-\u0026gt;NewIterator(); Log(options_.info_log, \u0026#34;Level-0 table #%llu: started\u0026#34;, (unsigned long long)meta.number); Status s; { mutex_.Unlock(); //调用BuildTable，将MemTable迭代器作为参数传入，生成一个SSTable  s = BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026amp;meta); mutex_.Lock(); } //...  return s; }   在这里首先会生成一个 MemTable 迭代器，调用 BuildTable ，将 MemTable 迭代器作为参数传入，生成一个 SSTable。\n我们接着来看 BuildTable 的逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  // https://github.com/google/leveldb/blob/master/db/builder.cc  Status BuildTable(const std::string\u0026amp; dbname, Env* env, const Options\u0026amp; options, TableCache* table_cache, Iterator* iter, FileMetaData* meta) { Status s; meta-\u0026gt;file_size = 0; //将迭代器移动到首部  iter-\u0026gt;SeekToFirst(); //生成SSTable文件名  std::string fname = TableFileName(dbname, meta-\u0026gt;number); if (iter-\u0026gt;Valid()) { WritableFile* file; s = env-\u0026gt;NewWritableFile(fname, \u0026amp;file); if (!s.ok()) { return s; } //创建一个TableBuilder  TableBuilder* builder = new TableBuilder(options, file); meta-\u0026gt;smallest.DecodeFrom(iter-\u0026gt;key()); Slice key; //遍历迭代器，将MemTable中的每一对K-V写入TableBuilder中  for (; iter-\u0026gt;Valid(); iter-\u0026gt;Next()) { key = iter-\u0026gt;key(); builder-\u0026gt;Add(key, iter-\u0026gt;value()); } if (!key.empty()) { meta-\u0026gt;largest.DecodeFrom(key); } //调用TableBuilder的Finish函数生成SSTable文件  s = builder-\u0026gt;Finish(); if (s.ok()) { meta-\u0026gt;file_size = builder-\u0026gt;FileSize(); assert(meta-\u0026gt;file_size \u0026gt; 0); } delete builder; //调用Sync将文件刷新到磁盘中\t if (s.ok()) { s = file-\u0026gt;Sync(); } //关闭文件  if (s.ok()) { s = file-\u0026gt;Close(); } //...  return s; }   核心逻辑如下：\n 将 MemTable 迭代器移动到首部。 生成 SSTable 文件名。 创建一个 TableBuilder。 遍历迭代器，将 MemTable 中的每一对 K-V 写入 TableBuilder 中。 调用 TableBuilder 的 Finish 函数生成 SSTable 文件。 调用 Sync 将文件刷新到磁盘中。 关闭文件。  ","date":"2022-05-23T23:40:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-memtable%E6%A8%A1%E5%9D%97/","title":"LevelDB MemTable模块"},{"content":"公共基础类 内存管理 Arena 内存频繁创建/释放的地方就会有内存池的出现，LevelDB 也不例外。在 Memtable 组件中，会有大量内存创建（数据持续 put）和释放（dump 到磁盘后内存结束），于是 LevelDB 通过 Arena 来管理内存。\n 它有什么好处呢？\n  提升性能：内存申请本身就需要占用一定的资源，消耗空间与时间。而 Arena 内存池的基本思路就是预先申请一大块内存，然后多次分配给不同的对象，从而减少 malloc 或 new 的调用次数。 提高内存利用率：频繁进行内存的申请与释放易造成内存碎片。即内存余量虽然够用，但由于缺乏足够大的连续空闲空间，从而造成申请一段较大的内存不成功的情况。而 Arena 具有整块内存的控制权，用户可以任意操作这段内存，从而避免内存碎片的产生。  结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // https://github.com/google/leveldb/blob/master/util/arena.h  class Arena { public: Arena(); Arena(const Arena\u0026amp;) = delete; Arena\u0026amp; operator=(const Arena\u0026amp;) = delete; ~Arena(); char* Allocate(size_t bytes); malloc. char* AllocateAligned(size_t bytes); size_t MemoryUsage() const { return memory_usage_.load(std::memory_order_relaxed); } private: char* AllocateFallback(size_t bytes); char* AllocateNewBlock(size_t block_bytes); char* alloc_ptr_; size_t alloc_bytes_remaining_; std::vector\u0026lt;char*\u0026gt; blocks_; std::atomic\u0026lt;size_t\u0026gt; memory_usage_; };   其组成如下：\n 成员变量 alloc_ptr_ ：当前已使用内存的指针  blocks_ ：实际分配的内存池_ alloc_bytes_remaining_ ：剩余内存字节数 memory_usage_ ：记录内存的使用情况 kBlockSize ：一个块大小（默认4k）   成员函数  AllocateFallback ：按需分配内存，可能会有内存浪费。 AllocateAligned ：分配偶数大小的内存，主要是 skiplist 节点时，目的是加快访问。 MemoryUsage：统计内存使用量。    内存分配 Arena 中与内存分配有关的两个接口函数：Allocate 与 AllocateAligned。\n首先我们来看看 Allocate 与它依赖的两个函数 AllocateFallback 与 AllocateNewBlock。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12  // https://github.com/google/leveldb/blob/master/util/arena.h  inline char* Arena::Allocate(size_t bytes) { assert(bytes \u0026gt; 0); if (bytes \u0026lt;= alloc_bytes_remaining_) { char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } return AllocateFallback(bytes); }   首先，对于 Allocate 来说，它存在两种情况\n 需要分配的字节数小于等于 alloc_bytes_remaining_：Allocate 直接返回 alloc_ptr_ 指向的地址空间，然后对 alloc_ptr_ 与 alloc_bytes_remaining_ 进行更新。 需要分配的字节数大于 alloc_bytes_remaining_：调用 AllocateFallback 方法进行扩容。  AllocateFallback 用于申请一个新 Block 内存空间，然后分配需要的内存并返回。因此当前 Block 剩余空闲内存就不可避免地浪费了。接着看看 AllocateFallback 的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateFallback(size_t bytes) { if (bytes \u0026gt; kBlockSize / 4) { char* result = AllocateNewBlock(bytes); return result; } alloc_ptr_ = AllocateNewBlock(kBlockSize); alloc_bytes_remaining_ = kBlockSize; char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; }   AllocateFallback 的使用也包括两种情况：\n 需要分配的空间大于 kBlockSize 的 1/4（即1024字节）：直接申请需要分配空间大小的 Block，从而避免剩余内存空间的浪费。 需要分配的空间小于等于 kBlockSize 的 1/4 ：申请一个大小为 kBlockSize 的新 Block 空间，然后在新的Block上分配需要的内存并返回其首地址。  对于 Block 的分配，这里又调用了 AllocateNewBlock，其逻辑如下：\n1 2 3 4 5 6 7 8 9  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); memory_usage_.fetch_add(block_bytes + sizeof(char*), std::memory_order_relaxed); return result; }   这里的逻辑非常简单，其通过 new，申请了一段大小为 block_bytes 的内存空间，并将这块空间的地址存储到 blocks_ 中，之后更新当前可用的总空间大小后将空间首地址返回。\n讲解完了 Allocate ，我们接着再来看看 AllocateAligned。虽然它也用于内存分配，但不同点在于它考虑了内存分配时的内存对齐问题（进行内存分配所返回的起始地址应为 b / 8 的倍数，在这里 b 代表操作系统平台的位数）。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateAligned(size_t bytes) { const int align = (sizeof(void*) \u0026gt; 8) ? sizeof(void*) : 8; static_assert((align \u0026amp; (align - 1)) == 0, \u0026#34;Pointer size should be a power of 2\u0026#34;); size_t current_mod = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(alloc_ptr_) \u0026amp; (align - 1); size_t slop = (current_mod == 0 ? 0 : align - current_mod); size_t needed = bytes + slop; char* result; if (needed \u0026lt;= alloc_bytes_remaining_) { result = alloc_ptr_ + slop; alloc_ptr_ += needed; alloc_bytes_remaining_ -= needed; } else { // AllocateFallback always returned aligned memory  result = AllocateFallback(bytes); } assert((reinterpret_cast\u0026lt;uintptr_t\u0026gt;(result) \u0026amp; (align - 1)) == 0); return result; }   由于主流的服务器平台采用 64 位的操作系统，64位 操作系统的指针同样为 64 位（即 8 个字节），因此这里的对齐就需要使分配的内存起始地址必然为 8 的倍数。要满足这一条件，采用的主要办法就是判断当前空闲内存的起始地址是否为 8 的倍数：如果是，则直接返回；如果不是，则对 8 求模，然后向后寻找最近的 8 的倍数的内存地址并返回。\n由于计算机进行乘除或求模运算的速度远慢于位操作运算，因此这里巧妙的用了位运算来进行优化。\n 用位运算判断某个数值是否为2的正整数幂：   static_assert((align \u0026amp; (align - 1)) == 0, \u0026quot;Pointer size should be a power of 2\u0026quot;);\n用位运算进行求模：  size_t current_mod = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(alloc_ptr_) \u0026amp; (align - 1);\n位运算的细节这里就i不提了，大家可以自行百度了解。\n内存使用率统计 memory_usage_ 用于存储当前 Arena 所申请的总共的内存空间大小，为了保证线程安全，其为 atomic\u0026lt;size_t\u0026gt; 变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/util/arena.h  size_t MemoryUsage() const { return memory_usage_.load(std::memory_order_relaxed); } // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); memory_usage_.fetch_add(block_bytes + sizeof(char*), std::memory_order_relaxed); return result; }   在 AllocateNewBock 申请内存的过程中，其会通过以下的公式来更新当前总大小\nmemory_usage_ = memory_usage_ + block_bytes + sizeof(char*) \n 为什么这里还需要加上一个sizeof（char*）呢？\n 因为在申请完 Block 后，Block 的首地址需要存储在一个 vector\u0026lt;char*\u0026gt; 的动态数组 blocks_ 中，因而需要额外占用一个指针的空间。\nTCMalloc LevelDB 中针对一些需要调用 new 或 malloc 方法进行堆内存操作的情况（即非内存池的内存分配），其使用 TCMalloc 进行优化。\nTCMalloc（Thread-Caching Malloc）是 google-perftool 中一个管理堆内存的内存分配器工具，可以降低内存频繁分配与释放所造成的性能损失，并有效控制内存碎片。默认 C/C++ 在编译器中主要采用 glibc 的内存分配器 ptmalloc2。同样的 malloc 操作，TCMalloc 比 ptmalloc2 更具性能优势。\nTCMalloc 的详细介绍可以参见http://goog-perftools.sourceforge.net/doc/tcmalloc.html。\nEnv家族 Env 是一个抽象接口类，用纯虚函数的形式定义了一些与平台操作的相关接口，如文件系统、多线程、时间操作等。接口定义如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT Env { public: Env(); Env(const Env\u0026amp;) = delete; Env\u0026amp; operator=(const Env\u0026amp;) = delete; virtual ~Env(); static Env* Default(); virtual Status NewSequentialFile(const std::string\u0026amp; fname, SequentialFile** result) = 0; virtual Status NewRandomAccessFile(const std::string\u0026amp; fname, RandomAccessFile** result) = 0; virtual Status NewWritableFile(const std::string\u0026amp; fname, WritableFile** result) = 0; virtual Status NewAppendableFile(const std::string\u0026amp; fname, WritableFile** result); virtual bool FileExists(const std::string\u0026amp; fname) = 0; virtual Status GetChildren(const std::string\u0026amp; dir, std::vector\u0026lt;std::string\u0026gt;* result) = 0; virtual Status RemoveFile(const std::string\u0026amp; fname); virtual Status DeleteFile(const std::string\u0026amp; fname); virtual Status CreateDir(const std::string\u0026amp; dirname) = 0; virtual Status RemoveDir(const std::string\u0026amp; dirname); virtual Status DeleteDir(const std::string\u0026amp; dirname); virtual Status GetFileSize(const std::string\u0026amp; fname, uint64_t* file_size) = 0; virtual Status RenameFile(const std::string\u0026amp; src, const std::string\u0026amp; target) = 0; virtual Status LockFile(const std::string\u0026amp; fname, FileLock** lock) = 0; virtual Status UnlockFile(FileLock* lock) = 0; virtual void Schedule(void (*function)(void* arg), void* arg) = 0; virtual void StartThread(void (*function)(void* arg), void* arg) = 0; virtual Status GetTestDirectory(std::string* path) = 0; virtual Status NewLogger(const std::string\u0026amp; fname, Logger** result) = 0; virtual uint64_t NowMicros() = 0; virtual void SleepForMicroseconds(int micros) = 0; };   Env 作为抽象类，有 3 个派生子类：PosixEnv、EnvWrapper 与 InMemoryEnv。\n这里就不过多的介绍它们的实现原理，只是大概的描述一下概念，方便理解。\nPosixEnv PosixEnv 是 LevelDB 中默认的 Env 实例对象。从字面意思上看，PosixEnv 就是针对 POSIX 平台的 Env 接口实现。\nEnvWrapper EnvWrapper 也是 Env 的一个派生类，与 PosixEnv 不同的是，EnvWrapper 中并没有定义众多纯虚函数接口的具体实现，而是定义了一个私有成员变量 Env* target_，并在构造函数中通过传递预定义的 Env 实例对象，从而实现对 target_ 的初始化操作。基于 EnvWrapper 的派生类，易于实现用户在某一个 Env 派生类的基础上改写其中一部分接口的需求。\nInMemoryEnv InMemoryEnv 就是 EnvWrapper 的一个子类，主要对 Env 中有关文件的接口进行了重写。InMemoryEnv 主要是将所有的操作都置于内存中，从而提升文件I/O的读取速度。\n文件操作 在 LevelDB 中，主要有三种文件 I/O 操作：\n SequentialFile：顺序读，如日志文件的读取、Manifest文件的读取。 WritableFile：顺序写，用于日志文件、SSTable文件、Manifest文件的写入。 RandomAccessFile：随机读，如SSTable文件的读取。  SequentialFile SequentialFile 定义了文件顺序读抽象接口，其接口定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT SequentialFile { public: SequentialFile() = default; SequentialFile(const SequentialFile\u0026amp;) = delete; SequentialFile\u0026amp; operator=(const SequentialFile\u0026amp;) = delete; virtual ~SequentialFile(); virtual Status Read(size_t n, Slice* result, char* scratch) = 0; virtual Status Skip(uint64_t n) = 0; };   其主要有两个接口方法，即 Read 与 Skip：\n Read：用于从文件当前位置顺序读取指定的字节数。 Skip：用于从当前位置，顺序向后忽略指定的字节数。   无论是Read方法还是Skip方法，对于多线程环境而言均不是线程安全的访问方法，需要开发者在调用过程中采用外部手段进行线程同步操作。\n PosixSequentialFile，是在符合POSIX标准的文件系统上对顺序读的实现。这里就不过多介绍，感兴趣的可以自己去了解。\nWritableFile WritableFile定义了文件顺序写抽象接口，其定义下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT WritableFile { public: WritableFile() = default; WritableFile(const WritableFile\u0026amp;) = delete; WritableFile\u0026amp; operator=(const WritableFile\u0026amp;) = delete; virtual ~WritableFile(); virtual Status Append(const Slice\u0026amp; data) = 0; virtual Status Close() = 0; virtual Status Flush() = 0; virtual Status Sync() = 0; };   WritableFile 主要有4个纯虚函数接口：Append、Close、Flush 与 Sync：\n Append：用于以追加的方式对文件顺序写入。 Close：用于关闭文件。 Flush：用于将 Append 操作写入到缓冲区的数据强制刷新到内核缓冲区。 Sync：用于将内存缓冲区的数据强制保存到磁盘。  PosixWritableFile 是对符合 POSIX 标准平台的 WritableFile 的派生实现。\nRandomAccessFile 随机读就是指可以定位到文件任意某个位置进行读取。RandomAccessFile 是文件随机读的抽象接口，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT RandomAccessFile { public: RandomAccessFile() = default; RandomAccessFile(const RandomAccessFile\u0026amp;) = delete; RandomAccessFile\u0026amp; operator=(const RandomAccessFile\u0026amp;) = delete; virtual ~RandomAccessFile(); virtual Status Read(uint64_t offset, size_t n, Slice* result, char* scratch) const = 0; };   与顺序读接口相比，随机读没有 Skip 操作，只有一个 Read 方法：\n Read：可从文件指定的任意位置读取一段指定长度数据。  在 LevelDB 中，RandomAccessFile 有两个派生类：PosixRandomAccessFile 与 PosixMmapReadableFile。这两个派生类是两种对随机文件操作的实现形式：一种是基于 pread() 方法的随机访问；另一种是基于 mmap() 方法的随机访问。\n数值编码 LevelDB 是一个嵌入式的存储库，其存储的内容可以是字符，也可以是数值。LevelDB 为了减少数值型内容对内存空间的占用，分别针对不同的需求定义了两种编码方式：一种是定长编码，另一种是变长编码。\n字节序 在讲编码之前，先聊聊字节序。字节序是处理器架构的特性，比如一个16 位的整数，他是由 2 个字节组成。内存中存储 2 个字节有两种方法：\n 将低序字节存储在起始地址，称为小端。 将高序字节存储在起始地址，称为大端。  LevelDB 为了便于操作，编码的数据统一采用小端模式，并存放到对应的字符串中，即数据低位存在内存低地址，数据高位存在内存高地址。\n具体的关于大小端的信息可以参考往期博客 大端小端存储解析以及判断方法。\n定长编码 定长的数值编码比较简单，主要是将原有的 uint64 或 uint32 的数据直接存储在对应的 8 字节或 4 字节中。而在直接存储的过程中，会基于大小端的不同，采取不同的执行逻辑（新版代码简化逻辑，统一按照大端逻辑处理）：\n 大端：在复制过程中调换字节顺序，以小端模式进行编码保存。 小端：直接利用 memcpy 进行内存间的复制。  具体实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  // https://github.com/google/leveldb/blob/master/util/coding.h  //32位数据定长编码 void EncodeFixed32(char* buf, uint32_t value) { if (port::kLittleEndian) { memcpy(buf, \u0026amp;value, sizeof(value)); } else { buf[0] = value \u0026amp; 0xff; buf[1] = (value \u0026gt;\u0026gt; 8) \u0026amp; 0xff; buf[2] = (value \u0026gt;\u0026gt; 16) \u0026amp; 0xff; buf[3] = (value \u0026gt;\u0026gt; 24) \u0026amp; 0xff; } } //64位数据定长编码 void EncodeFixed64(char* buf, uint64_t value) { if (port::kLittleEndian) { memcpy(buf, \u0026amp;value, sizeof(value)); } else { buf[0] = value \u0026amp; 0xff; buf[1] = (value \u0026gt;\u0026gt; 8) \u0026amp; 0xff; buf[2] = (value \u0026gt;\u0026gt; 16) \u0026amp; 0xff; buf[3] = (value \u0026gt;\u0026gt; 24) \u0026amp; 0xff; buf[4] = (value \u0026gt;\u0026gt; 32) \u0026amp; 0xff; buf[5] = (value \u0026gt;\u0026gt; 40) \u0026amp; 0xff; buf[6] = (value \u0026gt;\u0026gt; 48) \u0026amp; 0xff; buf[7] = (value \u0026gt;\u0026gt; 56) \u0026amp; 0xff; } } //32位数据定长解码 inline uint32_t DecodeFixed32(const char* ptr) { if (port::kLittleEndian) { // Load the raw bytes  uint32_t result; memcpy(\u0026amp;result, ptr, sizeof(result)); // gcc optimizes this to a plain load  return result; } else { return ((static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[0]))) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[1])) \u0026lt;\u0026lt; 8) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[2])) \u0026lt;\u0026lt; 16) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[3])) \u0026lt;\u0026lt; 24)); } } //64位数据定长解码 inline uint64_t DecodeFixed64(const char* ptr) { if (port::kLittleEndian) { // Load the raw bytes  uint64_t result; memcpy(\u0026amp;result, ptr, sizeof(result)); // gcc optimizes this to a plain load  return result; } else { uint64_t lo = DecodeFixed32(ptr); uint64_t hi = DecodeFixed32(ptr + 4); return (hi \u0026lt;\u0026lt; 32) | lo; } }   变长编码 对于 4 字节或 8 字节表示的无符号整型数据而言，数值较小的整数的高位空间基本为 0，如 uint32 类型的数据 128，高位的 3 个字节都是 0。为了更好的利用这些高位空间，如果能基于某种机制，将为 0 的高位数据忽略，有效地保留其有效位，从而减少所需字节数、节约存储空间。于是 LevelDB 就采用了 Google 的另一个开源项目 Protobuf 中提出的 varint 变长编码。\n**varint 将实际的一个字节分成了两个部分，最高位定义为 MSB（mostsignificant bit），后续低 7 位表示实际数据。**MSB 是一个标志位，用于表示某一数值的字节是否还有后续的字节，如果为 1 表示该数值后续还有字节，如果为 0 表示该数值所编码的字节至此完毕。每一个字节中的第 1 到第 7 位表示的是实际的数据，由于有 7 位，则只能表示大小为 0～127 的数值。\n下图给出了 127、300、2^28 - 1 的编码结果\n具体实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  // https://github.com/google/leveldb/blob/master/util/coding.cc  //32位数据变长编码 char* EncodeVarint32(char* dst, uint32_t v) { // Operate on characters as unsigneds  uint8_t* ptr = reinterpret_cast\u0026lt;uint8_t*\u0026gt;(dst); static const int B = 128; if (v \u0026lt; (1 \u0026lt;\u0026lt; 7)) { *(ptr++) = v; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 14)) { *(ptr++) = v | B; *(ptr++) = v \u0026gt;\u0026gt; 7; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 21)) { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = v \u0026gt;\u0026gt; 14; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 28)) { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = (v \u0026gt;\u0026gt; 14) | B; *(ptr++) = v \u0026gt;\u0026gt; 21; } else { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = (v \u0026gt;\u0026gt; 14) | B; *(ptr++) = (v \u0026gt;\u0026gt; 21) | B; *(ptr++) = v \u0026gt;\u0026gt; 28; } return reinterpret_cast\u0026lt;char*\u0026gt;(ptr); } //32位数据变长解码 const char* GetVarint32PtrFallback(const char* p, const char* limit, uint32_t* value) { uint32_t result = 0; for (uint32_t shift = 0; shift \u0026lt;= 28 \u0026amp;\u0026amp; p \u0026lt; limit; shift += 7) { uint32_t byte = *(reinterpret_cast\u0026lt;const uint8_t*\u0026gt;(p)); p++; if (byte \u0026amp; 128) { // More bytes are present  result |= ((byte \u0026amp; 127) \u0026lt;\u0026lt; shift); } else { result |= (byte \u0026lt;\u0026lt; shift); *value = result; return reinterpret_cast\u0026lt;const char*\u0026gt;(p); } } return nullptr; }   ","date":"2022-05-23T23:29:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%85%AC%E5%85%B1%E5%9F%BA%E7%A1%80%E7%B1%BB/","title":"LevelDB 公共基础类"},{"content":"LevelDB 架构 源码结构 LevelDB 的源码托管在 GitHub 上（https://github.com/google/leveldb），其中与程序实现源码相关的主要有以下几项：\n db：包含数据库的一些基本接口操作与内部实现。 table：为排序的字符串表 SSTable（Sorted String Table）的主体实现。 helpers：定义了 LevelDB 底层数据部分完全运行于内存环境的实现方法，主要用于相关的测试或某些全内存的运行场景。 util：包含一些通用的基础类函数，如内存管理、布隆过滤器、编码、CRC 等相关函数。 include：包含 LevelDB 库函数、可供外部访问的接口、基本数据结构等。 port：定义了一个通用的底层文件，以及多个进程操作接口，还有基于操作系统移植性实现的各平台的具体接口。  具体的环境搭建流程在前面的实战章节就讲过，这里就不过多介绍了。\n整体架构 LevelDB 总体模块架构主要包括接口 API（DB API 与 POSIX API）、Utility 公用基础类、LSM 树（Log、MemTable、SSTable）3个部分，如下图：\n 接口 API：接口 API 主要包括客户端调用的 DB API 以及针对操作系统底层的统一接口 POSIX API：  DB API：主要用于封装一些供客户端应用进行调用的接口，即头文件中的相关 API 函数接口，客户端应用可以通过这些接口实现数据引擎的各种操作。 POSIX API：实现了对操作系统底层相关操作的接口封装，主要用于保证 LevelDB 的可移植性，从而实现 LevelDB 在各 POSIX 操作系统的跨平台特性   Utility 公用基础类：主要用于实现主体功能所依赖的各种对象功能，例如内存管理 Arena、布隆过滤器、缓存、CRC 校验、哈希表、测试框架等。 LSM 树：LSM 树是 LevelDB 最重要的组件，也是实现其他功能的核心。一般而言，在常规的物理硬盘存储介质上，顺序写比随机写速度要快，而 LSM 树正是充分利用了这一物理特性，从而实现对频繁、大量数据写操作的支持。  LSM 架构如下：\n MemTable：内存数据结构，具体实现是 SkipList。接受用户的读写请求，新的数据修改会首先在这里写入。 Immutable MemTable：当 MemTable 的大小达到设定的阈值时，会变成 Immutable MemTable，只接受读操作，不再接受写操作，后续由后台线程 Flush 到磁盘上。 SSTable：Sorted String Table Files，磁盘数据存储文件。分为 Level0 到 LevelN 多层，每一层包含多个 SST 文件，文件内数据有序。Level0 直接由 Immutable Memtable Flush 得到，其它每一层的数据由上一层进行 Compaction 得到。 Manifest ：Manifest 文件中记录 SST 文件在不同 Level 的分布，单个 SST 文件的最大、最小 key，以及其他一些 LevelDB 需要的元信息。由于 LevelDB 支持 snapshot，需要维护多版本，因此可能同时存在多个 Manifest 文件。 Current ：由于 Manifest 文件可能存在多个，Current 记录的是当前的 Manifest 文件名。 Log ：用于防止 MemTable 丢数据的日志文件。  基本组件 Slice Slice 是 LevelDB 中的一种基本的、以字节为基础的数据存储单元，既可以存储 key，也可以存储 data。Slice 对数据字节的大小没有限制，其内部采用一个const char* 的常量指针存储数据，具有两个接口 data() 和size()，分别返回其表示的数据及数据的长度。\n此外，为了方便使用，其内部封装了 compare 函数，以及重载了用于比较的运算符 ==、!=，同时其可以与标准库中的 string 相互转换。\n 为什么 C++ 中已经有 string 了，还需要实现一个 Slice 呢？\n  string 默认语意为拷贝，传参和返回时会损失性能（在可预期的条件下，指针传递即可）。 Slice 不以 \u0026rsquo; \\0\u0026rsquo; 作为字符的终止符，可以存储值为 \u0026lsquo;\\0\u0026rsquo; 的数据。 string 不支持 remove_prefix 和 starts_with 等前缀操作函数，使用不方便。  具体实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  // https://github.com/google/leveldb/blob/master/include/leveldb/slice.h  class LEVELDB_EXPORT Slice { public: Slice() : data_(\u0026#34;\u0026#34;), size_(0) {} Slice(const char* d, size_t n) : data_(d), size_(n) {} Slice(const std::string\u0026amp; s) : data_(s.data()), size_(s.size()) {} Slice(const char* s) : data_(s), size_(strlen(s)) {} Slice(const Slice\u0026amp;) = default; Slice\u0026amp; operator=(const Slice\u0026amp;) = default; const char* data() const { return data_; } size_t size() const { return size_; } bool empty() const { return size_ == 0; } char operator[](size_t n) const { assert(n \u0026lt; size()); return data_[n]; } void clear() { data_ = \u0026#34;\u0026#34;; size_ = 0; } void remove_prefix(size_t n) { assert(n \u0026lt;= size()); data_ += n; size_ -= n; } std::string ToString() const { return std::string(data_, size_); } int compare(const Slice\u0026amp; b) const; bool starts_with(const Slice\u0026amp; x) const { return ((size_ \u0026gt;= x.size_) \u0026amp;\u0026amp; (memcmp(data_, x.data_, x.size_) == 0)); } private: const char* data_; size_t size_; }; inline bool operator==(const Slice\u0026amp; x, const Slice\u0026amp; y) { return ((x.size() == y.size()) \u0026amp;\u0026amp; (memcmp(x.data(), y.data(), x.size()) == 0)); } inline bool operator!=(const Slice\u0026amp; x, const Slice\u0026amp; y) { return !(x == y); } inline int Slice::compare(const Slice\u0026amp; b) const { const size_t min_len = (size_ \u0026lt; b.size_) ? size_ : b.size_; int r = memcmp(data_, b.data_, min_len); if (r == 0) { if (size_ \u0026lt; b.size_) r = -1; else if (size_ \u0026gt; b.size_) r = +1; } return r; }   Status 在 LevelDB 中，为了便于抛出异常，定义了一个 Status 类。Status 主要用于记录 LevelDB 中的状态信息，保存错误码和对应的字符串错误信息。（写死的，不支持拓展）\n对于错误代码，LevelDB 中定义了 6 种状态码在一个枚举类型 Code 中，如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/include/leveldb/status.h  class LEVELDB_EXPORT Status { private: enum Code { kOk = 0, kNotFound = 1, kCorruption = 2, kNotSupported = 3, kInvalidArgument = 4, kIOError = 5 }; }   其中 KoK 代表正常；kNotFound 代表未找到 ；kCorruption 代表数据异常崩溃；kNotSupported 代表不支持；kInvalidArgument 代表参数非法；kIOError 代表 I/O 错误；\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/include/leveldb/status.h  class LEVELDB_EXPORT Status { private: // OK status has a null state_. Otherwise, state_ is a new[] array  // of the following form:  // state_[0..3] == length of message  // state_[4] == code  // state_[5..] == message  const char* state_; }   在 Status 类中，所有状态信息，包括状态码与具体描述，都保存在一个私有的成员变量 const char*state_ 中。其具体表示方法如下。\n 当状态为 OK 时，state_ 为 NULL，说明当前操作一切正常。 否则，state_ 为一个 char 数组，即 state[0...3] 为 msg 的长度，state[4] 为状态码 code，state[5...] 为具体的 msg。  如下图：具体的处理逻辑如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // https://github.com/google/leveldb/blob/master/util/status.cc Status::Status(Code code, const Slice\u0026amp; msg, const Slice\u0026amp; msg2) { assert(code != kOk); const uint32_t len1 = static_cast\u0026lt;uint32_t\u0026gt;(msg.size()); const uint32_t len2 = static_cast\u0026lt;uint32_t\u0026gt;(msg2.size()); const uint32_t size = len1 + (len2 ? (2 + len2) : 0); char* result = new char[size + 5]; std::memcpy(result, \u0026amp;size, sizeof(size)); result[4] = static_cast\u0026lt;char\u0026gt;(code); std::memcpy(result + 5, msg.data(), len1); if (len2) { result[5 + len1] = \u0026#39;:\u0026#39;; result[6 + len1] = \u0026#39; \u0026#39;; std::memcpy(result + 7 + len1, msg2.data(), len2); } state_ = result; }   Comparator LevelDB 是按 key 排序后进行存储，因无论是插入还是删除，都必然少不了对 key 的比较。于是乎 LevelDB 抽象出了一个纯虚类 Comparator。\n具体定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/include/leveldb/comparator.h  class LEVELDB_EXPORT Comparator { public: virtual ~Comparator(); virtual int Compare(const Slice\u0026amp; a, const Slice\u0026amp; b) const = 0; virtual const char* Name() const = 0; virtual void FindShortestSeparator(std::string* start, const Slice\u0026amp; limit) const = 0; virtual void FindShortSuccessor(std::string* key) const = 0; };   在 LevelDB 中，有两个实现 Comparator 的类：一个是 BytewiseComparatorImpl，另一个是InternalKeyComparator。\n BytewiseComparatorImpl：默认比较器，主要采用字典序对两个字符串进行比较。 InternalKeyComparator：内部调用的也是 BytewiseComparatorImpl。  Iterate LevelDB 具有迭代器遍历功能，设计人员只需要调用相应的接口，就可以实现对容器数据类型的遍历访问。\n迭代器定义了一个纯虚类的接口，LevelDB 中的任何集合类型对象的迭代器均基于这一纯虚类进行实现。具体代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  // https://github.com/google/leveldb/blob/master/include/leveldb/iterator.h  class LEVELDB_EXPORT Iterator { public: Iterator(); Iterator(const Iterator\u0026amp;) = delete; Iterator\u0026amp; operator=(const Iterator\u0026amp;) = delete; virtual ~Iterator(); virtual bool Valid() const = 0; virtual void SeekToFirst() = 0; virtual void SeekToLast() = 0; virtual void Seek(const Slice\u0026amp; target) = 0; virtual void Next() = 0; virtual void Prev() = 0; virtual Slice key() const = 0; virtual Slice value() const = 0; virtual Status status() const = 0; using CleanupFunction = void (*)(void* arg1, void* arg2); void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2); private: struct CleanupNode { bool IsEmpty() const { return function == nullptr; } void Run() { assert(function != nullptr); (*function)(arg1, arg2); } CleanupFunction function; void* arg1; void* arg2; CleanupNode* next; }; CleanupNode cleanup_head_; };   从上面的接口可以看出，LevelDB 实现的是双向迭代器，并且支持 Seek 定位功能。同时还嵌套了一个子结构 CleanupNode 用于释放链表，其中的 CleanupFunction 即用户自定义的清除函数。当用户需要释放资源时，就会通过遍历 CleanupNode 中的 next 链表，并对每一个节点调用一次 CleanupFunction 将资源释放。\nOption 头文件 options.h 中定义了一系列与数据库操作相关的选项参数类型，例如与数据库操作相关的 Options，与读操作相关的 ReadOptions，与写操作相关的 WriteOptions。这几个类型均为结构体，在进行数据库的初始化、数据库的读写等操作时，这些参数直接决定了数据库相关的性能指标。\n Options（DB 参数）  Comparator：被用来表中 key 比较，默认是字典序。 create_if_missing：打开数据库，如果数据库不存在，是否创建新的。默认为 false。 error_if_exists：打开数据库，如果数据库存在，是否抛出错误。默认为 false。 paranoid_checks：默认为 false。如果为 true，则实现将对其正在处理的数据进行积极检查，如果检测到任何错误，则会提前停止。 这可能会产生不可预见的后果：例如，一个数据库条目的损坏可能导致大量条目变得不可读或整个数据库变得无法打开。 env：环境变量，封装了平台相关接口。 info_log：db 日志句柄。 write_buffer_size：memtable 的大小(默认4mb)  值大有利于性能提升 但是内存可能会存在两份，太大需要注意oom 过大刷盘之后，不利于数据恢复   max_open_files：允许打开的最大文件数。 block_cache：block 的缓存。 block_size：每个 block 的数据包大小(未压缩)，默认是4k。 block_restart_interval：block 中记录完整 key 的间隔。 max_file_size：生成新文件的阈值(对于性能较好的文件系统可以调大该阈值，但会增加数据恢复的时间)，默认 2k compression：数据压缩类型，默认是 kSnappyCompression，压缩速度快  kSnappyCompression 在 Intel(R) Core(TM)2 2.4GHz 上的典型速度：  ~200-500MB/s 压缩 ~400-800MB/s 解压     reuse_logs：是否复用之前的 MANIFES 和 log files。 filter_policy：block 块中的过滤策略，支持布隆过滤器。   ReadOptions（读操作参数）  verify_checksums：是否对从磁盘读取的数据进行校验。 fill_cache：读取到 block 数据，是否加入到 cache 中。 snapshot：记录的是当前的快照。   WriteOptions（写操作参数）  sync：是否同步刷盘，也就是调用完 write 之后是否需要显式 fsync。    ","date":"2022-05-23T23:22:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E6%9E%B6%E6%9E%84/","title":"LevelDB 架构"},{"content":"基本操作 环境搭建 1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 下载源码 git clone https://github.com.cnpmjs.org/google/leveldb.git # 下载依赖第三方库（benchmark、googletest） git submodule update --init # 执行编译 cd leveldb/ mkdir -p build \u0026amp;\u0026amp; cd build cmake -DCMAKE_BUILD_TYPE=Debug .. \u0026amp;\u0026amp; cmake --build . # 头文件加入系统目录 cp -r ./include/leveldb /usr/include/ cp build/libleveldb.a /usr/local/lib/   实战使用 创建、关闭数据库 创建数据库或打开数据库，均通过 Open 函数实现。Open 为一个静态成员函数，其函数声明如下所示：\n1 2  static Status Open(const Options\u0026amp; options, const std::string\u0026amp; name, DB** dbptr);    const Options \u0026amp; options：用于指定数据库创建或打开后的基本行为。 const std::string \u0026amp; name：用于指定数据库的名称。 DB** dbptr：定义了一个DB类型的指针的指针，该指针作为Open函数操作后传给调用者使用的DB类型的实际指针。 Status：当操作成功时，函数返回 status.ok() 的值为 True，*dbptr 分配了不为 NULL 的实际指针地址；若其中的操作存在错误，则 status.ok() 的值为 False，并且对应的 *dbptr 为 NULL。  关闭数据库非常简单，只需要使用 delete 释放 db 即可。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026lt;string\u0026gt;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //关闭数据库  delete db; return 0; }   数据读、写、删除 数据库中的读、写与删除操作分别用 Get 、 Put、Delete 这3个接口函数实现。接口定义如下：\n1 2 3 4 5  Status Get(const ReadOptions\u0026amp; options, const Slice\u0026amp; key, std::string* value); Status Put(const WriteOptions\u0026amp;, const Slice\u0026amp; key, const Slice\u0026amp; value); Status Delete(const WriteOptions\u0026amp;, const Slice\u0026amp; key);    ReadOptions\u0026amp; options：代表实际读操作传入的行为参数。 WriteOptions\u0026amp; options：代表实际写操作传入的行为参数。  这里需要注意的是 Delete 并不会直接删除数据，而是在对应位置插入一个 key 的删除标志，然后在后续的Compaction 过程中才最终去除这条 key-value 记录。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //写入数据  leveldb::Slice key(\u0026#34;hello\u0026#34;); string value(\u0026#34;world\u0026#34;); status = db-\u0026gt;Put(leveldb::WriteOptions(), key, value); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; \u0026#34; 写入成功。\u0026#34; \u0026lt;\u0026lt; endl; } //查找数据  status = db-\u0026gt;Get(leveldb::ReadOptions(), key, \u0026amp;value); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; \u0026#34; 查找成功。\u0026#34; \u0026lt;\u0026lt; endl; } //删除数据  status = db-\u0026gt;Delete(leveldb::WriteOptions(), key); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; 删除成功。\u0026#34; \u0026lt;\u0026lt; endl; } //关闭数据库  delete db; return 0; }   批量处理 针对大量的操作，LevelDB 不具有传统数据库所具备的事务操作机制，然而它提供了一种批量操作的方法。这种批量操作方法主要具有两个作用：一是提供了一种原子性的批量操作方法；二是提高了整体的数据操作速度。\nLevelDB 针对批量操作定义了 WriteBatch 的类型。WriteBatch 有 3 个非常重要的接口：数据写（Put）、数据删 除（Delete）以及清空批量写入缓存（Clear），具体定义如下所示：\n1 2 3 4 5 6 7 8 9 10  class LEVELDB_EXPORT WriteBatch { public:\t//...  void Put(const Slice\u0026amp; key, const Slice\u0026amp; value); void Delete(const Slice\u0026amp; key); void Clear(); //... };   当我们想将 WriteBatch 中的数据写入 DB 时，只需要调用 Write 接口，其主要用于处理之前保存在 WriteBatch 对象中的所有批量操作，其详细接口定义如下所示：\n1  Status Write(const WriteOptions\u0026amp; options, WriteBatch* updates);    注意：一旦我们写入完成后，就会调用 updates 中的 Clear 来清空之前保存的批量操作。  代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026#34;leveldb/write_batch.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //批量写入  leveldb::Slice key; string value; leveldb::WriteBatch batch; for(int i = 0; i \u0026lt; 10; i++) { value = (\u0026#39;0\u0026#39; + i); key = \u0026#34;k\u0026#34; + value; batch.Put(key, value); } status = db-\u0026gt;Write(leveldb::WriteOptions(), \u0026amp;batch); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;批量写入成功\u0026#34; \u0026lt;\u0026lt; endl; } //批量删除  for(int i = 0; i \u0026lt; 10; i++) { key = \u0026#34;k\u0026#34; + (\u0026#39;0\u0026#39; + i); batch.Delete(key); } status = db-\u0026gt;Write(leveldb::WriteOptions(), \u0026amp;batch); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;批量删除成功\u0026#34; \u0026lt;\u0026lt; endl; } //关闭数据库  delete db; return 0; }   迭代器遍历 针对 DB 中所有的数据记录，LevelDB 不仅支持前向的遍历，也支持反向的遍历。在 DB 对象类型中，通过调用NewIterator 创建一个新的迭代器对象，该接口具体定义如下：\n1  Iterator* NewIterator(const ReadOptions\u0026amp;);    ReadOptions\u0026amp; option：用于指定在遍历访问过程中的相关设置。  这里有一个需要注意的地方，这里返回的迭代器不能直接使用，而是需要先使用对应的 Seek 操作偏移到指定位置后才能进行对应的迭代操作。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026#34;leveldb/iterator.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } leveldb::Iterator* it = db-\u0026gt;NewIterator(leveldb::ReadOptions()); //正向遍历  cout \u0026lt;\u0026lt; \u0026#34;开始正向遍历:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;SeekToFirst(); it-\u0026gt;Valid(); it-\u0026gt;Next()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } //逆向遍历  cout \u0026lt;\u0026lt; \u0026#34;开始逆向遍历:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;SeekToLast(); it-\u0026gt;Valid(); it-\u0026gt;Prev()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } //范围查询  cout \u0026lt;\u0026lt; \u0026#34;开始范围查询:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;Seek(\u0026#34;k4\u0026#34;); it-\u0026gt;Valid() \u0026amp;\u0026amp; it-\u0026gt;key().ToString() \u0026lt; \u0026#34;k8\u0026#34;; it-\u0026gt;Next()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } delete it; //关闭数据库  delete db; return 0; }   常用优化方案 压缩 当每一个块写入存储设备中时，可以选择是否对块进行压缩后再存储，以及 Options 参数的 compression 成员变量决定是否开启压缩。默认情况下，压缩是开启的，且压缩的速度很快，基本对整体的性能没有太大影响\n用户在调用时，可以用 kNoCompression 或 kSnappyCompression 对 compression 参数进行设定，从而确定块在实际存储过程中是否进行压缩。\n1 2 3  leveldb::Options op; op.compression = leveldb::kNoCompression;\t//不启用压缩 op.compression = leveldb::kSnappyCompression;\t//Snappy压缩   缓存 Cache的作用是充分利用内存空间，减少磁盘的 I/O 操作，从而提升整体运行性能。LevelDB 默认的 Cache 采用的是 LRU 算法，即近期最少使用的数据优先从 Cache 中淘汰，而经常使用的数据驻留在内存，从而实现对需要频繁读取的数据的快速访问。\nLevelDB 中定义了一个全局函数 NewLRUCache 用于创建一个 LRUCache。\n1 2 3  leveldb::Options op; op.block_cache = leveldb::NewLRUCache(10 * 1024 * 1024); //参数主要用于指定LevelDB的块的Cache空间，如果为NULL则默认为8MB   过滤器 由于 LevelDB 中所有的数据均保存在磁盘中，因而一次 Get 的调用，有可能导致多次的磁盘 I/O 操作。为了尽可能减少读过程时磁盘 I/O 的操作次数，LevelDB 采用了 FilterPolicy 机制。LevelDB 中 Options 对象类型的filter_policy 参数，主要用于确定运行过程中 Get 所遵循的 FilterPolicy 机制。\n用户可以通过调用 NewBloomFilterPolicy 接口函数以创建布隆过滤器，并将其赋值给对应的 filter_policy 参数。\n1 2  leveldb::Options op; op.filter_policy = leveldb::NewBloomFilterPolicy(10);   命名 LevelDB 中磁盘数据读取与缓存均以块为单位，并且实际存储中所有的数据记录均以 key 进行顺序存储。根据排序结果，相邻的 key 所对应的数据记录一般均会存储在同一个块中。正是由于这一特性，用户针对自身的应用场景需要充分考虑如何优化 key 的命名设计，从而最大限度地提升整体的读取性能。\n为了提升性能，命名规则是：**针对需要经常同时访问的数据，其 key 在命名时，可以通过将这些 key 设置相同的前缀保证这些数据的 key 是相邻近的，从而使这些数据可存储在同一个块内。**基于此，那些不常用的数据记录自然会放置在其他块内。\n","date":"2022-05-23T23:12:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/","title":"LevelDB 基础操作"},{"content":"LevelDB 基本概念 LevelDB 是一个由 Google 公司所研发的 K-V 存储嵌入式数据库管理系统编程库，以开源的 BSD 许可证发布。其作为 LSM Tree 的经典实现，具有很高的随机写，顺序读/写性能，但是随机读的性能很一般，也就是说，LevelDB很适合应用在查询较少，而写很多的场景。\n为什么需求K-V数据库？ K-V 数据库主要用于存取、管理关联性的数组。关联性数组又称映射、字典，是一种抽象的数据结构，其数据对象为一个个的 key-value 数据对，且在整个数据库中每个 key 均是唯一的。\n随着近年来互联网的兴起，云计算、大数据应用越来越广泛，对于数据库来说也出现了一些需要面对的新情况：\n 数据量呈指数级增长，存储也开始实现分布式。 查询响应时间要求越来越快，需在1秒内完成查询操作。 应用一般需要 7 × 24 小时连续运行，因此对稳定性要求越来越高，通常要求数据库支持热备份，并实现故障下快速无缝切换。 在某些应用中，写数据比读数据更加频繁，对数据写的速度要求也越来越高。 在实际应用中，并不是所有环境下的数据都是完整的结构化数据，非结构化数据普遍存在，因此如何实现对灵活多变的非结构化数据的支持是需要考虑的一个问题。  正是在上述情况的催生下，2010年开始兴起了一场 NoSQL 运动，而 K-V 数据库作为 NoSQL 中一种重要的数据库也日益繁荣，因此催生出了许多成功的商业化产品，并得到了广泛应用。\nBigTable与LevelDB 早在 2004 年，Google 开始研发一种结构化的分布式存储系统，它被设计用来处理海量数据，通常是分布在数千台普通服务器上的 PB 级的数据——这一系统就是风靡全球的 Bigtable。\n2006 年，Google 发表了一篇论文——《Bigtable: A Distributed Storage System for StructuredData》。这篇论文公布了 Bigtable 的具体实现方法，揭开了 Bigtable 的技术面纱。Bigtable 虽然也有行、列、表的概念，但不同于传统的关系数据库，从本质上讲，它是一个稀疏的、分布式的、持久化的、多维的排序键-值映射。\n虽然 Google 公布了 Bigtable 的实现论文，但 Bigtable 依赖于 Google 其他项目所开发的未开源的库，Google 一直没有将 Bigtable 的代码开源。然而这一切在 2011 年迎来了转机。Sanjay Ghemawat 和 Jeff Dean 这两位来自 Google 的重量级工程师，为了能将 Bigtable 的实现原理与技术细节分享给大众开发者，于2011年基于 Bigtable 的基本原理，采用 C++ 开发了一个高性能的 K-V 数据库——LevelDB。由于没有历史的产品包袱，LevelDB 结构简单，不依赖于任何第三方库，具有很好的独立性，虽然其有针对性地对 Bigtable 进行了一定程度的简化，然而Bigtable的主要技术思想与数据结构均在 LevelDB 予以体现了。因此 LevelDB 可看作 Bigtable 的简化版或单机版。\n特点 优点： key 与 value 采用字符串形式，且长度没有限制。 数据能持久化存储，同时也能将数据缓存到内存，实现快速读取。 基于 key 按序存放数据，并且 key 的排序比较函数可以根据用户需求进行定制。 支持简易的操作接口 API，如 Put、Get、Delete，并支持批量写入。 可以针对数据创建数据内存快照。 支持前向、后向的迭代器。 采用 Google 的 Snappy 压缩算法对数据进行压缩，以减少存储空间。 基本不依赖其他第三方模块，可非常容易地移植到 Windows、Linux、UNIX、Android、iOS。  缺点： 不是传统的关系数据库，不支持SQL查询与索引。 只支持单进程，不支持多进程。 不支持多种数据类型。 不支持 C/S 的访问模式。用户在应用时，需要自己进行网络服务的封装。  应用场景 LevelDB主要应用于查少写多的场景，如：\n  常见的 Web 场景，可以存储用户的个人信息资料、对文章或博客的评论、邮件等。\n  具体到电子商务领域，可以存储购物车中的商品、产品类别、产品评论。\n  存储整个网页，其将网页的 URL 作为 key，网页的内容作为 value。\n  构建更为复杂的存储系统，如日志系统、缓存、消息队列等。\n  ……\n  RocksDB RocksDB 是基于 LevelDB 开发的，并保留、继承了 LevelDB 原有的基本功能，也是一个嵌入式的 K-V 数据存储库。RocksDB 设计之初，正值 SSD 硬盘兴起。然而在当时，无论是传统的关系数据库如 MySQL，还是分布式数据库如 HDFS、HBase，均没有充分发挥 SSD 硬盘的数据读写性能。因而 Facebook 当时的目标就是开发一款针对 SSD 硬盘的数据存储产品，从而有了后面的 RocksDB。RocksDB 采用嵌入式的库模式，充分发挥了 SSD 的性能。\n 为什么要基于LevelDB实现RocksDB？\n一般而言，数据库产品有两种访问模式可供选择。一种是直接访问本地挂载的硬盘，即嵌入式的库模式；另一种是客户端通过网络访问数据服务器，并获取数据。假设 SSD 硬盘的读写约 100 μs，机械硬盘的读写约 10 ms，两台 PC 间的网络传输延迟为 50 μs。可以分析得知，如果在机械硬盘时代，采用 C/S 的数据服务模式，客户端进行一次数据查询约为 10.05 ms，可见网络延迟对于数据查询速度的影响微乎其微；而在 SSD 硬盘时代，客户端进行一次数据查询约为 150 μs，但与直接访问 SSD 硬盘相比，整体速度慢了 50%，因而直接影响了整体性能。正是在这样的背景下，Facebook的工程师们选择了 LevelDB 来实现 RocksDB 的原型。\n RocksDB 使用了一个插件式的架构，这使得它能够通过简单的扩展适用于各种不同的场景。插件式架构主要体现在以下几个方面：\n 不同的压缩模块插件：例如 Snappy、Zlib、BZip 等（LevelDB 只使用 Snappy）。 Compaction 过滤器：一个应用能够定义自己的 Compaction 过滤器，从而可以在 Compaction 操作时处理键。例如，可以定义一个过滤器处理键过期，从而使 RocksDB 有了类似过期时间的概念。 MemTable 插件：LevelDB 中的 MemTable 是一个 SkipList，适用于写入和范围扫描，但并不是所有场景都同时需要良好的写入和范围扫描功能，此时用 SkipList 就有点大材小用了。因此 RocksDB 的 MemTable 定义为一个插件式结构，可以是 SkipList，也可以是一个数组，还可以是一个前缀哈希表（以字符串前缀进行哈希，哈希之后寻找桶，桶内的数据可以是一个 B 树）。因为数组是无序的，所以大批量写入比使用 SkipList 具有更好的性能，但不适用于范围查找，并且当内存中的数组需要生成为 SSTable 时，需要进行再排序后写入Level 0。前缀哈希表适用于读取、写入和在同一前缀下的范围查找。因此可以根据场景使用不同的MemTable。 SSTable 插件：SSTable 也可以自定义格式，使之更适用于 SSD 的读取和写入。除了插件式架构，RocksDB 还进行了一些写入以及 Compaction 操作方面的优化，主要有以下几个方面：  线程池：可以定义一个线程池进行 Level 0～Level 5 的 Compaction 操作，另一个线程池进行将MemTable 生成为 SSTable 的操作。如果 Level 0～Level 5 的 Compaction 操作没有重叠的文件，可以并行操作，以加快 Compaction 操作的执行。 多个 Immutable MemTable：当 MemTable 写满之后，会将其赋值给一个 ImmutableMemTable，然后由后台线程生成一个 SSTable。但如果此时有大量的写入，MemTable 会迅速再次写满，此时如果Immutable MemTable 还未执行完 Compaction 操作就会阻塞写入。因此 RocksDB 使用一个队列将Immutable MemTable 放入，依次由后台线程处理，实现同时存在多个 ImmutableMemTable。以此优化写入，避免写放大，当使用慢速存储时也能够加大写吞吐量。    ","date":"2022-05-23T23:11:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"LevelDB 基本概念"},{"content":"分布式原理 分布式存储 路由 当索引一个文档的时候，Elasticsearch会通过哈希来决定将文档存储到哪一个主分片中，路由计算公式如下：\n1 2 3 4  shard = hash(routing) % number_of_primary_shards //routing：默认为文档id,也可以自定义。 //number_of_primary_shards：主分片的数量     查询时指定routing：可以直接根据routing信息定位到某个分片查询，不需要查询所有的分配，经\n过协调节点排序。\n  查询时不指定routing：因为不知道要查询的数据具体在哪个分片上，所以整个过程分为 2 个步骤\n 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上。 聚合：协调节点搜集到每个分片上查询结果，在将查询的结果进行排序，之后给用户返回结果。    从上面的这个公式我们也可以看到一个问题，路由的逻辑与当前主分片的数量强关联，也就是说如果分片数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。这也就是为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量。\n 分片数量固定是否意味着会使索引难以进行扩容？\n 答案是否定的，Elasticsearch还提供了其他的一些方案来让我们轻松的实现扩容，如：\n 分片预分配：一个分片存在于单个节点，但一个节点可以持有多个分片。因此我们可以根据未来的数据的扩张状况来预先分配一定数量的分片到各个节点中。（注意⚠️：预先分配过多的分片会导致性能的下降以及影响搜索结果的相关度） 新建索引：分片数不够时，可以考虑新建索引，搜索1个有着50个分片的索引与搜索50个每个都有1个分片的索引完全等价。  更多关于水平拓展的内容可以参考官方文档扩容设计。\n新增、索引和删除文档 我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。 在下面的例子中，将所有的请求发送到 Node 1 ，我们将其称为协调节点(coordinating node) 。\n 当发送请求的时候， 为了扩展负载，更好的做法是轮询集群中所有的节点。\n 新建、索引和删除请求都是写操作， 必须在主分片上面完成之后才能被复制到相关的副本分片。\n流程如下：\n 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。  取回文档 由于取回文档为读操作，我们可以从主分片或者从其它任意副本分片检索文档。\n流程如下：\n 客户端向 Node 1 发送获取请求。 节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。 Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。  在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。\n并发控制 在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失：\n 悲观并发控制：这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。 乐观并发控制：Elasticsearch中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。 然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。 例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。  Elasticsearch是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许会乱序。所以Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。\n在Elasticsearch中，其通过版本号机制来实现乐观并发控制。即每一个文档中都会有一个_version版本号字段，当文档被修改时版本号递增。 Elasticsearch使用_version来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。\n我们可以利用_version号来确保应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的 version 号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  // 例如我们想更新文档的内容，并指定版本号为1 PUT /website/blog/1?version=1 { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Starting to get the hang of this...\u0026#34; } // 当文档的版本号为1时，次请求成功，同时响应体告诉我们版本号递增到2 { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2 \u0026#34;created\u0026#34;: false } // 此时我们再次尝试更新文档的内容，仍然指定版本号为1，由于版本号不符合，此时返回409 Conflict HTTP 响应码 { \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[blog][1]: version conflict, current [2], provided [1]\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;shard\u0026#34;: \u0026#34;3\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[blog][1]: version conflict, current [2], provided [1]\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;shard\u0026#34;: \u0026#34;3\u0026#34; }, \u0026#34;status\u0026#34;: 409 }   分布式搜索 搜索需要一种更加复杂的执行模型，因为我们不知道查询会命中哪些文档，这些文档有可能在集群的任何分片上。 一个搜索请求必须询问我们关注的索引的所有分片的某个副本来确定它们是否含有任何匹配的文档。\n但是找到所有的匹配文档仅仅完成事情的一半。 在 search 接口返回一个 page 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为query then fetch（查询后取回）。\n查询阶段 在查询阶段时， 查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的优先队列。\n查询阶段包含以下三个步骤\n 客户端发送一个 search 请求到 Node 3 ，此时Node 3成为协调节点，由它来负责本次的查询。 Node 3 将查询请求广播到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的ID和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。至此查询过程结束。   一个索引可以由一个或几个主分片组成， 所以一个针对单个索引的搜索请求需要能够把来自多个分片的结果组合起来。 针对 multiple 或者 all 索引的搜索工作方式也是完全一致的——仅仅是包含了更多的分片而已。\n 取回阶段 在查询阶段中，我们标识了哪些文档满足搜索请求，而接下来我们就需要取回这些文档。\n取回阶段由以下步骤构成\n 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。例如，如果我们的查询指定了 { \u0026quot;from\u0026quot;: 90, \u0026quot;size\u0026quot;: 10 } ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。 每个分片加载并丰富文档（如_source字段和高亮参数），接着返回文档给协调节点。 协调节点等待所有文档被取回，将结果返回给客户端。  集群内部原理 集群与节点 一个运行中的Elasticsearch实例称为一个节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n由于Elasticsearch采用了主从模式，所以当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 因为主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。\n作为用户，我们可以将请求发送到集群中的任何节点（这个处理请求的节点也叫做协调节点）。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。\n分片 在分布式系统中，单机无法存储规模巨大的数据，要依靠大规模集群处理和存储这些数据，一般通过增加机器数量来提高系统水平扩展能力。因此，需要将数据分成若干小块分配到各个机器上。然后通过某种路由策略找到某个数据块所在的位置。\n分片（shard）是底层的基本读写单元，分片的目的是分割巨大索引，让读写可以并行操作，由多台机器共同完成。读写请求最终落到某个分片上，分片可以独立执行读写工作。Elasticsearch利用分片将数据分发到集群内各处。分片是数据的容器，文档保存在分片内，不会跨分片存储。分片又被分配到集群内的各个节点里。当集群规模扩大或缩小时，Elasticsearch会自动在各节点中迁移分片，使数据仍然均匀分布在集群里。\n为了应对并发更新问题，Elasticsearch将分片分为两部分，即主分片（primary shard）和副本分片（replica shard）。主数据作为权威数据，写过程中先写主分片，成功后再写副分片，恢复阶段以主分片为准。\n一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。\n 那索引与分片之间又有什么关系呢？\n 一个Elasticsearch索引包含了很多个分片，每个分片又是一个Lucene的索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。Lucene索引又由很多分段组成，每个分段都是一个倒排索引。Elasticsearch每次refresh都会生成一个新的分段，其中包含若干文档的数据。在每个分段内部，文档的不同字段被单独建立索引。每个字段的值由若干词（Term）组成，Term是原文本内容经过分词器处理和语言处理后的最终结果。\n选举 在主节点选举算法的选择上，基本原则是不重复造轮子。最好实现一个众所周知的算法，这样的好处是其中的优点和缺陷是已知的。Elasticsearch的选举算法的选择上主要考虑下面两种。\n Bully算法：Leader选举的基本算法之一。它假定所有节点都有一个唯一的ID，使用该ID对节点进行排序。任何时候的当前Leader都是参与集群的最高ID节点。该算法的优点是易于实现。但是，当拥有最大ID的节点处于不稳定状态的场景下会有问题。例如，Master负载过重而假死，集群拥有第二大ID的节点被选为新主，这时原来的Master恢复，再次被选为新主，然后又假死…… Paxos算法：Paxos非常强大，尤其在什么时机，以及如何进行选举方面的灵活性比简单的Bully算法有很大的优势，因为在现实生活中，存在比网络连接异常更多的故障模式。但Paxos实现起来非常复杂。  Elasticsearch的选主算法是基于Bully算法的改进，主要思路是对节点ID排序，取ID值最大的节点作为Master，每个节点都运行这个流程。同时，为了解决Bully算法的缺陷，其通过推迟选举，直到当前的Master失效来解决上述问题，只要当前主节点不挂掉，就不重新选主。但是容易产生脑裂（双主），为此，再通过法定得票人数过半解决脑裂问题。\nElasticsearch对Bully附加的三个约定条件\n 参选人数需要过半。当达到多数时就选出临时主节点，为什么是临时的？每个节点运行排序取最大值的算法，结果不一定相同。举个例子，集群有5台主机，节点ID分别是1、2、3、4、5。当产生网络分区或节点启动速度差异较大时，节点1看到的节点列表是1、2、3、4，选出4；节点2看到的节点列表是2、3、4、5，选出5。结果就不一致了，由此产生下面的第二条限制。 得票数需要过半。某节点被选为主节点，必须判断加入它的节点数达到半数以上，才确认Master身份（推迟选举）。 当探测到节点离开事件时，必须判断当前节点数是否过半。如果达不到半数以上，则放弃Master身份，重新加入集群。如果不这么做，则设想以下情况：假设5台机器组成的集群产生网络分区，2台一组，3台一组，产生分区前，Master位于2台中的一个，此时3台一组的节点会重新并成功选取Master，产生双主，俗称脑裂。（节点失效检测）  流程如下图\n节点失效检测会监控节点是否离线，然后处理其中的异常。失效检测是选主流程之后不可或缺的步骤，不执行失效检测可能会产生脑裂（双主或多主）。在此我们需要启动两种失效探测器：\n 在Master节点，启动NodesFaultDetection，简称NodesFD。定期探测加入集群的节点是否活跃。 非Master节点启动MasterFaultDetection，简称MasterFD。定期探测Master节点是否活跃。  分片内部原理 索引不变性 早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。\n倒排索引被写入磁盘后是不可改变的，索引的不变性具有以下好处：\n 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。 一旦索引被读入内核的文件系统缓存，便会留在哪里。由于其不变性，只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。 缓存(像过滤器缓存)在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。  当然，一个不变的索引也有不好的地方。最大的缺点就是它是不可变的，我们无法对其进行修改。如果我们需要让一个新的文档可被搜索，就需要重建整个索引。这不仅对一个索引所能包含的数据量造成了巨大的限制，而且对索引可被更新的频率同样造成了影响。\n动态更新索引  那么我们如何能在保留不变性的前提下实现倒排索引的动态更新呢？\n 答案就是使用更多的索引，即新增内容并写到一个新的倒排索引中，查询时，每个倒排索引都被轮流查询，查询完再对结果进行合并。\nElasticsearch基于Lucene引入了按段写入的概念——每次内存缓冲的数据被写入文件时，会产生一个新的Lucene段，每个段都是一个倒排索引。同时，在提交点中描述了当前Lucene索引都含有哪些分段。\n按段写入的流程如下：\n 新文档被收集到内存的索引中缓存 当缓存堆积到一定规模时，就会进行提交  一个新的段（倒排索引）被写入磁盘。 一个新的提交点被写入磁盘。 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。   新的段被开启，让它包含的文档可见以被搜索 内存缓存被清空，等待接收新的文档  当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。\n 那插入和更新又如何实现呢？\n 段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。\n  当一个文档被删除时，它实际上只是在 .del 文件中被标记删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。\n  当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。\n  近实时搜索 Elasticsearch和磁盘之间是文件系统缓存，在执行写操作时，为了降低从索引到可被搜索的延迟，一般新段会被先写入到文件系统缓存，再将这些数据写入硬盘（磁盘I/O是性能瓶颈）。\n在写操作中，一般会先在内存中缓冲一段数据，再将这些数据写入硬盘，每次写入硬盘的这批数据称为一个分段。如同任何写操作一样，通过操作系统的write接口写到磁盘的数据会先到达系统缓存（内存）。write函数返回成功时，数据未必被刷到磁盘。通过手工调用flush，或者操作系统通过一定策略将文件系统缓存刷到磁盘。\n这种策略大幅提升了写入效率。从write函数返回成功开始，无论数据有没有被刷到磁盘，只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\nLucene允许新段被写入和打开——使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\nElasticsearch中将写入和打开一个新段的过程叫做refresh(刷新) 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说Elasticsearch是近实时搜索——文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n事务日志 由于系统先缓冲一段数据才写，且新段不会立即刷入磁盘，这两个过程中如果出现某些意外情况（如主机断电），则会存在丢失数据的风险。\n为了解决这个问题，Elasticsearch增加了一个translog（事务日志），在每一次对Elasticsearch进行操作时均进行了日志记录，当Elasticsearch启动的时候，重放translog中所有在最后一次提交后发生的变更操作。\n其执行流程如下：\n 一个文档被索引之后，就会被添加到内存缓冲区，并且追加到了translog  新的文档被添加到内存缓冲区并且被追加到了事务日志，如下图    分片会每秒自动执行一次刷新，这些内存缓冲区的文档被写入新的段中并打开以便搜索，同时清空内存缓冲区。  刷新完成后, 缓存被清空但是事务日志不会，同时新段写入文件系统缓冲区    这个进程继续工作，更多的文档被添加到内存缓冲区和追加到translog  事务日志不断积累文档    当translog足够大时，就会执行全量提交，对文件系统缓存执行flush，将其内容全部写入硬盘中，并清空事务日志。  在刷新（flush）之后，段被全量提交，并且事务日志被清空     除此之外，translog还有下面这些功能\n translog提供所有还没有被刷到磁盘的操作的一个持久化纪录。当Elasticsearch启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放translog中所有在最后一次提交后发生的变更操作。 translog也被用来提供实时CRUD 。当你试着通过ID查询、更新、删除一个文档，在从相应的段中检索之前， 首先检查translog任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。  段合并 由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段，所以段越多，搜索也就越慢。\nElasticsearch通过在后台进行段合并来解决这个问题，其会选择大小相似的分段进行合并。在合并过程中，标记为删除（更新）的数据不会写入新分段，当合并过程结束，旧的分段数据被删除，标记删除的数据才从磁盘删除。\n流程如下图\n合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。\n整体写入流程如下图\n ","date":"2022-05-23T22:20:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86/","title":"ElasticSearch 分布式原理"},{"content":"索引原理 倒排索引 例如，假设我们有两个文档，每个文档的 content 域包含如下内容：\n The quick brown fox jumped over the lazy dog Quick brown foxes leap over lazy dogs in summer  为了创建倒排索引，首先我们需要借助分词器，将每个文档的 content 域拆分成单独的词（我们称它为 词条 或 tokens、term ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  Term Doc_1 Doc_2 ------------------------- Quick | | X The | X | brown | X | X dog | X | dogs | | X fox | X | foxes | | X in | | X jumped | X | lazy | X | X leap | | X over | X | X quick | X | summer | | X the | X | ------------------------   如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档：\n1 2 3 4 5 6  Term Doc_1 Doc_2 ------------------------- brown | X | X quick | X | ------------------------ Total | 2 | 1   这里我们匹配到了两个文档（为了节省空间，这里返回的只是文档ID，最后再通过文档id去查询到具体文档。）。对于搜索引擎来说，用户总希望能够先看到相关度更高的结果，因此实际使用时我们通过一些算法来进行权重计算，将查询的结果按照权重降序返回。\nTerm dictionary与Term index Elasticsearch为了能够快速的在倒排索引中找到某个term，他会按照字典序对所有的term进行排序，再通过二分查找来找到term，这就是Term Dictionary，但即使有了Term Dictionary，O(logN)的磁盘读写仍然是影响性能的一大问题。\nB-Tree通过减少磁盘寻道次数来提高查询性能，Elasticsearch也是采用同样的思路，直接通过内存查找term，不读磁盘，但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index，从下图可以看出，Term Index其实就是一个Trie树（前缀树）\n这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。\n 为什么Elasticsearch/Lucene检索比mysql快呢？\n Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。检索一个term需要若干次的random access的磁盘操作。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。\n列式存储——Doc Values Doc values的存在是因为倒排索引只对某些操作是高效的。 倒排索引的优势在于查找包含某个项的文档，而对于从另外一个方向的相反操作并不高效，即：确定哪些项是否存在单个文档里，聚合需要这种次级的访问模式。\n以排序来举例——虽然倒排索引的检索性能非常快，但是在字段值排序时却不是理想的结构。\n 在搜索的时候，我们能通过搜索关键词快速得到结果集。 当排序的时候，我们需要倒排索引里面某个字段值的集合。换句话说，我们需要转置倒排索引。  转置 结构经常被称作 列式存储 。它将所有单字段的值存储在单数据列中，这使得对其进行操作是十分高效的，例如排序、聚合等操作。\n在Elasticsearch中，Doc Values就是一种列式存储结构，在索引时与倒排索引同时生成。也就是说Doc Values和倒排索引一样，基于 Segement生成并且是不可变的。同时Doc Values和倒排索引一样序列化到磁盘。\nDoc Values常被应用到以下场景：\n 对一个字段进行排序 对一个字段进行聚合 某些过滤，比如地理位置过滤 某些与字段相关的脚本计算  下面举一个例子，来讲讲它是如何运作的\n 假设存在以下倒排索引\n1 2 3 4 5 6  Term Doc_1 Doc_2 Doc_3 ------------------------------------ brown | X | X | dog | X | | X dogs | | X | X ------------------------------------   那么其生成的DocValues如下（实际存储时不会存储doc_id，值所在的顺位即为doc_id）\n1 2 3 4 5 6 7 8 9  Doc_id Values ------------------ Doc_1 | brown | Doc_1 | dog | Doc_2 | brown | Doc_2 | dogs | Doc_3 | dog | Doc_3 | dogs | ------------------   假设我们需要计算出brown出现的次数\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  GET /my_index/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;:{ \u0026#34;body\u0026#34;:\u0026#34;brown\u0026#34; } }, \u0026#34;aggs\u0026#34;:{ \u0026#34;popular_terms\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;body\u0026#34; } } }, \u0026#34;size\u0026#34; : 0 }   下面来分析上述请求在ES中是如何来进行查询的：\n 定位数据范围。通过倒排索引，来找到所有包含brown的doc_id。 进行聚合计算。借助doc_id在doc_values中定位到为brown的字段，此时进行聚合累加得到计算结果。browm的count=2。   但是doc_values仍存在一些问题，其不支持analyzed类型的字段，因为这些字段在进行文本分析时可能会被分词处理，从而导致doc_values将其存储为多行记录。但是在我们实际使用时，为什么仍然能对analyzed的字段进行聚合操作呢？这时就需要介绍一下Fielddata\n Fielddata doc values不生成分析的字符串，那为什么这些字段仍然可以使用聚合呢？是因为使用了fielddata的数据结构。与doc values不同，fielddata构建和管理100%在内存中，常驻于JVM内存堆。\n 从历史上看，fielddata 是所有字段的默认设置。但是Elasticsearch已迁移到doc values以减少 OOM 的几率。分析的字符串是仍然使用fielddata的最后一块阵地。 最终目标是建立一个序列化的数据结构类似于doc values ，可以处理高维度的分析字符串，逐步淘汰 fielddata。\n 它的一些特性如下\n 延迟加载。如果你从来没有聚合一个分析字符串，就不会加载fielddata到内存中，其是在查询时候构建的。 基于字段加载。 只有很活跃地使用字段才会增加fielddata的负担。 会加载索引中（针对该特定字段的） 所有的文档，而不管查询是否命中。逻辑是这样：如果查询会访问文档 X、Y 和 Z，那很有可能会在下一个查询中访问其他文档。 如果空间不足，使用最久未使用（LRU）算法移除fielddata。  因此，在聚合字符串字段之前，请评估情况：\n 这是一个not_analyzed字段吗？如果是，可以通过doc values节省内存 。 否则，这是一个analyzed字段，它将使用fielddata并加载到内存中。这个字段因为N-grams有一个非常大的基数？如果是，这对于内存来说极度不友好。  索引压缩 FOR编码 在Elasticsearch中，为了能够更方便的计算交集和并集，它要求倒排索引是有序的，而这个特点同时也带来了一个额外的好处，我们可以使用增量编码来压缩倒排索引，也就是FOR（Frame of Reference）编码\n 增量编码压缩，将大数变小数，按字节存储\n FOR编码分为三个步骤\n 增量编码 增量分区 位压缩  如下图所示，如果我们的倒排索引中存储的文档id为[73, 300, 302, 332, 343, 372]，那么经过增量编码后的结果则为[73, 227, 2, 30, 11, 29]。这种压缩的好处在哪里呢？我们通过增量将原本的大数变成了小数，使得所有的增量都在0～255之间，因此每一个值就只需要使用一个字节就可以存储，而不会使用int或者bigint，大大的节约了空间。\n接着，第二步我们将这些增量分到不同的区块中（Lucene底层用了256个区块，下面为了方便展示之用了两个）。\n第三步，我们计算出每组数据中最大的那个数所占用的bit位数，例如下图中区块1最大的为227，所以只占用8个bit位，所以三个数总共占用3 * 8bits即3字节。而区块2最大为29，只占用5个bit位，因此这三个数总共占用3 * 5bits即差不多2字节。通过这种方法，将原本6个整数从24字节压缩到了5字节，效果十分出色。\nRoaring Bitmaps FOR编码对于倒排索引来说效果很好，但对于需要存储在内存中的过滤器缓存等不太合适，两者之间有很多不同之处：\n 由于我们仅仅缓存那些经常使用的过滤器，因此它的压缩率并不需要像倒排索引那么高（倒排索引需要对每个词都进行编码）。 缓存过滤器的目的就是为了加速处理效率，因此它必须要比重新执行过滤器要快，因此使用一个好的数据结构和算法非常重要。 缓存的过滤器存储在内存之中，而倒排索引通常存储在磁盘中。  基于以上的不同，对于缓存来说FOR编码并不适用，因此我们还需要考虑其他的一些选择。\n 整数数组：数组可能是我们马上能想到的最简单的实现方式，我们将文档id存储在数组中，这样就使得我们的迭代变得非常简单，但是这种方法的内存利用率又十分低下，因为每个文档都需要4个字节。 Bitmaps：在数据分布密集的下，位图是一个很好的选择。它本质上就是一个数组，其中每一个文档id占据一个位，用0和1来标记文档是否存在。这种方法大大节约了内存，将一个文档从4字节降低到了一个位，但是一旦数据分布稀疏，此时的位图性能将大打折扣，因为无论数据量多少，位图的大小都是由数据的上下区间来决定的。 Roaring Bitmaps：Roaring Bitmaps即是对位图的一种优化，它会根据16位最高位将倒排索引划分为多块，如第一个块将对0到65535之间的值进行编码，第二个块将在65536和131071之间进行编码。在每一个块中，我们再对低16位进行编码，如果它的值小于4096在使用数组，否则就使用位图。由于我们编码的时候只会对低16位进行编码，因此在这里数组每个元素只需要2个字节   为什么要使用4096作为数组和位图选取的阈值呢？\n 下面是官方给出的数据报告，在一个块中只有文档数量超过4096，位图的内存优势才会凸显出来\n这就是Roaring Bitmaps高效率的原因，它基于两种完全不同的方案来进行编码，并根据内存效率来动态决定使用哪一种方案。\n官方也给出了几种方案的性能测试\n从上述对比可以看出，没有一种方法是完美的，但是以下两种方法的巨大劣势使得它们不会被选择\n 数组：性能很好，但是内存占用巨大。 Bitmaps：数据稀疏分布的时候内存和性能都会大打折扣。  因此在综合考量下，Elasticsearch还是选择使用Roaring Bitmaps，并且在很多大家了解的开源大数据框架中，也都使用了这一结构，如Hive、Spark、Kylin、Druid等。\n联合索引 如果多个字段索引的联合查询，倒排索引如何满足快速查询的要求呢？\n 跳表：同时遍历多个字段的倒排索引，互相skip。 位图：对多个过滤器分别求出位图，对这几个位图做AND操作。  Elasticsearch支持以上两种的联合索引方式，如果查询的过滤器缓存到了内存中（以位图的形式），那么合并就是两个位图的AND。如果查询的过滤器没有缓存，那么就用跳表的方式去遍历两个硬盘中的倒排索引。\n假设有下面三个倒排索引需要联合索引：\n 如果使用跳表，则对最短的倒排索引中的每一个id，逐个在另外两个倒排索引中查看是否存在，来判断是否存在交集。 如果使用位图，则直接将几个位图按位与运算，最终得到的结果就是最后的交集。  ","date":"2022-05-23T22:18:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/","title":"ElasticSearch 索引原理"},{"content":"权重计算原理 在Elasticsearch中，每个文档都有相关性评分，用一个正浮点数字段 _score 来表示 。 _score 的评分越高，相关性越高。为了保证搜索到的结果相关度更高，在默认情况下返回结果会按照相关度降序排序。\nElasticsearch使用布尔模型查找匹配文档，并用一个名为实用评分函数的公式来计算相关度。这个公式借鉴了词频/逆向文档频率和向量空间模型，同时也加入了一些现代的新特性，如协调因子，字段长度归一化，以及词或查询语句权重提升。\n布尔模型 布尔模型（Boolean Model） 适用于在查询中使用 AND 、 OR 和 NOT （与、或、非）这样的条件来查找匹配的文档，如以下查询：\n1  fullANDtextANDsearchAND(elasticsearchORlucene)  会将所有包括词 full 、 text 和 search ，以及 elasticsearch 或 lucene 的文档作为结果集。\n这个过程简单且快速，它将所有可能不匹配的文档排除在外。\n词频/逆向文档频率（TF/IDF） Elasticsearch的相似度算法被定义为检索词频率/反向文档频率（TF/IDF），主要依赖以下内容\n  检索词频率\n  检索词在该字段出现的频率。出现频率越高，相关性也越高。5次提到同一词的字段比只提到1次的更相关。\n  词频的计算方式如下：\n1  tf(t in d) = √frequency   词 t 在文档 d 的词频（ tf ）是该词在文档中出现次数的平方根。\n    逆向文档频率\n  每个检索词在索引中出现的频率。频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。\n  逆向文档频率的计算公式如下：\n1  idf(t) = 1 + log ( numDocs / (docFreq + 1))   词 t 的逆向文档频率（ idf ）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。\n    字段长度归一值\n  字段的长度。长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。\n  字段长度的归一值公式如下：\n1  norm(d) = 1 / √numTerms   字段长度归一值（ norm ）是字段中词数平方根的倒数。\n    以上三个因素是在索引时计算并存储的。最后将它们结合在一起计算单个词在特定文档中的权重 。\n向量空间模型 当然，查询通常不止一个词，所以需要一种合并多词权重的方式——向量空间模型（vector space model）。\n向量空间模型（vector space model）提供一种比较多词查询的方式，单个评分代表文档与查询的匹配程度，为了做到这点，这个模型将文档和查询都以 向量（vectors） 的形式表示，而向量实际上就是包含多个数的一维数组，例如：\n1  [1,2,5,22,3,8]   在向量空间模型里，向量空间模型里的每个数字都代表一个词的权重 ，与词频/逆向文档频率计算方式类似，下面举一个例子。\n设想如果查询 “happy hippopotamus” ，常见词 happy 的权重较低，不常见词 hippopotamus 权重较高，假设 happy 的权重是 2 ， hippopotamus 的权重是 5 ，可以将这个二维向量—— [2,5] ——在坐标系下作条直线，线的起点是 (0,0) 终点是 (2,5) ，如下图\n现在，设想我们有三个文档：\n I am happy in summer 。 After Christmas I’m a hippopotamus 。 The happy hippopotamus helped Harry 。  三篇文档的命中词如下\n 文档 1： (happy,____________) —— [2,0] 文档 2： ( ___ ,hippopotamus) —— [0,5] 文档 3： (happy,hippopotamus) —— [2,5]  可以为每个文档都创建包括每个查询词—— happy 和 hippopotamus ——权重的向量，然后将这些向量置入同一个坐标系中，如下图\n向量之间是可以比较的，只要测量查询向量和文档向量之间的角度就可以得到每个文档的相关度，文档 1 与查询之间的角度最大，所以相关度低；文档 2 与查询间的角度较小，所以更相关；文档 3 与查询的角度正好吻合，完全匹配。\n 在实际中，只有二维向量（两个词的查询）可以在平面上表示，幸运的是， 线性代数为我们提供了计算两个多维向量间角度工具，这意味着可以使用如上同样的方式来解释多个词的查询。\n ","date":"2022-05-23T22:17:18+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E6%9D%83%E9%87%8D%E8%AE%A1%E7%AE%97/","title":"ElasticSearch 权重计算"},{"content":"基本操作 环境搭建 搭建Elasticsearch环境 下载docker镜像\n1  docker pull elasticsearch:7.4.2   映射配置文件\n1 2 3 4 5 6 7 8 9  # 配置映射文件夹 mkdir -p /mydata/elasticsearch/config mkdir -p /mydata/elasticsearch/data # 设置文件夹权限任何用户可读可写 chmod 777 /mydata/elasticsearch -R # 配置 http.host echo \u0026#34;http.host: 0.0.0.0\u0026#34; \u0026gt;\u0026gt; /mydata/elasticsearch/config/elasticsearch.yml   启动容器\n1 2 3 4 5 6 7  docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\ -e \u0026#34;discovery.type\u0026#34;=\u0026#34;single-node\u0026#34; \\\t # 设置为单节点 -e ES_JAVA_OPTS=\u0026#34;-Xms64m -Xmx128m\u0026#34; \\ # 设置启动时ES的初始内存以及最大内存 -v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\ -v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\ -v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\ -d elasticsearch:7.4.2   访问ES服务，http://82.157.127.173:9200/\n得到相应体如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  { \u0026#34;name\u0026#34; : \u0026#34;de85ed684243\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;UeIP1PrXT2OFd7FlEEl3hQ\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.4.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;2f90bbf7b93631e52bafb59b3b049cb44ec25e96\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2019-10-28T20:40:44.881551Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.2.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; }   可以通过/_cat来获取节点信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # 访问http://82.157.127.173:9200/_cat # 属性列表 /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates   搭建Kibana环境 下载docker镜像\n1  docker pull kibana:7.4.2   启动容器\n1  docker run --name kibana -e ELASTICSEARCH_HOSTS=http://192.168.0.128:9200 -p 5601:5601 -d kibana:7.4.2   访问Kibana服务，http://192.168.0.128:5601/\nRESTful  一种软件架构风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。\n ES的基本REST命令：\n   Method Url 描述     PUT localhost:9200/索引名称/类型名称/文档id 创建文档（指定文档id）   GET localhost:9200/索引名称/类型名称/文档id 通过文档id查询文档   POST localhost:9200/索引名称/类型名称 创建文档（随机文档id）   POST localhost:9200/索引名称/类型名称/文档id/_update 修改文档   POST localhost:9200/索引名称/类型名称/_search 查询所有数据   DELETE localhost:9200/索引名称/类型名称/文档id 删除文档    CRUD 创建索引 在创建索引时，我们可以声明字段与数据类型的映射\n请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13  PUT /test0 { \u0026#34;mappings\u0026#34;:{ \u0026#34;properties\u0026#34;:{ \u0026#34;name\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; }, \u0026#34;author\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; } } } }   响应:\n1 2 3 4 5  { \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;index\u0026#34;: \u0026#34;test0\u0026#34; }   即使如果我们没有配置类型，ES也会根据字段的内容来自行推导。\n 注意⚠️：由于索引具有不变性，我们只能进行追加而不能更改已经存在的映射字段，必须创建新的索引后进行数据迁移。\n 1 2 3 4 5 6 7 8 9  POST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;test0\u0026#34; }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;test1\u0026#34; } }   插入文档 PUT和POST都可以插入文档：\n POST：如果不指定 id，自动生成 id。如果指定 id，则修改这条记录，并新增版本号。 PUT：必须指定 id，如果没有这条记录，则新增，如果有，则更新。   示例：在 test1 索引下的books类型中保存标识为 1 的文档。\n 请求：\n1 2 3 4 5  PUT /test1/books/1 { \u0026#34;name\u0026#34;:\u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34; : \u0026#34;Daniel Defoe\u0026#34; }   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test\u0026#34;,\t//索引  \u0026#34;_type\u0026#34;: \u0026#34;book\u0026#34;,\t//类型  \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;,\t//id  \u0026#34;_version\u0026#34;: 1,\t//版本号  \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;,\t//操作类型  \u0026#34;_shards\u0026#34;: {\t//分片  \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1,\t//并发控制字段，每次更新就会+1，用来做乐观锁  \u0026#34;_primary_term\u0026#34;: 1 //同上，主分片重新分配，如重启，就会变化 }   查询文档  示例：查询test1索引下的books类型中保存标识为 1 的文档的内容。\n 请求：\n1  GET /test1/books/1   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Daniel Defoe\u0026#34; } }   更新文档 使用POST命令，在ID后面加_update，并把需要修改的内容放入doc属性中\n 示例：更新test1 索引下的books类型中保存标识为 1 的文档的内容。\n 请求：\n1 2 3 4 5 6 7 8  POST /test1/books/1/_update { \u0026#34;doc\u0026#34; : { \u0026#34;name\u0026#34;:\u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34; : \u0026#34;Daniel Defoe\u0026#34;, \u0026#34;country\u0026#34; : \u0026#34;England\u0026#34; } }   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1, \u0026#34;_primary_term\u0026#34;: 1 }   删除文档和索引 删除使用DELETE命令\n 示例：删除文档/test1/books/1\n 请求：\n1  DELETE /test1/books/1   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 3, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 2, \u0026#34;_primary_term\u0026#34;: 1 }    示例：删除索引/test1\n 1  DELETE /test1   响应:\n1 2 3  { \u0026#34;acknowledged\u0026#34;: true }   Search 为了方便测试，可以从官网导入测试数据https://download.elastic.co/demos/kibana/gettingstarted/accounts.zip\n1 2 3 4 5 6  POST /test_data/account/_bulk {\u0026#34;index\u0026#34;:{\u0026#34;_id\u0026#34;:\u0026#34;1\u0026#34;}} {\u0026#34;account_number\u0026#34;:1,\u0026#34;balance\u0026#34;:39225,\u0026#34;firstname\u0026#34;:\u0026#34;Amber\u0026#34;,\u0026#34;lastname\u0026#34;:\u0026#34;Duke\u0026#34;,\u0026#34;age\u0026#34;:32,\u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;880 Holmes Lane\u0026#34;,\u0026#34;employer\u0026#34;:\u0026#34;Pyrami\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;amberduke@pyrami.com\u0026#34;,\u0026#34;city\u0026#34;:\u0026#34;Brogan\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;IL\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_id\u0026#34;:\u0026#34;6\u0026#34;}} {\u0026#34;account_number\u0026#34;:6,\u0026#34;balance\u0026#34;:5686,\u0026#34;firstname\u0026#34;:\u0026#34;Hattie\u0026#34;,\u0026#34;lastname\u0026#34;:\u0026#34;Bond\u0026#34;,\u0026#34;age\u0026#34;:36,\u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;671 Bristol Street\u0026#34;,\u0026#34;employer\u0026#34;:\u0026#34;Netagy\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;hattiebond@netagy.com\u0026#34;,\u0026#34;city\u0026#34;:\u0026#34;Dante\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;TN\u0026#34;} ....................   查询方式 ES支持两种查询方式，一种是直接在URL后加上参数，另一种是在URL后加上JSON格式的请求体。\n 示例：查找到收入最高的十条记录\n URL + 参数 常用的参数如下\n q：用于指定搜索的关键词。 from \u0026amp; size：类似于SQL中的offset和limit。 sort：对结果排序，默认为降序。 _source：指定想要返回的属性。  1  GET /test_data/_search?q=*\u0026amp;sort=balance:desc\u0026amp;from=0\u0026amp;size=10   URL + QueryDSL 1 2 3 4 5 6 7 8 9 10 11  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34; : {} }, \u0026#34;sort\u0026#34; : [{ \u0026#34;balance\u0026#34; : \u0026#34;desc\u0026#34; }], \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 10 }   虽然URL+参数的写法非常简洁，但是随着逻辑的复杂化，其可读性也越来越差，所以通常都会使用URL + QueryDSL的格式。\nmatch 匹配 match 匹配查询 无论你在任何字段上进行的是全文搜索还是精确查询，match 查询是你可用的标准查询。\n对于not_analyzed的字段，match能做到精确查询，而对于analyzed的字段，match能做到匹配查询（全文搜索）。\n 示例：查找所有年龄为25岁的记录（精确查询）\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34; : { \u0026#34;age\u0026#34;: 25 } } }    示例：查询所有地址与976 Lawrence Street相关的记录（全文搜索）\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34; : { \u0026#34;address\u0026#34;: \u0026#34;976 Lawrence Street\u0026#34; } } }   match_all 全部匹配 match_all 用于查询所有文档。在没有指定查询方式时，它是默认。\n 示例：查询年龄最小的十条记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34; : {} }, \u0026#34;sort\u0026#34; : [{ \u0026#34;age\u0026#34; : \u0026#34;asc\u0026#34; }], \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 10 }   match_phase 短语匹配 match_phase用于进行短语的匹配，它查询时并不是像term一样不进行分词直接查询，而是借助分析器返回的查询词的相对顺序以及偏移量来做判断——满足所有查询词且顺序完全相同的记录才会被匹配上。\n 示例：地址包含502 Baycliff Terrace的记录\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_phase\u0026#34; : { \u0026#34;address\u0026#34;: \u0026#34;502 Baycliff Terrace\u0026#34; } } }   multi_match 多字段匹配 multi_match 可以在多个字段上执行相同的 match 查询。\n 示例：查找city或address字段中包含Dixie或Street的记录。\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;Dixie Street\u0026#34;, \u0026#34;fields\u0026#34;:[ \u0026#34;city\u0026#34;, \u0026#34;address\u0026#34; ] } } }   term 精确查询 term即直接在倒排索引中查询，也就是精确查找，不进行分词器分析，文档中必须包含整个搜索的词汇。\nterm和match的区别:\n match是经过分析处理的，查询词先被文本分析器处理后再进行查询。所以根据不同的文本分析器，分析出  的结果也会不同。\n term是不经过分析处理的，直接去倒排索引查找精确的值。   由于text字段会被文本分析器处理，所以通常全文检索字段用match，其他非text字段（not_analyzed）匹配用term。\n 1 2 3 4 5 6 7 8 9 10  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;Street\u0026#34; } } } // 虽然文档中存在”702 Quentin Street“，但是由于文本分析器默认会转为小写，无法搜到任何数据   布尔查询（复合查询） 借助布尔查询可以实现如SQL中（and、or、!=）等逻辑条件的判断，并且可以合并任何其他查询语句，包括复合语句。复合语句之间可以相互嵌套，可以表达复杂的逻辑。\n  must（and）：文档必须匹配这些条件才能被包含进来。（影响相关性得分）\n  must_not（not）：文档必须不匹配这些条件才能被包含进来。（不影响相关性得分）\n  should（or）：如果满足这些语句中的任意语句，将增加得分 。（用于修正相关性得分）\n   示例：查找年龄不等于18的地址包含Street的男性，且优先展示居住在30岁以上的的记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;must\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;address\u0026#34;:\u0026#34;Street\u0026#34; } }, { \u0026#34;match\u0026#34;:{ \u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34; } } ], \u0026#34;must_not\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;age\u0026#34;:\u0026#34;18\u0026#34; } } ], \u0026#34;should\u0026#34;:[ { \u0026#34;range\u0026#34;:{ \u0026#34;age\u0026#34;:{ \u0026#34;gt\u0026#34;:30 } } } ] } } }   Filter 过滤器 Filter通常搭配布尔查询一起使用，用于过滤出所有满足Filter的记录，不影响相关性得分。\n 示例：查找年龄在30～60之间的记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;filter\u0026#34;:[ { \u0026#34;range\u0026#34;:{ \u0026#34;age\u0026#34;:{ \u0026#34;gte\u0026#34;:30, \u0026#34;lte\u0026#34;:60 } } } ] } } }   Aggregations 聚合 要掌握聚合，你只需要明白两个主要的概念：\n 桶（Buckets）：满足特定条件的文档的集合 指标（Metrics）：对桶内的文档进行统计计算  翻译成SQL的形式来理解的话：\n1 2 3 4 5  SELECTCOUNT(1),MAX(balance)FROMtableGROUPBYgender;  桶在概念上类似于 SQL 的分组（GROUP BY，如上面的GROUP BY gender），而指标则类似于 COUNT() 、 SUM() 、 MAX() 等统计方法，如MAX(balance)。\n聚合的语法如下：\n1 2 3 4 5 6 7 8 9 10  \u0026#34;aggregations\u0026#34; : { \u0026#34;\u0026lt;聚合名称 1\u0026gt;\u0026#34; : { \u0026#34;\u0026lt;聚合类型\u0026gt;\u0026#34; : { \u0026lt;聚合体内容\u0026gt; } [,\u0026#34;元数据\u0026#34; : { [\u0026lt;meta_data_body\u0026gt;] }]? [,\u0026#34;aggregations\u0026#34; : { [\u0026lt;sub_aggregation\u0026gt;]+ }]? } [\u0026#34;聚合名称 2\u0026gt;\u0026#34; : { ... }]* }    示例：按照性别进行分组，计算平均年龄和最高收入\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_all\u0026#34;: {} }, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_gender\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;gender\u0026#34; }, \u0026#34;aggs\u0026#34;:{ \u0026#34;avg_age\u0026#34;:{ \u0026#34;avg\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34; } }, \u0026#34;max_balance\u0026#34;:{ \u0026#34;max\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;balance\u0026#34; } } } } }, \u0026#34;size\u0026#34;:0 }   ","date":"2022-05-23T22:17:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"ElasticSearch 基本操作"},{"content":"ElasticSearch 基本概念 Elasticsearch是一个分布式的免费开源搜索和分析引擎，适用于包括文本、数字、地理空间、结构化和非结构化数据等在内的所有类型的数据。\nElasticsearch在Apache Lucene的基础上开发而成。然而，Elasticsearch不仅仅是Lucene，并且也不仅仅只是一个全文搜索引擎。 它可以被下面这样准确的形容：\n 一个分布式的实时文档存储，每个字段都可以被索引与搜索 一个分布式实时分析搜索引擎 能胜任上百个服务节点的扩展，并支持PB级别的结构化或者非结构化数据  Lucene Lucene是一套用于全文索引和搜索的开源程式库，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜索。但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎（英文与德文两种西方语言）。\nLucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。\n在Java开发环境里Lucene是一个成熟的免费开源工具。就其本身而言，Lucene是当前以及最近几年最受欢迎的免费Java信息检索程序库。人们经常提到信息检索程序库，虽然与搜索引擎有关，但不应该将信息检索程序库与搜索引擎相混淆。\nELK Elastic Stack是一套适用于数据采集、扩充、存储、分析和可视化的免费开源工具。人们通常将Elastic Stack称为ELK Stack（代指 Elasticsearch、Logstash 和 Kibana）。\n Logstash是什么？\n Logstash 是 Elastic Stack 的核心产品之一，可用来对数据进行聚合和处理，并将数据发送到 Elasticsearch。Logstash 是一个开源的服务器端数据处理管道，允许您在将数据索引到 Elasticsearch 之前同时从多个来源采集数据，并对数据进行充实和转换。\n Kibana是什么？\n Kibana 是一款适用于Elasticsearch的数据可视化和管理工具，可以提供实时的直方图、线形图、饼状图和地图。Kibana同时还包括诸如 Canvas和Elastic Maps等高级应用程序\n Canvas允许用户基于自身数据创建定制的动态信息图表， Elastic Maps用来对地理空间数据进行可视化。  Elasticsearch的特点   Elasticsearch 很快。 由于Elasticsearch是在Lucene基础上构建而成的，所以在全文本搜索方面表现十分出色。Elasticsearch同时还是一个近实时的搜索平台，这意味着从文档索引操作到文档变为可搜索状态之间的延时很短，一般只有一秒。因此，Elasticsearch 非常适用于对时间有严苛要求的用例，例如安全分析和基础设施监测。\n  Elasticsearch 具有分布式的本质特征。 Elasticsearch中存储的文档分布在不同的容器中，这些容器称为分片，可以进行复制以提供数据冗余副本，以防发生硬件故障。Elasticsearch的分布式特性使得它可以扩展至数百台（甚至数千台）服务器，并处理 PB 量级的数据。\n  Elasticsearch 包含一系列广泛的功能。 除了速度、可扩展性和弹性等优势以外，Elasticsearch 还有大量强大的内置功能（例如数据汇总和索引生命周期管理），可以方便用户更加高效地存储和搜索数据。\n  Elastic Stack 简化了数据采集、可视化和报告过程。 通过与 Beats 和 Logstash 进行集成，用户能够在向 Elasticsearch 中索引数据之前轻松地处理数据。同时，Kibana 不仅可针对 Elasticsearch 数据提供实时可视化，同时还提供 UI 以便用户快速访问应用程序性能监测 (APM)、日志和基础设施指标等数据。\n  应用场景 Elasticsearch在速度和可扩展性方面都表现出色，而且还能够索引多种类型的内容，这意味着其可用于多种用例：\n 应用程序搜索 网站搜索 企业搜索 日志处理和分析 基础设施指标和容器监测 应用程序性能监测 地理空间数据分析和可视化 安全分析 业务分析  架构设计  Gateway：Elasticsearch用来存储索引的文件系统，支持多种类型。ElasticSearch默认先把索引存储在内存中，然后当内存满的时候，再持久化到Gateway里。当ES集群关闭或重启的时候，它就会从Gateway里去读取索引数据。比如LocalFileSystem和HDFS、AS3等。 DistributedLucene Directory：Lucene里的一些列索引文件组成的目录。它负责管理这些索引文件。包括数据的读取、写入，以及索引的添加和合并等。 Mapping：映射解析模块。 Search Moudle：搜索模块。 Index Moudle：索引模块。 Disvcovery：节点发现模块。不同机器上的节点要组成集群需要进行消息通信，集群内部需要选举master节点，这些工作都是由Discovery模块完成。支持多种发现机制，如 Zen 、EC2、gce、Azure。 Scripting：Scripting用来支持在查询语句中插入javascript、python等脚本语言。 3rd plugins：第三方插件。 Transport：传输模块，支持多种传输协议，如 Thrift、memecached、http，默认使用http。 JMX：JMX是java的管理框架，用来管理ES应用。 Java(Netty)：java的通信框架。 RESTful Style API：提供给用户的接口，通过RESTful方式来实现API编程。  基本概念 Elasticsearch是一个文档型数据库，为了方便理解，下面给出其与传统的关系型数据库的对比\n   Relational DB Elasticsearch     数据库 Database 索引 Index   表 Table 类型 Type   行 Rows 文档 Document   列 columns 字段 fields   表结构 schema 映射 mapping    文档 Elasticsearch是面向文档的，索引和搜索数据的最小单位是文档，并且使用JSON来作为文档的序列化格式。每个文档可以由一个或多个字段组成，类似于关系型数据库中的一行记录，在Elasticsearch中，文档有几个重要属性\n 自我包含：一篇文档同时包含字段和对应的值。 层次型：文档中还可以包含新的文档，一个字段的取值也可以包含其他字段和取值。 结构灵活：文档不依赖预先定义的模式。在关系型数据库中，要提前定义字段才能使用，在Elasticsearch中，对于字段是非常灵活的，有时候，我们可以忽略该字段，或者动态的添加一个新的字段。  尽管我们可以随意的新增或者忽略某个字段，但是，每个字段的类型非常重要，比如一个年龄字段类型，可以是字 串也可以是整形。因为Elasticsearch会保存字段和类型之间的映射及其他的设置。这种映射具体到每个映射的每种类型，这也是为什么在Elasticsearch中，类型有时候也称为映射类型。\n类型 类型是文档的逻辑容器，类似于表格是行的容器。在不同的类型中，最好放入不同结构的文档。（有点类似于关系型数据库中的表的概念）。\n每个类型中对于字段的定义称为映射。比如name字段映射为string类型。 我们说文档是无模式的，它们不需要拥有映射中所定义的所有字段，例如我们可能会新增一个映射中不存在的字段，那么Elasticsearch是怎么做的呢?\nElasticsearch会自动的将新字段加入映射，由于不确定这个字段是什么类型，Elasticsearch就会根据字段的值来推导它的类型，如果这个值是10，那么Elasticsearch会认为它是整形。 但是这种推导也可能会存在问题，如果这里的10实际上是字符串“10”，如果后续在索引字符串“hello world”，就会因为类型不一致而导致索引失败。 所以最安全的方式就是在索引数据之前，就定义好所需要的映射，这点跟关系型数据库殊途同归了，先定义好字段，然后再使用。\n映射类型只是将文档进行逻辑划分。从物理角度来看，同一索引的文档都是写入磁盘，而不考虑它们所属的映射类型。\n 注意⚠️：为什么ElasticSearch要在7.X版本去掉type?\n 在Elasticsearch设计初期，是直接查考了关系型数据库的设计模式，存在了类型（表）的概念。 但是，其搜索引擎是基于Lucene的，这种基因决定了类型是多余的。Lucene的全文检索功能之所以快，是因为倒序索引的存在。 而这种倒序索引的生成是基于索引的，而并非 类型。多个类型反而会减慢搜索的速度——两个不同type下的两个user_name，在ES同一个索引下其实被认为是同一个filed，你必须在两个不同的type中定义相同的filed映射。否则，不同type中的相同字段名称就会在处理中出现冲突的情况，导致Lucene处理效率下降。\n索引 在Elasticsearch中，索引有两个含义：\n  名词\n 索引是映射类型的容器，Elasticsearch中的索引是一个非常大的文档集合，非常类似关系型数据库中库的概念。索引存储了所有映射类型的字段。    动词\n 索引一个文档就是存储一个文档到一个索引 （名词）中以便被检索和查询。这非常类似于SQL中的INSERT关键词，不同的是文档已存在时，新文档会替换旧文档（UPSERT）。    在Elasticsearch中，索引使用了一种称为倒排索引（即通过查询词索引到文档）的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。\n","date":"2022-05-23T22:16:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ElasticSearch 基本概念"},{"content":"集群 集群角色 通常在分布式系统中，构成一个集群的每一台机器都有自己的角色，最常见的集群模式就是Master/Slave模式（主从模式），在这种模式中，通常Maste服务器作为主服务器提供写服务，其他的Slave服务器从服务器通过异步复制的方式获取Master服务器最新的数据提供读服务。\n但是ZooKeeper并没有沿用传统的主从模式，而是引入了Leader、Follower和Observer三种角色，如下图所示：\nZooKeeper集群中的所有机器通过一个Leader 选举过程来选定一台称为Leader的机器，Leader既可以为客户端提供写服务又能提供读服务。除了Leader外，Follower和Observer都只能提供读服务。Follower和Observer唯一的区别在于Observer机器不参与Leader的选举过程，也不参与写操作的“过半写成功”策略，因此Observer机器可以在不影响写性能的情况下提升集群的读性能。\n Leader：为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。 Follower：为客户端提供读服务，如果是写服务则转发给Leader。在选举过程中参与投票。 Observer：与Follower工作原理基本一致，唯一的区别是Observer不参与任何形式的投票。Observer主要用来在不影响写性能的情况下提升集群的读性能，是ZooKeeper3.3中新增的角色。  ZooKeeper用以下四种状态来表示各个节点\n LOOKING：竞选状态。 LEADING：Leader状态。 FOLLOWING：Follower状态。 OBSERVING：Observer状态。  选举 Leader选举是ZooKeeper中最重要的技术之一，也是保证分布式数据一致性的关键所在。\n当ZooKeeper集群中的一台服务器出现以下两种情况之一时，就会开始进入Leader选举\n 服务器初始化启动 服务器运行期间无法和Leader保持连接  在介绍选举之前，首先介绍三个重要参数\n 服务器ID（myid）：编号越大在选举算法中权重越大。 事务ID（zxid）：值越大说明数据越新，权重越大。 逻辑时钟（epoch-logicalclock）：同一轮投票过程中的逻辑时钟值是相同的，每投完一次值会增加。  服务器启动时期的选举 ZooKeeper集群初始化时，当满足以下两个条件时，就会开始进入Leader选举流程：\n 集群规模至少为2台机器 集群内的机器能够互相通信  如上图，选举流程如下：\n 每个服务器会发出一个投票：由于是初始化状态，每一个节点都会将自己作为Leader来进行投票，每次投票的内容包含：推举服务器的myid（在集群中的机器序号）和ZXID（事务ID），以(myid, ZXID)的形式表示。初始化阶段每个节点都会将票投给自己，然后将这个投票发给集群总其他所有机器。 接受来自各个服务器的投票：每个服务器都会接收来自其他服务器的投票，在接收投票后首先会判断该投票的有效性，如是否为本轮投票（逻辑时钟）、是否来自LOOING状态的服务器。 处理投票：在接收到来自其他服务器的投票后，针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下  优先检查ZXID，ZXID较大的优先作为Leader。 如果Leader相同则比较myid，myid较大的作为Leader。    统计投票：每次投票后，服务器都会统计所有投票，判断是否已经有过半的机器接收到相同的投票信息。当票数达到半数以上时，此时就认为已经选出了Leader。 改变服务器状态：一旦确认了Leader，则每个服务器开始改变自己的状态。如果是Follower则更改成FOLLOWING；如果是Observer则更改成OBSERVING；如果是Leader则更改成LEADING；  服务器运行时期的选举 在ZooKeeper集群正常运行中，一旦选出一个Leader，那么所有服务器的集群角色一般不会再发生变化——也就是说Leader服务器将一直作为集群的Leader，即使集群中有非Leader机器挂了或者是有机器加入集群也不会影响Leader。但是一旦Leader所在的机器挂掉，那么此时整个集群将暂时无法对外服务，马上进入新一轮的Leader选举。\n服务器运行期间的Leader选举和启动时期的Leader选举基本是一致的，只增加了一个变更状态的步骤，流程如下：\n 变更状态：当Leader挂了之后，剩下的所有非Observer服务器都会将自己的服务器状态变更为LOOKING，然后开始进入Leader选举流程。 每个服务器会发出一个投票 接受来自各个服务器的投票 处理投票 统计投票 改变服务器状态  集群的机器数量  为什么ZooKeeper提倡集群的机器数量最好要做奇数台呢？\n ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n/2。先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。\n综上，何必增加那一个不必要的 ZooKeeper 呢？\n","date":"2022-05-23T21:13:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E9%9B%86%E7%BE%A4/","title":"ZooKeeper 集群"},{"content":"系统模型 数据模型 ZooKeeper数据存储的结构与标准的Unix文件系统非常类似，但是并没有引入传统文件系统中目录和文件等概念，而是使用了其特有的数据节点的概念，我们称之为ZNode。Znode是zookeeper中的最小数据单元，每个Znode上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。\n如上下图，Znode的节点路径标识方式和Unix文件系统路径非常相似，都是由一系列使用斜杠（/）进行分割的路径表示，最上层的根结点以/代表，并且每个Znode都有一个唯一的路径标识。\n ⚠️注意：ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的上限是每个结点的数据大小最大是 1M。\n 节点特性 节点类型 在ZooKeeper中，Znode节点类型分为持久节点（PERSISTENT）、临时节点（EPHEMERAL）、**顺序节点（SEQUENTIAL）**三大类，通过组合使用，可以生成以下四种组合型节点类型：\n 持久节点：该数据节点被创建后，就会一直存在于ZooKeeper的服务器上，直到有删除操作来主动清除这个节点。 持久顺序节点：基本特性与持久节点一致，额外的特性是顺序性，即一个父节点可以为其子节点维护一个创建的先后顺序 ，这个顺序体现在节点名称上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数，上限是整型最大值。 临时节点：临时节点的生命周期与客户端的会话绑定在一起，如果客户端会话失效，则这个节点就会自动被清理掉。同时，临时节点不能创建子节点，它只能作为叶子节点使用。 临时顺序节点：基本特性与临时节点一致，额外的特性是顺序性。  节点状态 每个数据节点除了存储数据内容之外，还存储了数据节点本身的一些状态信息（如子节点数量、事务id、版本信息等），这些状态信息由Stat这个类来维护。\n czxid：Created ZXID，该数据节点被创建时的事务ID。 mzxid：Modified ZXID，节点最后一次被更新时的事务ID。 ctime：Created Time，该节点被创建的时间。 mtime： Modified Time，该节点最后一次被修改的时间。 version：节点的版本号。 cversion：子节点的版本号。 aversion：节点的 ACL 版本号。 ephemeralOwner：创建该节点的会话的sessionID ，如果该节点为持久节点，该值为0。 dataLength：节点数据内容的长度。 numChildre：该节点的子节点个数，如果为临时节点为0。 pzxid：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的列表 ，不是内容。  版本 ZooKeeper中为节点引入了版本的概念，每个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化。\n dataVersion：当前Znode节点数据内容版本号 cversion：当前Znode节点的子节点版本号。 aclVersion：当前Znode的ACL变更版本号。  为了解决那些数据更新竞争激烈的场景，ZooKeeper借助版本号机制来实现乐观并发控制。\nWatcher Watcher（事件监听器），是ZooKeeper中的一个很重要的特性。ZooKeeper允许客户端在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。\n从上图中我们可以看到，ZooKeeper的Watcher机制主要包括客户端线程、客户端WatchManager、ZooKeeper服务器三个部分。工作流程如下：\n 客户端向ZooKeeper服务器注册Watcher对象 客户端将Watcher对象存储在客户端的WatchManager中。 当ZooKeeper服务器触发Watcher事件后，会向客户端发送通知。 客户端从WatchManager中取出对应的Watcher对象来执行回调逻辑。  ACL ZooKeeper提供了一套完善的ACL（Access Control Lists）权限控制机制来保障数据的安全，类似于Unix/Linux中的UGO权限控制机制。\nACL机制由以下三部分组成，我们通常使用scheme:id:permission来标识一个有效的ACL信息。\n 权限模式（Scheme） 授权对象（ID） 权限（Permission）  权限模式：Scheme 权限模式是用来确定权限验证过程中使用的校验策略。在ZooKeeper中最常用的就是以下四种权限模式\n IP：IP模式通过IP地址粒度来进行权限控制。 Digest：Digest是最常用的权限控制模式，其以类似于username:password形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。当我们配置了权限标识后，为了保证安全，ZooKeeper会分别使用SHA-1算法和BASE64编码进行处理，将其混淆为无法辨识的字符串。 World：World是最开放的权限控制模式，是一种特殊的Digest模式。在该模式下数据节点的访问权限对所有用户开放，即所有用户都可以在不进行任何数据校验的情况下操作ZooKeeper上的数据。权限标识为world:anyone Super：Super模式即超级用户模式，是一种特殊的Digest模式。在该模式下超级用户可以对任意ZooKeeper上的数据节点进行任何操作。  权限对象：ID 授权对象指的是权限赋予的用户或一个指定实体，例如IP地址或是机器等。在不同的权限关系下，授权对象是不同的，对应关系如下图：\n IP：通常是一个IP地址或者一个IP网段，如192.168.0.110或192.168.0.1/24 Digest：Digest是最常用的权限控制模式，其以类似于username:password形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。当我们配置了权限标识后，为了保证安全，ZooKeeper会分别使用SHA-1算法和BASE64编码进行处理，将其混淆为无法辨识的字符串。 World：只有一个ID：anyone Super：与Digest模式一致。  权限：Permission 权限就是指那些通过权限检查后可以被允许执行的操作。在ZooKeeper中，所有对数据的操作权限分为以下五大类：\n CREATE：数据节点的创建权限，允许授权对象在该数据节点下创建子节点。 DELETE：数据节点的删除权限，允许授权对象删除该数据节点的子节点。 READ：数据节点的读取权限，允许授权对象访问该数据节点并读取其数据内容或子节点列表等。 WRITE：数据节点的更新权限，允许授权对象对该数据节点进行更新操作。 ADMIN：数据节点的管理权限，允许授权对象在该数据节点进行ACL相关的设置操作。  Session Session（会话） 可以看作是ZooKeeper服务器与客户端的之间的一个TCP长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watcher事件通知。\nSession有一个属性叫做：sessionTimeout ，sessionTimeout 代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。\n另外在每次客户端向服务端发起会话创建请求时，服务端都会为其分配一个SessionID，SessionID用来唯一标识一个会话，因此ZooKeeper必须保证SessionID的全局唯一性。\n","date":"2022-05-23T21:11:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E7%B3%BB%E7%BB%9F%E6%A8%A1%E5%9E%8B/","title":"ZooKeeper 系统模型"},{"content":"ZooKeeper 基本概念 ZooKeeper是一个开源的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。\n 原语： 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。\n 特点 ZooKeeper可以保证如下分布式一致性特性：\n 顺序一致性：从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一视图：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性：一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 实时性：ZooKeeper保证在一定时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。  设计目标 ZooKeeper致力于实现一个高性能、高可用，具有严格顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务。高性能使得ZooKeeper能够应用于那些对系统吞吐有明确要求的大型分布式系统，高可用使得分布式的单点问题得到了很好的解决，而严格的顺序访问控制使得客户端能够基于ZooKeeper实现一些复杂的同步原语。\n针对以上需求，ZooKeeper的四个设计目标如下：\n  简单的数据模型：ZooKeeper使用一个共享的、树形结构的命名空间来协调分布式程序。其数据模型类似一个文件系统，不过与传统的文件系统不同，ZooKeeper将全量数据存储在内存中，以此来实现提高服务器吞吐、减少延迟的目的。\n  可以构建集群：一个ZooKeeper集群通常由一组机器组成，组成ZooKeeper集群的每台机器都会在内存中维护当前服务器状态，并且每台机器之间都互相保持通信。只要集群中存在半数以上的机器能够正常工作，整个集群就可以正常对外提供服务。\n  顺序访问：对于来自客户端的每个更新请求，ZooKeeper都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序，可以根据这个特性来实现更高层次的同步原语。\n  高性能：由于ZooKeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。\n  应用场景 ZooKeeper是一个典型的分布式数据一致性的解决方案，通常用于以下这些场景：\n 数据发布/订阅 负载均衡 命名服务 分布式协调/通知 集群管理 Master选举 分布式锁 分布式队列  ","date":"2022-05-23T21:10:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ZooKeeper 基本概念"},{"content":"分布式文件系统 基本概念 分布式文件系统（Distributed File System）是网络文件系统的延伸，其关键点在于存储端可以灵活地横向扩展。也就是可以通过增加设备（主要是服务器）数量的方法来扩充存储系统的容量和性能。同时，分布式文件系统还要对客户端提供统一的视图。也就是说，虽然分布式文件系统服务由多个节点构成，但客户端并不感知。在客户端来看就好像只有一个节点提供服务，而且是一个统一的分布式文件系统。\n分布式文件系统 VS 网络文件系统 从本质上来说，分布式文件系统其实也是网络文件系统的一种，其与网络文件系统的差异在于服务端包含多个节点，也就是服务端是可以横向扩展的，其可以通过增加节点的方式增加文件系统的容量，提升性能。\n由于其数据被存储在多个节点上，因此还有其他特点：\n 支持按照既定策略在多个节点上放置数据。 可以保证在出现硬件故障时，仍然可以访问数据。 可以保证在出现硬件故障时，不丢失数据。 可以在硬件故障恢复时，保证数据的同步。 可以保证多个节点访问的数据一致性。  横向拓展结构 对于存储集群端主要有两种类型的架构模式：一种是以有中心控制节点的分布式架构，另一种是对等的分布式架构，也就是没有中心控制节点的架构。\n中心架构 中心架构是指在存储集群中有一个或多个中心节点，中心节点维护整个文件系统的元数据，为客户端提供统一的命名空间。 在实际生产环境中，中心节点通常是多于一个的，其主要目的是保证系统的可用性和可靠性。\n在中心架构中，集群节点的角色分为两种：\n 控制节点：这种类型的节点会存储文件系统的元数据信息，并对请求进行协调与处理，根据元数据将请求转发只对应的节点上。 数据节点：这种类型的节点用于存储文件系统的用户数据。  架构示意图如下：\n当客户端需要对一个文件进行读/写时，首先会访问控制节点，控制节点通过对一些元数据进行处理（鉴权、文件锁、位置计算等），并将文件所在的数据节点的位置响应给客户端。此时客户端再与数据节点交互，完成数据的访问。\n对等架构 对等架构是没有中心节点的架构，集群中并没有一个特定的节点负责文件系统元数据的管理。在集群中所有节点既是元数据节点，也是数据节点。 在实际实现中，其实并不进行角色的划分，只是作为一个普通的存储节点。\n由于在对等架构中没有中心节点，因此主要需要解决两个问题：\n 客户端需要一种位置计算算法来计算数据应该存储的位置。 需要将元数据存储在各个存储节点，在某些情况下需要客户端来汇总。  关键原理 分布式文件系统本身也是文件系统，因此它与本地文件系统和网络文件系统等具备一些公共技术。除此之外，鉴于其分布式的特点，还涉及一些分布式的技术。\n数据布局 分布式文件系统的数据布局与本地文件系统不同，其关注的不是数据在磁盘的布局，而是数据在存储集群各个节点的放置问题。\n在分布式文件系统中，数据布局解决的主要问题是性能和负载均衡的问题。其解决方案就是通过多个节点来均摊客户端的负载，也就是实现存储集群的横向扩展。因此数据布局的核心，就是要保证数据量均衡与负载均衡。\n基于动态监测的数据布局 基于动态监测的数据布局是指通过监测存储集群各个节点的负载、存储容量和网络带宽等系统信息来决定新数据放置的位置。 另外，集群节点之间还要有一些心跳信息，这样当有数据节点故障的情况下，控制节点可以及时发现，保证在决策时剔除。\n由于需要汇总各个节点的信息进行决策，因此基于动态监测的数据布局通常需要一个中心节点。中心节点负责汇总各种信息并进行决策，并且会记录数据的位置信息等元数据信息。当客户端需要写入数据时，客户端首先与控制节点交互；控制节点根据汇总的信息计算出新数据的位置，然后反馈给客户端；客户端根据位置信息，直接与对应的数据节点交互。\n基于计算位置的数据布局 基于计算位置的数据布局是一种固定的数据分配方式。在该架构中通过一个算法来计算文件或数据存储的具体位置。 当客户端要访问某个文件时，请求在客户端或经过的某个代理节点计算出数据的具体位置，然后将请求路由到该节点进行处理。\n当客户端访问集群数据时，首先计算出数据的位置（根据请求的特征来计算数据具体应该放到哪个节点，例如一致性哈希算法），然后与该节点交互。\n数据可靠性 分布式数据的可靠性是指在出现组件故障的情况下依然能够能提供正常服务的能力。\n复制（Replication） 复制技术是通过将数据复制到多个节点的方式来实现系统的高可靠。 由于同一份数据会被复制到多个节点，这样同一个数据就存在多个副本，因此也称为多副本技术，这样当出现节点故障时就不会影响数据的完整性和可访问性。\n复制技术有两种不同的模式：\n  主从节点复制：即在副本节点中有一个节点是主节点，所有的数据请求先经过主节点。对于一个写数据请求，客户端将请求发送到主节点，主节点将数据复制到从节点，再给客户端应答。\n  无主节点复制：即在集群端并没有一个主节点，副本逻辑在客户端或代理层完成。 当客户端发送一个写数据请求时，客户端会根据策略自行（或者通过代理层）找到副本服务器，并将多个副本发送到副本服务器上。\n  纠删码（Erasure Code） 副本技术的本质就是冗余存储，因此需要消耗很多额外的存储空间。以 3 个副本为例，需要额外消耗 2 倍的存储空间来保证数据的可靠性。虽然副本技术在性能和可靠性方面优势明显，但成本明显比较高。为了降低存储的成本，很多公司采用纠删码技术来保证数据的可靠性。\n纠删码是一种通过校验数据来保证数据可靠性的技术，也就是该技术通过保存额外的一个或多个校验块来提供数据冗余。与副本技术不同，这种数据冗余技术不能通过简单复制来恢复数据，而是经过计算来得到丢失的数据。\n纠删码的基本原理是采用矩阵运算，将 n 个数据转换为 n+m 个数据进行存储。其基本流程如下：\n 校验数据生成：找到一个生成矩阵，通过该矩阵与原始数据的运算可以得到最终要存储的校验数据。 数据恢复：由于生成矩阵是可逆的，因此可利用生成矩阵和剩余可用数据来计算出原始数据。  数据一致性 在分布式文件系统中，由于同一个数据块被放置在不同的节点上，我们无法保证多个节点的数据时时刻刻是相同的，因此会出现一致性的问题。这里的一致性包括两个方面：一个方面是各个节点数据的一致性问题；另一个方面是从客户端访问角度一致性的问题。\n通常来说，我们是无法保证各个节点上数据是完全一致的（故障、宕机、延迟、网络分区等原因），只能保证客户端访问的一致性。为了保证客户端访问数据的一致性，通常需要对存储系统进行特殊的设计，从而在系统层面保证数据的一致性。通常提供的一致性保证有如下两种：\n 强一致性：当数据的写入操作反馈给客户端后，任何对该数据的读操作都会读到刚刚写入的数据。 最终一致性：在执行一个写入操作后，如果没有新的写入操作的情况下， 该写入的数据会最终同步到所有副本节点上，但中间会有时间窗口。  故障与容错 在分布式文件系统中必须要解决设备故障的问题。这是因为在大规模分布式文件系统中设备的总量达到数万个甚至数十万个，设备发生故障就会成为常态。\n设备的故障分为两种类型：\n 临时故障：指短时间可以恢复的故障，如服务器重启、网线松动或交换机掉电等。 永久故障：指设备下线，且永远不会恢复，如硬盘损坏等。  为了应对系统随时出现的故障，分布式文件系统在设计时必须要考虑容错处理。容错主要包括以下几方面内容：\n 故障预测：在故障发生前，预知设备故障，然后有计划地将该设备下线，避免突然下线导致的性能等问题。 故障检测：在故障发生时，及时发现故障原因，方便进行问题的修复。如检测磁盘、通信链路或服务的故障等。 故障恢复：在故障发生后，快速进行响应，保证系统仍然能够对外提供无损的服务。如通过部件冗余、主备链路等。当系统发生故障时，可以通过切换链路，或者通过冗余节点来提供服务。  ","date":"2022-05-23T18:55:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"分布式文件系统"},{"content":"网络文件系统 基本概念 网络文件系统（Network File System）是基于 TCP/IP 协议（整个协议可能会跨层）的文件系统，它可以将远端服务器文件系统的目录挂载到本地文件系统的目录上，允许用户或者应用程序像访问本地文件系统的目录结构一样，访问远端服务器文件系统的目录结构，而无需理会远端服务器文件系统和本地文件系统的具体类型，非常方便地实现了目录和文件在不同机器上进行共享。\n网络文件系统通常分为客户端和服务端，其中客户端类似本地文件系统，差异是在读/写数据时不是访问磁盘等设备，而是通过网络将请求传输到服务端。而服务端则是对数据进行管理的系统，将数据存储到磁盘等存储介质上。\n网络文件系统 VS 本地文件系统 网络文件系统与本地文件系统主要存在以下差异：\n 数据的访问过程。 本地文件系统的数据是持久化存储到磁盘上的，而网络文件系统则需要将数据传输到服务端进行持久化处理。 是否需要格式化。 本地文件系统需要进行格式化处理才可以使用。而网络文件系统则不需要客户端进行格式化操作，通常只需要挂载到客户端就可以直接使用。当然在服务端通常是要做一些配置工作的，包括格式化操作。  网络文件系统最主要的特性是实现了数据的共享。 基于数据共享的特性，使得网络文件系统有很多优势，如增大存储空间的利用效率（降低成本）、方便组织之间共享数据和易于实现系统的高可用等。\n关键原理 由于网络文件系统基于网络实现，其分为本地的客户端与远端的服务端。因此其除了之前提到的本地文件系统相关的技术，还引入文件系统协议、RPC，网络文件锁等机制。\n文件系统协议 网络文件系统本质上是一个基于 C/S（客户端/服务端）架构的应用，其大部分功能是通过客户端与服务端交互来实现的。因此，对于网络文件系统来说，其核心之一是客户端与服务端的交互语言——文件系统协议。\n网络文件系统的协议的定义类似函数调用，包含 ID（可以理解为函数名称），参数和返回值。这里以 NFS v3 协议举例：\n从上图可以看出，协议语义与文件系统操作的语义基本上一一对应，因此客户端对网络文件系统的访问都可以通过协议传输到服务端进行相应的处理。\n远程过程调用（RPC） 由于在客户端与服务端都要实现对协议数据的封装和解析，因此实现起来比较复杂。为了降低复杂性，通常会在文件系统业务层与 TCP/IP 层之间实现一层交互层，这就是 RPC 协议。\nRPC（Remote Procedure Call，远程过程调用） 是 TCP/IP 模型中应用层的网络协议（OSI 模型中会话层的协议）。RPC 协议通过一种类似函数调用的方式实现了客户端对服务端功能的访问，简化了客户端访问服务端功能的复杂度。\nRPC 协议通常架构如下：\n 客户端：文件系统。 服务端：文件系统服务。 存根：定义的函数集。以网络文件系统为例，函数集包括创建文件、删除文件、写数据和读数据等。函数集通常需要分别在客户端和服务端定义一套接口。 RPC 运行时库：实现了 RPC 协议的公共功能，如请求的封装与解析、消息收发和网络层面的错误处理等。  在客户端调用 RPC 函数时，会调用 RPC 库的接口将该函数调用转化为一个网络消息转发到服务端，而服务端的 RPC 库则对网络数据包进行反向解析，调用服务端注册的函数集（存根）中的函数实现功能，最后将执行的结果反馈给客户端。\n对于客户端的应用，这个函数调用与本地函数调用并没有明显的差异。\n网络文件锁 本地文件系统可以在文件系统内实现文件锁。但由于网络文件系统会有多个不同的客户端文件系统访问同一个服务端的文件系统，文件锁是无法在客户端的文件系统中实现的，只能在服务端实现。这样就需要一个协议将客户端的加锁、解锁等请求传输到服务端，并且在服务端维护文件锁的状态。\n在 NFS 协议族中，其通过 NLM（Network Lock Manager） 协议实现了一个网络文件锁服务。由于网络文件锁需要考虑到网络分区、丢包、服务端/客户端宕机等多方面因素，实现要比本地文件锁服务复杂得多，但其核心原理还是与本地文件锁一样：维护一个数据结构，负责记录每个文件的加锁情况。当客户端传来新的加锁请求时查找该结构，判断是否存在锁冲突，来考虑是允许加锁还是报错返回。\n","date":"2022-05-23T18:54:13+08:00","permalink":"https://blog.orekilee.top/p/%E7%BD%91%E7%BB%9C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"网络文件系统"},{"content":"本地文件系统 关键技术 虚拟文件系统 虚拟文件系统（Virtual File System，VFS） 是具体文件系统（如 Ext2、Ext4 和 XFS 等）与应用程序之间的一个接口层，它对 Linux 的每个文件系统的所有细节进行抽象，使得不同的文件系统在 Linux 核心以及系统中运行的其他进程看来都是相同的。以此达到使操作系统能够适配多种文件系统的目的。\nVFS 提供了一个文件系统框架，本地文件系统可以基于 VFS 实现，其主要做了如下几方面的工作：\n 作为抽象层为应用层提供了统一的接口（ read、write 和 chmod 等）。 实现了一些公共的功能，如 Inode 缓存（Inode Cache）和页缓存（Page Cache）等。 规范了具体文件系统应该实现的接口。  基于上述设定，其他具体的文件系统只需要按照 VFS 的约定实现相应的接口及内部逻辑，并注册在系统中，就可以完成文件系统的功能了。当用户调用操作系统提供的文件系统 API 时，会通过软中断的方式调用内核 VFS 实现的函数。\n磁盘空间布局 文件系统的核心功能是实现对磁盘空间的管理。对磁盘空间的管理是指要知道哪些空间被使用了，哪些空间没有被使用。这样，在用户层需要使用磁盘空间时，文件系统就可以从未使用的区域分配磁盘空间。\n为了对磁盘空间进行管理，文件系统往往将磁盘空间通常被划分为元数据区与数据区两个区域。数据区就是存储数据的地方，用户在文件中的数据都会存储在该区域；而元数据区则是对数据区进行管理的地方。\n基于固定功能区的磁盘空间布局 基于固定功能区的磁盘空间布局是指将磁盘的空间按照功能划分为不同的子空间，每种子空间有具体的功能。 以 Linux 中的 Ext 文件系统为例，其空间被划分为数据区和元数据区，而元数据区又被划分为数据块位图、inode 位图和 inode 表等区域。如下图：\n基于功能分区的磁盘空间布局空间职能清晰，便于手动进行丢失数据的恢复。但是由于元数据功能区大小固定，因此容易出现资源不足的情况==（位图长度有限）==。比如，在海量小文件的应用场景下，有可能会出现磁盘剩余空间充足，但 inode 不够用的情况\n基于非固定功能区的磁盘空间布局 在磁盘空间管理中有一种非固定功能区的磁盘空间管理方法。这种方法也分为元数据和数据，但是元数据和数据的区域并非固定的，而是随着文件系统对资源的需求而动态分配的。\n例如比较经典的实现 XFS，其它不是通过固定的位图区域来管理磁盘空间的，而是通过 B+ 树管理磁盘空间。这样做就可以动态的去分配元数据空间，而不是像位图一样具有限制。但是这也带来了新的问题，就是 B+ 树无法根据 inode 的偏移量来确定编号，因此 XFS 又引入复杂的编号机制来解决这个问题。\n基于数据追加的磁盘空间布局 前文介绍的磁盘空间布局方式对于数据的变化都是原地修改的，也就是对于已经分配的逻辑块，当对应的文件数据改动时都是在该逻辑块进行修改的。在文件随机 I/O 比较多的情况下，不太适合使用 SSD 设备，这主要由 SSD 设备的修改和擦写特性所决定。\n有一种基于数据追加的磁盘空间布局方式，也被称为基于日志（Log structured）的磁盘空间布局方式。这种磁盘空间布局方式对数据的变更并非在原地修改，而是以追加写的方式写到后面的剩余空间。这样，所有的随机写都转化为顺序写，非常适合用于 SSD 设备。\n文件数据管理 对于文件系统来说，无论文件是什么格式，存储的是什么内容，它都不关心。文件就是一个线性空间，类似一个大数组。而且文件的空间被文件系统划分为与文件系统块一样大小的若干个逻辑块。文件系统要做的事情就是将文件的逻辑块与磁盘的物理块建立关系。这样当应用访问文件的数据时，文件系统可以找到具体的位置，进行相应的操作。\n基于连续区域的文件数据管理 基于连续区域的文件数据管理方式是一次性为文件分配其所需要的空间，且空间在磁盘上是连续的。 由于文件数据在磁盘上是连续存储的，因此只要知道文件的起始位置所对应的磁盘位置和文件的长度就可以知道文件数据在磁盘上是如何存储的。如下图：\n这种文件数据管理方式的最大缺点是不够灵活，特别是对文件进行追加写操作非常困难。如果该文件后面没有剩余磁盘空间，那么需要先将该文件移动到新的位置，然后才能追加写操作。如果整个磁盘的可用空间没有能够满足要求的空间，那么会导致写入失败。\n除了追加写操作不够灵活，该文件数据管理方式还有另一个缺点就是容易形成碎片空间。由于文件需要占用连续的空间，因此很多小的可用空间就可能无法被使用，从而降低磁盘空间利用率。\n基于链表的文件数据管理 基于链表的文件数据管理方式将磁盘空间划分为大小相等的逻辑块。 在目录项中包含文件名、数据的起始位置和终止位置。在每个数据块的后面用一个指针来指向下一个数据块。如下图：\n这种方式可以有效地解决连续区域的碎片问题，但是对文件的随机读/写却无能为力。这主要是因为在文件的元数据中没有足够的信息描述每块数据的位置。为了实现随机读写，还需要在实现时附加一些额外的机制。\n基于索引的文件数据管理 索引方式的数据管理是指通过索引项来实现对文件内数据的管理。 如下图所示，与文件名称对应的是索引块在磁盘的位置，索引块中存储的并非用户数据，而是索引列表。当读/写数据时，根据文件名可以找到索引块的位置，然后根据索引块中记录的索引项可以找到数据块的位置，并访问数据。\n常见的索引方式有以下两种：\n 基于间接块的文件数据管理：在索引方式中，最为直观、简单的就是对文件的每个逻辑块都有一个对应的索引项，并将索引项用一个数组进行管理。当想要访问文件某个位置的数据时，就可以根据该文件逻辑偏移计算出数组的索引值，然后根据数组的索引值找到索引项，进而找到磁盘上的数据。    基于 Extent 的文件数据管理：在 Extent 文件数据管理方式中，每一个索引项记录的值不是一个数据块的地址，而是数据块的起始地址和长度。\n  缓存 文件系统的缓存（Cache）的作用主要用来解决磁盘访问速度慢的问题。缓存技术是指在内存中存储文件系统的部分数据和元数据而提升文件系统性能的技术。由于内存的访问延时是机械硬盘访问延时的十万分之一，因此采用缓存技术可以大幅提升文件系统的性能。\n文件系统缓存的原理主要还是基于数据访问的时间局部性和空间局部性特性。时间局部性就是如果一块数据之前被访问过，那么最近很有可能会被再次访问。空间局部性则是指在访问某一个区域之后，通常会访问临近的区域。\n缓存置换算法 由于内存的容量要比磁盘的容量小得多，当用户持续写入数据时就会面临缓存不足的情况，此时就涉及如何将缓存数据刷写到磁盘，然后存储新数据的问题。而将缓存数据落盘，并置换新数据的这一过程被称为缓存置换。\n在文件系统中通常会采取以下这些缓存置换算法：\n LRU（Least Recently Used）：LRU 基于时间局部性原理。将最久没有使用的数据淘汰出缓存。因此，当空间满时，最久没有使用的数据即被淘汰。 LFU（Least Frequently Used）：LFU 基于缓存命中频次作为置换依据。如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，将命中频次最少的数据淘汰掉。  预读算法 预读算法是针对读数据的一种缓存算法。预读算法通过识别 I/O 模式方式来提前将数据从磁盘读到缓存中。这样，应用读取数据时就可以直接从缓存读取数据，从而极大地提高读数据的性能。\n通常有两种情况会触发预读操作：\n 当有多个地址连续地读请求时会触发预读操作。 应用访问到有预读标记的缓存时会触发预读操作。这里，预读标记的缓存是在预读操作完成时在缓存页做的标记，当应用读到有该标记的缓存时会触发下一次的预读，从而省略对 I/O 模式的识别。  快照与克隆 快照（Snapshot）和克隆（Clone）技术可以应用于整个文件系统或单个文件。快照技术可以实现文件的可读备份，而克隆技术则可以实现文件的可写备份。针对文件，用的更多的是克隆技术。\n快照 目前快照技术有两种实现方式：\n 写时拷贝（Copy-On-Write，COW）：刚开始创建快照时原始文件和快照文件指向相同的存储区域。当原始文件被修改时，就需要将该位置的原始数据拷贝到新的位置，并且更新快照文件中的地址信息。 写时重定向（Redirect-On-Write，ROW）：当原始文件写数据时并不在原始位置写入数据，而是分配一个新的位置写入。在读取时，创建快照前的数据从源卷读，创建快照后产生的数据，从快照卷读。  可以看出 COW 在写入时需要拷贝一份数据到快照卷的动作，而 ROW 是直接重定向到快照卷写入，所以 ROW 适合写密集型；相反，由于 ROW 不断进行指针重定向，读性能会有较大影响，而 COW 不会，所以 COW 适合读密集型。对文件系统而言，用得最多的是 ROW 方式。\n克隆 克隆技术的原理与快照技术的原理类似，其相同点在于其实现方式依然是 ROW 或 COW，而差异点则主要表现在两个方面：一个方面，克隆生成的克隆文件是可以写的；另一个方面，克隆的数据最终会与原始文件的数据完全隔离。\n日志 在文件系统中，一个简单的写操作在底层可能会分解为多个步骤的修改。如果不能保证操作的原子性，那么倘若出现宕机、断电的情况，此时就会导致数据的丢失、不一致，甚至是文件系统的不可用。为了解决这个问题，在文件系统中引入了日志机制。\n日志通常是一块独立的空间，其采用环形缓冲区的结构，当进行文件修改操作时，相关数据块会被打包成一组操作写入日志空间，再更新实际数据。这里的一组操作被称为一个事务。\n由于实际数据的更新在写入日志之后，如果在数据更新过程中出现了系统崩溃，那么通过读取日志来更新。这样就能保证数据是我们所期望的数据。还有一种异常场景是日志数据刷写过程中。由于此时日志完成标记还没有置位，而且实际数据还没有更新，那么只需要放弃该条日志即可。\n权限管理 在多用户操作系统中，由于多个用户的存在，就必须实现用户之间资源访问的隔离。这种管理用户可访问资源的特性就是权限管理。\nRWX 权限管理 Linux最常用的权限管理就是 RWX-UGO 权限管理。其中，RWX 是 Read、Write 和 eXecute 的缩写。而 UGO 则是 User、Group 和 Other 的缩写。通过该机制建立用户和组实现对文件的不同的访问权限。\n如下图所示：\n当我们对文件进行操作时，首先会调用 inode_permission() 函数来判断执行用户的访问权限，如果权限不足则阻止对应操作的执行。\nACL 权限管理 ACL（Access Control List，访问控制列表），是一个针对文件/目录的访问控制列表。它在 RWX-UGO 权限管理的基础上为文件系统提供一个额外的、更灵活的权限管理机制。ACL 允许给任何用户或用户组设置任何文件/目录的访问权限，这样就能形成网状的交叉访问关系。\n根据是否继承父目录的 ACL 属性，ACL 分为以下两类：\n access ACL：每一个对象（文件/目录）都可以关联一个 ACL 来控制其访问权限，这样的 ACL 被称为 access ACL。 default ACL：目录关联的一种 ACL。当目录具备该属性时，在该目录中创建的对象（文件或子目录）默认具有相同的 ACL。  ACL 在操作系统内部是通过文件的扩展属性实现的。 当用户为文件添加一个 ACL 规则时，其实就是为该文件添加一个扩展属性。如下图：\n需要注意的是，ACL 的数据与文件的普通扩展属性数据存储在相同的位置，只不过通过特殊的标记进行了区分，这样就可以屏蔽普通用户对 ACL 的访问。\nSELinux 权限管理 SELinux（Security-Enhanced Linux）是一个在内核中实现的强制存取控制（MAC）安全性机制。SELinux 与 RWX、ACL 最大的区别是基于访问者（应用程序）与资源的规则，而不是用户与资源的规则，因此其安全性更高。这里基于应用程序与资源的规则是指规定了哪些应用程序可以访问哪些资源，而与运行该应用程序的用户无关。\nSELinux 通过将权限从用户收缩到应用，即使应用被攻破，黑客也只能访问应用程序所限定的资源，而不会扩散到其他地方。\n其原理图如下：\n当访问者访问被访问者（资源）时，需要调用内核的接口，此时会经过 SELinux 内核的判断逻辑，该判断逻辑根据策略数据库的内容确定访问者是否有权访问被访问者，如果允许访问则放行，否则拒绝该请求并记录审计日志\n配额管理 在多用户环境中，不仅要防止用户对其他用户数据的非法访问，还要确保某些用户所使用的存储空间不能太多。在这种情况下，就需要一种配额 （Quota）管理技术。\n配额管理是一种对使用空间进行限制的技术，其主要包括针对用户（或组）的限制和针对目录的限制两种方式。针对用户（或组）的限制是指某一个用户（或组）对该文件系统的空间使用不能超过设置的上限，如果超过上限则无法写入新的数据。针对目录的限制是指该目录中的内容总量不能超过设置的上限。\n在配额管理中，通常涉及三个基本概念：\n 软上限（Soft Limit）：指数据总量可以超过该上限。如果超过该上限则会有一个告警信息。 硬上限（Hard Limit）：指数据总量不可以超过该上限，如果超过该上限则无法写入新的数据。 宽限期（Grace Period）：宽限期通常是针对软上限而言的。如果设置了该值（如 3 天），则在 3 天内允许数据量超过软上限，当超过3天后，无法写入新的数据。  其实现的原理非常简单，当有新的写入请求时，配额模块就会去分析该请求。如果需要计入配额管理中，则进行配额上限的检查，在小于上限的情况下会更新配额管理数据，否则将阻止新的数据写入，并发出告警信息。\n文件锁 文件锁的基本作用就是保证多个进程对某个文件并发访问时数据的一致性。如果没有文件锁，就有可能出现多个进程同时访问文件相同位置数据的问题，从而导致文件数据的不一致性。\n文件锁分为以下两种类型：\n 劝告锁（Advisory Lock）：劝告锁是一种建议性的锁，通过该锁告诉访问者现在该文件被其他进程访问，但不强制阻止访问。 强制锁（Mandatory Lock）：强制锁则是在有进程对文件锁定的情况下，其他进程是无法访问该文件的。  为了提高性能，减少等待锁时间，以上两种锁都引入了共享锁、排他锁机制。\n 共享锁（Shared Lock）：在任意时间内，一个文件的共享锁可以被多个进程拥有，共享锁不会导致系统进程的阻塞。 排他锁（Exclusive Lock）：在任意时间内，一个文件的排他锁只能被一个进程拥有。  当有新的加锁请求到来时，其会与已有的锁信息逐一比对，判断是否存在锁冲突。如果存在，则将该进程置为休眠状态（阻塞，如果为非阻塞则返回直接错误码）。如果不存在，则将锁信息记录下来后允许用户访问。\n","date":"2022-05-23T18:53:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"本地文件系统"},{"content":"文件系统 基础 什么是文件系统？  文件系统是一种存储和组织计算机数据的方法，它使得对其访问和查找变得容易，文件系统使用文件和树形目录的抽象逻辑概念代替了硬盘和光盘等物理设备使用数据块的概念，用户使用文件系统来保存数据不必关心数据实际保存在硬盘（或者光盘）的地址为多少的数据块上，只需要记住这个文件的所属目录和文件名。在写入新数据之前，用户不必关心硬盘上的那个块地址没有被使用，硬盘上的存储空间管理（分配和释放）功能由文件系统自动完成，用户只需要记住数据被写入到了哪个文件中。\n 上文是维基百科对于文件系统的描述，我们将其精炼一下，得到以下结论：\n 文件系统是一套实现了数据的存储、分级组织、访问和获取等操作的抽象数据类型（Abstract data type）。其可以构建在磁盘、内存、甚至是网络上。  用于解决什么问题？  我们为什么要使用文件系统呢？换句话说，文件系统它解决了什么问题呢？\n 文件系统解决的是对磁盘空间使用的问题。 如果我们不抽象出一个文件系统，而是每个应用程序都独立去访问磁盘空间，那会出现什么样的情况呢？\n 磁盘空间的访问会存在冲突。 如果没有一个第三方去统一管理磁盘空间，那么此时应用程序的数据访问并不会进行隔离，而是会互相影响，产生冲突。 磁盘空间的管理会非常复杂。 由于文件有不同格式、大小，存储的设备与位置也有所不同（本地磁盘、内存、甚至远端机器）。直接管理磁盘空间将会非常的复杂。  文件系统正是为了解决这两个问题，而在应用程序与磁盘空间之间抽象出的一层。文件系统对下实现了对磁盘空间的管理，对上为应用程序呈现层级数据组织形式和统一的访问接口。\n基于文件系统，应用程序只需要创建、删除或读取文件即可，他们并不需要关注磁盘空间的细节，所有磁盘空间管理相关的动作交由文件系统来处理。\n相关概念  目录：在文件系统中目录是一种容器，它可以容纳子目录和普通文件。但同时目录本身也是一种文件。只不过目录中存储的数据是特殊的数据，这些数据就是关于文件名称等元数据（管理数据的数据）的信息。 文件：在文件系统中，最基本的概念是文件，文件是存储数据的实体。从用户的角度来看，文件是文件系统中最小粒度的单元。为了便于用户对文件进行识别和管理，文件系统为每个文件都分配了一个名称，称为文件名。同时为了方便用户辨别文件的类型，每个文件都会有一个拓展名来标识其类型。（但在文件系统中都以字节流存储） 链接：Linux 中的链接分为软链接（Soft Link）和硬链接（Hard Link）两种。其中，软链接又被称为符号链接（Symbolic Link），它是文件的另外一种形态，其内容指向另外一个文件路径（相对路径或绝对路径）。硬链接则不同，它是一个已经存在文件的附加名称，也就是同一个文件的第 2 个或第 N 个名称。  基本原理  文件系统的主要核心就是对下实现了对磁盘空间的管理，对上为应用程序呈现层级数据组织形式和统一的访问接口。\n 在文件系统中，为了简化用户对数据的访问，向用户屏蔽数据的存储方式，其会对磁盘空间进行规划、组织和编号处理。\n以 Ext4 文件系统为例，它会将磁盘空间进行划分。 首先将磁盘空间划分为若干个子空间，这些子空间称为块组。然后将每个子空间划分为等份的逻辑块，逻辑块是最小的管理单元。\n为了管理这些逻辑块，需要一些区域来记录哪些逻辑块已经被使用了，哪些还没有被使用。记录这些数据的数据通常在磁盘的特殊区域，我们称这些数据为文件系统的元数据（Metadata）。通过元数据，文件系统实现了对磁盘空间的管理，最终为用户提供了简单易用的接口。\n当我们调用这些接口时，用户对文件的操作就转化为文件系统对磁盘空间的操作。 比如，当用户向某个文件写入数据时，文件系统会将该请求转换为对磁盘的操作，包括分配磁盘空间、写入数据等。而对文件的读操作则转换为定位到磁盘的某个位置、从磁盘读取数据等。\n分类 目前，常见的文件系统有几十种。虽然文件系统的具体实现形式纷繁复杂，具体特性也各不相同，但是有一定规律可循。下面将介绍一下常见的文件系统都有哪些。\n本地文件系统 本地文件系统是对磁盘空间进行管理的文件系统，也是最常见的文件系统形态。从呈现形态上来看，本地文件系统就是一个树形的目录结构。本地文件系统本质上就是实现对磁盘空间的管理，实现磁盘线性空间与目录层级结构的转换。\n从普通用户的角度来说，本地文件系统主要方便了对磁盘空间的使用，降低了使用难度，提高了利用效率。常见的本地文件系统有 ExtX、Btrfs、XFS 和 ZFS 等。\n伪文件系统 伪文件系统是 Linux 中的概念，它是对传统文件系统的延伸。伪文件系统存在于内存，不占用硬盘。它以文件系统的形态实现用户与内核数据交互的接口，其通过文件的形式向用户提供一些系统信息，用户通过读写这些文件就可以读取、修改系统的一些信息。 常见的伪文件系统有 proc、sysfs 和 configfs 等。\n例如我们在 Linux 中通常用来检测性能的一些工具（如 iostat、top 等），其本质上就是通过访问 /proc/cpuinfo、/proc/diskstats、/proc/meminfo 等文件来获取内核信息。\n网络文件系统 网络文件系统（Network File System）是基于 TCP/IP 协议（整个协议可能会跨层）的文件系统，它可以将远端服务器文件系统的目录挂载到本地文件系统的目录上，允许用户或者应用程序像访问本地文件系统的目录结构一样，访问远端服务器文件系统的目录结构，而无需理会远端服务器文件系统和本地文件系统的具体类型，非常方便地实现了目录和文件在不同机器上进行共享。\n网络文件系统通常分为客户端和服务端，其中客户端类似本地文件系统，而服务端则是对数据进行管理的系统。网络文件系统的使用与本地文件系统的使用没有任何差别，只需要执行 mount 命令挂载远端文件系统即可。常见的网络文件系统如 NFS、SMB 等。\n集群文件系统 集群文件系统（Clustered File System）是指运行在多台计算机之上，之间通过某种方式相互通信从而将集群内所有存储空间资源整合、虚拟化并对外提供文件访问服务的文件系统。\n其本质上还是一种本地文件系统，但与 NTFS、EXT 等本地文件系统的目的不同，它通常构建在基于网络的 区域存储网络（SAN） 设备上，且在多个节点中共享 SAN 磁盘。前者是为了扩展性，后者运行在单机环境，纯粹管理块和文件之间的映射以及文件属性。\n集群文件系统最大的特点是可以实现客户端节点对磁盘介质的共同访问，且视图具有一致性。其访问模式如下图：\n其最大的特点是多个节点可以同时为应用层提供文件系统服务，特别适合用于业务多活的场景，通过集群文件系统提供高可用集群机制，避免因为宕机造成服务失效。\n分布式文件系统 分布式文件系统（Distributed File System）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点（可简单的理解为一台计算机）相连；或是若干不同的逻辑磁盘分区或卷标组合在一起而形成的完整的有层次的文件系统。\n从本质上来说，分布式文件系统其实也是网络文件系统的一种，其与网络文件系统的差异在于服务端包含多个节点，也就是服务端是可以横向扩展的。\n","date":"2022-05-23T18:52:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-%E5%9F%BA%E7%A1%80/","title":"文件系统 基础"},{"content":"Git 分支 什么是分支？ Git 保存的不是文件的变化或者差异，而是一系列不同时刻的文件快照。 在进行提交操作时，Git 会保存一个提交对象，该提交对象会包含一个指向暂存内容快照的指针，以及作者的姓名和邮箱、提交时输入的信息以及指向它的父对象的指针。\n 首次提交产生的提交对象没有父对象，普通提交操作产生的提交对象有一个父对象，而由多个分支合并产生的提交对象有多个父对象，\n 假设现在有一个工作目录，里面包含了三个将要被暂存和提交的文件。暂存操作会为每一个文件计算校验和，然后会把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 对象来保存它们），最终将校验和加入到暂存区域等待提交。\n当进行提交操作时，Git 会先计算每一个子目录的校验和，然后将 Git 仓库中这些校验和保存为树对象。 随后 Git 便会创建一个提交对象，它不仅包含上面提到的那些信息，还包含指向这个树对象（项目根目录）的指针。如此一来，Git 就可以在需要的时候重现此次保存的快照。\n如下图，此时Git 仓库中有五个对象：三个 blob 对象（保存着文件快照）、一个树对象（记录着目录结构和 blob 对象索引）以及一个提交对象（包含着指向前述树对象的指针和所有提交信息）。\n做些修改后再次提交，那么这次产生的提交对象会包含一个指向上次提交对象（父对象）的指针。\nGit 的分支，其实本质上仅仅是指向提交对象的可变指针。 Git 的默认分支名字是 master。 在多次提交操作之后， master 分支会在每次的提交操作中自动向前移动，并指向最后一个提交对象。\n分支管理 查看分支 当我们使用 git branch 命令，并不加任何参数时，就可以得到当前所有分支的列表：\n1 2 3  $ git branch master * test   ps：* 代表当前所在的分支，即当前 HEAD 指针所指向的分支。\n我们可以在后面加上这两个参数，来获取到这个列表中已经合并或尚未合并到当前分支的分支。\n1 2  git branch --merged # 查看哪些分支已经合并到当前分支 git branch --no-merged # 查看所有包含未合并工作的分支   我们还可以通过 -v 选项，查看每一个分支的最后一次提交：\n1 2 3  $ git branch -v master f876558 [ahead 1, behind 1] test * test da8f5f4 commit   创建分支 当我们要创建一个分支时，可以使用以下命令：\n1  git branch [branch_name]   例如我们创建一个 testing 分支，这会在当前所在的提交对象上创建一个指针。\n 那么，Git 又是怎么知道当前在哪一个分支上呢？ 也很简单，它有一个名为 HEAD 的特殊指针。\n 此时指针关系如下图：\n想要新建一个分支并同时切换到那个分支上，你可以运行一个带有 -b 参数的 git checkout 命令：\n1 2 3 4  git checkout -b [branch_name] 等价于 git branch [branch_name] git checkout [branch_name]   切换分支 当我们要切换到一个已经存在的分支上时，可以使用下面这个命令将 HEAD 指向该分支：\n1  git checkout [branch_name]   例如我们切换到 testing 分支上，此时指向关系如下图：\n如果我们在这个新分支上做了修改，并将修改提交后，此时 testing 分支就会向前移动，如下图：\n此时由于两个分支的版本不同，当我们再次切换回 master 分支时，此时会执行两个操作：\n 使 HEAD 指向 master。 将工作目录恢复成 master 分支所指向的快照内容。  如果我们再次在 master 分支上进行修改后提交，此时两个分支就会完全分叉，走上不同的路线，如下图：\n此时我们就需要通过 merge 来对分支进行合并，使其重新回到统一的一个版本。\n合并分支 当我们在特性分支上完成开发后，此时就需要将特性分支合并入 master 分支。此时我们就需要先切换到主分支，在主分支上执行 git merge：\n1 2  git checkout master git merge [branch]   Git 会自行决定选取哪一个提交作为最优的共同祖先，并以此作为合并的基础。接着它会把两个分支的最新快照以及二者最近的共同祖先进行三方合并，合并的结果是生成一个新的快照（并提交）。\n但事实上，只有在这些分支没有任何冲突时才能这么顺利的进行合并，如果存在冲突，我们必须要将冲突的内容解决后，才能进行合并。\n合并冲突解决 如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法合并它们。此时 Git 会暂停下来，等待你去解决合并产生的冲突。\n1 2 3 4 5  $ git merge test Removing test.md Auto-merging README.md CONFLICT (content): Merge conflict in README.md Automatic merge failed; fix conflicts and then commit the result.   此时我们可以使用 git status 命令来查看那些因包含合并冲突而处于未合并状态的文件，并查看它的内容：\n1 2 3 4 5  \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD Hello world123123:` ======= HELLO WORLD !!! \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; test   此时 Git 用 ======= 分割了两个分支的冲突的内容，\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; 后面显示了 HEAD 所指向的版本，\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 后面展示了 test 指向的版本。此时我们就需要进行权衡，选择应该怎么样去解决冲突。\n当我们将 =======、\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;、\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 删除后，并对冲突内容进行解决。此时我们可以对每个文件使用 git add 命令来将其标记为冲突已解决。 一旦暂存这些原本有冲突的文件，Git 就会将它们标记为冲突已解决。此时我们再进行 git commit，就会自动完成 merge 操作。\n删除分支 当我们想要删除某个分支的时候，可以使用下面这个命令：\n1  git branch -d [branch name]   如果该分支还未被 merge，则此时会报错\n1 2 3  $ git branch -d testing error: The branch \u0026#39;testing\u0026#39; is not fully merged. If you are sure you want to delete it, run \u0026#39;git branch -D testing\u0026#39;.   如果仍然需要删除，则可以使用 -D 选项强制删除分支。\n分支开发的工作流程 长期分支 在项目的开发过程中，为了维护不同层次的稳定性，通常会拥有几个长期存在的分支。大多数人采用下面这种开发模式，只在 master 分支上保留完全稳定的代码，而在 develop、next 等分支上进行后续的开发，当这些分支通过测试，达到稳定状态时就可以将它们并入到 master 分支中。\n如上图，稳定分支的指针总是在提交历史中落后一大截，而前沿分支的指针往往比较靠前。\n特性分支 特性分支是一种短期分支，它被用来实现单一特性或其相关工作。例如我们需要快速实现一个新功能，或者修复一些遗留的问题，就可以在特性分支上进行开发，然后将其并入主分支后再将其删除。\n由于工作被分散到不同的流水线中，在不同的流水线中每个分支都仅与其目标特性相关，因此，在做代码审查之类的工作的时候就能更加容易地看出你做了哪些改动。 我们可以把做出的改动在特性分支中保留几分钟、几天甚至几个月，等它们成熟之后再合并，而不用在乎它们建立的顺序或工作进度。对于不想要的分支，也可以直接将其抛弃，并且不会造成任何影响。\n远程分支 远程引用是对远程仓库的引用（指针），包括分支、标签等等。 可以通过 git ls-remote (remote) 来显式地获得远程引用的完整列表，或者通过 git remote show (remote) 获得远程分支的更多信息。 然而，一个更常见的做法是利用远程跟踪分支。\n远程跟踪分支是远程分支状态的引用。 它们是你不能移动的本地引用，当你做任何网络通信操作时，它们会自动移动。 远程跟踪分支像是你上次连接到远程仓库时，那些分支所处状态的书签。它们以 (remote)/(branch) 形式命名。\n如上图，例如我们从 Git 服务器上 git clone 一个仓库时，会为自动将其该仓库命名为 origin，拉取它的所有数据，创建一个指向它的 master 分支的指针，并且在本地将其命名为 origin/master。 Git 也会给你一个与 origin 的 master 分支在指向同一个地方的本地 master 分支，这样我们就可以在本地进行开发工作。\n推送 当你想要公开分享一个分支时，需要将其推送到有写入权限的远程仓库上，此时可以使用以下命令：\n1 2 3  git push [remote] [branch] # 如果想要修改远程仓库的分支名，可以使用下面这个命令 git push [remote] [local_branch_name]:[remote_branch_name]   跟踪 从一个远程跟踪分支检出一个本地分支会自动创建一个跟踪分支。 跟踪分支是与远程分支有直接关系的本地分支。 如果在一个跟踪分支上输入 git pull，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。（例如当克隆一个仓库时，自动创建的跟踪 origin/master 的 master 分支）。\n如果我们想要设置一个新的跟踪分支，可以使用下面这个命令：\n1  git checkout -b [branch] [remote]/[branch]   如果我们想要让已有的本地分支跟踪一个远程分支，或者是修改已有分支的跟踪目标，就可以使用下面这个命令：\n1 2 3  git branch -u [remote]/[branch] 或者 git branch --set-upstream-to [remote]/[branch]   如果我们想查看设置的所有跟踪分支，可以使用下面这个命令：\n1  git branch -vv   这些数字的值来自于你从每个服务器上最后一次抓取的数据。 这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。\n1 2  git fetch --all git branch -vv   拉取 如果我们想要同步远程分支上的数据时，可以使用下面两种方式抓取本地没有的数据，并且更新本地数据库，移动 remote/branch 指针指向新的、更新后的位置。\n  git fetch ：该命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。 它只会获取数据然后让你自己使用 git merge 进行合并。\n1 2 3 4 5 6 7 8  # 下载远端分支到本地 git fetch [remote] [remote_branch]:[local_branch] # 比较差异 git diff local_branch # 合并分支 git merge local_branch # 旧删除分支 git branch -d local_branch     git pull：它其实相当于 git fetch + git merge，它会查找当前分支所跟踪的服务器与分支，从服务器上抓取数据然后尝试合并入那个远程分支。\n1  git pull [remote] [remote_branch]:[local_branch]     虽然 git pull 简化了流程，但由于 git pull 会对本地工作目录造成修改，所以通常都会使用 git fetch 来拉取分支。\n删除 当我们在某个分支上已经完成了开发，并将其合并到了远程仓库的 master 分支后，此时该分支就失去了作用，可以将其从服务器上删除。此时我们可以使用带有 --delete 选项的 git push 命令来删除一个远程分支：\n1  git push [remote] --delete [branch]   这个命令做的只是从服务器上移除这个指针。 Git 服务器通常会保留数据一段时间直到垃圾回收运行，所以如果不小心删除掉了，通常是很容易恢复的。\nRebase 在 Git 中整合不同分支的方法除了 merge 以外，还有 rebase。\n什么是 Rebase？ 例如下图这个场景，此时出现了两个不同的分支，并且各自又提交了更新，导致出现了分叉：\n如果我们要整合这两个分支，最简单的方法就是使用 merge，它会两个分支的最新快照（C3 和 C4）以及二者最近的共同祖先（C2）进行三方合并，合并的结果是生成一个新的快照（并提交）。\n但除此之外，还有另一种方法，即提取在 C4 中引入的补丁和修改，然后在 C3 的基础上应用一次。在 Git 中，这种操作就叫做 rebase（变基）。 你可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上。\n1 2 3  git rebase [topicbranch] # 此时basebranch为当前HEAD指向的分支 or git rebase [basebranch] [topicbranch]   rebase 的原理是首先找到这两个分支的最近共同祖先，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底，最后以此将之前另存为临时文件的修改依序应用。\n例如执行下面这个命令：\n1 2 3 4  git checkout experiment git rebase master 或者 git rebase master experiment # 等价于前两句，将master变基到experiment上，避免切换分支   此时就会将 C4 中的修改变基到 C3 上，如下图：\n此时 experiment 与 master 处于同一条之间中，我们回到 master 分支上执行一次快进合并，即可再次回到一条直线：\n1 2  git checkout master git merge experiment   此时就已经完成了分支的合并，不仅和 merge 的效果相同，而且还是得提交历史更加整洁，是一条没有分叉的直线。\n因此当我们需要确保在向远程分支推送时能保持提交历史的整洁时就可以使用 rebase，我们在自己的分支开发完毕后将代码 rebase 到 master 分支上，master 在进行快进合并即可整合分支。\n 除了直接 rebase 一个分支，rebase 还支持更细粒度的操作\n 1 2 3  git rebase --onto [branch] [from] [to] # from 待合并片段的起始commitId（不包含） # to 待合并片段的结束commitId（包含）   即取出 to 分支，找出处于 from 分支和 to 分支的共同祖先之后的修改，然后把它们在 base 分支上重放一遍。 以下图为例，执行 git rebase --onto master server client，此时会将 client 的 c8 和 c9 在 master 上重放：\nRebase 的风险 如果要使用 rebase，就必须记住这条原则：只对尚未推送或分享给别人的本地修改执行变基操作清理历史，从不对已推送至别处的提交执行变基操作。\n假设这样一个场景，我们与项目的其他成员在同一个远程仓库中进行协作开发，此时我们拉取下来项目，并在其基础上进行开发，如图：\n此时用户 A 也在该仓库进行开发，它首先提交了修改 c4 和 c5，并将它们 merge 得到 c6。此时我们使用 pull 将这些修改拉取到本地，此时提交历史如下图：\n此时用户 A 对 merge 不满意，他想通过 rebase 来使提交记录更加清晰，于是将 merge 回滚后改为使用 rebase，并使用 git push --force 强制覆盖了提交历史。此时提交记录如下：\n如果我们再次使用 pull 拉取数据，此时提交历史如下：\n此时就出现了意料之外的情况：\n 对我们而言，我们将相同的内容又合并了一次，生成了一个新的提交，并且用户 A 使用了 rebase，因此这两次提交的 log 中作者、日期、日志等信息是一模一样的。 对于用户 A 而言，如果我们此时将修改 push 到远程仓库中，又再次将用户 A 丢弃的 c4 和 c6 提交记录找回，而这又违背了用户 A 简化提交历史的初衷。   那在这种情况下，有什么方法能解决这个问题呢？\n 可以使用 git pull --rebase 命令而不是直接 git pull。 又或者可以自己手动完成这个过程，先 git fetch，再 git rebase teamone/master。但这种方法需要每一个人都执行该命令，因此我们通常把变基命令当作是在推送前清理提交使之整洁的工具，并且只在从未推送至共用仓库的提交上执行变基命令，以避免这种场景的出现。\n","date":"2022-05-23T18:39:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%88%86%E6%94%AF/","title":"Git 分支"},{"content":"Git 基础 Git 的安装与配置 这里以 CentOS 8 举例。\n首先使用 yum 安装 Git\n1  sudo yum install git   检查是否安装成功并查看版本号\n1  git --version   接着开始配置用户信息\n1 2 3 4 5  # 配置用户名 git config --global user.name \u0026#34;yourname\u0026#34; # 配置邮箱 git config --global user.email \u0026#34;yourname@email.com\u0026#34;   接着列出所有配置，查看是否配置成功\n1  git config --list   获取仓库 获取 Git 项目仓库的方法有两种：\n 在现有项目或目录下导入所有文件到 Git 中； 从一个服务器克隆一个现有的 Git 仓库。  初始化新仓库 如果打算使用当前已有的项目来初始化一个新仓库，你只需要进入该项目目录并输入：\n1  git init   该命令将创建一个名为 .git 的子目录，这个子目录含有你初始化的 Git 仓库中所有的必须文件。接下来使用 git add 来跟踪已有的文件，再使用 git commit 来进行提交，就可以完成当前仓库的构建。\n克隆已有仓库 如果想获得一份已经存在了的 Git 仓库的拷贝，可以使用下面这个命令：\n1  git clone [url] (可选，本地仓库名称)   其会在当前目录下创建一个与项目同名（默认）的目录，并在这个目录下初始化一个 .git 文件夹，从远程仓库拉取下所有数据放入 .git 文件夹，然后从中读取最新版本的文件的拷贝。此时所有的项目文件已经存放在本地仓库中，准备就绪等待后续的开发和使用。\n Git 克隆的是该 Git 仓库服务器上的几乎所有数据，而不是仅仅复制完成你的工作所需要文件。 当你执行 git clone 命令的时候，默认配置下远程 Git 仓库中的每一个文件的每一个版本都将被拉取下来。\n 记录更新至仓库 查看文件状态 当我们要查看文件的状态时，可以使用 git status 命令。例如：\n1 2 3 4 5 6 7 8 9  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) test2.cpp   我们可以看到，其说明了我们当前所处的分支，以及展示了当前未被跟踪的文件，以及暂存区中待提交的文件。\n我们还可以在后面加上 -s 或者 --short 参数来使输出更加简洁\n1 2 3 4  $ git status -s M README.md A test.cpp ?? test2.cpp   左边的状态码的含义如下；\n  新添加的未跟踪文件前面有 ?? 标记。\n  新添加到暂存区中的文件前面有 A 标记。\n  修改过的文件前面有 M 标记。出现在右边的 M 表示该文件被修改了但是还没放入暂存区，出现在靠左边的 M 表示该文件被修改了并放入了暂存区。\n  跟踪新文件  git add 是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。\n 我们可以使用命令 git add 来跟踪一个文件，例如：\n1  git add test.cpp    git add 命令使用文件或目录的路径作为参数；如果参数是目录的路径，该命令将递归地跟踪该目录下的所有文件。\n 此时用 git status 就可以看到文件已经被跟踪，并放入暂存区中：\n1 2 3 4 5  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp   暂存已修改文件 如果我们修改了一个已经被跟踪的文件，此时它的状态如下：\n1 2 3 4 5 6 7 8 9 10  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: README.md   此时文件内容发生了变化，但是还没有被放入暂存区中，此时就需要我们使用 git add 将其添加到暂存区\n1 2 3 4 5 6 7  $ git add README.md $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp   此时文件已被加入到暂存区中，在下次使用 git commit 提交时就会被添加到仓库。但是，如果我们此时修改一个暂存区中的文件，会怎么样呢？\n1 2 3 4 5 6 7 8 9 10 11  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: README.md   此时看起来像是文件同时出现在了暂存区和非暂存区，但事实是这样吗？答案是否定的，git 中有版本的概念，此时存在暂存区中的是我们上一次提交的那个版本，而此时在非暂存区中的即是最新的版本。\n此时，我们就需要再次使用 git add 来将最新的版本暂存起来\n1 2 3 4 5 6 7  $ git add README.md $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp   忽略文件 文件 .gitignore 的格式规范如下：\n 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。   glob 模式是指 shell 所使用的简化了的正则表达式。\n 星号（*）匹配零个或多个任意字符； [abc] 匹配任何一个列在方括号中的字符； 问号（?）只匹配一个任意字符； 如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（*) 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。   例如一个 C++ 的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # Prerequisites *.d # Compiled Object files *.slo *.lo *.o *.obj # Precompiled Headers *.gch *.pch # Compiled Dynamic libraries *.so *.dylib *.dll # Fortran module files *.mod *.smod # Compiled Static libraries *.lai *.la *.a *.lib # Executables *.exe *.out *.app    GitHub 有一个十分详细的针对数十种项目及语言的 .gitignore 文件列表：https://github.com/github/gitignore 。\n 差异比较 git diff 用于比较已写入暂存区和已经被修改但尚未写入暂存区文件的区别。\n例如此时我们修改了一个已写入暂存区的文件，此时结果如下：\n1 2 3 4 5 6 7 8  $ git diff diff --git a/README.md b/README.md index d0e7d60..a261b14 100644 --- a/README.md +++ b/README.md @@ -1 +1 @@ -HELLO WORLD +HELLO WORLD !!!   如果要查看已暂存的将要添加到下次提交里的内容，可以用下面这些命令：\n1 2 3  git diff --cached\t// 老版本 或者 git diff --staged // 新版本   请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。\n提交更新 如果暂存区中的内容已经准备就绪了，此时就可以使用 git commit 来提交更新，例如：\n1 2  $ git commit -m \u0026#34;commit\u0026#34; # -m 用于将提交信息与命令放在同一行   提交完成后，它会告诉你当前是在哪个分支提交的，本次提交的完整 SHA-1 校验和是什么，以及在本次提交中，有多少文件修订过，多少行添加和删改过。\n1 2 3  [test da8f5f4] commit 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 test.md   跳过使用暂存区域 如果觉得每次提交前都要将已跟踪的文件存入暂存区过于麻烦，可以为 git commit 加上一个 -a 参数，例如：\n1 2 3 4 5 6 7  $ git commit -a -m README.md [test fa38827] README.md 1 file changed, 1 insertion(+) $ git status On branch test nothing to commit, working tree clean   此时 Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。\n删除文件 如果我们想要从 Git 中删除一个文件，就必须要将其从已跟踪文件清单移除，然后 commit 提交。\n我们可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。\n1 2 3 4 5 6 7 8  $ git rm test.md rm \u0026#39;test.md\u0026#39; $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) deleted: test.md    如果我们直接删除文件，会怎么样呢？\n 1 2 3 4 5 6 7 8 9  $ rm test.md $ git status On branch test Changes not staged for commit: (use \u0026#34;git add/rm \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) deleted: test.md no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;)   此时这个操作会被加入未暂存清单中，我们仍然需要使用 git rm 来进行删除。\n 如果只是想从仓库中删除文件（或者暂存区），而不想在本地删除，那该怎么做呢？\n 此时可以使用 –cached 参数：\n1  git rm --cached [file]    如果在删除前，这个文件就已经在暂存区中，那该怎么办呢？\n 1 2 3 4 5  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: test.md   此时文件已经存储在暂存区了，我们尝试使用 git rm 删除它：\n1 2 3 4  $ git rm test.md error: the following file has changes staged in the index: test.md (use --cached to keep the file, or -f to force removal)   此时我们发现其无法直接被删除，原因是 Git 为了防止误删还没有添加到快照中的数据（这种数据无法被 Git 恢复），为其添加了安全策略，需要使用强制删除的选项 -f 才行。\n1  git rm -rf test.md   移动/重命名文件 当我们想要将一个文件移动到其他目录时，可以使用下面命令：\n1  git mv [源文件] [新路径]   同样，我们也可以使用 git mv 来完成文件的重命名，例如：\n1  git mv [原文件名] [新文件名]   那它是如何做到既能移动，又能够重命名呢？其实运行 git mv 就相当于运行了下面三条命令：\n1 2 3  mv [原文件名] [新文件名] git rm [原文件名] git add [新文件名]   此操作必须要在暂存区或者文件commit之后才能进行\n查看提交记录 当我们想要查看提交记录时，可以使用 git log 命令，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  $ git log commit af1248beeb9de2b58d339441c5fccc2cbd652374 (HEAD -\u0026gt; master, origin/master, origin/HEAD) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 11 14:43:39 2022 +0800 update commit 1801c1003b9250e7854eec43dd53c4974b5b04b2 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Sat Apr 9 18:13:42 2022 +0800 C++ 20 done commit 11ed22aab5099608427492711c560a2c967ba7d3 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Fri Apr 8 21:34:39 2022 +0800 Cpp 17 done 20 doing commit 9d519fa0ada5991fc8791012d98c736704ee3b68 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Thu Apr 7 21:49:42 2022 +0800   我们可以看到，其默认以提交时间降序排序，同时列出了每个提交的 SHA-1 校验和、作者和邮箱、提交时间以及提交说明。\n其常用选项如下：\n   选项 说明     -p 按补丁格式显示每个更新之间的差异。   \u0026ndash;stat 显示每次更新的文件修改统计信息。   \u0026ndash;shortstat 只显示 \u0026ndash;stat 中最后的行数修改添加移除统计。   \u0026ndash;name-only 仅在提交信息后显示已修改的文件清单。   \u0026ndash;name-status 显示新增、修改、删除的文件清单。   \u0026ndash;abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。   \u0026ndash;relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。   \u0026ndash;graph 显示 ASCII 图形表示的分支合并历史。   \u0026ndash;pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。    其常用的限制输出的参数如下：\n   选项 说明     -(n) 仅显示最近的 n 条提交   \u0026ndash;since, \u0026ndash;after 仅显示指定时间之后的提交。   \u0026ndash;until, \u0026ndash;before 仅显示指定时间之前的提交。   \u0026ndash;author 仅显示指定作者相关的提交。   \u0026ndash;committer 仅显示指定提交者相关的提交。   \u0026ndash;grep 仅显示含指定关键字的提交   -S 仅显示添加或移除了某个关键字的提交    撤销操作 补充文件/信息 如果我们已经 git commit 提交之后，才发现可能有几个文件忘记提交，又或者是提交的信息写错了。这时可以使用下列命令来尝试重新提交\n1  git commit --amend   这个命令会将暂存区中的文件提交。 如果自上次提交以来你还未做任何修改，那么快照会保持不变，而你所修改的只是提交信息。而如果进行了修改，则第二次提交将代替第一次提交的结果。\n撤销暂存的文件 如果我们不小心使用 git add 将不必要的文件加入暂存区，如何将这些文件撤销呢？这时候就要用到下述命令：\n1 2 3  git reset HEAD \u0026lt;file\u0026gt;...\t// 老版本 或者 git restore --staged \u0026lt;file\u0026gt;... // 新版本   在调用时加上 \u0026ndash;hard 选项会令 git reset 成为一个危险的命令，即可能导致工作目录中所有当前进度丢失。\n撤销对文件的修改 当我们对一个文件进行修改后，如果不满意刚才的修改，像将其回滚到上次提交的状态时，可以使用如下命令：\n1 2 3  git checkout -- \u0026lt;file\u0026gt;...\t// 老版本 或者 git restore \u0026lt;file\u0026gt;... // 新版本   git checkout -- [file] 是一个危险的命令。 你对那个文件做的任何修改都会消失， 除非确定不想要那个文件，否则不要使用这个命令。\n远程仓库 查看远程仓库 如果想查看你已经配置的远程仓库服务器，可以运行 git remote 命令。 它会列出你指定的每一个远程服务器的简写。也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。例如：\n1 2 3  $ git remote -v origin https://gitee.com/HONGYU-LEE/git-tes.git (fetch) origin https://gitee.com/HONGYU-LEE/git-tes.git (push)   如果还想要查看某一个远程仓库的更多信息，可以使用 git remote show [remote-name] 命令。例如：\n1 2 3 4 5 6 7 8 9 10 11  $ git remote show origin * remote origin Fetch URL: https://gitee.com/HONGYU-LEE/git-tes.git Push URL: https://gitee.com/HONGYU-LEE/git-tes.git HEAD branch: master Remote branch: master tracked Local branch configured for \u0026#39;git pull\u0026#39;: master merges with remote master Local ref configured for \u0026#39;git push\u0026#39;: master pushes to master (local out of date)   它同样会列出远程仓库的 URL 与跟踪分支的信息，并且告诉你正处于 master 分支，并且如果运行 git pull，就会抓取所有的远程引用，然后将远程 master 分支合并到本地 master 分支。 它也会列出拉取到的所有远程引用。\n添加远程仓库并拉取数据 可以通过以下命令来添加一个新的远程 Git 仓库：\n1  git remote add \u0026lt;shortname\u0026gt; \u0026lt;url\u0026gt;    如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写。\n 从远程仓库中拉取数据 使用下面这个命令，就可以从远端仓库中获得数据：\n1  git fetch [remote-name]   这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。\n虽然 git fetch 命令会将数据拉取到你的本地仓库，但是它并不会自动合并或修改你当前的工作，所以在准备好时必须手动将其合并入你的工作。\n如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。\n推送到远程仓库 如果我们想分享项目时，必须将其推送到上游。此时可以使用下面这个命令：\n1  git push [remote-name] [branch-name]   只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。\n远端仓库移除/重命名 如果想要修改一个远程仓库的简写名，可以使用以下命令：\n1  git remote rename [old_name] [new_name]   值得注意的是这同样也会修改你的远程分支名字。 例如那些过去引用 old_name/master 的现在会引用 new_name/master。\n如果因为一些原因想要移除一个远程仓库，可以使用以下命令：\n1  git remote rm [remote-name]   标签（Tag） 与其他版本控制系统一样，Git 可以给历史中的某一个提交打上标签，以示重要。 Git 使用两种主要类型的标签：轻量标签（lightweight）与附注标签（annotated）。\n查看标签 在 Git 中列出已有的标签是非常简单直观的， 只需要输入 git tag，就会以字典序排列出所有的标签：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  $ git tag 1.21 1.22 1.23 v1.10 v1.11 v1.12 v1.13 v1.14 v1.15 v1.16 v1.17 v1.18 v1.19 v1.20 v1.3 v1.4 v1.5 v1.6 v1.7 v1.8 v1.9   也可以使用特定的模式查找标签，例如想查找 v1.10 ~ v1.13 版本的标签，则可以使用正则表达式：\n1 2 3 4 5  $ git tag -l \u0026#39;v1.1[0-3]\u0026#39; v1.10 v1.11 v1.12 v1.13   附注标签 附注标签是存储在 Git 数据库中的一个完整对象。 它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。\n 通常建议创建附注标签，这样你可以拥有以上所有信息；但是如果你只是想用一个临时的标签，或者因为某些原因不想要保存那些信息，轻量标签也是可用的。\n 它的创建也非常简单，只需要加上参数 -a 即可：\n1  $ git tag -a v1.0 -m \u0026#39;version 1.0\u0026#39;   如果是想要对过去的提交打上标签，只需要在末尾指定已提交的校验和（部分校验和也可以，会自动识别）即可：\n1  $ git tag -a v0.8 fa38827eb9d27a2559e8178aa67189105d882fca   此时查看 git tag，发现已经给那个位置追加上了标签\n1 2 3 4  $ git tag v0.8 v1.0 v1.0-test   通过使用 git show 命令可以看到标签信息与对应的提交信息：\n1 2 3 4 5 6 7 8 9 10  $ git show commit da8f5f463cad799940543c7304df4d436a9efb2d (HEAD -\u0026gt; test, tag: v1.0) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 18 17:35:08 2022 +0800 commit diff --git a/test.md b/test.md deleted file mode 100644 index e69de29..0000000   输出显示了打标签者的信息、打标签的日期时间、附注信息，然后显示具体的提交信息。\n轻量标签 轻量标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。 本质上是将提交校验和存储到一个文件中（没有保存任何其他信息）。\n创建一个轻量标签时只需要使用 git tag，而不需要加别的参数，例如：\n1 2 3 4 5 6 7 8 9 10 11  $ git tag v1.0-test $ git show commit da8f5f463cad799940543c7304df4d436a9efb2d (HEAD -\u0026gt; test, tag: v1.0-test, tag: v1.0) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 18 17:35:08 2022 +0800 commit diff --git a/test.md b/test.md deleted file mode 100644 index e69de29..0000000   此时不会看到额外的标签信息， 命令只会显示出提交信息。\n发布标签 默认情况下，git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。\n1  git push origin [tagname]   如果想要一次性推送很多标签，也可以使用带有 --tags 选项的 git push 命令。 这将会把所有不在远程仓库服务器上的标签全部传送到那里。\n1  git push origin --tags   Tag 与 Commit Git 的标签虽然是版本库的快照，但其实它就是指向某个 commit 的指针，但是它与 commit 又有些不同（分支可以移动，标签不能移动）。\n那么为什么有了 commit 后，还要引入 tag 呢？\n \u0026ldquo;请把上周一的那个版本打包发布，commit号是6a5819e…\u0026rdquo;\n\u0026ldquo;一串乱七八糟的数字不好找！\u0026rdquo;\n如果换一个办法：\n\u0026ldquo;请把上周一的那个版本打包发布，版本号是v1.2\u0026rdquo;\n\u0026ldquo;好的，按照tag v1.2查找commit就行！\u0026rdquo;\n所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。\n ","date":"2022-05-23T18:38:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"Git 基本操作"},{"content":"Git 介绍 Git 是什么？ Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。也是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。\n与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。\n版本控制 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。\n本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。 这么做唯一的好处就是简单，但是特别容易犯错。有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。\n为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。\n这时人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 为了解决这个问题，集中化的版本控制系统（Centralized Version Control Systems，CVCS）应运而生。\n集中化版本控制系统 这类系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 多年以来，这已成为版本控制系统的标准做法。\n但此时人们发现，集中化的版本控制系统存在单点问题，如果中央服务器宕机，则在宕机的这段时间所有人都无法提交更新，也就无法协同工作。而如果数据库发生损坏，而有没有及时备份时，所有的数据都将会丢失，只剩下人们在各自机器上保留的单独快照。\n为了解决这个问题，分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。\n分布式版本控制系统 在这类系统中，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。\n更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。 你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。\nGit 的特性   直接记录快照，而非差异比较：Git 把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。\n  近乎所有操作都是本地执行：在 Git 中的绝大多数操作都只需要访问本地文件和资源，一般不需要来自网络上其它计算机的信息。\n  完整性保证：Git 中所有数据在存储前都计算校验和（SHA-1），然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。\n  一般只添加数据：你执行的 Git 操作，几乎只往 Git 数据库中增加数据。 很难让 Git 执行任何不可逆操作，或者让它以任何方式清除数据。\n  Git 状态与执行流程 文件状态 Git 中文件有四种状态：\n 未跟踪（Untracked）：标识未被纳入版本控制的文件，它们既不存在于上次快照的记录中，也没有放入暂存区。除此状态之外的所有状态都是已跟踪。 未修改（Unmodified）：表示数据已经安全的提交，并且从上次提交到现在都没有被修改过。 已修改（Modified）：表示自上次提交后修改了文件，但还没保存到数据库中。 已暂存（Staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。  状态转移图如下：\n工作区域 Git 中存在三个工作区域：Git 仓库、工作目录以及暂存区域。\n  Repository（Git 仓库）：是 Git 用来保存项目的元数据和对象数据库的地方。 这是 Git 中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。\n  Working Directory（工作目录）：是对项目的某个版本独立提取出来的内容。 这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。\n  Staging Area（暂存区）：是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。 有时候也被称作`‘索引’\u0026rsquo;，不过一般说法还是叫暂存区域。\n  工作流程 对应上面提到的状态，我们来根据一个 Git 文件的工作流程，来分析一个 Git 文件的状态变化以及所处区域：\n 当我们在工作目录下创建一个新文件，此时文件的状态是未跟踪。 我们使用 git add 将文件加入跟踪列表，此时文件的状态变为已暂存，同时被放入暂存区中。 此时我们使用 git commit 将文件进行提交，此时文件被保存到 Git 仓库中，状态变化为未修改。 此时我们再次修改文件，文件状态变为已修改，同时处于未暂存区（工作目录）。  Git VS SVN 我们常用 SVN 与其进行对比，那么它们有什么区别呢？\n Git 是分布式的，SVN 不是：这是 Git 和其它非分布式的版本控制系统，例如 SVN，CVS 等，最核心的区别。 Git 把内容按元数据方式存储，而 SVN 是按文件：所有的资源控制系统都是把文件的元信息隐藏在一个类似 .svn、.cvs 等的文件夹里。 Git 分支和 SVN 的分支不同：分支在 SVN 中一点都不特别，其实它就是版本库中的另外一个目录。 Git 没有一个全局的版本号，而 SVN 有：目前为止这是跟 SVN 相比 Git 缺少的最大的一个特征。 Git 的内容完整性要优于 SVN：Git 的内容存储使用的是 SHA-1 哈希算法。这能确保代码内容的完整性，确保在遇到磁盘故障和网络问题时降低对版本库的破坏。  ","date":"2022-05-23T18:34:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%9F%BA%E7%A1%80/","title":"Git 基础"},{"content":"Distributed 引擎 Distributed 表引擎是分布式表的代名词，它自身不存储任何数据，而是作为数据分片的透明代理，能够自动路由数据至集群中的各个节点，所以 Distributed 表引擎需要和其他数据表引擎一起协同工作。\nClickHouse 并不像其他分布式系统那样，拥有高度自动化的分片功能。ClickHouse 提供了**本地表（Local Table）与分布式表（Distributed Table）**的概念\n 本地表：通常以 _local 为后缀进行命名。本地表是承接数据的载体，可以使用非 Distributed 的任意表引擎，一张本地表对应了一个数据分片。 分布式表：通常以 _all 为后缀进行命名。分布式表只能使用 Distributed 表引擎，它与本地表形成一对多的映射关系，日后将通过分布式表代理操作多张本地表。  分布式写入流程 在向集群内的分片写入数据时，通常有两种思路\n  借助外部计算系统，事先将数据均匀分片，再借由计算系统直接将数据写入 ClickHouse 集群的各个本地表。\n  通过 Distributed 表引擎代理写入分片数据。\n  第一种方案通常拥有更好的写入性能，因为分片数据是被并行点对点写入的。但是这种方案的实现主要依赖于外部系统，而不在于 ClickHouse 自身，所以这里主要会介绍第二种思路。为了便于理解整个过程，这里会将分片写入、副本复制拆分成两个部分进行讲解。\n数据写入分片   在第一个分片节点写入本地分片数据：首先在 CH5 节点，对分布式表 test_shard_2_all 执行 INSERT，尝试写入 10、30、200 和 55 四行数据。执行之后分布式表主要会做两件事情：\n  根据分片规则划分数据\n  将属于当前分片的数据直接写入本地表 test_shard_2_local。\n    第一个分片建立远端连接，准备发送远端分片数据：将归至远端分片的数据以分区为单位，分别写入 /test_shard_2_all存储目录下的临时 bin 文件，接着，会尝试与远端分片节点建立连接。\n  第一个分片向远端分片发送数据：此时，会有另一组监听任务负责监听 /test_shard_2_all 目录下的文件变化，这些任务负责将目录数据发送至远端分片，其中，每份目录将会由独立的线程负责发送，数据在传输之前会被压缩。\n  第二个分片接收数据并写入本地：CH6 分片节点确认建立与 CH5 的连接，在接收到来自 CH5 发送的数据后，将它们写入本地表。\n  由第一个分片确认完成写入：最后，还是由 CH5 分片确认所有的数据发送完毕。\n  可以看到，在整个过程中，Distributed 表负责所有分片的写入工作。本着谁执行谁负责的原则，在这个示例中，由 CH5 节点的分布式表负责切分数据，并向所有其他分片节点发送数据。\n在由 Distributed 表负责向远端分片发送数据时，有异步写和同步写两种模式：如果是异步写，则在 Distributed 表写完本地分片之后，INSERT 查询就会返回成功写入的信息；如果是同步写，则在执行 INSERT 查询之后，会等待所有分片完成写入。\n副本复制数据 如果在集群的配置中包含了副本，那么除了刚才的分片写入流程之外，还会触发副本数据的复制流程。数据在多个副本之间，有两种复制实现方式：\n  Distributed 表引擎：副本数据的写入流程与分片逻辑相同，所以 Distributed 会同时负责分片和副本的数据写入工作。但在这种实现方案下，它很有可能会成为写入的单点瓶颈，所以就有了接下来将要说明的第二种方案。\n  ReplicatedMergeTree 表引擎：如果使用 ReplicatedMergeTree 作为本地表的引擎，则在该分片内，多个副本之间的数据复制会交由 ReplicatedMergeTree 自己处理，不再由 Distributed 负责，从而为其减负。\n  分布式查询流程 与数据写入有所不同，在面向集群查询数据的时候，只能通过 Distributed 表引擎实现。当 Distributed 表接收到SELECT查询的时候，它会依次查询每个分片的数据，再合并汇总返回，流程如下：\n多副本的路由规则 在查询数据的时候，如果集群中的某一个分片有多个副本，此时 Distributed 引擎就会通过负载均衡算法从众多的副本中选取一个，负载均衡算法有以下四种。\n在 ClickHouse 的服务节点中，拥有一个全局计数器errors_count，当服务发生任何异常时，该计数累积加1。\n random（默认）：random 算法会选择errors_count 错误数量最少的副本，如果多个副本的errors_count计数相同，则在它们之中随机选择一个。 nearest_hostname：nearest_hostname 可以看作 random 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个 replica 的errors_count计数相同，则选择集群配置中 host 名称与当前 host 最相似的一个。而相似的规则是以当前 host 名称为基准按字节逐位比较，找出不同字节数最少的一个。 in_order：in_order 同样可以看作 random 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个副本的errors_count计数相同，则按照集群配置中 replica 的定义顺序逐个选择。 first_or_random：first_or_random 可以看作 in_order 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个副本的errors_count计数相同，它首先会选择集群配置中第一个定义的副本，如果该副本不可用，则进一步随机选择一个其他的副本。  多分片查询的流程 分布式查询与分布式写入类似，同样本着谁执行谁负责的原则，它会由接收SELECT查询的 Distributed 表，并负责串联起整个过程。首先它会将针对分布式表的 SQL 语句，按照分片数量将查询拆分成若干个针对本地表的子查询，然后向各个分片发起查询，最后再汇总各个分片的返回结果。\n1 2 3 4 5  --查询分布式表 SELECT*FROMdistributor_table--转换为查询本地表，并将该命令推送到各个分片节点上执行 SELECT*FROMlocal_table  如下图\n 查询各个分片数据：One 和 Remote 步骤是并行执行的，它们分别负责了本地和远端分片的查询动作。 合并返回结果：多个分片数据均查询返回后，在执行节点将所有数据union合并  使用 Global 优化分布式子查询 如果现在有一项查询需求，例如要求找到同时拥有两个仓库的用户，应该如何实现？对于这类交集查询的需求，可以使用IN子查询，此时你会面临两难的选择：IN查询的子句应该使用本地表还是分布式表？（使用JOIN面临的情形与IN类似）。\n使用本地表的问题（可能查询不到结果） 如果在IN查询中使用本地表时，如下列语句\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidIN(SELECTidFROMlocal_tableWHERErepo=200)  在实际执行时，分布式表在接收到查询后会将上述 SQL 替换成本地表的形式，再发送到每个分片进行执行，此时，每个分片上实际执行的是以下语句\n1  SELECTuniq(id)FROMlocal_tableWHERErepo=100ANDidIN(SELECTidFROMlocal_tableWHERErepo=200)  那么此时查询的最终结果就有可能是错误的，因为在单个分片上只保存了部分的数据，这就导致该 SQL 语句可能没有匹配到任何数据，如下图\n使用分布式表的问题（查询请求被放大 N^2 倍，N 为节点数量） 如果在 IN 查询中使用本地表时，如下列语句\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidIN(SELECTidFROMdistributed_tableWHERErepo=200)  对于此次查询，每个分片节点不仅需要查询本地表，还需要再次向其他的分片节点再次发起远端查询，如下图\n因此可以得出结论，在 IN 查询子句使用分布式表的时候，虽然查询的结果得到了保证，但是查询请求会被放大 N 的平方倍，其中 N 等于集群内分片节点的数量，假如集群内有 10 个分片节点，则在一次查询的过程中，会最终导致 100 次的查询请求，这显然是不可接受的。\n使用 GLOBAL 优化查询 为了解决查询放大的问题，我们可以使用 GLOBAL IN 或 GLOBAL JOIN 进行优化，下面就简单介绍一下 GLOBAL 的执行流程\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidGLOBALIN(SELECTidFROMdistributed_tableWHERErepo=200)  如上图，主要有以下五个步骤\n 将IN子句单独提出，发起了一次分布式查询。 将分布式表转 local 本地表后，分别在本地和远端分片执行查询。 将IN子句查询的结果进行汇总，并放入一张临时的内存表进行保存。 将内存表发送到远端分片节点。 将分布式表转为本地表后，开始执行完整的 SQL 语句，IN 子句直接使用临时内存表的数据。  在使用 GLOBAL 修饰符之后，ClickHouse 使用内存表临时保存了 IN 子句查询到的数据，并将其发送到远端分片节点，以此到达了数据共享的目的，从而避免了查询放大的问题。由于数据会在网络间分发，所以需要特别注意临时表的大小，IN 或者 JOIN 子句返回的数据不宜过大。如果表内存在重复数据，也可以事先在子句 SQL 中增加 DISTINCT 以实现去重。\n","date":"2022-05-23T18:27:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86distributed%E5%BC%95%E6%93%8E/","title":"ClickHouse 分布式原理：Distributed引擎"},{"content":"ReplicatedMergeTree引擎 ReplicatedMergeTree是MergeTree的派生引擎，它在MergeTree的基础上加入了分布式协同的能力，只有使用了ReplicatedMergeTree复制表系列引擎，才能应用副本的能力。或者用一种更为直接的方式理解，即使用ReplicatedMergeTree的数据表就是副本。\n在MergeTree中，一个数据分区由开始创建到全部完成，会历经两类存储区域。\n 内存：数据首先会被写入内存缓冲区。 本地磁盘：数据接着会被写入tmp临时目录分区，待全部完成后再将临时目录重命名为正式分区。  ReplicatedMergeTree在上述基础之上增加了ZooKeeper的部分，它会进一步在ZooKeeper内创建一系列的监听节点，并以此实现多个实例之间的通信。在整个通信过程中，ZooKeeper并不会涉及表数据的传输。\n特点 作为数据副本的主要实现载体，ReplicatedMergeTree在设计上有一些显著特点。\n  依赖ZooKeeper：在执行INSERT和ALTER查询的时候，ReplicatedMergeTree需要借助ZooKeeper的分布式协同能力，以实现多个副本之间的同步。但是在查询副本的时候，并不需要使用ZooKeeper。\n  表级别的副本：副本是在表级别定义的，所以每张表的副本配置都可以按照它的实际需求进行个性化定义，包括副本的数量，以及副本在集群内的分布位置等。\n  多主架构（Multi Master）：可以在任意一个副本上执行INSERT和ALTER查询，它们的效果是相同的。这些操作会借助ZooKeeper的协同能力被分发至每个副本以本地形式执行。\n  Block数据块：在执行INSERT命令写入数据时，会依据max_insert_block_size的大小（默认1048576行）将数据切分成若干个Block数据块。所以Block数据块是数据写入的基本单元，并且具有写入的原子性和唯一性。\n  原子性：在数据写入时，一个Block块内的数据要么全部写入成功，要么全部失败。\n  唯一性：在写入一个Block数据块的时候，会按照当前Block数据块的数据顺序、数据行和数据大小等指标，计算Hash信息摘要并记录在案。在此之后，如果某个待写入的Block数据块与先前已被写入的Block数据块拥有相同的Hash摘要（Block数据块内数据顺序、数据大小和数据行均相同），则该Block数据块会被忽略。\n  数据结构 ZooKeeper内的节点结构 ReplicatedMergeTree需要依靠ZooKeeper的事件监听机制以实现各个副本之间的协同。所以，在每张ReplicatedMergeTree表的创建过程中，它会以zk_path为根路径，在Zoo-Keeper中为这张表创建一组监听节点。按照作用的不同，监听节点可以大致分成如下几类：\n 元数据  /metadata：保存元数据信息，包括主键、分区键、采样表达式等。 /columns：保存列字段信息，包括列名称和数据类型。 /replicas：保存副本名称，对应设置参数中的replica_name。   判断标识  /leader_election：用于主副本的选举工作，主副本会主导MERGE和MUTATION操作（ALTER DELETE和ALTER UPDATE）。这些任务在主副本完成之后再借助ZooKeeper将消息事件分发至其他副本。 /blocks：记录Block数据块的Hash信息摘要，以及对应的partition_id。通过Hash摘要能够判断Block数据块是否重复；通过partition_id，则能够找到需要同步的数据分区。 /block_numbers：按照分区的写入顺序，以相同的顺序记录partition_id。各个副本在本地进行MERGE时，都会依照相同的block_numbers顺序进行。 /quorum：记录quorum的数量，当至少有quorum数量的副本写入成功后，整个写操作才算成功。quorum的数量由insert_quorum参数控制，默认值为0。   操作日志  /log：常规操作日志节点（INSERT、MERGE和DROP PARTITION），它是整个工作机制中最为重要的一环，保存了副本需要执行的任务指令。log使用了ZooKeeper的持久顺序型节点，每条指令的名称以log-为前缀递增，例如log-0000000000、log-0000000001等。每一个副本实例都会监听/log节点，当有新的指令加入时，它们会把指令加入副本各自的任务队列，并执行任务。 /mutations：MUTATION操作日志节点，作用与log日志类似，当执行ALERT DELETE和ALERTUPDATE查询时，操作指令会被添加到这个节点。mutations同样使用了ZooKeeper的持久顺序型节点，但是它的命名没有前缀，每条指令直接以递增数字的形式保存，例如0000000000、0000000001等。 /replicas/{replica_name}/*：每个副本各自的节点下的一组监听节点，用于指导副本在本地执行具体的任务指令，其中较为重要的节点有如下几个：  /queue：任务队列节点，用于执行具体的操作任务。当副本从/log或/mutations节点监听到操作指令时，会将执行任务添加至该节点下，并基于队列执行。 /log_pointer：log日志指针节点，记录了最后一次执行的log日志下标信息。 /mutation_pointer：mutations日志指针节点，记录了最后一次执行的mutations日志名称。      Entry日志对象的数据结构 ReplicatedMergeTree在ZooKeeper中有两组非常重要的父节点，那就是/log和/mutations。它们的作用犹如一座通信塔，是分发操作指令的信息通道，而发送指令的方式，则是为这些父节点添加子节点。所有的副本实例，都会监听父节点的变化，当有子节点被添加时，它们能实时感知。这些被添加的子节点在ClickHouse中被统一抽象为Entry对象，而具体实现则由LogEntry和MutationEntry对象承载，分别对应/log和/mutations节点\n LogEntry  source replica：发送这条Log指令的副本来源，对应replica_name。 type：操作指令类型，主要有get、merge和mutate三种，分别对应从远程副本下载分区、合并分区和MUTATION操作。 block_id：当前分区的BlockID，对应/blocks路径下子节点的名称。 partition_name：当前分区目录的名称。   MutationEntry  source replica：发送这条MUTATION指令的副本来源，对应replica_name。 commands：操作指令，主要有ALTER DELETE和ALTER UPDATE。 mutation_id：MUTATION操作的版本号。 partition_id：当前分区目录的ID。    副本协同的核心流程 副本协同的核心流程主要有INSERT、MERGE、MUTATION和ALTER四种，分别对应了数据写入、分区合并、数据修改和元数据修改。INSERT和ALTER是分布式执行的，借助ZooKeeper的事件通知机制，多个副本之间会自动进行有效协同，但是它们不会使用ZooKeeper存储任何分区数据。而其他操作并不支持分布式执行，包括SELECT、CREATE、DROP、RENAME和ATTACH。\n在下列例子中，使用ReplicatedMergeTree实现一张拥有1分片、1副本的数据表来分别执行INSERT、MERGE、MUTATION和ALTER操作，演示执行流程。\nINSERT 当需要在ReplicatedMergeTree中执行INSERT查询以写入数据时，即会进入INSERT核心流程，它的核心流程如下图所示\n 向副本A写入数据 由副本A推送Log日志 各个副本拉取Log日志 各个副本向远端副本发起下载请求  选择一个远端的其他副本作为数据的下载来源。远端副本的选择算法大致是这样的：  从/replicas节点拿到所有的副本节点。 遍历这些副本，选取其中一个。选取的副本需要拥有最大的log_pointer下标，并且/queue子节点数量最少。log_pointer下标最大，意味着该副本执行的日志最多，数据应该更加完整；而/queue最小，则意味着该副本目前的任务执行负担较小。     远端副本响应其它副本的数据下载 各个副本下载数据并完成本地写入  在INSERT的写入过程中，ZooKeeper不会进行任何实质性的数据传输。本着谁执行谁负责的原则，在这个案例中由CH5首先在本地写入了分区数据。之后，也由这个副本负责发送Log日志，通知其他副本下载数据。如果设置了insert_quorum并且insert_quorum\u0026gt;=2，则还会由该副本监控完成写入的副本数量。其他副本在接收到Log日志之后，会选择一个最合适的远端副本，点对点地下载分区数据。\nMERGE 当ReplicatedMergeTree触发分区合并动作时，即会进入这个部分的流程，它的核心流程如下图所示\n无论MERGE操作从哪个副本发起，其合并计划都会交由主副本来制定。\n 创建远程连接，尝试与主副本通信 主副本接收通信 由主副本制定MERGE计划并推送Log日志 各个副本分别拉取Log日志 各个副本分别在本地执行MERGE  可以看到，在MERGE的合并过程中，ZooKeeper也不会进行任何实质性的数据传输，所有的合并操作，最终都是由各个副本在本地完成的。而无论合并动作在哪个副本被触发，都会首先被转交至主副本，再由主副本负责合并计划的制定、消息日志的推送以及对日志接收情况的监控。\nMUTATION 当对ReplicatedMergeTree执行ALTER DELETE或者ALTER UPDATE操作的时候（ClickHouse把delete和update操作也加入到了alter table的范畴中，它并不支持裸的delete或者update操作），即会进入MUTATION部分的逻辑\n与MERGE类似，无论MUTATION操作从哪个副本发起，首先都会由主副本进行响应。\n 推送MUTATION日志 所有副本实例各自监听MUTATION日志 由主副本实例响应MUTATION日志并推送Log日志 各个副本实例分别拉取Log日志 各个副本实例分别在本地执行MUTATION  在MUTATION的整个执行过程中，ZooKeeper同样不会进行任何实质性的数据传输。所有的MUTATION操作，最终都是由各个副本在本地完成的。而MUTATION操作是经过/mutations节点实现分发的。CH6负责了消息的推送。但是无论MUTATION动作从哪个副本被触发，之后都会被转交至主副本，再由主副本负责推送Log日志，以通知各个副本执行最终的MUTATION逻辑。同时也由主副本对日志接收的情况实行监控。\nALTER 当对ReplicatedMergeTree执行ALTER操作进行元数据修改的时候，即会进入ALTER部分的逻辑，例如增加、删除表字段等，核心流程如下图\nALTER的流程与前几个相比简单很多，其执行过程中并不会涉及/log日志的分发，整个流程大致分成3个步骤\n 修改共享元数据 监听共享元数据变更并各自执行本地修改 确认所有副本完成修改  在ALTER整个的执行过程中，ZooKeeper不会进行任何实质性的数据传输。所有的ALTER操作，最终都是由各个副本在本地完成的。本着谁执行谁负责的原则，在这个案例中由CH6负责对共享元数据的修改以及对各个副本修改进度的监控。\n","date":"2022-05-23T18:19:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%89%AF%E6%9C%AC%E5%8D%8F%E5%90%8C%E5%8E%9F%E7%90%86replicatedmergetree%E5%BC%95%E6%93%8E/","title":"ClickHouse 副本协同原理：ReplicatedMergeTree引擎"},{"content":"MergeTree 引擎 存储结构  partition：分区目录，余下各类数据文件（primary.idx、[Column].mrk、[Column]. bin等）都是以分区目录的形式被组织存放的，属于相同分区的数据，最终会被合并到同一个分区目录，而不同分区的数据，永远不会被合并在一起。 checksums：校验文件，使用二进制格式存储。它保存了余下各类文件(primary. idx、count.txt等)的size大小及size的哈希值，用于快速校验文件的完整性和正确性。 columns.txt：列信息文件，使用明文格式存储，用于保存此数据分区下的列字段信息。 count.txt：计数文件，使用明文格式存储，用于记录当前数据分区目录下数据的总行数。 primary.idx：一级索引文件，使用二进制格式存储。用于存放稀疏索引，一张MergeTree表只能声明一次一级索引。借助稀疏索引，在数据查询的时能够排除主键条件范围之外的数据文件，从而有效减少数据扫描范围，加速查询速度。 [Column].bin：数据文件，使用压缩格式存储，用于存储某一列的数据。由于MergeTree采用列式存储，所以每一个列字段都拥有独立的.bin数据文件，并以列字段名称命名。 [Column].mrk：使用二进制格式存储。标记文件中保存了.bin文件中数据的偏移量信息。标记文件与稀疏索引对齐，又与.bin文件一一对应，所以MergeTree通过标记文件建立了primary.idx稀疏索引与.bin数据文件之间的映射关系。即首先通过稀疏索引（primary.idx）找到对应数据的偏移量信息（.mrk），再通过偏移量直接从.bin文件中读取数据。由于.mrk标记文件与.bin文件一一对应，所以MergeTree中的每个列字段都会拥有与其对应的.mrk标记文件 [Column].mrk2：如果使用了自适应大小的索引间隔，则标记文件会以．mrk2命名。它的工作原理和作用与．mrk标记文件相同。 partition.dat与minmax_[Column].idx：如果使用了分区键，例如PARTITION BY EventTime，则会额外生成partition.dat与minmax索引文件，它们均使用二进制格式存储。partition.dat用于保存当前分区下分区表达式最终生成的值；而minmax索引用于记录当前分区下分区字段对应原始数据的最小和最大值。 skp_idx_[Column].idx与skp_idx_[Column].mrk：如果在建表语句中声明了二级索引，则会额外生成相应的二级索引与标记文件，它们同样也使用二进制存储。二级索引在ClickHouse中又称跳数索引。  一级索引 稀疏索引 当我们定义主键之后，MergeTree会依据index_granularity间隔（默认8192行），为数据表生成一级索引并保存至primary.idx文件内，索引数据按照主键排序。相比使用主键定义，更为常见的简化形式是通过ORDER BY指代主键。在此种情形下，主键与ORDER BY定义相同，所以索引（primary.idx）和数据（.bin）会按照完全相同的规则排序。\n一级索引底层采用了稀疏索引来实现，从下图我们可以看出它和稠密索引的区别。\n对于稠密索引而言，每一行索引标记都会对应到具体的一行记录上。而在稀疏索引中，每一行索引标记对应的一大段数据，而不是具体的一行（他们之间的区别就有点类似mysql中innodb的聚集索引与非聚集索引）。\n稀疏索引的优势是显而易见的，它只需要使用少量的索引标记就能够记录大量数据的区间位置信息，并且数据量越大优势愈发明显。例如我们使用默认的索引粒度（8192）时，MergeTree只需要12208行索引标记就能为1亿行数据记录提供索引。由于稀疏索引占用空间小，所以primary.idx内的索引数据能够常驻内存，取用速度自然极快。\n索引粒度index_granularity 索引粒度就如同标尺一般，会丈量整个数据的长度，并依照刻度对数据进行标注，最终将数据标记成多个间隔的小段。数据以index_granularity的粒度(老版本默认8192，新版本实现了自适应粒度)被标记成多个小的区间，其中每个区间最多8192行数据，MergeTree使用MarkRange表示一个具体的区间，并通过start和end表示其具体的范围。\n如下图所示。\nindex_granularity的命名虽然取了索引二字，但它不单只作用于一级索引(.idx)，同时也会影响数据标记(.mrk)和数据文件(.bin)。因为仅有一级索引自身是无法完成查询工作的，它需要借助数据标记才能定位数据，所以一级索引和数据标记的间隔粒度相同(同为index_granularity行)，彼此对齐。而数据文件也会依照index_granularity的间隔粒度生成压缩数据块。\n索引的查询过程 索引查询其实就是两个数值区间的交集判断。其中，一个区间是由基于主键的查询条件转换而来的条件区间；而另一个区间是刚才所讲述的与MarkRange对应的数值区间。\n整个索引的查询过程可以分为三大步骤\n  生成查询条件区间： 将查询条件转换为条件区间。即便是单个值的查询条件，也会被转换成区间的形式。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  --举例-- WHEREID=\u0026#39;A000\u0026#39;=[\u0026#39;A000\u0026#39;,\u0026#39;A000\u0026#39;]WHEREID\u0026gt;\u0026#39;A000\u0026#39;=(\u0026#39;A000\u0026#39;,\u0026#39;+inf\u0026#39;)WHEREID\u0026lt;\u0026#39;A000\u0026#39;=(\u0026#39;-inf\u0026#39;,\u0026#39;A000\u0026#39;)WHEREIDLIKE\u0026#39;A000%\u0026#39;=[\u0026#39;A000\u0026#39;,\u0026#39;A001\u0026#39;)      递归交集判断： 以递归的形式，依次对MarkRange的数值区间与条件区间做交集判断。从最大的区间[A000 , +inf)开始。\n 如果不存在交集，则直接通过剪枝算法优化此整段MarkRange 如果存在交集，且MarkRange步长大于N，则将这个区间进一步拆分为N个子区间，并重复此规则，继续做递归交集判断（N由merge_tree_coarse_index_granularity指定，默认值为8） 如果存在交集，且MarkRange不可再分解，则记录MarkRange并返回    合并MarkRange区间： 将最终匹配的MarkRange聚在一起，合并它们的范围。\n  MergeTree通过递归的形式持续向下拆分区间，最终将MarkRange定位到最细的粒度，以帮助在后续读取数据的时候，能够最小化扫描数据的范围。\n联合主键 当我们以需要以多个字段为主键时，此时数据的查询和存储就涉及到另外一种规则。\n例如以 (CounterID, Date) 以主键，片段中数据首先按 CounterID 排序，具有相同 CounterID 的部分按 Date 排序。排序好的索引的图示会是下面这样：\n1 2 3 4 5 6  全部数据:[-------------------------------------------------------------------------] CounterID:[aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]Date:[1111111222222233331233211111222222333211111112122222223111112223311122333]标记:|||||||||||a,1a,2a,3b,3e,2e,3g,1h,2i,1i,3l,3标记号:012345678910  如果指定查询如下：\n CounterID in ('a', 'h')，服务器会读取标记号在 [0, 3) 和 [6, 8) 区间中的数据。 CounterID IN ('a', 'h') AND Date = 3，服务器会读取标记号在 [1, 3) 和 [7, 8) 区间中的数据。 Date = 3，服务器会读取标记号在 [1, 10] 区间中的数据。  上面例子可以看出使用索引通常会比全表描述要高效。\n  稀疏索引会引起额外的数据读取。当读取主键单个区间范围的数据时，每个数据块中最多会多读 index_granularity * 2 行额外的数据。\n  稀疏索引使得你可以处理极大量的行，因为大多数情况下，这些索引常驻与内存（RAM）中。\n  从上面可以看出，ClickHouse的联合主键在某种程度上与我们熟知的最左前缀规则有点类似，通常在以下几种场景下我们才会考虑使用联合索引\n 查询会使用 b 列作为条件 很长的数据范围（ index_granularity 的数倍）里 a 都是相同的值，并且这样的情况很普遍。换言之，就是加入另一列后，可以让你的查询略过很长的数据范围。 数据量大，需要改善数据压缩（以主键排序片段数据，数据的一致性越高，压缩越好）  长的主键会对插入性能和内存消耗有负面影响，但主键中额外的列并不影响 SELECT 查询的性能。\n二级索引 除了一级索引之外，MergeTree同样支持二级索引。二级索引又称跳数索引，由数据的聚合信息构建而成。根据索引类型的不同，其聚合信息的内容也不同。跳数索引的目的与一级索引一样，也是帮助查询时减少数据扫描的范围。\n==（二级索引目前还处于测试阶段，官方不建议大量使用）==\n跳数索引 目前，MergeTree共支持4种跳数索引，分别是minmax（最值）、set（集合行数）、ngrambf_v1（N-Gram布隆过滤器）和tokenbf_v1（Token布隆过滤器）。一张数据表支持同时声明多个跳数索引。\n  minmax（最值索引）：minmax索引记录了一段数据内的最小和最大极值，其索引的作用类似分区目录的minmax索引，能够快速跳过无用的数据区间。\n  1  示例：INDEX[index_name][column]TYPEminmaxGRANULARITY[GRANULARITYSIZE]      set（集合行数索引）：set索引直接记录了声明字段或表达式的不重复值，用于检测数据块是否满足WHERE条件。\n  1 2 3  示例：INDEX[index_name][column]TYPEset(max_rows)GRANULARITY[index_granularity]-- max_rows是一个阈值，表示在一个index_granularity内，索引最多记录的数据行数。(如果max_rows=0，则表示无限制)       ngrambf_v1（N-Gram布隆过滤器）：ngrambf_v1索引记录的是指定长度的数据短语的布隆表过滤器，只支持String和FixedString数据类型，同时只能够提升in、notIn、like、equals和notEquals查询的性能。\n  1 2 3 4 5 6 7 8  示例：INDEX[index_name][column]TYPEngrambf_v1(n,size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed)GRANULARITY[index_granularity]/* n:token长度，依据n的长度将数据切割为token短语。 size_of_bloom_filter_in_bytes：布隆过滤器的大小。 number_of_hash_functions：布隆过滤器中使用Hash函数的个数。 random_seed: Hash函数的随机种子。 */    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  布隆过滤器可能会包含不符合条件的匹配，所以ngrambf_v1,tokenbf_v1和bloom_filter索引不能用于负向的函数，例如：--可以用来优化的场景 sLIKE\u0026#39;%test%\u0026#39;NOTsNOTLIKE\u0026#39;%test%\u0026#39;s=1NOTs!=1startsWith(s,\u0026#39;test\u0026#39;)i--不能用来优化的场景 NOTsLIKE\u0026#39;%test%\u0026#39;sNOTLIKE\u0026#39;%test%\u0026#39;NOTs=1s!=1NOTstartsWith(s,\u0026#39;test\u0026#39;)      tokenbf_v1（Token布隆过滤器）：tokenbf_v1索引是ngrambf_v1的变种，同样也是一种布隆过滤器索引。tokenbf_v1除了短语token的处理方法外，其他与ngrambf_v1是完全一样的。tokenbf_v1会自动按照非字符的、数字的字符串分割token。\n  1  示例：INDEXdIDTYPEtokenbf_v1(size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed)      granularity 对于跳数索引而言，index_granularity定义了数据的粒度，而granularity定义了聚合信息汇总的粒度。换言之，granularity定义了一行跳数索引能够跳过多少个index_granularity区间的数据。\n作用规则如下：首先，按照index_granularity粒度间隔将数据划分成n段，总共有[0 , n-1]个区间（n = total_rows /index_granularity，向上取整）。接着，根据索引定义时声明的表达式，从0区间开始，依次按index_granularity粒度从数据中获取聚合信息，每次向前移动1步(n+1)，聚合信息逐步累加。最后，当移动granularity次区间时，则汇总并生成一行跳数索引数据。\n以minmax索引为例，假设index_granularity=8192且granularity=3，则数据会按照index_granularity划分为n等份，MergeTree从第0段分区开始，依次获取聚合信息。当获取到第3个分区时（granularity=3），则汇总并会生成第一行minmax索引（前3段minmax极值汇总后取值为[1 , 9]），如下图\n数据标记 如果把MergeTree比作一本书，primary.idx一级索引好比这本书的一级章节目录，.bin文件中的数据好比这本书中的文字，那么数据标记(.mrk)就好比书签一样，会为一级章节目录和具体的文字之间建立关联。\n对于数据标记而言，它记录了两点重要信息：\n 一级章节对应的页码信息。 一段文字在某一页中的起始位置信息。  这样一来，通过数据标记就能够很快地从一本书中立即翻到关注内容所在的那一页，并知道从第几行开始阅读。\n标记数据与一级索引数据不同，它并不能常驻内存，而是使用LRU（最近最少使用）缓存策略加快其取用速度。\n生成规则 从上图可以看出，数据标记和索引区间是对齐的，均按照index_granularity的粒度间隔。如此一来，只需简单通过索引区间的下标编号就可以直接找到对应的数据标记。\n为了能够与数据衔接，数据标记文件也与.bin文件一一对应。即每一个列字段[Column].bin文件都有一个与之对应的[Column].mrk数据标记文件，用于记录数据在.bin文件中的偏移量信息。同时，.mrk包含了.bin压缩和解压缩这两种不同状态的偏移量，如下图\n工作方式 MergeTree在读取数据时，必须通过标记数据的位置信息才能够找到所需要的数据。整个查找过程大致可以分为读取压缩数据块和读取数据两个步骤。\n对于下图来说，表的index_granularity粒度为8192，所以一个索引片段的数据大小恰好是8192B。按照压缩数据块的生成规则，如果单个批次数据小于64KB，则继续获取下一批数据，直至累积到size\u0026gt;=64KB时，生成下一个压缩数据块。因此在JavaEnable的标记文件中，每8行标记数据对应1个压缩数据块（1B * 8192 = 8192B, 64KB = 65536B, 65536 / 8192 =8）。\n从图能够看到，其左侧的标记数据中，8行数据的压缩文件偏移量都是相同的，因为这8行标记都指向了同一个压缩数据块。而在这8行的标记数据中，它们的解压缩数据块中的偏移量，则依次按照8192B（每行数据1B，每一个批次8192行数据）累加，当累加达到65536(64KB)时则置0。因为根据规则，此时会生成下一个压缩数据块。\n  读取压缩数据块： 在查询某一列数据时，MergeTree无须一次性加载整个.bin文件，而是可以根据需要，只加载特定的压缩数据块。而这项特性需要借助标记文件中所保存的压缩文件中的偏移量。\n  读取数据： 在读取解压后的数据时，MergeTree并不需要一次性扫描整段解压数据，它可以根据需要，以index_granularity的粒度加载特定的一小段。为了实现这项特性，需要借助标记文件中保存的解压数据块中的偏移量。\n  数据标记与压缩数据块的对应关系 由于压缩数据块的划分，与一个间隔index_granularity内的数据大小相关，每个压缩数据块的体积都被严格控制在64KB～1MB。而一个间隔index_granularity的数据，又只会产生一行数据标记。那么根据一个间隔内数据的实际字节大小，数据标记和压缩数据块之间会产生三种不同的对应关系。\n 一对一  一个数据标记对应一个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size大于等于64KB且小于等于1MB时，会出现这种对应关系。    一对多  一个数据标记对应多个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size直接大于1MB时，会出现这种对应关系。    多对一  多个数据标记对应一个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size小于64KB时，会出现这种对应关系。     工作流程 存储流程 数据的存储流程主要有以下几个步骤\n  首先生成分区目录，伴随着每一批数据的写入，都会生成一个新的分区目录。\n  在后续的某一时刻，属于相同分区的目录会依照规则合并到一起\n  接着，按照index_granularity索引粒度，会分别生成primary.idx一级索引（如果声明了二级索引，还会创建二级索引文件）、每一个列字段的．mrk数据标记和．bin压缩数据文件。\n  查询流程 数据查询的本质，可以看作一个不断减小数据范围的过程。在最理想的情况下，MergeTree首先可以依次借助分区索引、一级索引和二级索引，将数据扫描范围缩至最小。然后再借助数据标记，将需要解压与计算的数据范围缩至最小。\n如果一条查询语句没有指定任何WHERE条件，或是指定了WHERE条件，但条件没有匹配到任何索引（分区索引、一级索引和二级索引），那么MergeTree就不能预先减小数据范围。在后续进行数据查询时，它会扫描所有分区目录，以及目录内索引段的最大区间。虽然不能减少数据范围，但是MergeTree仍然能够借助数据标记，以多线程的形式同时读取多个压缩数据块，以提升性能。\n","date":"2022-05-23T18:18:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86mergetree%E5%BC%95%E6%93%8E/","title":"ClickHouse 数据存储原理：MergeTree引擎"},{"content":"ClickHouse 基本概念 ClickHouse 是一个用于联机分析（OLAP）的列式数据库管理系统（DBMS）。\nOLAP 什么是 OLAP? OLAP 名为联机分析，又可以称为多维分析，是由关系型数据库之父埃德加·科德（EdgarFrank Codd）于 1993 年提出的概念。顾名思义，它指的是通过多种不同的维度审视数据，进行深层次分析。\n维度可以看作观察数据的一种视角，例如人类能看到的世界是三维的，它包含长、宽、高三个维度。直接一点理解，维度就好比是一张数据表的字段，而多维分析则是基于这些字段进行聚合查询。\n如上图，多维分析包含以下几种操作：\n 下钻： 从高层次向低层次明细数据穿透，例如从省下钻到市。 上卷： 和下钻相反，从低层次向高层次汇聚，例如从市汇聚成省。 切片： 观察立方体的一层，将一个或多个维度设为单个固定值，然后观察剩余的维度，例如将商品维度固定为足球。 切块： 与切片类似，只是将单个固定值变成多个值。例如将商品维度固定成足球、篮球。 旋转： 旋转立方体的一面，如果要将数据映射到一张二维表，那么就要进行旋转，这就等同于行列置换。  OLAP 与 OLTP OLTP（on-line transaction processing）翻译为联机事务处理， OLAP（On-Line Analytical Processing）翻译为联机分析处理。\n  从字面上来看 OLTP 是做事务处理，OLAP 是做分析处理。\n  从对数据库操作来看，OLTP 主要是对数据的增删改，OLAP 是对数据的查询。\n  因为 OLTP 所产生的业务数据分散在不同的业务系统中，而 OLAP 往往需要将不同的业务数据集中到一起进行统一综合的分析，这时候就需要根据业务分析需求做对应的数据清洗后存储在数据仓库中，然后由数据仓库来统一提供 OLAP 分析。\n  OLTP 是数据库的应用，OLAP 是数据仓库的应用\n  下面用一张图来简要对比。\n列式存储 列式存储与行式存储 在传统的行式数据库系统中，处于同一行中的数据总是被物理的存储在一起，存储方式如下图：\n在列式数据库系统中，来自不同列的值被单独存储，来自同一列的数据被存储在一起，数据按如下的顺序存储：\n不同的数据存储方式适用不同的业务场景，而对于 OLAP 来说，列式存储是最适合的选择。\n列式存储与 OLAP 为什么列式数据库更适合于 OLAP 场景呢？下面这两张图就可以给你答案\n 行式数据库     列式数据库      下面分别从两个 I/O 和 CPU 两个角度来分析为什么他们有如此之大的差别\n I/O  针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。 由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了 I/O 的体积。 由于 I/O 的降低，这将帮助更多的数据被系统缓存,进一步降低了数据传输的成本。   CPU  由于执行一个查询需要处理大量的行，因此在整个向量上执行所有操作将比在每一行上执行所有操作更加高效。同时这将有助于实现一个几乎没有调用成本的查询引擎。如果你不这样做，使用任何一个机械硬盘，查询引擎都不可避免的停止 CPU 进行等待。所以，在数据按列存储并且按列执行是很有意义的。    列式存储与数据压缩 如果你想让查询变得更快，最简单且有效的方法是减少数据扫描范围和数据传输时的大小，而列式存储和数据压缩就可以帮助我们实现上述两点。\n列式存储和数据压缩通常是伴生的。数据按列存储。而具体到每个列字段，数据也是独立存储的，每个列字段都拥有一个与之对应的 .bin 数据文件，相同类型的数据放在同一个文件中，对压缩更加友好。数据默认使用 LZ4 算法压缩，在 Yandex.Metrica 的生产环境中，数据总体的压缩比可以达到 8:1（未压缩前 17PB，压缩后 2PB）。\n核心特点 完备的 DBMS 功能 ClickHouse 拥有完备的管理功能，所以它称得上是一个 DBMS（Database Management System，数据库管理系统），而不仅是一个数据库。作为一个 DBMS，它具备了一些基本功能，\n如下所示。\n DDL（数据定义语言）：可以动态地创建、修改或删除数据库、表和视图，而无须重启服务。 DML（数据操作语言）：可以动态查询、插入、修改或删除数据。 权限控制：可以按照用户粒度设置数据库或者表的操作权限，保障数据的安全性。 数据备份与恢复：提供了数据备份导出与导入恢复机制，满足生产环境的要求。 分布式管理：提供集群模式，能够自动管理多个数据库节点。  关系模型与 SQL 查询 相比 HBase 和 Redis 这类 NoSQL 数据库，ClickHouse 使用关系模型描述数据并提供了传统数据库的概念（数据库、表、视图和函数等）。与此同时，ClickHouse 完全使用 SQL 作为查询语言（支持 GROUP BY、ORDER BY、JOIN、IN 等大部分标准 SQL），这使得它平易近人，容易理解和学习。\n向量化表引擎 向量化执行，可以简单地看作从硬件的角度上消除程序中循环的优化。\n为了实现向量化执行，需要利用 CPU 的 SIMD 指令。SIMD 的全称是 Single Instruction MultipleData，即用单条指令操作多条数据。现代计算机系统概念中，它是通过数据并行以提高性能的一种实现方式，它的原理是在 CPU 寄存器层面实现数据的并行操作。例如有 8 个 32 位整形数据都需要进行移位运行，则由一条对 32 位整形数据进行移位的指令重复执行 8 次完成。SIMD 引入了一组大容量的寄存器，一个寄存器包含 8 * 32 位，可以将这 8 个数据按次序同时放到一个寄存器。同时，CPU 新增了处理这种 8 * 32 位寄存器的指令，可以在一个指令周期内完成 8 个数据的位移运算。（本质就是将每次处理的数据从一条变为一批）\n多样化的表引擎 与 MySQL 类似，ClickHouse 也将存储部分进行了抽象，把存储引擎作为一层独立的接口。ClickHouse 共拥有合并树、内存、文件、接口和其他 6 大类 20 多种表引擎。其中每一种表引擎都有着各自的特点，用户可以根据实际业务场景的要求，选择合适的表引擎使用。\n多主架构 ClickHouse 则采用 Multi-Master多主架构，集群中的每个节点角色对等，客户端访问任意一个节点都能得到相同的效果。这种多主的架构有许多优势，例如对等的角色使系统架构变得更加简单，不用再区分主控节点、数据节点和计算节点，集群中的所有节点功能相同。所以它天然规避了单点故障的问题，非常适合用于多数据中心、异地多活的场景。\n多线程与分布式 在各服务器之间，通过网络传输数据的成本是高昂的，所以相比移动数据，更为聪明的做法是预先将数据分布到各台服务器，将数据的计算查询直接下推到数据所在的服务器。ClickHouse 在数据存取方面，既支持分区（纵向扩展，利用多线程原理），也支持分片（横向扩展，利用分布式原理），可以说是将多线程和分布式的技术应用到了极致。\n分片与分布式查询 数据分片是将数据进行横向切分，这是一种在面对海量数据的场景下，解决存储和查询瓶颈的有效手段，是一种分治思想的体现。ClickHouse 支持分片，而分片则依赖集群。每个集群由 1 到多个分片组成，而每个分片则对应了 ClickHouse 的 1 个服务节点。分片的数量上限取决于节点数量（1 个分片只能对应 1 个服务节点）。\nClickHouse 并不像其他分布式系统那样，拥有高度自动化的分片功能。ClickHouse 提供了**本地表（Local Table）与分布式表（Distributed Table）**的概念。一张本地表等同于一份数据的分片，而分布式表本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。借助分布式表，能够代理访问多个数据分片，从而实现分布式查询。\n应用场景 擅长的场景  绝大多数是读请求 数据以相当大的批次（\u0026gt; 1000 行）更新，而不是单行更新;或者根本没有更新。 已添加到数据库的数据不能修改。 对于读取，从数据库中提取相当多的行，但只提取列的一小部分。 宽表，即每个表包含着大量的列。 查询相对较少（通常每台服务器每秒查询数百次或更少）。 对于简单查询，允许延迟大约 50 毫秒。 列中的数据相对较小：数字和短字符串（例如，每个 URL 60 个字节）。 处理单个查询时需要高吞吐量（每台服务器每秒可达数十亿行）。 事务不是必须的。 对数据一致性要求低。 每个查询有一个大表。除了他以外，其他的都很小。 查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的 RAM 中。  不擅长的场景  OLTP 事务性操作（不支持事务，不支持真正的更新/删除） 不擅长根据主键按行粒度进行查询（如 select * from table where user_id in (xxx, xxx, xxx, ...)） 不擅长存储和查询 blob 或者大量文本类数据（按列存储） 不擅长执行有大量 join 的查询（Distributed 引擎局限） 不支持高并发，官方建议 QPS \u0026lt;= 100  Clickhouse 为什么会这么快？ 首先亮出官方的测试报告：Clickhouse 性能对比报告\n所有用于对比的数据库都使用了相同配置的服务器，在单个节点的情况下，对一张拥有 133 个字段的数据表分别在 1000 万、1 亿和 10 亿这三种数据体量下执行基准测试，基准测试的范围涵盖 43 项 SQL 查询。\n市面上有很多与 Clickhouse 采用了同样技术(如列式存储、向量化引擎等)的数据库，但为什么 ClickHouse 的性能能够将其他数据库远远甩在身后呢？这主要依赖于下面几个方面\n 着眼硬件，先想后做  ClickHouse 会在内存中进行 GROUP BY，并且使用 HashTable 装载数据。 ClickHouse 非常在意 CPU L3 级别的缓存，因为一次 L3 的缓存失效会带来 70 ～ 100ns 的延迟。这意味着在单核CPU上，它会浪费 4000 万次/秒的运算；而在一个 32 线程的 CPU 上，则可能会浪费 5 亿次/秒的运算。   算法在前，抽象在后  对于常量，使用 Volnitsky 算法； 对于非常量，使用 CPU 的向量化执行 SIMD（用于文本转换、数据过滤、数据解压和 JSON 转换等），暴力优化； 正则匹配使用 re2 和 hyperscan 算法。性能是算法选择的首要考量指标。   勇于尝鲜，不行就换  除了字符串之外，其余的场景也与它类似，ClickHouse 会使用最合适、最快的算法。如果世面上出现了号称性能强大的新算法，ClickHouse 团队会立即将其纳入并进行验证。如果效果不错，就保留使用；如果性能不尽人意，就将其抛弃。   特定场景，特殊优化  针对同一个场景的不同状况，选择使用不同的实现方式，尽可能将性能最大化。 例如去重计数 uniqCombined 函数，会根据数据量的不同选择不同的算法：当数据量较小的时候，会选择 Array 保存；当数据量中等的时候，会选择 HashSet；而当数据量很大的时候，则使用 HyperLogLog 算法。 针对不同的场景，Clickhouse 提供了 MergeTree 引擎家族，如 MergeTree、ReplacingMergeTree、SummingMergeTree、AggregatingMergeTree、CollapsingMergeTree和VersionedCollapsingMergeTree 等。   持续测试，持续改进  由于 Yandex 的天然优势，ClickHouse 经常会使用真实的数据进行测试，这一点很好地保证了测试场景的真实性。 ClickHouse 差不多每个月都能发布一个版本，正因为拥有这样的发版频率，ClickHouse 才能够快速迭代、快速改进。    ClickHouse 的架构 目前 ClickHouse 公开的资料相对匮乏，比如在架构设计层面就很难找到完整的资料，甚至连一张整体的架构图都没有，根据官网提供的信息，我们能够得出一个大概的架构，如下图\n  Parser： Parser 分析器可以将一条 SQL 语句以递归下降的方法解析成 AST 语法树的形式。不同的 SQL 语句，会经由不同的 Parser 实现类解析。\n  Interpreter：Interpreter 解释器的作用就像 Service 服务层一样，起到串联整个查询过程的作用，它会根据解释器的类型，聚合它所需要的资源。首先它会解析AST对象；然后执行“业务逻辑”（例如分支判断、设置参数、调用接口等）；最终返回IBlock对象，以线程的形式建立起一个查询执行管道。\n  Tables：Tables由 IStorage 接口表示。该接口的不同实现对应不同的表引擎。比如 StorageMergeTree、StorageMemory 等。这些类的实例就是表。\n IStorage 接口定义了DDL（如 ALTER、RENAME、OPTIMIZE 和 DROP 等）、read 和 write 方法，它们分别负责数据的定义、查询与写入。在数据查询时，IStorage 负责根据 AST 查询语句的指示要求，返回指定列的原始数据。 后续对数据的进一步加工、计算和过滤，则会统一交由Interpreter解释器对象处理。对Table发起的一次操作通常都会经历这样的过程，接收AST查询语句，根据AST返回指定列的数据，之后再将数据交由Interpreter做进一步处理。    Block与Block Streams：ClickHouse 内部的数据操作是面向 Block 对象进行的，并且采用了流的形式。\n Block：虽然 Column 和 Filed 组成了数据的基本映射单元，但对应到实际操作，它们还缺少了一些必要的信息，比如数据的类型及列的名称。于是 ClickHouse 设计了 Block对象，Block 对象可以看作数据表的子集。Block 对象的本质是由数据对象、数据类型和列名称组成的三元组，即 Column、DataType 及列名称字符串。Column 提供了数据的读取能力，而DataType知道如何正反序列化，所以 Block 在这些对象的基础之上实现了进一步的抽象和封装，从而简化了整个使用的过程，仅通过Block对象就能完成一系列的数据操作。在具体的实现过程中，Block 并没有直接聚合Column和DataType对象，而是通过ColumnWithTypeAndName对象进行间接引用。 Block Streams：Block Streams 用于处理数据。我们可以使用 Block Streams 从某个地方读取数据，执行数据转换，或将数据写到某个地方。IBlockInputStream 具有 read 方法，其能够在数据可用时获取下一个块。IBlockOutputStream 具有 write 方法，其能够将块写到某处。    Functions：ClickHouse 主要提供两类函数——普通函数和聚合函数。\n Function：普通函数由IFunction接口定义，其是没有状态的，函数效果作用于每行数据之上。当然，在函数具体执行的过程中，并不会一行一行地运算，而是采用向量化的方式直接作用于一整列数据。 AggregateFunction：聚合函数由IAggregateFunction接口定义，相比无状态的普通函数，聚合函数是有状态的，并且聚合函数的状态支持序列化与反序列化，所以能够在分布式节点之间进行传输，以实现增量计算。    DataType：数据的序列化和反序列化工作由 DataType 负责。根据不同的数据类型，IDataType 接口会有不同的实现类。DataType 虽然会对数据进行正反序列化，但是它不会直接和内存或者磁盘做交互，而是转交给 Column 和 Filed 处理。\n  Column 与 Field：Column 和 Field 是 ClickHouse 数据最基础的映射单元。\n Column：内存中的一列数据由一个 Column 对象表示。Column 对象分为接口和实现两个部分，在IColumn接口对象中，定义了对数据进行各种关系运算的方法，例如插入数据的insertRangeFrom和insertFrom方法、用于分页的cut，以及用于过滤的filter方法等。而这些方法的具体实现对象则根据数据类型的不同，由相应的对象实现。 Field：在大多数场合，ClickHouse 都会以整列的方式操作数据，但凡事也有例外。如果需要操作单个具体的数值（也就是单列中的一行数据），则需要使用 Field 对象，Field 对象代表一个单值。与 Column 对象的泛化设计思路不同，Field 对象使用了聚合的设计模式。在 Field 对象内部聚合了 Null、UInt64、String 和 Array 等 13 种数据类型及相应的处理逻辑。    ","date":"2022-05-23T18:16:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ClickHouse 基本概念"}]