[{"content":"微调 条款 41：如果参数可拷⻉并且移动操作开销低，考虑直接按值传递   对于可拷贝、移动开销低，且一定会被拷贝的形参而言，按值传递的效率基本和按引用传递一样，而且可能生成更少的目标代码。\n  当我们在编写构造函数时，考虑到对左值和右值需要区分对待（拷贝语义和移动语义），通常采用下面三种方法进行编写：\n  针对左值和右值分别重载\n1 2 3 4 5 6 7 8 9 10 11  class Widget { //方法1：对左值和右值重载 public: void addName(const std::string\u0026amp; newName) { names.push_back(newName); } // rvalues  void addName(std::string\u0026amp;\u0026amp; newName) { names.push_back(std::move(newName)); } … private: std::vector\u0026lt;std::string\u0026gt; names; };   优点：\n 左值对应一次拷贝构造，右值对应一次移动构造。  缺点：\n 代码冗余，同一份代码需要编写两个不同版本。    万能引用模板\n1 2 3 4 5 6 7  class Widget { //方法2：使用通用引用 public: template\u0026lt;typename T\u0026gt; void addName(T\u0026amp;\u0026amp; newName) { names.push_back(std::forward\u0026lt;T\u0026gt;(newName)); } … };   优点：\n 性能高，左值对应一次拷贝构造，右值对应一次移动构造。 代码量少，简洁。  缺点：\n 作为模板，声明和实现必须都置于头文件中，且还可能根据不同的模板类型实例化出不同的模板函数，导致代码增大。 有些类型不能以万能引用传参，如果传入了这些类型则会导致报错。    按值传递\n1 2 3 4 5 6  class Widget { //方法3：传值 public: void addName(std::string newName) { names.push_back(std::move(newName)); } … };   缺点：\n 性能低，无论传入的是左值还是右值，首先都需要对形参 newName 进行一次拷贝/移动构造，接着将构造出的 newName 作为右值传入容器中，对应一次移动构造。对于左值而言需要一次拷贝构造和一次移动构造，而对于右值而言则是两次移动构造。比起前两个方法，多了一次移动构造操作。 可能导致切片问题，这个下面会具体介绍。  优点：\n 只需要编写单个函数，且没有万能模板的一些隐患。 实现简单，代码简洁。    我们需要根据具体需求场景进行选择，当对性能要求高时，可以选择使用重载或者万能引用模板。而参数可拷贝，且移动开销低时，就可以考虑使用按值传递。\n    通过构造函数拷贝参数可能比通过赋值拷贝开销大得多。\n  按值传递会导致切片问题，所以不适合基类类型的参数。\n  当将派生类的对象按值传递给基类时，由于类型不同，会自动进行隐式的类型转换，而在转换中则会对派生类对象进行切割，将派生类比基类多出的成员全部切割掉，以保证其能够转换为基类类型。\n1 2 3 4 5 6 7 8  class Widget { … }; //基类 class SpecialWidget: public Widget { … }; //派生类 void processWidget(Widget w); //对任意类型的Widget的函数，包括派生类型 … //遇到对象切片问题 SpecialWidget sw; … processWidget(sw); //processWidget看到的是Widget，  //不是SpecialWidget！       条款 42：使⽤ emplacement 代替 insertion   原则上，emplacement 函数有时比 insertion 高效，并且不会更差。\n  insertion 函数接受的参数是待插入对象，而 emplacement 接受的参数是待插入对象的构造函数参数，这就导致 emplacement 避免了临时对象的创建和析构，而 insertion 则无法做到。\n例如：\n1 2 3 4 5 6  std::vector\u0026lt;std::string\u0026gt; vs; //std::string的容器  vs.push_back(\u0026#34;xyzzy\u0026#34;); //添加字符串字面量，改代码等价于下面这一行 vs.push_back(std::string(\u0026#34;xyzzy\u0026#34;)); //创建临时std::string，把它传给push_back  vs.emplace_back(\u0026#34;xyzzy\u0026#34;); //直接用“xyzzy”在vs内构造std::string       实际上，当满足以下条件时，emplacement 函数更快：\n 待添加的值是以构造而非赋值的方式加入容器。  当以构造的方式放入容器时，insertion 就有可能创建和析构临时对象。   传入的实参类型与容器类型不一致。  当传入的实参类型与容器类型不一致时，如果使用 insertion 则需要创建一个该类型的临时对象，并进行隐式类型转换，之后还要析构临时对象。   容器不会拒绝已经存在的重复值。  为了判断一个元素是否已存在于容器中，emplacement 需要创建一个具有新值的节点，以便将该节点的值与容器中现有节点的值进行比较。如果该元素不存在，则将其 emplacement。而如果存在，则终止 emplacement，并将节点析构，这时就浪费了构造和析构的成本。      emplacement 函数可能会执行 insertion 函数中被拒绝的类型转换。\n  在 emplacement 使用的是直接初始化，所以它能够调用带有 explicit 声明的构造函数，而使用 insertion 函数时使用的是拷贝初始化，所以它不能调用带有 explicit 声明的构造函数。这就导致 emplacement 可能存在隐式的类型转换。\n1 2 3 4 5 6 7 8  std::vector\u0026lt;std::regex\u0026gt; regexes; regexes.push_back(nullptr); //错误！拷贝初始化不允许用那个构造函数  regexes.emplace_back(nullptr); //可编译。直接初始化允许使用接受指针的  //std::regex的explicit构造函数 //上面的代码等价于隐式构造了一个regex对象 std::regex r(nullptr);       ","date":"2022-07-12T18:54:13+08:00","permalink":"https://blog.orekilee.top/p/effective-modern-c-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%83%E5%BE%AE%E8%B0%83/","title":"Effective Modern C++ 读书笔记（七）：微调"},{"content":"并发 API 条款 35：优先基于任务编程而非基于线程  std::thread 的 API 没有提供直接获取异步运行函数的返回值的途径，如果那些函数抛出异常，则程序会终止运行。 基于线程的程序设计要求手动管理线程耗尽、资源超额、负载均衡，以及跨平台适配。 基于任务的程序设计中 std::async 会默认解决以上两个问题。  std::async 返回的 future 提供了 get 函数，从而可以获取返回值。即使函数发生了异常，get 函数也能够访问到异常。 当使用默认启动策略调用 std::async 时，系统不保证会创建一个新的线程，它允许调度器把指定函数运行在请求结果的线程中。合理的调度器在系统资源超额或者线程耗尽时就会利用这个自由度。    条款 36：如果必须使用异步，则指定 std::launch::async   std::async 的默认启动策略既允许任务以异步方式执行，也允许使用同步方式。\n  std::launch::async 启动策略意味着函数必须以异步方式运行，即在另一个线程上执行。\n  std::launch::deferred 启动策略意味着函数只会在 std::async 返回的 future 上调用 get 或者 wait 时才会同步执行（即调用方阻塞到函数运行结束为止）。如果 get 或 wait 没有调用则函数不会运行。\n  std::async在不指定启动策略时会使用默认启动策略，即采用对上述两者进行或运算的结果：\n1 2 3 4 5 6  auto fut1 = std::async(f); //使用默认启动策略运行f  //等价于上一条 auto fut2 = std::async(std::launch::async | //使用async或者deferred运行f  std::launch::deferred, f);   默认启动策略运行函数以同步或者异步的方式运行，也因此导致函数具有不确定性。\n    这样的灵活性导致使用 thread_local 变量时的不确定性，隐含了任务可能永远不会被执行的意思，还会影响运用了基于超时的 wait 调用的程序逻辑。\n  由于默认启动策略的不确定性，导致在使用 thread_local 变量时，如果函数读/写该线程级局部存储（thread local storage，TLS）时无法预知到读取的是哪个线程。\n  如果使用基于 wait 的循环超时机制，很可能会导致任务永远不会被执行：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  using namespace std::literals; //为了使用C++14中的时间段后缀；参见条款34  void f() //f休眠1秒，然后返回 { std::this_thread::sleep_for(1s); } auto fut = std::async(f); //异步运行f（理论上）  while (fut.wait_for(100ms) != //循环，直到f完成运行时停止...  std::future_status::ready) //但是有可能永远不会发生！ { … }   对于上面这个代码，如果采用的是 std::launch::deferred 策略，则 fut.wait_for 将总是返回 std::future_status::deferred。这永远不等于 std::future_status::ready ，循环会永远执行下去。\n想要解决这个问题也很简单，只需要加个判断，查看返回值是否为 std::future_status::deferred 即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  auto fut = std::async(f); //同上  if (fut.wait_for(0s) == //如果task是deferred（被延迟）状态  std::future_status::deferred) { … //在fut上调用wait或get来异步调用f } else { //task没有deferred（被延迟）  while (fut.wait_for(100ms) != //不可能无限循环（假设f完成）  std::future_status::ready) { … //task没deferred（被延迟），也没ready（已准备）  //做并行工作直到已准备  } … //fut是ready（已准备）状态 }       如果必须使用异步，则指定 std::launch::async。\n 如果下面条件满足不了，则你可能要想确保任务以异步执行，此时就需要指定 std::launch::async：  任务不需要与调用 get 或 wait 的线程并发执行。 读/写哪个线程的 thread_local 变量并无影响。 可以保证会在 std::async 返回的 future 之上调用 get 或 wait，或者可以接受任务永不执行。 使用 wait_for 或者 wait_until 的代码会考虑任务被延迟的可能。      条款 37：使 std::thread 类型对象在所有路径皆 unjoinable   在所有路径上保证 std::thread 类型对象最终是 unjoinable。\n unjoinable 的 std::thread 对象包括：  默认构造的 std::thread。此类 std::thread 没有可以执行的函数，因此也没有对应的底层执行线程。 已经被移动走的 std::thread。一个 std::thread 所对应的底层执行线程被对应到另一个 std::thread 上。 已经被 join 的 std::thread。在 join 之后，std::thread 不再对应已结束运行的底层执行线程。 已经被 detach 的 std::thread。detach 断开了 std::thread 和它的底层执行线程的连接。      在析构时调用 join 可能导致难以调试的性能异常。\n std::thread 的析构函数会等待底层异步执行线程完成。    在析构时调用 detach 可能导致难以调试的未定义行为。\n std::thread 的析构函数会分离 std::thread 类型对象与底层执行线程之间的连接，而该底层执行线程会继续执行。    声明类的数据成员时，最后再声明 std::thread 类型对象。\n 通常一个数据成员的初始化会依赖另一个成员，而 std::thread 对象会在初始化结束后立即执行函数，因此最好将其在最后再声明，这样就能保证一旦构造结束，在前面的所有数据成员都会初始化完毕，可以供 std::thread 数据成员绑定的异步运行的线程安全使用。    条款 38：关注不同线程句柄的析构行为   future 的正常析构行为就是销毁 future 本身的数据。\n future 的正常析构行为仅会析构 future 对象，并对共享状态里的引用计数（由引用它的 future 和被调用者的 std::promise 共同控制的）进行一次自减。它不会对任何东西实施 join 或 detach。    最后一个经由 std::async 创建的共享状态的 future 析构函数会在任务结束前阻塞。\n 本质上，这种 future 的析构函数对执行异步任务的线程执行了隐式的 join。    条款 39：对于一次性事件的通信使用无返回值的 future   对于简单的时间通信，基于条件变量的设计需要一个多余的互斥锁，这不仅会给相互关联的检测和反应任务带来约束，还需要反应任务来验证事件是否已发生。\n  基于条件变量的设计如下面的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  std::condition_variable cv; //事件的条件变量 std::mutex m; //配合cv使用的mutex  //检测任务 … //检测事件 cv.notify_one(); //通知反应任务  //反应任务 … //反应的准备工作 { //开启关键部分  std::unique_lock\u0026lt;std::mutex\u0026gt; lk(m); //锁住互斥锁  cv.wait(lk); //等待通知，但是这是错的！  … //对事件进行反应（m已经上锁） } //关闭关键部分；通过lk的析构函数解锁m … //继续反应动作（m现在未上锁）   由于条件变量需要确保线程安全，因此需要使用互斥锁。而在检测和反应任务这一场景下，检测线程和反应线程之间并不存在数据竞争，这就导致锁不仅带来了性能损耗，还会阻止两个线程同时运行。\n除了互斥锁，条件变量本身也存在问题，即使检测线程没有发出信号，反应线程也有可能会被虚假唤醒。或者在反应线程条用 wait 之间，检测线程发出了信号，这也会导致信号的丢失。\n    使用标志位的设计可以避免上述问题，但这一设计基于轮询而非阻塞。\n  除了条件变量，我们也可以使用原子的布尔类型作为标记，当标记为 true 时则代表事件发生：\n1 2 3 4 5 6 7 8 9 10  std::atomic\u0026lt;bool\u0026gt; flag(false); //共享的flag；std::atomic见条款40  //检测任务 … //检测某个事件 flag = true; //告诉反应线程  //反应任务 … //准备作出反应 while (!flag); //等待事件 … //对事件作出反应   这种方法不需要互斥锁，也没有虚假唤醒的问题，但是其基于轮询而非阻塞，这就导致其大量占用了 cpu 资源。\n    条件变量和标志位可以一起使用，但这样的通信机制设计结果并不让人愉快。\n  我们也可以将上述两种方案结合起来，用标志位标记事件，用条件变量阻塞线程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  std::condition_variable cv; //跟之前一样 std::mutex m; bool flag(false); //不是std::atomic  //检测任务 … //检测某个事件 { std::lock_guard\u0026lt;std::mutex\u0026gt; g(m); //通过g的构造函数锁住m  flag = true; //通知反应任务（第1部分） } //通过g的析构函数解锁m cv.notify_one(); //通知反应任务（第2部分）  //反应任务 … //准备作出反应 { //跟之前一样  std::unique_lock\u0026lt;std::mutex\u0026gt; lk(m); //跟之前一样  cv.wait(lk, [] { return flag; }); //使用lambda来避免虚假唤醒  … //对事件作出反应（m被锁定） } … //继续反应动作（m现在解锁）   这种方法既避免了方法一的事件丢失和虚假唤醒，又弥补了方法二基于轮询的缺点。但是仍然有不好的地方，即就算反应任务被唤醒，他也要检测标志位的值后才决定是否做出反应，这就导致大量无意义的线程阻塞与唤醒，浪费了大量 CPU 时间的同时带来了上下文切换的损耗。\n    使用 std::promise 和 future 的方案就能够回避这些问题，但是这样就需要为了共享状态而使用堆内存，且需要考虑堆内存的分配和销毁开销，且同时有只能使用一次通信的限制。\n  除了上面提到的方案，还有一个方法能够避免使用条件变量、互斥锁和标记位。即让反应任务在检测任务设置的 future 上 wait 等待通知，例如下面的代码：\n1 2 3 4 5 6 7 8 9 10  std::promise\u0026lt;void\u0026gt; p; //通信信道的promise  //检测任务 … //检测某个事件 p.set_value(); //通知反应任务  //反应任务 … //准备作出反应 p.get_future().wait(); //等待对应于p的那个future … //对事件作出反应 //继续反应动作（m现在解锁）   这个方案不需要互斥锁与条件变量，也不存在虚假唤醒，同时还保证了事件基于阻塞，且在等待时不会浪费系统资源。但是它仍然有许多缺点：\n std::promise 和 future 之间有个共享状态，且共享状态是动态分配的，因此就会导致产生在堆上进行内存分配与释放的成本。 std::promise 类型对象只能设置一次，不能用于重复通信。      条款 40：当需要并发时使用 std::atomic，特殊内存才使用 volatile   std::atomic 用于无锁、多线程读写变量的场景。它是用来编写并发程序的工具。\n  volatile 用于读写操作不可以被优化的特殊内存，它可以避免编译器优化内存带来的一些问题。\n  volatile 的作用就是告诉编译器当前处理的内存是特殊内存，不要对这块内存做任何特殊优化。如果没有声明 volatile，则编译器会消除一些代码中的重复赋值或者中间结果，例如下面这个代码：\n1 2 3 4 5 6 7 8 9  int x; auto y = x; //读x y = x; //再次读x x = 10; //写x x = 20; //再次写x  //编译器优化后 auto y = x; //读x x = 20; //写x       ","date":"2022-07-12T17:54:13+08:00","permalink":"https://blog.orekilee.top/p/effective-modern-c-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E5%85%AD%E5%B9%B6%E5%8F%91-api/","title":"Effective Modern C++ 读书笔记（六）：并发 API"},{"content":"lambda 表达式 条款 31：避免使用默认捕获模式   默认的按引用捕获可能会导致悬空指针问题。\n  按引用捕获会导致闭包中包含指涉到局部变量的的引用，或者是指涉到定义 lambda 表达式的作用域内的形参的引用。一旦 lambda 表达式所创建的闭包越过了该局部变量/形参的生命周期，就会导致引用空悬。\n例如下面的代码，由于 lambda 中指涉了局部变量 divisor，因此当 addDivisorFilter 的生命周期结束时，该变量也会消亡，而我们添加进去的那个 labmda 表达式则操作了一个已经释放的空间，因此会产生未定义的行为。\n1 2 3 4 5 6 7 8 9 10 11  void addDivisorFilter() { auto calc1 = computeSomeValue1(); auto calc2 = computeSomeValue2(); auto divisor = computeDivisor(calc1, calc2); filters.emplace_back( //危险！对divisor的引用  [\u0026amp;](int value) { return value % divisor == 0; } //将会悬空！  ); }   解决这个问题最简单的方法就是改为按值的默认捕获模式：\n1 2 3  filters.emplace_back( //现在divisor不会悬空了  [=](int value) { return value % divisor == 0; } );       默认的按值捕获对于悬空指针较为敏感（尤其是 this 指针），并且它会误导人们认为 lambda 表达式是独立的。\n  当我们使用按值捕获时，可能会出现一些隐含的问题，例如下面的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  using FilterContainer = //跟之前一样  std::vector\u0026lt;std::function\u0026lt;bool(int)\u0026gt;\u0026gt;; FilterContainer filters; //跟之前一样  void doSomeWork() { auto pw = //创建Widget；std::make_unique  std::make_unique\u0026lt;Widget\u0026gt;(); //见条款21  pw-\u0026gt;addFilter(); //添加使用Widget::divisor的过滤器  … } void Widget::addFilter() const { filters.emplace_back( [=](int value) { return value % divisor == 0; } ); }   在该代码中，实际上并没有将 divisor 的值拷贝进 lambda 中，而是隐式的拷贝了 this 指针，并隐式使用这个 this 指针的拷贝来访问 divisor。一旦 pw 的生命周期结束，make_unique 就会将它的资源释放掉，这时 lambda 中访问到的就是一个悬空的 this 指针。\n要解决这个问题，我们可以使用一个临时变量来拷贝类的数据成员，并以传值的形式传递进 lambda 中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  //c++11 void Widget::addFilter() const { auto divisorCopy = divisor; //拷贝数据成员  filters.emplace_back( [divisorCopy](int value) //捕获副本  { return value % divisorCopy == 0; }\t//使用副本  ); } //c++14 void Widget::addFilter() const { filters.emplace_back( //C++14：  [divisor = divisor](int value) //拷贝divisor到闭包  { return value % divisor == 0; }\t//使用这个副本  ); }     默认的值捕获模式看起来似乎是独立的，与外界绝缘，然而实际并非如此，因为 lambda 表达式还依赖于静态存储期的对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13  void addDivisorFilter() { static auto calc1 = computeSomeValue1(); //现在是static  static auto calc2 = computeSomeValue2(); //现在是static  static auto divisor = //现在是static  computeDivisor(calc1, calc2); filters.emplace_back( [=](int value) //什么也没捕获到！  { return value % divisor == 0; } //引用上面的static  ); ++divisor; //调整divisor }       条款 32：使用初始化捕获将对象移入闭包   使用 C++ 14 的初始化捕获将对象移动到闭包中。\n在 C++ 11 中没有办法将一个对象移动到 lambda 的闭包中，而 C++ 14 提出的初始化捕获机制则可以做到这件事：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Widget { //一些有用的类型 public: … bool isValidated() const; bool isProcessed() const; bool isArchived() const; private: … }; auto pw = std::make_unique\u0026lt;Widget\u0026gt;(); //创建Widget；使用std::make_unique  //的有关信息参见条款21  … //设置*pw  auto func = [pw = std::move(pw)] //使用std::move(pw)初始化闭包数据成员  { return pw-\u0026gt;isValidated() \u0026amp;\u0026amp; pw-\u0026gt;isArchived(); };   位于 = 号左侧的是指定的闭包类成员变量名，它的作用域就是闭包类的作用域。位于 = 号右侧则是该变量的初始化表达式，它的作用域则和外面一层的相同。\n  在 C++ 11 中通过自定义类或 std::bind 的方式来模拟初始化捕获。\n  自定义类。lambda 表达式的原理其实也就是生成一个类并重载它的()操作符，那么我们完全可以手动模拟这个过程，例如下面的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class IsValAndArch { public: using DataType = std::unique_ptr\u0026lt;Widget\u0026gt;; explicit IsValAndArch(DataType\u0026amp;\u0026amp; ptr) : pw(std::move(ptr)) {} bool operator()() const { return pw-\u0026gt;isValidated() \u0026amp;\u0026amp; pw-\u0026gt;isArchived(); } private: DataType pw; }; auto func = IsValAndArch(std::make_unique\u0026lt;Widget\u0026gt;());     std::bind。std::bind 同样可以生成函数对象，因此我们也可以使用它来模拟初始化捕获：\n1 2 3 4 5 6 7 8 9 10  std::vector\u0026lt;double\u0026gt; data; //同上  … //同上  auto func = std::bind( //C++11模拟初始化捕获  [](const std::vector\u0026lt;double\u0026gt;\u0026amp; data) { /*使用data*/ }, std::move(data) );       条款 33：对于 std::forward 的 auto\u0026amp;\u0026amp; 形参使⽤ decltype   对于 std::forward 的 auto\u0026amp;\u0026amp; 形参使⽤ decltype。\n  C++ 14 中支持使用泛型 lambda 表达式，即我们可以在 lambda 中使用 auto 推导形参的类型。假设我们想实现一个泛型的 lambda 函数，并在其内部将 auto 推导出的形参转发给另一个函数，此时很容易就能够写出下面这样的代码：\n1  auto f = [](auto x){ return func(normalize(x)); };   这样的代码存在问题，因为它无法区分左值和右值，即使你传入的参数是右值，它转发后的结果却丢失了右值的属性，仍然为左值。\n为了能够支持在内部能够完美转发左值与右值，就需要将参数类型声明为 auto\u0026amp;\u0026amp;，并使用 std::forward 对参数进行转发。同时，使用 decltype 推导出 std::forward 所需要的参数的类型。\n1 2 3 4 5 6 7  //C++14 支持多个参数并进行完美转发的泛型lambda模板 auto f = [](auto\u0026amp;\u0026amp;... params) { return func(normalize(std::forward\u0026lt;decltype(params)\u0026gt;(params)...)); };       条款 34：优先使用 lambda 表达式而非 std::bind   相比于 std::bind，lambda 表达式可读性更好、表达力更强、运行效率更高。\n  这里以一段代码来举例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  //一个时间点的类型定义（语法见条款9） using Time = std::chrono::steady_clock::time_point; //“enum class”见条款10 enum class Sound { Beep, Siren, Whistle }; //时间段的类型定义 using Duration = std::chrono::steady_clock::duration; //在时间t，使用s声音响铃时长d void setAlarm(Time t, Sound s, Duration d); //lambda写法 auto setSoundL = [](Sound s) { //使std::chrono部件在不指定限定的情况下可用  using namespace std::chrono; setAlarm(steady_clock::now() + hours(1), s, seconds(30)); }; //bind写法 using namespace std::chrono; using namespace std::placeholders; auto setSoundB = std::bind(setAlarm, std::bind(std::plus\u0026lt;steady_clock::time_point\u0026gt;(), std::bind(steady_clock::now), hours(1)), _1, seconds(30));   对于以上代码：\n 可读性：由于参数中存在一个表达式，而 std::bind 会在调在 std::bind 时就计算出表达式的结果，并绑定在 setSoundB 对象中，而不是在调用 setAlarm 时再计算表达式结果，因此会导致结果有误差。为了解决这个问题，就需要在 std::bind 中再嵌套一个 std::bind 并将表达式改为模板 std::plus，以此将表达式求值推迟。这样带来的问题就是代码的可读性变差，理解成本增高。 运行效率：lambda 中调用 setAlarm 时，会使用 inline 对其进行优化，而 std::bind 是通过传入的函数指针调用函数，因此编译器不太可能将其内联，此时 lambda 效率更高。      仅在 C++ 11 中，std::bind 在实现移动捕获、使用模板化函数调用运算符来绑定对象这两个场景才能发挥作用。而在 C++ 14 及以后的版本，lambda 已经完全可以替代 std::bind 了。\n   https://zh.wikipedia.org/zh-cn/GNU%E4%BE%A6%E9%94%99%E5%99%A8)  ","date":"2022-07-12T16:54:13+08:00","permalink":"https://blog.orekilee.top/p/effective-modern-c-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%BA%94lambda-%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"Effective Modern C++ 读书笔记（五）：lambda 表达式"},{"content":"右值引用、移动语义和完美转发 条款 23：std::move 与 std::forward   std::move 执行的是无条件的向右值类型的强制类型转换，就其本身而言并不会执行移动操作。\n  下面是 std::move 在 C++ 11 与 C++ 14 中的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  //C++11 template\u0026lt;typename T\u0026gt; typename remove_reference\u0026lt;T\u0026gt;::type\u0026amp;\u0026amp; move(T\u0026amp;\u0026amp; param) { using ReturnType = typename remove_reference\u0026lt;T\u0026gt;::type\u0026amp;\u0026amp;; return static_cast\u0026lt;ReturnType\u0026gt;(param); } //C++14 template\u0026lt;typename T\u0026gt; decltype(auto) move(T\u0026amp;\u0026amp; param) //C++14，仍然在std命名空间 { using ReturnType = remove_referece_t\u0026lt;T\u0026gt;\u0026amp;\u0026amp;; return static_cast\u0026lt;ReturnType\u0026gt;(param); }   可以看到，std::move 只是简单的将类型原本的引用属性去掉，再将其强转为右值版本后返回，本身并不存在移动操作。\n    std::forward 仅当传入的实参被绑定到右值时，才会将参数强制转换为右值类型。\n 与 std::move 的无条件转换不同，std::forward 是有条件的转换：仅当参数是使用右值完成初始化时，它才会执行向右值类型的强制转换（通过解码模板参数 T 来分辨是左值还是右值）。    在运行期，std::move 与 std::forward 都不会做任何操作。\n  条款 24：万能引用与右值引用   如果函数模板形参具备 T\u0026amp;\u0026amp; 类型，且 T 的类型是推导得来的，又或者对象使用 auto\u0026amp;\u0026amp; 声明类型（必定会类型推导），则表示这个形参或者对象是一个万能引用。\n  例如：\n1 2 3 4 5 6  //函数模板形参具备T\u0026amp;\u0026amp;类型，且T的类型是推导得来的 template\u0026lt;typename T\u0026gt; void f(T\u0026amp;\u0026amp; param); //param是一个万能引用  //使用 auto\u0026amp;\u0026amp; 声明类型 auto\u0026amp;\u0026amp; var2 = var1; //var2是一个万能引用       如果类型声明不是标准的 type\u0026amp;\u0026amp;，或者类型推导没有发生，则 type\u0026amp;\u0026amp; 就代表右值引用。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  template \u0026lt;typename T\u0026gt;\t//加上了const属性，不是标准的type\u0026amp;\u0026amp; void f(const T\u0026amp;\u0026amp; param); //param是一个右值引用  void f(Widget\u0026amp;\u0026amp; param); //没有类型推导，  //param是一个右值引用  Widget\u0026amp;\u0026amp; var1 = Widget(); //没有类型推导，  //var1是一个右值引用  template\u0026lt;class T, class Allocator = allocator\u0026lt;T\u0026gt;\u0026gt; class vector { public:\t//需要使用一个类型来实例化模板类，不存在类型推导  void push_back(T\u0026amp;\u0026amp; x);\t//右值引用  … }       如果用一个右值来初始化万能引用，则会得到一个右值引用。同理，如果使用左值来初始化万能引用，就会得到一个左值引用。\n  条款 25：对右值引用使用 std::move，对万能引用使用 std::forward   在最后一次使用时，对右值引用使用 std::move，对万能引用使用 std::forward。\n 当转发右值引用给其他函数时，应当对它使用向右值的无条件转换（std::move），因为它们一定绑定到右值。而转发万能引用时，应该对它使用向右值的有条件转换转换（std::forward），因为它们不一定绑定到右值。    对按值返回的函数，无论是右值引用和万能引用，都按照上一条采取相同的行为。\n  若局部变量适用于返回值优化时，避免使用 std::move 或者 std::forward。\n  在 C++ 中有返回值优化（return value optimization，RVO）机制，可以直接在函数返回值分配的内存上创建的临时对象，然后可以达到少调用拷贝/移动构造的操作目的。\n但是使用这个机制有两个前提：\n 局部对象类型和函数返回值类型相同。 返回的就是局部变量本身。  而 std::move 和 std::forward 会将返回值变为该局部变量的引用，此时不满足上述的前提，因此限制了编译器优化。\n    条款 26：避免用万能引用类型进行重载   把万能引用作为重载的参数，总是会让该重载版本在始料未及的情况下被调用。\n  万能引用在具限过程中几乎能够和任何实参类型产生精确匹配，一旦将万能引用作为重载的参数，它们就会在预期之外的情况下进行调用，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  template\u0026lt;typename T\u0026gt; void logAndAdd(T\u0026amp;\u0026amp; name) { auto now = std::chrono::system_lock::now(); log(now, \u0026#34;logAndAdd\u0026#34;); names.emplace(std::forward\u0026lt;T\u0026gt;(name)); } void logAndAdd(int idx) //新的重载 { auto now = std::chrono::system_lock::now(); log(now, \u0026#34;logAndAdd\u0026#34;); names.emplace(nameFromIdx(idx)); } logAndAdd(\u0026#34;Patty Dog\u0026#34;); //T\u0026amp;\u0026amp;重载版本  logAndAdd(22); //调用int重载版本  short nameIdx; … //给nameIdx一个值 logAndAdd(nameIdx); //错误！   在上面的代码中，我们传递了一个 short 类型的 index，我们的预期是将他匹配到同为整数类型的 int 版本上，但由于 short 类型在转换为 int 类型时需要进行隐式类型转换（整型提升）后才能匹配，此时就会精准的匹配到万能引用版本上，从而导致报错。\n    完美转发构造函数的问题尤其严重，因为对于非常量的左值类型而言，它门不会调用拷贝构造，而是调用完美转发构造，并且还会劫持派生类中对基类的拷贝和移动构造的调用。\n  对于非常量的左值类型而言，它门不会调用拷贝构造，而是调用完美转发构造，例如下面的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Person { public: template\u0026lt;typename T\u0026gt; //完美转发的构造函数  explicit Person(T\u0026amp;\u0026amp; n) : name(std::forward\u0026lt;T\u0026gt;(n)) {} explicit Person(int idx); //int的构造函数  Person(const Person\u0026amp; rhs); //拷贝构造函数  Person(Person\u0026amp;\u0026amp; rhs); //移动构造函数  … }; Person p(\u0026#34;Nancy\u0026#34;); auto cloneOfP(p); //调用完美转发构造；报错！  const Person cp(\u0026#34;Nancy\u0026#34;); //现在对象是const的 auto cloneOfP(cp); //调用拷贝构造函数！   在使用非常量左值类型时，如果要调用拷贝构造函数，就需要为该类型添加 const 属性，而完美转发构造函数却不要求添加任何饰词，因此是精准匹配，此时就会调用完美转发构造函数。\n  完美转发构造函数会劫持派生类中对基类的拷贝和移动构造的调用，例如下面的代码：\n1 2 3 4 5 6 7 8 9 10  class SpecialPerson: public Person { public: SpecialPerson(const SpecialPerson\u0026amp; rhs) //拷贝构造函数，调用基类的  : Person(rhs) //完美转发构造函数！  { … } SpecialPerson(SpecialPerson\u0026amp;\u0026amp; rhs) //移动构造函数，调用基类的  : Person(std::move(rhs)) //完美转发构造函数！  { … } };   这里也是一样，由于我们将派生类对象传递给了基类的构造函数，而如果要想使用基类的拷贝/移动构造，就需要进进行对象切割，而完美转发构造函数则可以直接进行精准匹配，因此调用会被劫持。\n    条款 27：替代万能引用类型进行重载的方案   如果不使用万能引用和重载的组合，则替代方案包括使用不同的函数名、传递 const T\u0026amp; 类型的形参、按值传参、标签分派。\n  使用不同的函数名\n 最简单的方法就是舍弃重载，将重载的不同版本重新命名为不同名字的函数，这样就可以避免万能引用劫持匹配。    传递 const T\u0026amp; 类型的形参\n 回归 C++ 98 的写法，放弃高效率而选择保持代码逻辑的简洁，继续使用 const T\u0026amp; 类型。    按值传参\n  把传递的参数类型从引用类型改为值类型，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12  class Person { public: explicit Person(std::string n) //代替T\u0026amp;\u0026amp;构造函数，  : name(std::move(n)) {} //std::move的使用见条款41  explicit Person(int idx) //同之前一样  : name(nameFromIdx(idx)) {} … private: std::string name; };       标签分派\n  如果既不想放弃重载，又不想放弃万能引用，那么我们可以采用标签分派的方式来重写代码，即使用一个标签来辨别该调用哪种类型的重载函数。我们可以使用类型萃取的方式，根据不同的类型生成不同的标签，从而调用不同的重载函数。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  template\u0026lt;typename T\u0026gt; void logAndAdd(T\u0026amp;\u0026amp; name) { logAndAddImpl( std::forward\u0026lt;T\u0026gt;(name), std::is_integral\u0026lt;typename std::remove_reference\u0026lt;T\u0026gt;::type\u0026gt;() ); } template\u0026lt;typename T\u0026gt; void logAndAddImpl(T\u0026amp;\u0026amp; name, std::false_type)\t//std::false_type { auto now = std::chrono::system_clock::now(); log(now, \u0026#34;logAndAdd\u0026#34;); names.emplace(std::forward\u0026lt;T\u0026gt;(name)); } std::string nameFromIdx(int idx); void logAndAddImpl(int idx, std::true_type) //std::true_type { logAndAdd(nameFromIdx(idx)); }         经由 std::enable_if 对模板施加限制，就可以将万能引用和重载一起使用，std::enable_if 控制了编译器调用到接受万能引用的重载版本的条件。\n  通过这种方式，我们就能够解决条款 26 中提到的使用完美转发构造函数的几个问题：\n  对于非常量的左值类型而言，它门不会调用拷贝构造，而是调用完美转发构造。解决方法如下，我们可以使用 std::decay 来去掉参数原有的引用、const、volatile 饰词，消除隐式转换的可能，再以 std::is_same 来判断类型是否相同，来达到精确匹配。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  class Person { public: template\u0026lt; typename T, typename = typename std::enable_if\u0026lt; !std::is_same\u0026lt;Person, typename std::decay\u0026lt;T\u0026gt;::type \u0026gt;::value \u0026gt;::type \u0026gt; explicit Person(T\u0026amp;\u0026amp; n); … };     完美转发构造函数会劫持派生类中对基类的拷贝和移动构造的调用。解决方案如下，我们可以使用 is_base_of 来区分对待派生类和基类对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  //C++11 class Person { public: template\u0026lt; typename T, typename = typename std::enable_if\u0026lt; !std::is_base_of\u0026lt;Person, typename std::decay\u0026lt;T\u0026gt;::type \u0026gt;::value \u0026gt;::type \u0026gt; explicit Person(T\u0026amp;\u0026amp; n); … }; //C++14，可以使用别名模板简化代码 class Person { public: template\u0026lt; typename T, typename = std::enable_if_t\u0026lt; !std::is_base_of\u0026lt;Person, std::decay_t\u0026lt;T\u0026gt; \u0026gt;::value \u0026gt; \u0026gt; explicit Person(T\u0026amp;\u0026amp; n); … };         万能引用形参在性能方面具备优势，但是在易用性方面则具有劣势。\n  条款 28：引用折叠   引用折叠会在四种情况下发生：模板实例化、auto 类型推导、typedef 和别名声明、decltype。\n  当编译器在引用折叠的情况下生成引用的引用时，通过引用折叠会变成单个引用。如果原始的引用中有左值引用，则结果位左值引用。反之，则为右值引用。\n  1 2 3 4 5 6 7 8 9 10 11  Widget widgetFactory(); //返回右值的函数 Widget w; //一个变量（左值） func(w); //用左值调用func；T被推导为Widget\u0026amp; func(widgetFactory()); //用又值调用func；T被推导为Widget  auto\u0026amp;\u0026amp; w1 = w; Widget\u0026amp; \u0026amp;\u0026amp; w1 = w;\t//等价于上一条，auto推导出Widget\u0026amp; Widget\u0026amp; w1 = w\t//引用折叠后为左值  auto\u0026amp;\u0026amp; w2 = widgetFactory(); Widget\u0026amp;\u0026amp; w2 = widgetFactory()\t//等价于上一条，auto推导出Widget，为右值       万能引用通过类型推导区分左值和右值，以及会发生引用折叠情况下的右值引用。\n 万能引用的类型推导会区分左值和右值。T 类型的左值会推导为 T\u0026amp;，而 T 类型的右值会推导为 T 类型。接着会发生引用折叠。    条款 29：假定移动操作不存在、成本高、不可用   假定移动操作不存在、成本高、不可用。\n 在下面这几个场景中，C++ 11 的移动语义并不会有任何优势：  没有移动操作。待移动对象不支持移动操作，因此移动请求就变为拷贝请求。 移动操作成本高。移动操作的成本不比拷贝操作更快。 移动操作不可用。要求移动操作不可抛出异常，但该操作没加上 noexcept。      对于那些类型或者移动语义的支持情况已知的代码，则无需做以上假定。\n  条款 30：完美转发的失败情形   完美转发的失败原因是源于模板类型推导失败，或是推导类型错误。\n  完美转发会在下面两个条件中的任何一个成立时失败：\n 编译器无法为一个或者多个转发函数模板形参推导出类型。这种情况下代码无法通过编译。 编译器为一个或多个转发函数模板的形参推导出了错误的结果。这种情况即可以指转发函数模板根据类型推导结果的实例化无法通过编译，也可以指转发函数模板推导出的类型与直接传递给转发函数模板的实参调用的行为不一致。  例如下面这些代码：\n1 2 3 4 5 6 7 8 9  template\u0026lt;typename... Ts\u0026gt; void fwd(Ts\u0026amp;\u0026amp;... params) //接受任何实参 { f(std::forward\u0026lt;Ts\u0026gt;(params)...); //转发给f } f( expression ); //调用f执行某个操作  fwd( expression );\t//但调用fwd执行另一个操作，则fwd不能完美转发expression给f       导致完美转发失败的实参种类有大括号初始化；以 0 或者 NULL 表示的空指针；仅有声明的整型 static、const 数据成员；模板或重载函数的名字；位域。\n  大括号初始化\n  如下面的代码，由于 fwd 的形参并未声明为 std::initializer_list\u0026lt;T\u0026gt;，因此在使用大括号初始化时，编译器就会被禁止在 fwd 的调用过程中从表达式 {1，2，3} 中推导出类型，从而导致编译错误。\n1 2 3 4 5  void f(const std::vector\u0026lt;int\u0026gt;\u0026amp; v); f({ 1, 2, 3 }); //可以，“{1, 2, 3}”隐式转换为std::vector\u0026lt;int\u0026gt;  fwd({ 1, 2, 3 }); //错误！不能编译   这有一个简单的方法可以转发大括号初始化，即先用一个 auto 变量来接收初始化表达式，接着再将这个变量转发给函数：\n1 2  auto il = { 1, 2, 3 }; //il的类型被推导为std::initializer_list\u0026lt;int\u0026gt; fwd(il); //可以，完美转发il给f       以 0 或者 NULL 表示的空指针\n 使用 0 或者 NULL 表示空指针时会被推导为整数类型而不是所传递实参的指针类型。    仅有声明的整型 static、const 数据成员\n C++ 允许我们只声明一个 static、const 成员而不去定义它。因为编译器会根据这些成员的值实行常量传播，从而不必再为它们保留内存。但是由于引用与指针的本质相同，都需要获取到对象的地址，而这些没有存储在内存中的对象自然没有地址，如果之后没有人去定义这些变量，则它们会在链接时报错。    模板或重载函数的名字\n  当向转发函数模板传递模板/重载函数名时，由于它没有任何关于类型的信息，这就导致编译器无法决定选择调用哪个重载版本。\n1 2 3 4 5 6  void f(int pf(int)); int processVal(int value); int processVal(int value, int priority); f(processVal); //可以 fwd(processVal); //错误！那个processVal？   要想适用于转发函数模板，就得将函数名转换为对应类型的函数指针，再将这个明确类型的函数指针传递给转发函数模板。\n1 2 3 4 5 6 7  using ProcessFuncType = //写个类型定义  int (*)(int); ProcessFuncType processValPtr = processVal; //指定所需的processVal签名  fwd(processValPtr); //可以 fwd(static_cast\u0026lt;ProcessFuncType\u0026gt;(workOnVal)); //也可以       位域\n  C++ 标准规定非 const 引用无法绑定到位域。因为位域是由机器字的若干个任意部分组成的，这样的实体无法直接对其取址，而引用与指针的本质相同，既然无法创建指向任意比特的指针，自然也无法创建这样的引用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  struct IPv4Header { std::uint32_t version:4, IHL:4, DSCP:6, ECN:2, totalLength:16; … }; void f(std::size_t sz); //要调用的函数  IPv4Header h; … f(h.totalLength); //可以 fwd(h.totalLength); //错误！   如果一定要转发位域也不是不行，我们可以通过强制类型转换，将位域的值提取出来：\n1 2 3 4  //拷贝位域值； auto length = static_cast\u0026lt;std::uint16_t\u0026gt;(h.totalLength); fwd(length); //转发这个副本         ","date":"2022-07-12T15:54:13+08:00","permalink":"https://blog.orekilee.top/p/effective-modern-c-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E5%9B%9B%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%E7%A7%BB%E5%8A%A8%E8%AF%AD%E4%B9%89%E5%92%8C%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/","title":"Effective Modern C++ 读书笔记（四）：右值引用、移动语义和完美转发"},{"content":"智能指针 条款 18：使用 std::unique_ptr 管理具备专属所有权的资源   std::unique_ptr 是轻量级、高速、只能 move 的智能指针，对托管资源实施专属所有权语义（通过 delete 拷贝操作实现）。\n  默认情况下，资源销毁采用 delete 来实现，但我们也可以自定义删除器。有状态的删除器和函数指针会增加 std::unique_ptr 的大小（函数指针的大小 + 状态的大小）。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  auto delInvmt1 = [](Investment* pInvestment) //无状态lambda的  { //自定义删除器  makeLogEntry(pInvestment); delete pInvestment; }; template\u0026lt;typename... Ts\u0026gt; //返回类型大小是 std::unique_ptr\u0026lt;Investment, decltype(delInvmt1)\u0026gt; //Investment*的大小 makeInvestment(Ts\u0026amp;\u0026amp;... args); void delInvmt2(Investment* pInvestment) //函数形式的 { //自定义删除器  makeLogEntry(pInvestment); delete pInvestment; } template\u0026lt;typename... Ts\u0026gt; //返回类型大小是 std::unique_ptr\u0026lt;Investment, void (*)(Investment*)\u0026gt; //Investment*的指针 makeInvestment(Ts\u0026amp;\u0026amp;... params); //加至少一个函数指针的大小       很容易就可以将 std::unique 转化为 std::shared_ptr。\n  1 2 3 4 5  std::shared_ptr\u0026lt;std::string\u0026gt; str_sp2 = std::unique_ptr\u0026lt;std::string\u0026gt;(new std::string(\u0026#34;TEST\u0026#34;))) //如果要以已经存在的unique_ptr，则需要move std::unique_ptr\u0026lt;std::string\u0026gt; str_up = std::make_unique\u0026lt;std::string\u0026gt;(\u0026#34;TEST\u0026#34;); std::shared_ptr\u0026lt;std::string\u0026gt; str_sp = std::move(str_up);       条款 19：使用 std::shared_ptr 管理具备共享所有权的资源   std::shared_ptr 实现了任意资源在共享所有权语义下进行生命周期管理的垃圾回收（通过引用计数实现）。\n  与 std::unique_ptr 相比，std::shared_ptr 的大小通常是裸指针的两倍，且还会带来控制块的开销，并要求使用原子化的引用计数操作。\n 引用计数的内存必须要动态分配，并且原子计数会消耗大量 CPU 资源。 std::shared_ptr 的大小是裸指针的两倍，成员中包含了一个指向数据块的指针与一个指向控制块的指针。同时控制块中包含了引用计数、weak 计数、其他数据（例如分配器、自定义删除）。    默认的资源析构通过 delete 进行，但也支持自定义删除器。删除器的类型对 std::shared_ptr 的类型没有影响。\n  对于std::unique_ptr来说，删除器类型是智能指针类型的一部分。而对于std::shared_ptr 则不是，其将删除器作为成员的一部分，将其拷贝后保存至控制块中。由于删除器保存控制块中，如果删除器过大，则会浪费内存。\n1 2 3 4 5 6 7  auto customDeleter1 = [](Widget *pw) { … }; //自定义删除器， auto customDeleter2 = [](Widget *pw) { … }; //每种类型不同 std::shared_ptr\u0026lt;Widget\u0026gt; pw1(new Widget, customDeleter1); std::shared_ptr\u0026lt;Widget\u0026gt; pw2(new Widget, customDeleter2); //删除器对类型没有影响，因此它们类型相同，可以放到同一个容器中，也可以互相给对方赋值。 std::vector\u0026lt;std::shared_ptr\u0026lt;Widget\u0026gt;\u0026gt; vpw{ pw1, pw2 };       避免使用裸指针类型的变量来创建 std::shared_ptr。\n  如果用一个裸指针来创建多个 std::shared_ptr，则会导致多个控制块的建立，并进行多重的引用计数。这不仅仅只是空间的浪费，还会导致同一个对象被多次析构，导致程序报错。\n1 2 3 4 5  auto pw = new Widget; //pw是原始指针 … std::shared_ptr\u0026lt;Widget\u0026gt; spw1(pw, loggingDel); //为*pw创建控制块 … std::shared_ptr\u0026lt;Widget\u0026gt; spw2(pw, loggingDel); //为*pw创建第二个控制块       条款 20：当 std::shared_ptr 可能空悬时使用 std::weak_ptr   当 std::shared_ptr 可能空悬时使用 std::weak_ptr。\n  由于 std::weak_ptr 并不参与 std::shared_ptr 的计数与资源管理，且它们指向的是同一块资源，因此我们可以使用 std::weak_ptr 来检测 std::shared_ptr 所指向的资源是否已销毁（当 weak_count 不为零时，控制块并不会销毁，因此可以使用 std::weak_ptr 查看资源是否析构。）。\n1 2 3 4 5 6  //方法一：调用成员函数expired()，但是在并发环境下可能存在问题，即在if判断未过期后，结果另一个线程却将数据析构了，此时就访问到了已删除数据。 if (wpw.expired()) … //如果wpw没有指向对象…  //方法二：使用lock将weak_ptr升级为shared_ptr，或者直接用其构造出shared_ptr\tstd::shared_ptr\u0026lt;Widget\u0026gt; spw1 = wpw.lock(); //如果wpw过期，spw1就为空 std::shared_ptr\u0026lt;Widget\u0026gt; spw3(wpw); //如果wpw过期，抛出std::bad_weak_ptr异常       std::weak_ptr 主要用于缓存、观察者模式，以及避免 std::shared_ptr 出现循环引用。\n  条款 21：优先使用 std::make_unique 和 std::make_shared 而非 new   相比于直接使用 new，make 系列函数消除了重复代码，提高了异常安全性。并且对于 std::make_shared 和 std::allcoated_shared 而言，生成的目标代码更小、更快。\n  make 系列函数消除了重复代码，避免重复撰写类型：\n1 2 3 4 5 6  //不使用make函数时要多写一次类型 auto upw1(std::make_unique\u0026lt;Widget\u0026gt;()); //使用make函数 std::unique_ptr\u0026lt;Widget\u0026gt; upw2(new Widget); //不使用make函数  auto spw1(std::make_shared\u0026lt;Widget\u0026gt;()); //使用make函数 std::shared_ptr\u0026lt;Widget\u0026gt; spw2(new Widget); //不使用make函数     make 函数提高了异常安全性：\n1 2 3 4 5 6 7 8 9  void processWidget(std::shared_ptr\u0026lt;Widget\u0026gt; spw, int priority); int computePriority(); processWidget(std::shared_ptr\u0026lt;Widget\u0026gt;(new Widget), //潜在的资源泄漏！  computePriority()); processWidget(std::make_shared\u0026lt;Widget\u0026gt;(), //没有潜在的资源泄漏  computePriority());   由于 new 和 computePriority()、构造 std::shared_ptr 的操作顺序并不确定，因此有可能先 new 资源，然后执行 computePriority()，最后再构造 std::shared_ptr。而如果 computePriority() 中出现了异常，就会导致 new 出来的资源无法被用来构造 shared_ptr，因此出现了内存泄漏的问题。而如果使用 make 函数，则将 new 和构造 std::shared_ptr 放到了一起操作，因此不会出现这样的问题。\n    不适合使用 make 函数的情况包括需要自定义删除器，以及使用大括号初始化。\n  make 系列函数不允许指定自定义删除器，此时只能使用 new：\n1 2 3 4 5 6  auto widgetDeleter = [](Widget* pw) { … }; std::unique_ptr\u0026lt;Widget, decltype(widgetDeleter)\u0026gt; upw(new Widget, widgetDeleter); std::shared_ptr\u0026lt;Widget\u0026gt; spw(new Widget, widgetDeleter);     此时又出现了条款 7 中提到的问题，但 make 系列中对形参进行完美转发的代码使用的是小括号，因此不会有不确定性。这也就导致了 make 无法使用大括号，此时只能用 new。\n1 2 3  //完美转发的代码使用的是()，因此vector中有10个值为20的元素 auto upv = std::make_unique\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;(10, 20); auto spv = std::make_shared\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;(10, 20);   但这个问题其实还有一个另类的解决方法，即用 auto 推导出大括号初始化的类型 std::initializer_list\u0026lt;T\u0026gt;，并将这个 auto 对象传递给 make，即可间接使用大括号初始化：\n1 2 3 4  //创建std::initializer_list auto initList = { 10, 20 }; //使用std::initializer_list为形参的构造函数创建std::vector auto spv = std::make_shared\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;(initList);     对于直接使用 new 来构造的 std::shared_ptr，其首先会分配 Widget 的内存，接着将数据块指针指向这块内存，接着再去构建控制块，此时进行了两次内存分配，且两块内存不连续，在析构时还需要析构两次。而使用 make 函数构造的 std::shared_ptr，一次性申请/释放了数据块和控制块的内存，大大提升了性能，并且两者都是连续的，还能避免内存碎片的问题。\n1 2 3  std::shared_ptr\u0026lt;Widget\u0026gt; spw(new Widget);\t//两次内存分配  auto spw = std::make_shared\u0026lt;Widget\u0026gt;();\t//一次内存分配       对于 std::shared_ptr，不建议使用 make 系列函数的额外场景包括：\n  自定义内存管理的类。\n 例如自定义了 operator new 和 operator delete 的类只处理了原本数据块的内存，而并没有考虑到控制块的内存。    内存紧张的系统、非常大的对象，以及指向同一资源的 std::weak_ptr 比对应的 std::shared_ptr 的生命周期更长。\n 如果 std::weak_ptr 比对应的 std::shared_ptr 的生命周期更长，则控制块的内存一直无法释放，而由于 make_shared 的控制块和数据块是连续的，则这块内存也就都一直无法释放，直到 weak_count 清零。      条款 22：使用 PImpl 用法时，将特殊成员函数的定义放到实现文件中   Pimpl 惯用法通过降低类实现和类使用者之间的编译依赖，来减少编译时间。\n  这里以一个 Widget 类举例。由于 Widget 中有着 std::string、std::vector、Gadget 等类型，因此其依赖头文件 \u0026lt;string\u0026gt;、\u0026lt;vector\u0026gt;、Gadget.h。只有这些头文件存在时，它才能通过编译。这些头文件不仅增加了编译时间，还由于它们之间存在依赖关系，如果某个头文件的内容发现了变化，Widget 也需要重新编译。\n1 2 3 4 5 6 7 8 9  class Widget() { //定义在头文件“widget.h” public: Widget(); … private: std::string name; std::vector\u0026lt;double\u0026gt; data; Gadget g1, g2, g3; //Gadget是用户自定义的类型 };   此时就可以考虑使用 Pimpl 惯用法来优化这个问题，即把类的数据成员替代为一个指向实现类（或结构体）的指针，例如下面的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  class Widget //仍然在“widget.h”中 { public: Widget(); ~Widget(); //析构函数在后面会分析  … private: struct Impl; //声明一个 实现结构体  Impl *pImpl; //以及指向它的指针 }; #include \u0026#34;widget.h\u0026#34; //以下代码均在实现文件“widget.cpp”里#include \u0026#34;gadget.h\u0026#34;#include \u0026lt;string\u0026gt;#include \u0026lt;vector\u0026gt; struct Widget::Impl { //含有之前在Widget中的数据成员的  std::string name; //Widget::Impl类型的定义  std::vector\u0026lt;double\u0026gt; data; Gadget g1,g2,g3; }; Widget::Widget() //为此Widget对象分配数据成员 : pImpl(new Impl) {} Widget::~Widget() //销毁数据成员 { delete pImpl; }   此时因为 Widget 不再依赖这些类型（依赖关系从头文件转移到实现文件，用户不可见），也就不再需要这些头文件，大大的提升了编译速度。且因为摆脱了依赖关系，即使这些头文件的内容发生了改变，也不会对 Widget 造成任何影响。\n    对于 std::unique_ptr 类型的 pImpl 指针，需要在类的头文件中声明特殊成员函数，并在实现文件中实现它们。即使默认生成的函数可以正常工作，我们也必须这样做。\n  当我们想使用 std::unique_ptr 来管理 PImpl 指针时，通常会出现一个奇怪的问题，例如下面这个代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  class Widget { //在“widget.h”中 public: Widget(); … private: struct Impl; std::unique_ptr\u0026lt;Impl\u0026gt; pImpl; //使用智能指针而不是原始指针 }; #include \u0026#34;widget.h\u0026#34; //在“widget.cpp”中#include \u0026#34;gadget.h\u0026#34;#include \u0026lt;string\u0026gt;#include \u0026lt;vector\u0026gt; struct Widget::Impl { //跟之前一样  std::string name; std::vector\u0026lt;double\u0026gt; data; Gadget g1,g2,g3; }; Widget::Widget() //根据条款21，通过std::make_unique : pImpl(std::make_unique\u0026lt;Impl\u0026gt;()) //来创建std::unique_ptr {} //上述代码能够通过编译（前提是没有构造Widget对象） #include \u0026#34;widget.h\u0026#34; Widget w; //错误！   声明和定义 Wigdet 的代码都能够通过编译，但是只要我们创建了一个 Widget 对象，就会导致编译不通过。\n当我们进行排错时，发现原因竟然是 Widget 并没有生成析构函数。为什么会这样呢？在我们的印象中，即使我们没有显式声明，编译器也会默认提供，但此时为什么没有呢？这就需要提到 unique_ptr 的内部机制了。\n在介绍之前，首先我们要知道什么是不完整类型。\n 不完整类型指的是该类型缺乏足够的信息例如长度去描述一个完整的对象。也就是说，如果在编译期编译器能计算出一个类型的size，那么它就是一个完整类型，否则就是不完整类型。\n因此声明了但是未定义的类、未知大小的数组、包含不完整成员、void类型，都属于不完整类型。\n 当编译器发现 Widget 中并没有显式定义构造函数时，就会开始自动生成构造函数，而在生成的过程中，就需要考虑析构掉 unique_ptr 所管理的 Impl 对象。由于我们并没有自定义删除器，因此 unique_ptr 内部会尝试调用 delete 来删除 Impl。但在删除之前，unique_ptr 会通过 static_assert 去确保管理的裸指针未涉及非完整类型，这时就会产生报错。\n**为了解决这个问题，我们只需要保证在编译器生成析构函数时，Widget::Impl 是一个完整类型。**因此我们可以将这个过程后推到实现文件中——即在头文件中加上析构函数的声明，避免编译器自动生成。而在声明函数中使用 default 告诉编译器生成析构函数，由于 Widget::Impl 的定义也在定义文件中，此时它已经成为了完整类型，就不再有上面提到的这个问题。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  class Widget { //跟之前一样，在“widget.h”中 public: Widget(); ~Widget(); //只有声明语句  … private: //跟之前一样  struct Impl; std::unique_ptr\u0026lt;Impl\u0026gt; pImpl; }; #include \u0026#34;widget.h\u0026#34; //跟之前一样，在“widget.cpp”中#include \u0026#34;gadget.h\u0026#34;#include \u0026lt;string\u0026gt;#include \u0026lt;vector\u0026gt; struct Widget::Impl { //跟之前一样，定义Widget::Impl  std::string name; std::vector\u0026lt;double\u0026gt; data; Gadget g1,g2,g3; } Widget::Widget() //跟之前一样 : pImpl(std::make_unique\u0026lt;Impl\u0026gt;()) {} Widget::~Widget() = default; //析构函数的定义       上述建议仅适用于 std::unique_ptr，不适用于 std::shared_ptr。\n 对于 std::unique_ptr 而言，自定义删除器是类型的一部分，这虽然使得编译器生成了更小的运行期数据结构以及更快的运行期代码，但这带来了一个后果——如果要使用编译器生成的特殊函数，就要求指涉的类型必须是完整类型。 对于 std::shared_ptr 而言，自定义删除器不是类型的一部分，而是控制块的一部分。这就使得编译器生成了更大的运行期数据结构以及更慢的运行期代码，但使用编译器生成的特殊函数时，就要求指涉的类型就不要求是完整类型。    ","date":"2022-07-12T14:54:13+08:00","permalink":"https://blog.orekilee.top/p/effective-modern-c-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%89%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/","title":"Effective Modern C++ 读书笔记（三）：智能指针"},{"content":"现代 C++ 条款 7：在创建对象时区分 () 与   大括号初始化可以引用的语境最广泛，还可以阻止隐式的窄化类型转换，以及对 C++ 中令人头疼的解析语法具有免疫。\n  大括号初始化又被称为统一初始化，因为其适用于 C++ 中所有的初始化场景：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  //大括号初始化让你可以表达以前表达不出的东西，如指定一个容器中的元素 std::vector\u0026lt;int\u0026gt; v{ 1, 3, 5 }; //v初始内容为1,3,5  //大括号初始化也能被用于为非静态数据成员指定默认初始值，C++11允许\u0026#34;=\u0026#34;初始化也拥有这种能力： class Widget{ … private: int x{ 0 }; //没问题，x初始值为0  int y = 0; //也可以  int z(0); //错误！ } //不可拷贝的对象可以使用大括号初始化或者小括号初始化，但是不能使用=初始化： std::atomic\u0026lt;int\u0026gt; ai1{ 0 }; //没问题 std::atomic\u0026lt;int\u0026gt; ai2(0); //没问题 std::atomic\u0026lt;int\u0026gt; ai3 = 0; //错误！     大括号初始化禁止内置类型间隐式的窄化类型转换，而小括号为了兼容老旧代码则放宽了这一限制：\n1 2 3 4  double x, y, z; int sum1{ x + y + z }; //错误！double的和可能不能表示为int int sum2(x + y +z); //可以（表达式的值被截为int）     C++ 规定任何能被解析为声明的都要被解析为声明，因此有时候想要使用默认构造函数或者全缺省构造函数创建对象时，就会被视为一个函数声明，而大括号初始化则会对此免疫：\n1 2 3 4 5 6 7 8 9 10 11 12  class Widget{ Widget(int x = 0){ x_ = x; } private: int x_; } Widget w1(10); //使用实参10调用Widget的一个构造函数 Widget w2(); //最令人头疼的解析！声明一个函数w2，返回Widget Widget w3{}; //调用没有参数的构造函数构造对象       在构造函数重载决议期间，大括号初始化会尽可能的与带有 std::initializer_list 类型的形参相匹配，即使其他重载版本有着更加匹配的形参。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  class Widget { public: Widget(int i, bool b); Widget(int i, double d); Widget(std::initializer_list\u0026lt;long double\u0026gt; il); … }; Widget w1(10, true); //使用小括号初始化，同之前一样调用第一个构造函数  Widget w2{10, true}; //使用花括号初始化，但是现在  //调用std::initializer_list版本构造函数  //(10 和 true 转化为long double)  Widget w3(10, 5.0); //使用小括号初始化，同之前一样调用第二个构造函数  Widget w4{10, 5.0}; //使用花括号初始化，但是现在  //调用std::initializer_list版本构造函数  //(10 和 5.0 转化为long double)       () 和 {} 在有些情况下结果可能大相径庭。\n  大括号在初始化时会调用 std::initializer_list 构造函数，而小括号则调用非 std::initializer_list 构造函数：\n1 2 3 4 5 6  std::vector\u0026lt;int\u0026gt; v1(10, 20); //使用非std::initializer_list构造函数  //创建一个包含10个元素的std::vector，  //所有的元素的值都是20 std::vector\u0026lt;int\u0026gt; v2{10, 20}; //使用std::initializer_list构造函数  //创建包含两个元素的std::vector，  //元素的值为10和20       在模板类创建对象时，选择使用小括号和大括号是一个棘手的问题。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  template\u0026lt;typename T, //要创建的对象类型  typename... Ts\u0026gt; //要使用的实参的类型 void doSomeWork(Ts\u0026amp;\u0026amp;... params) { create local T object from params... … } T localObjectA(std::forward\u0026lt;Ts\u0026gt;(params)...); //使用小括号 T localObjectB{std::forward\u0026lt;Ts\u0026gt;(params)...}; //使用大括号  //假设以下面这样的参数进行调用，此时就会如上一条一样，如果使用小括号则调用非std::initializer_list构造函数，而使用大括号则调用std::initializer_list构造函数，出现两种结果。 std::vector\u0026lt;int\u0026gt; v; … doSomeWork\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;(10, 20);   这也是 std::make_unique 与 std::make_shared 面临的问题，它们的解决方法是是使用小括号，并将这个决定记录在文档中，作为接口的一部分。\n    条款 8：优先选用 nullptr，而非 0 或 NULL   相对于 0 或者 NULL，优先选用 nullptr。\n 在 C++ 中，0 是一个 int 而不是指针，同样 NULL 也只是一个整数类型（根据编译器的实现不同，可能会为 int 或 long）。它们都是整数类型而不是指针类型，如果采用 C++ 98 的写法使用它们作为空指针，可能会出现未定义的行为。    避免在整型和指针类型之间重载。\n  1 2 3 4 5 6 7 8 9 10  void f(int); //三个f的重载函数 void f(bool); void f(void*); f(0); //调用f(int)而不是f(void*)  f(NULL); //可能不会被编译，一般来说调用f(int)，  //绝对不会调用f(void*)  f(nullptr); //调用重载函数f的f(void*)版本       条款 9：优先选用 using 而非 typedef   typedef 不支持模板化，但别名声明支持。\n  假设我们需要定义一个链表，并且这个链表使用了我们自定义的内存分配器：\n1 2 3 4 5 6 7 8 9 10  //别名模板 template\u0026lt;typename T\u0026gt; //MyAllocList\u0026lt;T\u0026gt;是 using MyAllocListA = std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt;; //std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt;  //的同义词  //如果使用typedef，则只能在一个模板类中用typedef来为该类型起别名 template\u0026lt;typename T\u0026gt; //MyAllocList\u0026lt;T\u0026gt;是 struct MyAllocListB { //std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt;  typedef std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt; type; //的同义词 };       别名模板可以免写 [::类型名] 后缀，并且不会像 typedef 在模板中使用时还需要增加 typename 前缀。\n  由于 typedef 是模板类中的某个类型的别名，因此还需要在作用域后面加上 [::类型名] 后缀 ：\n1 2  MyAllocListA\u0026lt;Widget\u0026gt; lw; MyAllocListB\u0026lt;Widget\u0026gt;::type lw; //typedef需要加上::type     C++ 中规定，在使用依赖类型时必须要加上 typename 前缀，而 MyAllocListB\u0026lt;T\u0026gt;::type 依赖于模板参数 T，因此它是一个依赖类型。而别名模板 MyAllocListA\u0026lt;T\u0026gt; 是一个类型的名称，因此它是一个非依赖类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  //typedef需要加上typename来告诉编译器这是一个类型的别名 template\u0026lt;typename T\u0026gt; class Widget { //Widget\u0026lt;T\u0026gt;含有一个 private: //MyAllocLIst\u0026lt;T\u0026gt;对象  typename MyAllocList\u0026lt;T\u0026gt;::type list; //作为数据成员  … }; //而using声明的别名模板则不需要 template\u0026lt;typename T\u0026gt; class Widget { private: MyAllocList\u0026lt;T\u0026gt; list; //没有“typename” };   为什么要有这个规定呢？因为对于编译器来说，它无法判断 MyAllocListB\u0026lt;T\u0026gt;::type 究竟代表的是一个类型的名字，还是一个变量的名字，又或是别的一些东西。\n    条款 10：优先选用限定作用域的枚举而非未限定作用域的枚举   C++ 98 风格的枚举类型被称为未限定作用域的枚举。\n  1 2  enum Color { black, white, red }; //C++98 未限域枚举 enum class Color { black, white, red }; //C++11 限域枚举类       限定作用域的枚举类型仅能在枚举类型中可见。它们只能通过强制转换才可以转换到别的类型。\n  未限域枚举会将枚举名泄露，从而导致命名污染，而限域枚举类仅能在枚举类型中可见。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  //未限域枚举会导致枚举名泄露 enum Color { black, white, red }; //black, white, red在  //Color所在的作用域 auto white = false; //错误! white早已在这个作用  //域中声明  //限域枚举类仅能在枚举类型中可见 enum class Color { black, white, red }; //black, white, red  //限制在Color域内 auto white = false; //没问题，域内没有其他“white”  Color c = white; //错误，域中没有枚举名叫white  Color c = Color::white; //没问题 auto c = Color::white; //也没问题（也符合Item5的建议）     未限域枚举可能会隐式转换为整数类型或者浮点数类型，而限域枚举类则不会。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  enum Color { black, white, red }; //未限域枚举  std::vector\u0026lt;std::size_t\u0026gt; primeFactors(std::size_t x); Color c = red; … //未限域枚举在比较时会隐式类型转换 if (c \u0026lt; 14.5) { auto factors = primeFactors(c); … } enum class Color { black, white, red }; //限域枚举类  Color c = Color::red; ... //只能通过强制类型转换 if (static_cast\u0026lt;double\u0026gt;(c) \u0026lt; 14.5) { auto factors = primeFactors(static_cast\u0026lt;std::size_t\u0026gt;(c)); … }       两种枚举类型都支持执行底层的存储类型。限定作用域的枚举类型默认是 int，而不限范围的枚举类型则没有默认底层类型。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  //默认情况下，限域枚举的底层类型是int： enum class Status; //底层类型是int  //如果默认的int不适用，你可以重写它： enum class Status: std::uint32_t; //Status的底层类型  //是std::uint32_t  //（需要包含 \u0026lt;cstdint\u0026gt;）  //为了高效使用内存，编译器通常在确保能包含所有枚举值的前提下为enum选择一个最小的底层类型。在一些情况下，编译器将会优化速度，舍弃大小，这种情况下它可能不会选择最小的底层类型，而是选择对优化大小有帮助的类型。  //只需要表示三个值，底层类型是int enum Color { black, white, red }; //这里值的范围从0到0xFFFFFFFF，编译器会选择一个能表示这个范围的的整型类型 enum Status { good = 0, failed = 1, incomplete = 100, corrupt = 200, indeterminate = 0xFFFFFFFF };       限定作用域的枚举类型总是可以前置声明，而未限定作用域的枚举只有在制定了默认底层类型的时候才可以前置声明。\n  1 2 3 4 5 6 7  enum Color; //错误！ enum class Color; //没问题  //当指定默认底层类型时，才可以前置声明 enum Color: std::uint8_t; //非限域enum前向声明  //底层类型为  //std::uint8_t       条款 11：优先使用 delete 函数而非 private 未定义函数   优先选择使用 delete 而非 private。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // C++98写法，此时仍然可以通过成员函数或者类的友元调用它们，由于没有函数定义，在链接时会报错 template \u0026lt;class charT, class traits = char_traits\u0026lt;charT\u0026gt; \u0026gt; class basic_ios : public ios_base { public: … private: basic_ios(const basic_ios\u0026amp; ); // not defined  basic_ios\u0026amp; operator=(const basic_ios\u0026amp;); // not defined }; // C++11写法，无法通过任何途径调用这些函数 template \u0026lt;class charT, class traits = char_traits\u0026lt;charT\u0026gt; \u0026gt; class basic_ios : public ios_base { public: … basic_ios(const basic_ios\u0026amp; ) = delete; basic_ios\u0026amp; operator=(const basic_ios\u0026amp;) = delete; … };       任何函数都可以删除，包括非成员函数和模板实例。\n  由于 C++ 继承了 C 的隐式类型转换，所有的数值类型在传参给一个参数为 int 的函数时都能够隐式转换为 int，为了避免这样的操作带来的一些错误行为，delete 允许删除非成员函数，因此我们可以 delete 它们的重载函数：\n1 2 3 4 5 6 7 8  bool isLucky(int number); //原始版本 bool isLucky(char) = delete; //拒绝char bool isLucky(bool) = delete; //拒绝bool bool isLucky(double) = delete; //拒绝float和double  if (isLucky(\u0026#39;a\u0026#39;)) … //错误！调用deleted函数 if (isLucky(true)) … //错误！ if (isLucky(3.5f)) … //错误！     delete 同样也能删除某个特化的模板类，甚至是类内部的模板函数的特化版本，这都是 private 无法做到的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  template\u0026lt;typename T\u0026gt; void processPointer(T* ptr); //删除void*和char*的特化版本 template\u0026lt;\u0026gt; void processPointer\u0026lt;void\u0026gt;(void*) = delete; template\u0026lt;\u0026gt; void processPointer\u0026lt;char\u0026gt;(char*) = delete; //删除类内部的模板函数的某个特化版本 class Widget { public: … template\u0026lt;typename T\u0026gt; void processPointer(T* ptr) { … } … }; template\u0026lt;\u0026gt; //还是public， void Widget::processPointer\u0026lt;void\u0026gt;(void*) = delete; //但是已经被删除了       条款 12：为重写函数添加 override   为需要重写的函数加上 override。\n  1 2 3 4 5 6 7 8 9  class Base { public: virtual void mf1() const; }; class Derived: public Base { public: virtual void mf1() const override; };   override 用于确保派生类虚函数是否构成重写，当不构成重写时会编译错误。\n    成员函数引用饰词可以让我们区别对待左值和右值对象（针对 *this）。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  class Widget { public: using DataType = std::vector\u0026lt;double\u0026gt;; … DataType\u0026amp; data() \u0026amp; //对于左值Widgets,  { return values; } //返回左值  DataType data() \u0026amp;\u0026amp; //对于右值Widgets,  { return std::move(values); } //返回右值  … private: DataType values; }; auto vals1 = w.data(); //调用左值重载版本的Widget::data，  //拷贝构造vals1 auto vals2 = makeWidget().data(); //调用右值重载版本的Widget::data,  //移动构造vals2       条款 13：优先选用 const_iterator 而非 iterator   优先选用 const_iterator 而非 iterator（对于不需要修改的操作，尽可能收缩权限，避免出现意外）。\n  若要实现通用的代码，优先选用非成员函数版本的 begin、end 和 rbegin 等，而不是成员函数版本。\n  比起成员函数版本，非成员函数版本更加的通用，因为其能够支持原生数组，以及一些以自由函数提供接口的第三方库，例如下面这个通用版本的 findAndInsert：\n1 2 3 4 5 6 7 8 9 10 11 12 13  template\u0026lt;typename C, typename V\u0026gt; void findAndInsert(C\u0026amp; container, //在容器中查找第一次  const V\u0026amp; targetVal, //出现targetVal的位置，  const V\u0026amp; insertVal) //然后在那插入insertVal { using std::cbegin; using std::cend; auto it = std::find(cbegin(container), //非成员函数cbegin  cend(container), //非成员函数cend  targetVal); container.insert(it, insertVal); }       条款 14：如果函数不抛出异常，就使用 noexcept   noexcept 是函数接口的一部分，这意味着调用方可能对它有依赖。\n  带有 noexcept 的函数更容易优化。\n  在运行期时，如果一个异常跳出函数的作用域，则说明函数的异常说明被违反。在 C++ 98 的异常说明中，调用栈会展开至函数的调用者，并在执行一系列动作后将程序终止。而 C++ 11 异常说明的运行时行为又有所不同，调用栈只是可能在程序终止前展开。\n  展开调用栈和可能展开调用栈两者对于代码生成有很大的影响。在带有 noexcept 声明的函数中，优化器不需要在异常传出函数的前提下，保证运行时栈处于可展开的状态；也不需要在异常离开函数时保证 noexcept 函数中的对象按照构造的逆序析构。而那些以 throw() 异常声明的函数则享受不到这样的优化灵活性，和那些没有加上异常声明的函数一样。\n1 2 3  RetType function(params) noexcept; //C++11写法 极尽所能优化 RetType function(params) throw(); //C++98写法 较少优化 RetType function(params); //较少优化       noexcept 对于移动操作、swap、资源释放函数和析构函数非常有用。\n  大多数函数都是异常中立的，不具备 noexcept。\n 这类函数自身并不抛出异常，但是它们所调用的函数则可能会抛出异常。为了处理这种情况，异常中立的函数会允许那些抛出的异常就由它传至调用栈中更深的一层，直到遇到异常处理的程序。    条款 15：尽可能使用 constexpr   constexpr 对象都具备 const 属性，并由编译期已知的值完成初始化。\n  1 2 3 4 5 6 7 8 9  int sz; //non-constexpr变量 … constexpr auto arraySize1 = sz; //错误！sz的值在编译期不可知  std::array\u0026lt;int, sz\u0026gt; data1; //错误！一样的问题  constexpr auto arraySize2 = 10; //没问题，10是编译期可知常量  std::array\u0026lt;int, arraySize2\u0026gt; data2; //没问题, arraySize2是constexpr     注意：const 不提供 constexpr 所能保证之事，因为 const 对象不需要在编译期初始化它的值。\n1 2 3 4 5  int sz; //和之前一样 … const auto arraySize = sz; //没问题，arraySize是sz的const复制  std::array\u0026lt;int, arraySize\u0026gt; data; //错误，arraySize值在编译期不可知   简而言之，所有 constexpr 对象都是 const，但不是所有 const 对象都是 constexpr。\n    constexpr 函数在调用时如果传入的实参也是编译期已知的，则会产出编译期结果。\n  当 constexpr 函数接受的参数为编译期已知的，则会在编译期间计算出结果。而如果参数是编译期未知的，则它的运作与普通函数一样，在运行期去执行计算。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  constexpr int pow(int base, int exp) noexcept //constexpr函数 { //C++11写法，constexpr 函数的代码不超过一行语句 \t// return (exp == 0 ? 1 : base * pow(base, exp - 1));  //C++14放宽了限制  auto result = 1; for (int i = 0; i \u0026lt; exp; ++i) result *= base; return result; } //传入编译器实参，生成编译器结果 constexpr auto numConds = 5; std::array\u0026lt;int, pow(3, numConds)\u0026gt; results; //传入运行时参数，则在运行时进行计算 auto base = readFromDB(\u0026#34;base\u0026#34;); //运行时获取这些值 auto exp = readFromDB(\u0026#34;exponent\u0026#34;); auto baseToExp = pow(base, exp); //运行时调用pow函数       比起非 constexpr 的对象和函数，constexpr 可以用在作用域更广的语境（编译期、运行期）中。\n  条款 16：保证 const 成员函数的线程安全   保证 const 成员函数的线程安全性，除非可以确定他们不会用在并发语境中。\n const 本身代表了只读的语义，但也有一些例外情况，例如存在 mutable 变量时，这些变量能够突破 const 的限制，此时在并发环境下就会出现数据竞争的情况。    std::atmoic 类型的变量会比使用 std::mutex 性能更佳，但前者仅适用于单个变量或者内存区域的操作。\n  条款 17：特殊成员函数的生成机制  特殊成员函数指的是 C++ 中会自动生成的成员函数，如：默认构造函数、析构函数、拷贝操作、移动操作。 移动操作只有在类中未包含用户显式声明的拷贝操作、移动操作和析构函数时才会生成。 拷贝构造函数和拷贝赋值运算符仅会在用户没有显式声明它们的时候才会自动生成，且如果该类声明了它们的移动版本，则拷贝版本会被 delete。在新版本的 C++ 中，显式声明析构函数后，自动生成拷贝操作是被废弃的行为。 成员函数模板在任何情况下都不会抑制特殊成员函数的生成。  ","date":"2022-07-12T13:54:13+08:00","permalink":"https://blog.orekilee.top/p/effective-modern-c-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%BA%8C%E7%8E%B0%E4%BB%A3-c-/","title":"Effective Modern C++ 读书笔记（二）：现代 C++"},{"content":"类型推导 条款 1：模板类型推导   在模板类型推导时，具有引用类型的实参会被视为非引用类型，即引用会被忽视。\n  1 2 3 4 5 6 7 8 9 10  template\u0026lt;typename T\u0026gt; void f(T \u0026amp; param); int x = 27; //x是int const int cx = x; //cx是const int const int \u0026amp; rx = cx; //rx是指向const int的引⽤  f(x); //T是int，param的类型是int\u0026amp; f(cx); //T是const int，param的类型是const int \u0026amp; f(rx); //T是const int，param的类型是const int \u0026amp;       对万能引用形参进行推导时，左值参数会特殊处理。\n 左值：T 和 paramType 都为左值引用。 右值：与第一条原则相同，即 paramType 为右值引用，T 为非引用类型。    对按值传递的形参进行推导时，若实参类型中带有 const 或 volatile 饰词，会将其忽视，当作不带这些饰词的类型。\n  在模板类型推导时，数组或者函数类型的实参会退化成对应的指针，除非它们用来初始化引用。\n  1 2 3 4 5 6 7 8 9 10 11 12  void someFunc(int, double); //someFunc是一个函数，  //类型是void(int, double) template\u0026lt;typename T\u0026gt; void f1(T param); //传值给f1  template\u0026lt;typename T\u0026gt; void f2(T \u0026amp; param); //传引用给f2  f1(someFunc); //param被推导为指向函数的指针，  //类型是void(*)(int, double) f2(someFunc); //param被推导为指向函数的引用，  //类型是void(\u0026amp;)(int, double)       条款 2：auto 类型推导   在正常情况下，auto 类型推导和模板类型推导的结果是一样的。但是也有例外，当我们使用大括号括起的初始化表达式时，auto 会将其推导为 std::initializer_list，而模板推导则不会。\n  1 2 3 4 5 6  auto x = { 11, 23, 9 }; //x的类型是std::initializer_list\u0026lt;int\u0026gt;  template\u0026lt;typename T\u0026gt; //带有与x的声明等价的 void f(T param); //形参声明的模板  f({ 11, 23, 9 }); //错误！不能推导出T   若是想要推导出 T 的类型，则需要将模板中的 param 指定为 std::initializer_list\u0026lt;T\u0026gt; ：\n1 2 3 4 5  template\u0026lt;typename T\u0026gt; void f(std::initializer_list\u0026lt;T\u0026gt; initList); f({ 11, 23, 9 }); //T被推导为int，initList的类型为  //std::initializer_list\u0026lt;int\u0026gt;       在函数返回值或 lambda 表达式中使用 auto 时，使用模板类型推导而非 auto 类型推导。\n  自从 C++ 14 开始允许使用 auto 来推导函数返回值，但是其虽然是 auto，但是实际上确实模板类型推导。我们怎样确认呢？可以尝试返回一个大括号括起的初始化表达式。\n1 2 3 4 5 6 7 8 9 10 11  auto createInitList() { return { 1, 2, 3 }; //错误！不能推导{ 1, 2, 3 }的类型 } std::vector\u0026lt;int\u0026gt; v; … auto resetV = [\u0026amp;v](const auto\u0026amp; newValue){ v = newValue; }; //C++14 … resetV({ 1, 2, 3 }); //错误！不能推导{ 1, 2, 3 }的类型       条款 3：decltype   通常 decltype 会得出变量的类型并且不做任何修改。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  const int i = 0; //decltype(i)是const int  bool f(const Widget\u0026amp; w); //decltype(w)是const Widget\u0026amp;  //decltype(f)是bool(const Widget\u0026amp;)  struct Point{ int x,y; //decltype(Point::x)是int }; //decltype(Point::y)是int  Widget w; //decltype(w)是Widget  if (f(w))… //decltype(f(w))是bool  template\u0026lt;typename T\u0026gt; //std::vector的简化版本 class vector{ public: … T\u0026amp; operator[](std::size_t index); … }; vector\u0026lt;int\u0026gt; v; //decltype(v)是vector\u0026lt;int\u0026gt; … if (v[0] == 0)… //decltype(v[0])是int\u0026amp;       对于类型为 T 的左值表达式，decltype 总是推导出类型 T\u0026amp;。\n  1 2 3 4 5 6 7 8 9 10 11 12  decltype(auto) f1() { int x = 0; … return x; //decltype(x）是int，所以f1返回int } decltype(auto) f2() { int x = 0; return (x); //decltype((x))是int\u0026amp;，所以f2返回int\u0026amp; }       C++ 14 支持 decltype(auto)，与 auto 一样，它会依据初始化表达式来推导类型，但它使用的是 decltype 的规则。\n  1 2 3 4 5 6 7 8  Widget w; const Widget\u0026amp; cw = w; auto myWidget1 = cw; //auto类型推导  //myWidget1的类型为Widget decltype(auto) myWidget2 = cw; //decltype类型推导  //myWidget2的类型是const Widget\u0026amp;       条款 4：查看类型推导结果的方法   利用 IDE 编辑器、编译器错误消息 和 Boost.TypeIndex 库通常能够查看到推导的类型。\n  工具判断的结果可能不准确，因此理解 C++ 类型推导规则是必要的。\n  ​\nauto 条款 5：优先选用 auto，而非显式类型声明   auto 变量必须初始化，它通常可以避免⼀些移植性和效率性的问题，也使得重构更⽅便，比显式指定类型更加简洁。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  //大大的简化了代码 //使用auto auto derefUPLess = [](const std::unique_ptr\u0026lt;Widget\u0026gt; \u0026amp;p1, const std::unique_ptr\u0026lt;Widget\u0026gt; \u0026amp;p2) { return *p1 \u0026lt; *p2; }; //显式执行类型 std::function\u0026lt;bool(const std::unique_ptr\u0026lt;Widget\u0026gt; \u0026amp;, const std::unique_ptr\u0026lt;Widget\u0026gt; \u0026amp;)\u0026gt; derefUPLess = [](const std::unique_ptr\u0026lt;Widget\u0026gt; \u0026amp;p1, const std::unique_ptr\u0026lt;Widget\u0026gt; \u0026amp;p2) { return *p1 \u0026lt; *p2; }; //可能会避开一些潜在的风险 unordered_map\u0026lt;std::string, int\u0026gt; m; //看起来没有问题，但是实际unordered_map的key是cosnt std::string //类型不匹配，虽然没有报错，但是编译器会将m的值拷贝到一个临时变量中，再将临时变量的引用绑定到p上，且在循环结束后再将这个临时变量销毁，增加了大量拷贝、销毁资源的成本 for(const std::pair\u0026lt;std::string, int\u0026gt;\u0026amp; p : m) { … //用p做一些事 } for(const auto\u0026amp; p : m) { … }       auto 类型的变量有着条款 2 和 条款 6 中描述的毛病。\n  条款 6：当 auto 推导的类型不符合要求时，使用显式类型初始化   隐形的代理类可能会导致 auto 根据初始化表达式推导出错误的类型。\n  1 2 3 4 5 6  std::vector\u0026lt;bool\u0026gt; features(const Widget\u0026amp; w); Widget w; … bool var_a = features(w)[5]; //bool，存在隐式类型转换 auto vat_b = features(w)[5]; //代理类型std::vector\u0026lt;bool\u0026gt;::reference，如果不明确类型则可能导致未定义的行为。   对于 std::vector\u0026lt;bool\u0026gt; 来说，为了节省空间，底层存储时使用了 1 bit 而非 1 byte 的 bool 类型来存储数据。由于 C++ 禁止对 bits 的引用，因此当我们调用 operator[] 时返回的其实是一个代理类型 std::vector\u0026lt;bool\u0026gt;::reference，其能够模拟 bool\u0026amp; 的行为。\n    显式类型的初始化惯用法会强制 auto 推导出想要的类型。\n  1 2 3 4  double calcEpsilon(); //返回公差值  float ep = calcEpsilon(); //double到float隐式转换 auto ep = static_cast\u0026lt;float\u0026gt;(calcEpsilon());\t//显式类型初始器惯用法       ","date":"2022-07-12T12:54:13+08:00","permalink":"https://blog.orekilee.top/p/effective-modern-c-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E4%B8%80%E7%B1%BB%E5%9E%8B%E6%8E%A8%E5%AF%BC%E4%B8%8E-auto/","title":"Effective Modern C++ 读书笔记（一）：类型推导与 auto"},{"content":"基本概念 什么是 GDB？ GDB（GNU Debugger），是一个由 GNU 开源组织发布的、UNIX/LINUX 操作系统下的、基于命令行的、功能强大的程序调试工具；GDB 支持断点、单步执行、打印变量、观察变量、查看寄存器、查看堆栈等调试手段；GDB 支持调试多种编程语言编写的程序，包括 C、C++、Go、Objective-C、OpenCL、Ada 等。实际场景中，GDB 更常用来调试 C 和 C++ 程序。\nGDB 可以用来做些什么？ 一般来说，借助 GDB 调试器可以实现以下几个功能：\n 程序启动时，可以按照我们自定义的要求运行程序，例如设置参数和环境变量。 可使被调试程序在指定代码处暂停运行，并查看当前程序的运行状态（例如当前变量的值，函数的执行结果等），即支持断点调试。 程序执行过程中，可以改变某个变量的值，还可以改变代码的执行顺序，从而尝试修改程序中出现的逻辑错误。  调试模型 根据 GDB 程序与被调试程序是否运行在同一台机器中，可以把 GDB 的调试模型分为以下两种：\n 本地调试 远程调试  本地调试 本地调试指的是调试程序和被调试程序运行在同一台机器中，如下图所示：\n 可视化调试程序只是对 GDB 操作的一层封装，例如 Visual Studio、CLion 等 IDE 中的可视化调试。当然我们也可以直接通过 bash 来手动输入调试命令。\n 远程调试 调试程序运行在一台机器中，被调试程序运行在另一台机器中，如下图所示：\n GdbServer 的主要工作是负责完成 GDB 与目标程序之间的通信，其采用了 RSP（GDB Remote Serial Protocol） 协议。\n 安装 GDB 这里以 CentOS 8 举例，来演示 GDB 的安装。\n首先查看当前机器中是否存在 GDB：\n1  gdb -v   如果提示 bash: gdb: command not found，则说明当前机器没有，继续执行下面的步骤，反之则无需安装。\n通过 yum 安装 GDB：\n1  sudo yum -y install gdb   如果使用的是官方的 yum 下载源，这里就会报错 Error: Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist，这主要是因为在 2021 年底官方就停止对 CentOS 8 提供服务，这时我们就需要将 yum 源切换为国内的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 进入yum的repos目录 cd /etc/yum.repos.d/ # 修改所有的CentOS文件内容 sed -i \u0026#39;s/mirrorlist/#mirrorlist/g\u0026#39; /etc/yum.repos.d/CentOS-* sed -i \u0026#39;s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\u0026#39; /etc/yum.repos.d/CentOS-* # 更新yum源为阿里镜像 wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo yum clean all yum makecache   这时候重新通过 yum 安装即可。\n实战 启动调试  GDB 的使用前提：需要在编译时加上-g 参数，保留调试信息，否则会提示 no debugging symbols found，无法使用 GDB 进行调试。\n 调试未运行的程序 无参数 使用 GDB 可执行程序名 开启调试，输入 run 命令运行程序：\n1 2  gdb test1 (gdb) run   有参数 直接在 run 后面跟上参数：\n1 2  gdb test1 (gdb) run \u0026#34;hello world\u0026#34;   也可以通过 set args 命令指定参数列表：\n1 2 3 4  gdb test1 (gdb) set args \u0026#34;hello world\u0026#34; (gdb) show args # 查看参数列表 (gdb) run   调试运行中的程序 如果程序已经处于运行中的状态，那我们该如何对其进行调试呢？\n已生成调试信息 对于已生成调试信息的，我们只需要找到它的进程 id，再使用 attach 命令绑定进程即可：\n1 2 3 4 5 6 7 8 9 10 11 12  # 步骤一：找到进程id ps -ef|grep 进程名 # 或者 pidof 进程名 # 步骤二：开启GDB调试 gdb # 步骤三：attach绑定对应的进程id attach [进程id] # 或者在启动时直接指定程序名与进程id gdb [程序名] --pid [进程id]   未生成调试信息 对于已运行且未生成调试信息的程序，我们可以重新编译出一个带调试信息的版本，再使用 file 命令将这个版本的符号表读取出来，此时我们再次 attach 程序，就能够进行调试了，并且不需要重新启动程序：\n1 2 3 4 5 6  # 步骤一：重新编译一个带调试信息的版本 # 步骤二：使用file加载这个版本的符号表 file [程序名] # 接下来的步骤与已生成调试信息的一样   调试 core 文件 当一个程序因为出错而导致异常中断的时候，操作系统会将程序当前的状态（如程序运行时的内存，寄存器状态，堆栈指针，内存管理信息）保存为一个 core 文件。我们可以通过使用 GDB 来调试这个文件，来迅速定位导致程序出错的问题。\n配置 core 文件生成 首先我们需要使用 ulimit -a 命令查看系统有没有限制 core 文件的生成：\n1 2 3  ulimit -c unlimited # 代表没有限制 0 # 如果结果为零则代表无法生成，如果为其他数字则代表限制生成的个数   配置 coredump 生成，有临时配置和永久配置两种。\n  临时配置：只需要简单的命令即可，但是退出 bash 后就会失效。\n  1 2  $ ulimit -c unlimited #表示不限制core文件大小 $ ulimit -c n #n为数字，表示core文件大小上限，单位为块，一块默认为512字节       永久配置：需要修改内核参数，指定 core 文件名、存放路径与永久配置。\n  1 2 3 4 5 6 7 8 9 10  mkdir -p /www/coredump/ chmod 777 /www/coredump/ /etc/profile ulimit -c unlimited /etc/security/limits.conf * soft core unlimited echo \u0026#34;/www/coredump/core-%e-%p-%h-%t\u0026#34; \u0026gt; /proc/sys/kernel/core_pattern       调试 core 文件 这里以一个简单的访问空指针的示例来演示：\n1 2 3 4 5 6 7 8 9 10  #include \u0026lt;stdio.h\u0026gt; void fault_test(void) { *((int *)NULL) = 100;\t//解引用空指针并尝试修改它的值 } int main() { fault_test(); return 0; }   当我们编译后执行程序时，此时就会因为访问空指针而导致段错误：\n1 2 3  gcc -g -o test_coredump test_coredump.c ./test_coredump Segmentation fault (core dumped)   此时查看 core 文件的位置：\n1 2  cat /proc/sys/kernel/core_pattern |/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e   这时我们发现这个路径下并没有 core 文件，上述信息表明了 core 文件已经被系统转储，此时有两种方法获取到 core 文件：\n  修改生成路径：\n  1 2  # 直接在程序所在目录生成core文件 echo \u0026#34;core-%e-%p-%t\u0026#34; \u0026gt; /proc/sys/kernel/core_pattern       取出 core 文件：\n 执行 coredumpctl 命令，查看所有 coredump 的程序，找到我们需要调试的那个。 执行 coredumpctl -o 自定义core文件名 dump Pid 取回 core 文件。    当生成完毕 core 文件后，执行以下命令开始调试：\n1  gdb 程序名 core文件名   此时我们就能够看到 coredump 的原因：\n1 2 3 4 5  [New LWP 1065384] Core was generated by `./test_coredump\u0026#39;. Program terminated with signal SIGSEGV, Segmentation fault. #0 fault_test () at test_coredump.c:4 4\t*((int *)NULL) = 100;   接着执行 where 命令就能够查看调用堆栈：\n1 2 3  (gdb) where #0 fault_test () at test_coredump.c:4 #1 0x0000000000400551 in main () at test_coredump.c:8   常用命令 在 GDB 中，最常用的命令有以下这些：\n下面就来详细的介绍这些命令。\n断点 断点是最常用的调试功能之一，它可以让程序中断在需要的地方，从而方便我们对程序分析。GDB 中断点主要分为以下三类：\n breakpoint。 watchpoint。 catchpoint。  breakpoint 可以根据行号、函数、条件生成断点，下面是相关命令以及对应的作用说明：\n   命令 作用     break [file]:function 在文件file的function函数入口设置断点   break [file]:line 在文件file的第line行设置断点   info breakpoints 查看断点列表   break [+-]offset 在当前位置偏移量为[+-]offset处设置断点   break *addr 在地址addr处设置断点   break \u0026hellip; if expr 设置条件断点，仅仅在条件满足时   ignore n count 接下来对于编号为n的断点忽略count次   clear 删除所有断点   clear function 删除所有位于function内的断点   delete n 删除指定编号的断点   enable n 启用指定编号的断点   disable n 禁用指定编号的断点   save breakpoints file 保存断点信息到指定文件   source file 导入文件中保存的断点信息   break 在下一个指令处设置断点   clear [file:]line 删除第line行的断点    watchpoint watchpoint 是一种特殊类型的断点，其类似于一个表达式的监视器，即当某个表达式改变了值时，它就会让 GDB 发出暂停执行的命令。\n   命令 作用     watch variable 设置变量数据断点   watch var1 + var2 设置表达式数据断点   rwatch variable 设置读断点，仅支持硬件实现   awatch variable 设置读写断点，仅支持硬件实现   info watchpoints 查看数据断点列表   set can-use-hw-watchpoints 0 强制基于软件方式实现    使用数据断点时，需要注意：\n 当监控变量为局部变量时，一旦局部变量失效，数据断点也会失效 如果监控的是指针变量p，则watch *p监控的是p所指内存数据的变化情况，而watch p监控的是p指针本身有没有改变指向  最常见的数据断点应用场景：定位堆上的结构体内部成员何时被修改。由于指针一般为局部变量，为了解决断点失效，一般有两种方法。\n   命令 作用     print \u0026amp;variable 查看变量的内存地址   watch *(type *)address 通过内存地址间接设置断点   watch -l variable 指定location参数   watch variable thread 1 仅编号为1的线程修改变量var值时会中断    catchpoint catchpoint 是捕获断点，其主要监测信号的产生。\n   命令 含义     catch fork 程序调用fork时中断   tcatch fork 设置的断点只触发一次，之后被自动删除   catch syscall ptrace 为ptrace系统调用设置断点    调用栈    命令 作用     backtrace [n] 打印栈帧   frame [n] 选择第n个栈帧，如果不存在，则打印当前栈帧   up n 选择当前栈帧编号+n的栈帧   down n 选择当前栈帧编号-n的栈帧   info frame [addr] 描述当前选择的栈帧   info args 当前栈帧的参数列表   info locals 当前栈帧的局部变量    输出 变量信息    命令 作用     whatis variable 查看变量的类型   ptype variable 查看变量详细的类型信息   info variables var 查看定义该变量的文件，不支持局部变量    字符串    命令 作用     x/s str 打印字符串   set print elements 0 打印不限制字符串长度/或不限制数组长度   call printf(\u0026quot;%s\\n\u0026quot;,xxx) 这时打印出的字符串不会含有多余的转义符   printf \u0026ldquo;%s\\n\u0026rdquo;,xxx 同上    数组    命令 作用     print *array@10 打印从数组开头连续10个元素的值   print array[60]@10 打印array数组下标从60开始的10个元素，即第60~69个元素   set print array-indexes on 打印数组元素时，同时打印数组的下标    指针    命令 作用     print ptr 查看该指针指向的类型及指针地址   print *(struct xxx *)ptr 查看指向的结构体的内容    内存地址 使用 x 命令来打印内存的值，格式为 x/nfu addr，以 f 格式打印从 addr 开始的 n 个长度单元为 u 的内存值。\n n：输出单元的个数 f：输出格式，如 x 表示以 16 进制输出，o 表示以 8 进制输出，默认为 x u：一个单元的长度，b 表示 1 个 byte，h 表示 2 个 byte（half word），w 表示 4 个 byte，g 表示 8 个 byte（giant word）     命令 作用     x/8xb array 以16进制打印数组array的前8个byte的值   x/8xw array 以16进制打印数组array的前16个word的值    局部变量    命令 作用     info locals 打印当前函数局部变量的值   backtrace full 打印当前栈帧各个函数的局部变量值，命令可缩写为bt   bt full n 从内到外显示n个栈帧及其局部变量   bt full -n 从外向内显示n个栈帧及其局部变量    结构体    命令 作用     set print pretty on 每行只显示结构体的一名成员   set print null-stop 不显示\u0026rsquo;\\000\u0026rsquo;这种    函数跳转    命令 作用     set step-mode on 不跳过不含调试信息的函数，可以显示和调试汇编代码   finish 执行完当前函数并打印返回值，然后触发中断   return 0 不再执行后面的指令，直接返回，可以指定返回值   call printf(\u0026quot;%s\\n\u0026quot;, str) 调用printf函数，打印字符串(可以使用call或者print调用函数)   print func() 调用func函数(可以使用call或者print调用函数)   set var variable=xxx 设置变量variable的值为xxx   set {type}address = xxx 给存储地址为address，类型为type的变量赋值   info frame 显示函数堆栈的信息（堆栈帧地址、指令寄存器的值等）    多线程、多进程 多进程 GDB 在调试多进程程序（程序含 fork 调用）时，默认只追踪父进程。可以通过命令设置，实现只追踪父进程或子进程，或者同时调试父进程和子进程。\n   命令 作用     info inferiors 查看进程列表   attach pid 绑定进程id   inferior num 切换到指定进程上进行调试   print $_exitcode 显示程序退出时的返回值   set follow-fork-mode child 追踪子进程   set follow-fork-mode parent 追踪父进程   set detach-on-fork on fork调用时只追踪其中一个进程   set detach-on-fork off fork调用时会同时追踪父子进程    在调试多进程程序时候，默认情况下，除了当前调试的进程，其他进程都处于挂起状态，所以，如果需要在调试当前进程的时候，其他进程也能正常执行，那么通过设置 set schedule-multiple on 即可。\n 在默认情况下，GDB 只支持调试主进程（即 main），只有在 GDB 7.0 后才支持单独调试(调试父进程或者子进程)和同时调试多个进程。\n 多线程 默认调试多线程时，一旦程序中断，所有线程都将暂停。如果此时再继续执行当前线程，其他线程也会同时执行。\n   命令 作用     info threads 查看线程列表   thread [thread_id] 切换进该线程   print $_thread 显示当前正在调试的线程编号   set scheduler-locking on 调试一个线程时，其他线程暂停执行   set scheduler-locking off 调试一个线程时，其他线程同步执行   set scheduler-locking step 仅用step调试线程时其他线程不执行，用其他命令如next调试时仍执行    如果只关心当前线程，建议临时设置 scheduler-locking 为 on，避免其他线程同时运行，导致命中其他断点分散注意力。\n其他 图形化 tui为terminal user interface的缩写，在启动时候指定-tui参数，或者调试时使用ctrl+x+a组合键，可进入或退出图形化界面。\n   命令 含义     layout src 显示源码窗口   layout asm 显示汇编窗口   layout split 显示源码 + 汇编窗口   layout regs 显示寄存器 + 源码或汇编窗口   winheight src +5 源码窗口高度增加5行   winheight asm -5 汇编窗口高度减小5行   winheight cmd +5 控制台窗口高度增加5行   winheight regs -5 寄存器窗口高度减小5行    汇编    命令 含义     disassemble function 查看函数的汇编代码   disassemble /mr function 同时比较函数源代码和汇编代码    其他工具 pstack pstack 是一个 shell 脚本，用于打印正在运行的进程的栈跟踪信息。pstack 命令必须由相应进程的属主或 root 运行。可以使用 pstack 来确定进程挂起的位置。\n使用方式如下：\n1  pstack [pid]   ldd 当我们编译链接时找不到静态库而导致链接失败，又或者是编译成功，在运行时加载动态库失败时，我们可以使用 ldd 命令来分析该程序依赖了哪些库以及这些库所在的路径，从而解程序因缺少某个库文件而不能运行的一些问题。使用方式如下：\n1  ldd 程序名   例如：\n1 2 3 4 5 6 7  ldd test linux-vdso.so.1 (0x00007ffeefb2f000) libstdc++.so.6 =\u0026gt; /lib64/libstdc++.so.6 (0x00007fa30f54a000) libm.so.6 =\u0026gt; /lib64/libm.so.6 (0x00007fa30f1c8000) libgcc_s.so.1 =\u0026gt; /lib64/libgcc_s.so.1 (0x00007fa30efb0000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007fa30ebeb000) /lib64/ld-linux-x86-64.so.2 (0x00007fa30f8df000)   其中每一行的第一个参数程序依赖的库名，第二个则是系统所提供的对应的库（如果系统找不到，则会显示 not found），第三个是库加载的起始地址。\nstrings、c++filt C++ 为了支持函数重载功能，需要编译器在使用 name mangling 机制将符号表中的函数进行重命名，而我们使用 strings 就可以查看到重命名后的函数名：\n1  strings 程序名   如果使用 c++filt 工具，就可以根据符号表中的函数名，还原为原始的函数定义：\n1  c++filt 重命名后的函数名   原理 调试原理 当我们开始使用 GDB 开始调试时，系统首先会启动一个 GDB 进程，紧接着这个进程会 fork 出一个子进程来控制被调试程序，它会执行以下操作：\n 调用 ptrace(PTRACE_TRACEME) 来让 GDB 进程接管本进程的执行。 通过 execv 来将被调试程序替换到子进程中。  详细流程如下图所示：\n接下来就介绍里面最关键的 ptrace。\nptrace ptrace 是 Linux 内核提供的一个用于进程跟踪的系统调用，通过它，一个进程（GDB）可以读写另外一个进程（被调试进程）的指令空间、数据空间、堆栈和寄存器的值，并且 GDB 进程接管了被调试进程的所有信号，这样一来，被调试进程的执行就被 GDB 控制，从而达到调试的目的。\nptrace 的定义如下：\n1 2  #include \u0026lt;sys/ptrace.h\u0026gt; long ptrace(enum __ptrace_request request, pid_t pid, void *addr, void *data)    enum __ptrace_request request：指示了 ptrace 要执行的命令。 pid_t pid：指示 ptrace 要跟踪的进程 void *addr：指示要监控的内存地址 void *data：存放读取出的或者要写入的数据。  调试运行中程序 如果想要调试一个已经执行的进程，就需要在 GDB 进程中调用 ptrace(PTRACE_ATTACH,...)，此时 GDB 进程会 attach 已经执行的被调试进程，将其收养为自己的子进程，接着会向被调试进程发送一个 SIGSTO 信号，当被调试进程接收到这个信号时，就会暂时执行并进入 TASK_STOPED 状态，表示其已经准备好接受调试。\n attach 的一些限制：不予许 attach 自己；不允许多次 attach 到同一个进程；不允许 attach 1 号进程；\n 断点 break 当我们使用 break 命令设置断点的时候，GDB 首先会将原来的汇编代码存储到一个断点链表中，接着会在对应的汇编代码的位置插入一个 INT3 中断指令。\n当被调试的程序运行到这个位置时，就会执行 INT3 指令，此时会发生软中断，接着内核会向被调试进程发送一个 SIGTRAP 信号，由于当前 GDB 通过 ptrace 接管了被调试进程，所以这个信号自然又转发到了 GDB 进程中。GDB 此时会对比当前停止的位置和断点链表中存储的断点位置，将该位置的代码恢复成断点链表中存储的原来的代码，接着将程序计数器（pc 指针）回退一步，指向用户 break 的位置。\n此时就达到了断点的目的，接着 GDB 会一直等待用户输入调试指令。\n单步 next 当我们使用 next 执行单步命令时，此时 GDB 会控制其执行一行代码，它首先会计算出这一行代码所对应的汇编代码的位置，接着控制程序计数器一直往下执行，直到执行到这个位置时就会停下来，继续等待用户输入调试指令。\n这个功能其实借助 ptrace 就可以直接实现，只需要在第一个参数中传递 PTRACE_SINGLESTEP 即可。\n参考  GDB调试-从入门实践到原理 GDB调试入门，看这篇就够了 图文并茂 | 彻底弄懂GDB调试原理 GDB教程 GNU调试器  ","date":"2022-07-11T18:55:13+08:00","permalink":"https://blog.orekilee.top/p/gdb/","title":"GDB"},{"content":"流处理 在批处理中，输入数据是有界的（已知和有限的大小），所以批处理系统能直到它何时完成输入的读取。但事实上，很多场景下数据都是无界的，数据会随着时间的推移不断到来，并且这个过程永远不会结束，所以我们没有办法能掌握数据的大小以及它该何时结束。\n为了解决这个问题，批处理程序以时间来划分数据块，但这也就导致了一个问题：划分的时间越长，则延迟越长。倘若我们的服务一天更新一次数据，这会使得用户的体验直线下降，为了减少延迟，我们就需要将这个时间缩短，更加频繁的处理数据，这也就是流处理的原理。\n发送事件流 在批处理中任务的输入和输出都是文件，而在流处理领域中，输入则变成了一系列的事件。\n 事件：一个小的、自包含的、不可变的对象，包含某个时间点发生的某件事情的细节。一个事件通常包含一个来自日历时钟的时间戳，以指明事件发生的时间。\n 在批处理中，文件只被写入一次，然后可能被多个作业读取。类似地，在流处理领域中，一个事件由生产者（producer）生成一次，然后可能由多个消费者（consumer）进行处理。在文件系统中，文件名标识一组相关记录；在流式系统中，相关的事件通常被聚合为一个主题（topic） 。\n消息传递系统 向消费者通知新事件的常用方式是使用消息传递系统（messaging system）：生产者发送包含事件的消息，然后将消息推送给消费者。\n不同的消息传递系统可能会采取不同的实现方案，我们可以借助下面两个问题来区分这些系统：\n 如果生产者发送消息的速度比消费者能够处理的速度快会发生什么？\n 解决方案主要有三种：\n 丢弃这些来不及处理的消息。 将消息放入缓冲队列，延迟处理。 采取背压（backpressure）机制，阻塞生产者，以免其发送更多的消息。   如果节点崩溃或暂时脱机，会发生什么情况？是否会有消息丢失？\n 如果要想保证持久性，就必须定期写入磁盘和复制数据。而如果允许一定程序的消息丢失，则可以获得更高的吞吐量和更低的延迟。\n直接消息传递系统 许多消息传递系统使用生产者和消费者之间的直接网络通信，而不通过中间节点：\n UDP 组播广泛应用于金融行业，因为这些常见需要保证低延时（虽然 UDP 本身是不可靠的，但应用层的协议可以恢复丢失的数据包）。 无代理的消息库，如 ZeroMQ 和 nanomsg 采取类似的方法，通过 TCP 或 IP 多播实现发布 / 订阅消息传递。 StatsD 和 Brubeck 使用不可靠的 UDP 消息传递来收集网络中所有机器的指标并对其进行监控（在 StatsD 协议中，只有接收到所有消息，才认为计数器指标是正确的；使用 UDP 将使得指标处在一种最佳近似状态）。 如果消费者在网络上公开了服务，生产者可以直接发送 HTTP 或 RPC 请求消息推送给使用者。  虽然这些系统的性能很好，但是它们的容错程度极为有限。即使协议检测到并重传在网络中丢失的数据包，它们通常也只是假设生产者和消费者始终在线。\n如果消费者处于脱机状态，则可能会丢失其不可达时发送的消息。一些协议允许生产者重试失败的消息传递，但当生产者崩溃时，它可能会丢失消息缓冲区及其本应发送的消息，这种方法可能就没用了。\n消息代理 目前的主流方案则是使用消息代理（也称为消息队列）来发送消息。消息代理实质上是一种针对处理消息流而优化的数据库。它作为服务器运行，生产者和消费者作为客户端连接到服务器。生产者将消息写入代理，消费者通过从代理那里读取来接收消息。\n通过将数据集中在代理上，这些系统可以更容易地容忍来来去去的客户端（连接，断开连接和崩溃），而持久性问题则转移到代理的身上。一些消息代理只将消息保存在内存中，而另一些消息代理（取决于配置）将其写入磁盘，以便在代理崩溃的情况下不会丢失。\n消息代理 VS 数据库  数据库通常保留数据直至显式删除，而大多数消息代理在消息成功递送给消费者时会自动删除消息。这样的消息代理不适合长期的数据存储。 数据库通常支持次级索引和各种搜索数据的方式，而消息代理通常支持按照某种模式匹配主题，订阅其子集。 由于它们很快就能删除消息，因此大多数消息代理的队列很短。如果代理需要缓冲很多消息，比如因为消费者速度较慢（如果内存装不下消息，可能会溢出到磁盘），每个消息需要更长的处理时间，整体吞吐量可能会恶化。 查询数据库时，结果通常基于某个时间点的数据快照；如果另一个客户端随后向数据库写入一些改变了查询结果的内容，则第一个客户端不会发现其先前结果现已过期（除非它重复查询或轮询变更）。相比之下，消息代理不支持任意查询，但是当数据发生变化时（即新消息可用时），它们会通知客户端。  多个消费者 当多个消费者从同一主题中读取消息时，有两种主要的消息传递模式：\n 负载均衡（load balancing）：每条消息都被传递给消费者之一，所以处理该主题下消息的工作能被多个消费者共享。代理可以为消费者任意分配消息。 扇出（fan-out）：每条消息都被传递给所有消费者。扇出允许几个独立的消费者各自监听相同的消息广播，而不会相互影响。  两种模式可以组合使用：例如，两个独立的消费者组可以每组各订阅同一个主题，每一组都共同收到所有消息，但在每一组内部，每条消息仅由单个节点处理。\n确认与重传 由于消费者随时可能会崩溃，所以可能会存在这么一个场景：代理向消费者递送消息，但消费者没有处理，或者在消费者崩溃之前只进行了部分处理。为了确保消息不会丢失，消息代理引入了**确认（acknowledgments）**机制，即客户端必须显式告知代理消息处理完毕的时间，以便代理能将消息从队列中移除。\n如果与客户端的连接关闭，或者代理超出一段时间未收到确认，代理则认为消息没有被处理，因此它将消息再递送给另一个消费者。\n当结合上文提到的负载均衡时，这种重传行为会对消息的顺序产生影响。如下图，当某个消费者在处理消息时崩溃了，此时这个未处理的消息就会被重传到其他消费者手中，这也就可能导致消息的交付顺序（那个消费者可能正在处理别的消息）与生产者的发送顺序不一致。\n即使消息代理试图保留消息的顺序，负载均衡与重传的组合也不可避免地导致消息被重新排序。为了避免此问题，可以让每个消费者使用单独的队列（即不使用负载均衡功能）。\n分区日志  有没有一种方法既有数据库的持久存储，又能保证消息传递的低延迟？这时就需要提到基于日志的消息代理。\n 使用日志进行消息存储 基于日志的消息代理的实现原理如下：生产者通过将消息追加到日志末尾来发送消息，而消费者通过依次读取日志来接收消息。如果消费者读到日志末尾，则会等待新消息追加的通知。\n为了伸缩超出单个磁盘所能提供的更高吞吐量，可以对日志进行分区。不同的分区可以托管在不同的机器上，使得每个分区都有一份能独立于其他分区进行读写的日志。一个主题可以定义为一组携带相同类型消息的分区。如下图所示：\n在每个分区内，代理为每个消息分配一个单调递增的序列号或偏移量（因为是追加写入，保证了单分区的完全有序，但无法保证不同分区有序）。\n日志 VS 消息传递 在消息处理代价高昂，希望逐条并行处理，以及消息的顺序并没有那么重要的情况下，基于队列的消息代理是可取的。另一方面，在消息吞吐量很高，处理迅速，顺序很重要的情况下，基于日志的方法表现得非常好。\n消费者偏移量 由于日志是追加写入的，因此仅仅需要通过偏移量就可以判断消息是否已被处理：**所有偏移量小于消费者的当前偏移量的消息已经被处理，而具有更大偏移量的消息还没有被看到。**因此，代理不需要跟踪确认每条消息，只需要定期记录消费者的偏移即可。\n磁盘空间使用 为了避免数据不断写入而导致磁盘空间耗尽，通常日志会被分为多个段，并不定时将旧段删除或归档存储。为了避免生产者写入过快，导致消费者读取到被删除的数据，从而丢失数据，日志通常会实现一个循环缓冲区，当缓冲区填满后再丢弃数据。\n重播旧消息 在基于队列的消息代理中，处理和确认消息后会导致该消息被删除。而在基于日志的代理中，则仅仅是读取日志，并不会做任何的修改操作。这也就使得我们能够重放数据，即从之前的偏移量开始重新读取。\n数据库与流 保持系统同步 通常情况下，没有一个系统能够满足所有的存储需求，这就要求当一个数据发生变动时，这个变更应该同步到所有的相关系统中。\n对于数据仓库而言，通常的作法是转储数据库——取得数据库的完整副本，然后执行 ETL 将数据加载到数据仓库中。但是由于这样做的效率过低，有时人们又会采取双写来进行替代——代码在写入数据库的同时写入到每个系统中。\n但是双写在改善效率的同时，又带来了一系列的问题：\n 数据竞争：在并发写入的时候，一台机器可能会覆盖掉另一个机器的写入结果，并且双方都无法感知这个过程。 原子提交：双写需要保证写入操作是原子的，否则一个写入成功、一个写入失败时就会导致数据的不一致。  变更数据捕获 变更数据捕获（change data capture, CDC） 指的是记录是写入数据库的所有数据变更，将其提取并转换为可以复制到其他系统中的形式的过程。\n如下图，我们可以捕获数据中的变更，并将变更日志以相同的顺序应用于其他系统中，则能够保证多个系统中的数据与数据库一致。\n通常的实现方案有如下两种：\n 数据库触发器：注册观察所有变更的触发器，并将相应的变更项写入变更日志表中。 解析复制日志：解析数据库的日志，将解析出来的修改传递给下游。  事件溯源 事件溯源是一种强大的数据建模技术：从应用的角度来看，将用户的行为记录为不可变的事件更有意义，而不是在可变数据库中记录这些行为的影响。事件溯源使得应用随时间演化更为容易，通过更容易理解事情发生的原因来帮助调试的进行，并有利于防止应用 Bug。\n 事件溯源和变更数据捕获都将所有对应用状态的变更存储为变更事件日志，那么它们有什么区别呢？\n  在变更数据捕获中，应用以可变方式使用数据库，可以任意更新和删除记录。变更日志是从数据库的底层提取的（例如，通过解析复制日志），从而确保从数据库中提取的写入顺序与实际写入的顺序相匹配。 在事件溯源中，应用逻辑显式构建在写入事件日志的不可变事件之上。在这种情况下，事件存储是仅追加写入的，更新与删除是不鼓励的或禁止的。事件被设计为旨在反映应用层面发生的事情，而不是底层的状态变更。  事件溯源的核心是区分事件（event）和命令（command）。当来自用户的请求刚到达时，它一开始是一个命令：在这个时间点上它仍然可能可能失败，比如，因为违反了一些完整性条件。应用必须首先验证它是否可以执行该命令。如果验证成功并且命令被接受，则它变为一个持久化且不可变的事件。\n在事件生成的时刻，它就成为 事实（fact）。即使用户对其进行删除或者修改，也只是在后续单独添加了删除、修改事件。\n流处理 应用场景 除了传统的监控（欺诈检测、机器状态检测、金融交易等），流处理还有以下这些应用场景：\n 复合事件处理（CEP） 数据分析 物化视图 搜索引擎 消息传递和 PRC  时间推理 事件时间和处理时间 在流处理中有事件时间和处理时间的概念：\n 事件时间：事件发生的时间。 处理时间：机器接收到事件时，对其进行处理的时间。  如果我们以处理时间作为标准，则可能会出现逻辑上的紊乱（与时间实际发生的顺序不同），而如果是事件时间作为标准，则考虑到延迟、排队、网络故障等因素，则可能会出现某些事件延迟很久才会到来，甚至丢失的情况。\n知道什么时候准备好了 当我们使用事件时间定义窗口时，就会遇到上面说的问题，我们无法判断当前这个窗口的事件有没有完整的到来。\n通常情况下，我们会设定一个超时时间，当到达这个时间后我们就认为当前窗口已经就绪，开始聚合计算。倘若后续有延迟的数据到来，此时可以采用两种方案进行处理：\n 丢弃数据：丢弃这些延迟的数据。为了防止丢失的数据过多，可以设定一个监控的阈值，当丢弃数据过多时发出警报，重放数据。 修正数据：将该数据放入旧窗口中，同时需要回撤以前的输出，确保数据的正确性。  时钟 考虑到各个机器的物理时钟可能会存在误差（客户端的时间可能因为用户的错误设置，出现明显的错误），通常需要记录三个时间戳来校准时间：\n 事件发生的时间（依赖于客户端时钟）。 事件发送给服务器的时间（依赖于客户端时钟） 服务器接收事件的时间（依赖于服务器时钟）。  通过从第三个时间戳中减去第二个时间戳，可以估算设备时钟和服务器时钟之间的偏移（假设网络延迟与所需的时间戳精度相比可忽略不计）。然后可以将该偏移应用于事件时间戳，从而估计事件实际发生的真实时间（假设设备时钟偏移在事件发生时与送往服务器之间没有变化）。\n窗口类型 常见的窗口类型有以下几种：\n 滚动窗口（Tumbling Window）：滚动窗口有着固定的长度，每个事件都仅能属于一个窗口。例如，假设你有一个 1 分钟的滚动窗口，则所有时间戳在 10:03:00 和 10:03:59 之间的事件会被分组到一个窗口中，10:04:00 和 10:04:59 之间的事件被分组到下一个窗口，依此类推。 跳动窗口（Hopping Window）：跳动窗口也有着固定的长度，但允许窗口重叠以提供一些平滑。例如，一个带有 1 分钟跳跃步长的 5 分钟窗口将包含 10:03:00 至 10:07:59 之间的事件，而下一个窗口将覆盖 10:04:00 至 10:08:59 之间的事件，等等。通过首先计算 1 分钟的滚动窗口（tunmbling window），然后在几个相邻窗口上进行聚合，可以实现这种跳动窗口。 滑动窗口（Sliding Window）：滑动窗口包含了彼此间距在特定时长内的所有事件。例如，一个 5 分钟的滑动窗口应当覆盖 10:03:39 和 10:08:12 的事件，因为它们相距不超过 5 分钟。通过维护一个按时间排序的事件缓冲区，并不断从窗口中移除过期的旧事件，可以实现滑动窗口。 会话窗口（Session window）：与其他窗口类型不同，会话窗口没有固定的持续时间，它将同一用户出现时间相近的所有事件分组在一起，而当用户一段时间没有活动时结束窗口。  流连接 在流处理中主要有以下三种连接类型：\n  流流连接（窗口连接）：两个输入流都由活动事件组成，而连接算子在某个时间窗口内搜索相关的事件。如果你想要找出一个流内的相关事件，连接的两侧输入可能实际上都是同一个流（自连接）。\n  流表连接（流扩充）：一个输入流由活动事件组成，另一个输入流是数据库变更日志。变更日志保证了数据库的本地副本是最新的。对于每个活动事件，连接算子将查询数据库，并输出一个扩展的活动事件。\n  表表连接（维护物化视图）：两个输入流都是数据库变更日志。在这种情况下，一侧的每一个变化都与另一侧的最新状态相连接。结果是两表连接所得物化视图的变更流。\n  容错 微批量与存档点 考虑到流处理没有边界，数据永远不会停止，为了尽可能的减少延迟，Spark Streaming 将流分解为一个个小块，并像微型批处理一样处理每个块。这种方法被称为微批（microbatching）。\n而 Flink 则采取了另一种方案，它会定期生成状态的滚动存档点并将其写入持久存储。如果流算子崩溃，它可以从最近的存档点重启，并丢弃从最近检查点到崩溃之间的所有输出。\n在流处理中，这两种方法都满足于恰好一次语义（exactly-once semantics）。\n原子提交 为了在出现故障时满足 exactly-once ，我们需要确保事件处理的所有输出和副作用当且仅当处理成功时才会生效。这些事情要么都原子地发生，要么都不发生，但是它们不应当失去同步。\n幂等性 我们的目标是丢弃任何失败任务的部分输出，以便能安全地重试，而不会生效两次。分布式事务是实现这个目标的一种方式，而另一种方式是依 幂等性（idempotence）。\n 幂等操作指的是是多次重复执行与单次执行效果相同的操作。\n 那么如何实现幂等呢？可以参考 Kafka 的解决方案，即为每个消息附带一个持久的、单调递增的偏移量，通过这个偏移量就可以判断这个消息是否被执行过，从而避免重复执行。\n失败后重建状态 任何需要状态的流处理以及任何用于连接的表和索引，都必须确保在失败之后能恢复其状态。通常会采用如下解决方案：\n 将状态保存在远程数据存储中，并进行复制。 在流处理器本地保存状态，并定期复制。当流处理器从故障中恢复时，新任务可以读取状态副本，恢复处理而不丢失数据。 不需要复制状态，直接从输入流中重建状态。  至于要选择哪个方案，需要根据业务场景、底层架构的性能来进行分析。\n","date":"2022-07-10T10:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%8D%81%E4%B8%80%E6%B5%81%E5%A4%84%E7%90%86/","title":"数据密集型应用系统设计 学习笔记（十一）：流处理"},{"content":"批处理 从高层次来看，存储和处理数据的系统可以分为两大类：\n 记录系统：其持有数据的权威版本。当新的数据进入时首先会记录在这里。每个记录在系统中只表示一次。如果其他系统和记录系统之间存在任何差异，那么此时将以记录系统中的值为准。 派生数据系统：派生数据系统中的数据通常是另一个系统中的现有数据以某种方式进行转换或处理的结果。如果丢失派生数据，可以从原始数据源中重新创建。典型的例子是缓存（cache）：如果数据在缓存中，则可以从缓存中读取；如果缓存不包含所需数据，则降级由底层数据库提供。非规范化的值，索引和物化视图亦属此类。在推荐系统中，预测汇总数据通常衍生自用户日志。   派生数据是冗余的（redundant），因为它重复了已有的信息。但是派生数据对于获得良好的只读查询性能通常是至关重要的。它通常是非规范化的。可以从单个源头衍生出多个不同的数据集，使我们能从不同的视角观察数据。\n 根据响应时间的不同，数据处理系统通常分为以下三种类型：\n 在线服务（在线系统）：服务等待客户的请求或指令到达。每收到一个，服务会试图尽快处理它，并发回一个响应。 批处理系统（离线系统）：一个批处理系统有大量的输入数据，通过运行一个 job 来处理这些数据，并生成一些输出数据，这往往需要一段时间（从几分钟到几天），所以通常不会有用户等待 job 完成。相反，批量作业通常会定期运行。 流处理系统（准实时系统）：处于在线和离线系统之间。像批处理系统一样，流处理消费输入并产生输出（并不需要响应请求）。但是，流式作业在事件发生后不久就会对事件进行操作，而批处理作业则需等待固定的一组输入数据。这种差异使流处理系统比起批处理系统具有更低的延迟。  基于 UNIX 工具的批处理 最简便的批处理方案是基于 UNIX 工具实现的，例如我们需要在一个网站中找出访问最高频的五个网页：\n1 2 3 4 5 6 7 8 9 10 11 12  # 原始数据 216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] \u0026#34;GET /css/typography.css HTTP/1.1\u0026#34; 200 3377 \u0026#34;http://martin.kleppmann.com/\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36\u0026#34; # 处理方案 cat /var/log/nginx/access.log | #1 awk \u0026#39;{print $7}\u0026#39; | #2 sort | #3 uniq -c | #4 sort -r -n | #5 head -n 5 #6    cat 读取日志文件。 awk 每行只获取第七个字段，恰好是 URL 的那个。 sort 按照字段序排列 URL。 uniq -c 计算出每一个 URL 重复的次数。 sort -r -n 按照每一个 URL 出现的次数降序排序。 head -n 5 取出现次数最高的五个 URL。  使用 awk、sed、grep、sort、uniq 和 xargs 的组合，可以在几分钟内完成许多数据分析，并且它们的性能也相当不错。\n然而这些 Unix 工具有一个致命的缺陷，它们只能在一台机器上运行。为了解决这个问题，MapReduce 诞生了。\nMapReduce MapReduce 作业执行 MapReduce 是一个编程框架，你可以使用它编写代码来处理 HDFS 等分布式文件系统中的大型数据集。其数据处理流程如下：\n 读取一组输入文件，并将其分解成记录（records）。 调用 Mapper 函数，从每条输入记录中提取一对键值。 按键排序所有的键值对。 调用 Reducer 函数遍历排序后的键值对。  这四个步骤可以作为一个 MapReduce 作业执行。由于步骤 1（将文件分解成记录）由输入格式解析器处理，而步骤 3 中的排序步骤隐含在 MapReduce，因此我们去需要自己去实现 Mapper 和 Reducer 函数即可：\n Mapper：Mapper 会在每条输入记录上调用一次，其工作是从输入记录中提取键值。对于每个输入，它可以生成任意数量的键值对。它不会保留从一个输入记录到下一个记录的任何状态，因此每个记录都是独立处理的。 Reducer：MapReduce 框架拉取由 Mapper 生成的键值对，收集属于同一个键的所有值，并在这组值上迭代调用 Reducer。 Reducer 可以产生输出记录。  分布式执行 MapReduce MapReduce 与 Unix 命令管道的主要区别在于：MapReduce 可以在多台机器上并行执行计算，而无需编写代码来显式处理并行问题。\n下图即为 Hadoop MapReduce 作业中的数据流，其并行化基于分区实现：作业的输入通常是 HDFS 中的一个目录，输入目录中的每个文件或文件块都被认为是一个单独的分区，可以单独处理 map 任务。\n 只要有足够的空闲内存和 CPU 资源，MapReduce 调度器就会尝试在其中一台存储输入文件副本的机器上运行 Mapper 任务。这个原则被称为将计算放在数据附近：它节省了通过网络复制输入文件的开销，减少网络负载并增加局部性。\n 如上图，MapReduce 的完整执行流程如下：\n MapReduce 框架首先将代码复制到适当的机器。然后启动 Map 任务并开始读取输入文件，一次将一条记录传入 Mapper 回调函数。 键值对必须进行排序，但数据集可能太大，无法在单台机器上使用常规排序算法进行排序。因此每个 Map 任务都按照 Reducer 对输出进行分区。每个分区都被写入 Mapper 程序的本地磁盘 当 Mapper 读取完输入文件，并写完排序后的输出文件后，MapReduce 调度器就会通知 Reducer 可以从该 Mapper 中获取输出文件。 Reducer 连接到每个 Mapper，并下载自己相应分区的有序键值对文件。按 Reducer 分区，排序，从 Mapper 向 Reducer 复制分区数据，这一整个过程被称为 shuffle。 Reduce 任务从 Mapper 获取文件，并将它们 merge 在一起，并保留有序特性。因此，如果不同的 Mapper 生成了键相同的记录，则在 Reducer 的输入中，这些记录将会相邻。 Reducer 调用时会收到一个键，和一个迭代器作为参数，迭代器会顺序地扫过所有具有该键的记录。Reducer 可以使用任意逻辑来处理这些记录，并且可以生成任意数量的输出记录。这些输出记录会写入分布式文件系统上的文件中。   考虑到单个 MapReduce 作业可以解决的问题范围很有限，因此我们可以将 MapReduce 作业链接成一个工作流，即一个作业的输出成为下一个作业的输入。\n只有当作业成功完成后，批处理作业的输出才会被视为有效的。因此，工作流中的一项作业只有在先前的作业成功完成后才能开始。\n Reduce 侧 join 与分组  在许多数据集中，一条记录与另一条记录存在关联是很常见的，例如关系模型中的外键、文档模型中的文档引用、图模型中的边。当你需要同时访问这一关联的两侧时，就必须进行 join。\n 排序合并 join 在 Reducer 中执行实际的 join 逻辑，被称为 Reduce 侧 join 。\n通常我们会采用排序合并 join，其原理如下：每个参与 join 的输入都会由一个提取 join 键的 Mapper 进行处理。通过分区、排序和合并，具有相同键的所有记录最终都会进入相同的 Reducer 调用，然后这个函数输出 join 好的记录。\n分组 除了 join 之外，还有另一种方法能将相关数据放在一起，即按某个键对记录分组（如 SQL 中的 GROUP BY ）。\n使用 MapReduce 实现这种分组操作的最简单方法是设置 Mapper，使它们生成的键值对使用所需的分组键。然后在分区和排序过程将所有具有相同分区键的记录导向同一个 Reducer。（因此在 MapReduce 之上实现分组和 join 看上去非常相似。）\n数据倾斜 如果存在与单个键关联的大量数据，则将具有相同键的所有记录放到相同的位置这种模式就会产生问题：大量的数据放到一台机器上，从而导致负载不均衡。这种情况也被称为数据倾斜，而这种数据被称为热点数据（热键）。\n为了处理这种情况，当 join 的输入存在热键的时候，可以采取一些补偿机制，例如下面几种方法：\n Pig 的解决方案：首先运行一个抽样作业来确定哪些键是热键。join 实际执行时，Mapper 会将热键的关联记录随机发送到几个 Reducer 之一。对于另外一侧的 join 输入，与热键相关的记录需要被复制所有处理该键的 Reducer 上。 Hive 的解决方案：在表格元数据中显式指定热键，并将与这些键相关的记录单独存放，与其它文件分开。当在该表上执行连接时，对于热键，它会使用 Map 端 join。  Map 侧 join Reduce 侧 join 的优点是不需要对输入数据做任何假设：无论其属性和结构如何，Mapper 都可以对其预处理以备连接。然而不利的一面是，排序，复制至 Reducer，以及合并 Reducer 输入，所有这些操作可能开销巨大。当数据通过 MapReduce 阶段时，数据可能需要落盘好几次（次数取决于可用的内存缓冲区）。\n倘若我们能够对输入数据做一些假设，我们就可以使用 Map 侧 join 来加快我们的 join 速度。\n广播哈希 join 适用于执行 Map 端连接的最简单场景是大数据集与小数据集连接的情况。\n其要求小数据集需要足够小，不需要进行分区，以便可以将其完全加载进一个哈希表中。因此，你可以为连接输入大端的每个分区启动一个 Mapper，将输入小端的哈希表加载到每个 Mapper 中，然后扫描大端，一次一条记录，并为每条记录查询哈希表。\n 除了将较小的连接输入加载到内存哈希表中，另一种方法是将较小输入存储在本地磁盘上的只读索引中。索引中经常使用的部分将保留在操作系统的页面缓存中，因而这种方法可以提供与内存哈希表几乎一样快的随机查找性能。\n 分区哈希 join 如果两个连接输入以相同的方式分区（使用相同的键，相同的哈希函数和相同数量的分区），则可以独立地对每个分区应用哈希表方法。\nMap 侧合并 join 如果输入数据集不仅以相同的方式进行分区，而且还基于相同的键进行排序，则可适用另一种 Map 侧连接的变体。\n在这种情况下，输入是否小到能放入内存并不重要，因为这时候 Mapper 同样可以执行归并操作（通常由 Reducer 执行）：按键递增的顺序依次读取两个输入文件，将具有相同键的记录配对。\nMapreduce 工作流与 Map 侧 join 当下游作业使用 MapReduce join 的输出时，选择 Map 侧 join 或 Reduce 侧 join 会影响输出的结构。Reduce 侧 join 的输出是按照 join 键进行分区和排序的，而 Map 端 join 的输出则按照与较大输入相同的方式进行分区和排序。\n批处理的应用场景 批处理有以下几种常见的使用场景：\n 构建搜索引擎。 构建机器学习系统，例如分类器（比如垃圾邮件过滤器，异常检测，图像识别）与推荐系统（例如，你可能认识的人，你可能感兴趣的产品或相关的搜索）。 ……  MapReduce 之后 物化中间状态 在很多情况下，一个作业的输出只能用作另一个作业的输入。在这种情况下，分布式文件系统上的文件只是简单的中间状态（intermediate state）：一种将数据从一个作业传递到下一个作业的方式。将这个中间状态写入文件的过程称为物化（materialization）。\n在之前的例子中，Unix 利用管道将一个命令的输出与另一个命令的输入连接起来。管道并没有完全物化中间状态，而是只使用一个小的内存缓冲区，将输出增量地流（stream） 向输入。与 Unix 管道相比，MapReduce 完全物化中间状态的方法存在以下不足之处：\n MapReduce 作业只有在前驱作业中的所有任务都完成时才能启动，而由 Unix 管道连接的进程会同时启动，输出一旦生成就会被消费。不同机器上的数据偏斜或负载不均意味着一个作业往往会有一些掉队的任务，比其他任务要慢得多才能完成，拖慢了整个工作流程的执行。 Mapper 通常是多余的：它们仅仅是读取刚刚由 Reducer 写入的同样文件，为下一个阶段的分区和排序做准备。在许多情况下，Mapper 代码可能是前驱 Reducer 的一部分：如果 Reducer 和 Mapper 的输出有着相同的分区与排序方式，那么 Reducer 就可以直接串在一起，而不用与 Mapper 相互交织。 将中间状态存储在分布式文件系统中意味着这些文件被复制到多个节点，对这些临时数据来说显得有些浪费。  数据流引擎 为了解决 MapReduce 的这些问题，几种用于分布式批处理的新执行引擎被开发出来（如 Spark、Flink 等）。它们的设计方式有一个共同点：把整个工作流作为单个作业来处理，而不是把它分解为独立的子作业。\n由于它们将工作流显式建模为数据从几个处理阶段穿过，所以这些系统被称为数据流引擎（dataflow engines）。像 MapReduce 一样，它们在一条线上通过反复调用用户定义的函数来一次处理一条记录，它们通过输入分区来并行化载荷，它们通过网络将一个函数的输出复制到另一个函数的输入。\n与 MapReduce 不同，这些函数不需要严格扮演交织的 Map 与 Reduce 的角色，而是可以以更灵活的方式进行组合。我们称这些函数为算子（operators），数据流引擎提供了几种不同的选项来将一个算子的输出连接到另一个算子的输入：\n 对记录按键重新分区并排序，就像在 MapReduce 的 shuffle 阶段一样。 接受多个输入，并以相同的方式进行分区，但跳过排序。 将一个算子的输出，发送到连接算子的所有分区。  与 MapReduce 模型相比，它有几个优点：\n 排序等昂贵的工作只需要在实际需要的地方执行，而不是默认地在每个 Map 和 Reduce 阶段之间出现。 没有不必要的 Map 任务，因为 Mapper 所做的工作通常可以合并到前面的 Reduce 算子中。 由于工作流中的所有连接和数据依赖都是显式声明的，因此调度程序能够总览全局，知道哪里需要哪些数据，因而能够利用局部性进行优化。 算子间的中间状态足以保存在内存中或写入本地磁盘，这比写入 HDFS 需要更少的 I/O（必须将其复制到多台机器，并将每个副本写入磁盘）。 算子可以在输入就绪后立即开始执行；后续阶段无需等待前驱阶段整个完成后再开始。 与 MapReduce（为每个任务启动一个新的 JVM）相比，现有JVM 进程可以重用来运行新算子，从而减少启动开销。  容错 完全物化中间状态至 HDFS 的一个优点是，它具有持久性，这使得 MapReduce 中的容错相当容易：如果一个任务失败，它可以在另一台机器上重新启动，并从文件系统重新读取相同的输入。\nSpark、Flink 和 Tez 避免将中间状态写入 HDFS，因此它们采取了不同的方法来容错：如果一台机器发生故障，并且该机器上的中间状态丢失，则它会从其他仍然可用的数据重新计算（在可行的情况下是先前的中间状态，要么就只能是原始输入数据，通常在 HDFS 上）。\n 为了实现重新计算，框架需要获取一个数据的计算信息（输入分区、算子等）。 Spark 使用**弹性分布式数据集（RDD，Resilient Distributed Dataset）**的抽象来跟踪数据的谱系，而 Flink 对算子状态存档，允许恢复运行在执行过程中遇到错误的算子。\n 图与迭代处理  像 Spark、Flink 和 Tez 这样的数据流引擎通常将算子作为**有向无环图（DAG）**的一部分安排在作业中。这与图处理不一样：在数据流引擎中，从一个算子到另一个算子的数据流被构造成一个图，而数据本身通常由关系型元组构成。在图处理中，数据本身具有图的形式。\n 许多图算法是通过一次遍历一条边来表示的，将一个顶点与近邻的顶点连接起来，以传播一些信息，并不断重复，直到满足一些条件为止（例如，直到没有更多的边要跟进，或直到一些指标收敛）。\n可以在分布式文件系统中存储图（包含顶点和边的列表的文件），但是这种重复至完成的想法不能用普通的 MapReduce 来表示，因为它只扫过一趟数据。这种算法因此经常以迭代的风格实现：\n 外部调度程序运行批处理来计算算法的一个步骤。 当批处理过程完成时，调度器检查它是否完成（基于完成条件 —— 例如，没有更多的边要跟进，或者与上次迭代相比的变化低于某个阈值）。 如果尚未完成，则调度程序返回到步骤 1 并运行另一轮批处理。  ","date":"2022-07-10T09:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%8D%81%E6%89%B9%E5%A4%84%E7%90%86/","title":"数据密集型应用系统设计 学习笔记（十）：批处理"},{"content":"一致性与共识 可线性化（强一致性） 在最终一致性数据库中，同时查询两个不同的副本可能会得到两个不同的答案。这会 使应用层感到困惑。如果数据库能够对上提供只有单个副本的假象，情况会不会大为简化呢？\n这样让每个客户端都拥有相同的数据视图，而不必担心复制滞后的思想，就是可线性化（强一致性）。其基本的想法是让一个系统看起来好像只有一个数据副本，且所有的操作都是原子的。 有了这个保证，应用程序就不需要关心系统内部的多个副本。\n在一个可线性化的系统中，一旦某个客户端成功提交写请求，所有客户端的读请求一定都能看到刚刚写入的值。这种看似单一副本的假象意味着它可以保证读取最近最新值，而不是过期的缓存。换句话说，可线性化是一种就近的保证。\n可线性化与可串行化   可串行化：可串行化是事务的隔离级别，其中每个事务可以读写多个对象。它用来确保事务执行的结果与串行执行（即每次执行一个事务）的结果完全相同，即使串行执行的顺序可能与事务实际执行顺序不同。\n  可线性化：可线性化是读写寄存器（单个对象）的最新值保证。它并不要求将操作组合到事务中，因此无法避免写倾斜等问题，除非采取其他额外措施。\n  数据库可以同时支持可串行化与线性化，这种组合又被称为严格的可串行化或者强的单副本可串行化。基于两阶段加锁或者实际以串行执行都是典型的可线性化。\n使用场景 在有些场景下，线性化对于保证系统正确工作至关重要。\n加锁与主节点选举 主从复制的系统需要确保有且只有一个主节点，否则会产生脑裂。选举新的主节点常见的方法是使用锁，即每个启动的节点都试图获得锁，其中只有一个可以成功即成为主节点。不管锁具体如何实现，它都必须满足可线性化，即所有节点都必须同意哪个节点持有锁，否则就会出现问题。\n约束与唯一性保证 唯一性约束在数据库中很常见。例如，用户名或电子邮件地址必须唯一标识一个用户、文件存储服务中两个文件不能具有相同的路径和文件名。如果要在写入数据时强制执行这些约束（例如，如果两个人试图同时创建具有相同名称的用户或文件，其中一个必须返回错误），则也需要线性化。\n这种情况本质上与加锁非常类似，用户注册等同于试图对用户名进行加锁操作。该操作也类似于原子比较和设置，如果当前用户名尚未被使用，就设置用户名与客户 ID 进行关联。\n跨通道的时间依赖 如果某个系统存在多个通信通道（如消息队列、文件存储），如果没有线性化的就近性保证，这两个通道之间就会产生竞争（如消息队列比存储服务内部复制更快），在这种情况下就会导致数据的不一致出现。\n实现 由于线性化本质上意味着表现得好像只有一个数据副本，且其上的所有操作都是原子的，所以最简单的方案自然是只用一个数据副本。但显然，该方法无法容错。如果仅有的副本所在的节点发生故障，就会导致数据丢失，或者至少在重启之前都无法访问服务。\n系统容错最常见的方法就是采用复制机制，那就来分析以下各个复制方案能否满足可线性化：\n 主从复制（部分支持线性化）：在主从复制的系统中，只有主节点承担数据写入，从节点则在各自节点上维护数据的备份副本。如果从主节点或者同步更新的从节点上读取，则可以满足线性化。但并非每个主从复制的具体数据库示例都是可线性化的，主要是因为它们可能采用了快照隔离的设计，或者实现时存在并发方面的 bug（例如异步复制后数据丢失、从节点自认为是主节点等）。 共识算法（可线性化）：共识算法通常内置一些措施来防止脑裂和副本过期。正是由于这些专门的设计，共识算法可以安全地实现线性化存储。 多主复制（不可线性化）：具有多主节点复制的系统通常无法线性化的，主要由于它们同时在多个节点上执行并发写入，并将数据异步复制到其他节点 。因此它们可能会产生冲突的写入，需要额外的解决方案。 无主复制（可能不可线性化）：对于无主节点复制的系统，有些人认为只要配置法定读取和写入满足 w+r\u0026gt;n 就可以获得强一致性。但这完全取决于具体的 quorum 的配置，以及如何定义强一致性，它可能并不保证线性化。例如最后写入获胜几乎肯定是非线性化，因为这种时间戳无法保证与实际事件顺序一致（例如由于时钟偏移）不规范的 quorum 也会破坏线性化。  顺序保证 顺序与因果关系 如果系统服从因果关系所规定的顺序，我们称之为因果一致性。 例如，快照隔离提供了因果一致性，当从数据库中读数据时，如果查询到了某些数据，一定能看到触发该数据的前序事件（ 假设期间没有发生删除操作）。\n因果顺序并非全序 全序关系支持任何两个元素之间进行比较，即对于任意两个元素，总是可以指出哪个更大，哪个更小。但是，有些集合并不符合全序，因为它们都不是对方的子集，所以无法直接比较它们。我们称之为不可比较，数学集合只能是偏序。\n全序和偏序的差异也会体现在不同的数据库一致性模型中：\n 可线性化：在一个可线性化的系统中，存在全序操作关系。系统的行为就好像只有一个数据副本，且每个操作都是原子的，这意味着对于任何两个操作，我们总是可以指出哪个操作在先。 因果关系：如果两个操作都没有发生在对方之前，那么这两个操作是并发关系。换言之，如果两个事件是因果关系（一个发生在另一个之前），那么这两个事件可以被排序。而并发的事件则无法排序比较。这表明因果关系至少可以定义为偏序，而非全序。  因此，根据这个定义，在可线性化数据存储中不存在并发操作，一定有一个时间线将所有操作都全序执行。可能存在多个请求处于等待处理的状态，但是数据存储保证了在特定的时间点执行特定的操作，所以是单个时间轴，单个数据副本，没有并发。\n可线性化强于因果一致性 那么因果序和可线性化之间是什么关系呢？\n答案是可线性化一定意味着因果关系，任何可线性化的系统都将正确地保证因果关系。 特别是，如果系统存在多个通信通道，可线性化确保了因果关系会自动全部保留，而不需要额外的工作（比如在不同组件之间的传递时间戳）。\n可线性化虽然可以确保因果性，但其会显著降低性能和可用性，尤其是在严重网络延迟的情况下。正因如此，一些分布式数据系统已经放弃了线性化，以换来更好的性能，但也存在可能无法正确工作的风险。好消息是线性化并非是保证因果关系的唯一途径 ，还有其他方法使得系统可以满足因果一致性而免于线性化所带来的性能问题。\n因果一致性可以认为是，不会由于网络延迟而显著影响性能，又能对网络故障提供容错的最强的一致性模型。\n捕获因果依赖关系 为保持因果关系，需要知道哪个操作发生在前。这里只需偏序关系，或并发操作会以任意顺序执行，但如果一个操作发生在另一个操作之前，那么每个副本都应该按照相同的顺序处理。因此，当某个副本在处理一个请求时，必须确保所有因果在前的请求都已完成处理，否则，后面的请求必须等待直到前序操作处理完毕。\n为了确定请求的因果依赖关系，它需要跟踪整个数据库请求的因果关系，而不仅仅针对某个主键。版本向量技术可以推广为一种通用的解决方案。\n序列号排序 虽然因果关系很重要，但实际上跟踪所有的因果关系不切实际 。在许多应用程序中，客户端在写入之前会先读取大量数据，系统无法了解之后的写入究竟是依赖于全部读取内容， 还是仅仅是一小部分。但很明显，显式跟踪所有已读数据意味着巨大的运行开销。\n这里还有一个更好的方法，我们可以使用序列号或时间戳来排序事件。这样的序列号或时间戳非常紧凑（只有几字节大小），但它保证了全序关系。也就是说，每一个操作都有唯一的顺序号，并且总是可以通过比较来确定哪个更大（即操作发生在后）。这样的全局排序可以捕获所有的因果信息，但也强加了比因果关系更为严格的顺序性。\n非因果序列发生器 如果系统不存在这样唯一的主节点（例如可能是多主或者无主类型的数据库，或者数据库本身是分区的），产生序列号就不是那么简单了。实践中可以采用以下方法：\n 每个节点都独立产生自己的一组序列号。 例如，如果有两个节点，则一个节点只生成奇数，而另一个节点只生成偶数。还可以在序列号中保留一些位用于嵌入所属节点的唯一标识符，确保不同的节点永远不会生成相同的序列号。 可以把日历时钟信息（物理时钟）附加到每个操作上。 时间戳可能是不连续的，但是只要它们有足够高的分辨率，就可以用来区分操作。最后写获胜的冲突解决方案也使用类似的方法。 可以预先分配序列号的区间范围。 例如，节点 A 负责区间 1~1000 的序列号，节点 B 负责 1001~2000。然后每个节点独立地从区间中分配序列号，当序列号出现紧张时就分配更多的区间。  上述三种思路都可行，相比于把所有请求全部压给唯一的主节点具有更好的扩展性。它们为每个操作生成一个唯 一的、近似增加的序列号。不过，它们也都存在一个问题，即所产生的序列号与因果关系井不严格一致。\n所有这些序列号发生器都无法保证正确捕获跨节点操作的顺序，因而存在因果关系方面的问题：\n 每个节点可能有不同的处理速度，如每秒请求数。因此，某个节点产生偶数而另一个产生奇数，偶数的计数器产生速度可能落后于奇数的计数器，反之亦然。这样就无法准确地知道哪个操作在先。 物理时钟的时间戳会受到时钟偏移的影响，也可能导致与实际因果关系不一致。 对于区间分配器，一个操作可能被赋予从 1001~2000 之间的某个序列号，而后发生的操作则路由到另一个节点，拿到了某个 1~1000 之间的序列号，导致与因果序不一致。  Lamport 时间戳 刚才所描述的三个序列号发生器可能与因果关系存在不一致，但还有一个简单的方法可以产生与因果关系一致的序列号，即 Lamport 时间戳。\n首先每个节点都有一个唯一的标识符，且每个节点都有一个计数器来记录各自已处理的请求总数。 Lamport 时间戳是一个键值对（计数器，节点 ID）。两个节点可能会有相同的计数器值，但时间戳中还包含节点 ID 信息，因此可以确保每个时间戳都是唯一的。 原理如下图所示：\nLamport 时间戳与物理时钟并不存在直接对应关系，但它可以保证全序。给定两个 Lamport 时间戳，计数器较大那个时间戳大，而如果计数器值正好相同，则节点 ID 越大，时间戳越大。\n上面的描述看起来和前面提过的奇偶发生器类似，那它是如何保证因果一致性的呢？每个节点以及每个客户端都跟踪迄今为止所见到的最大计数器值，井在每个请求中附带该最大计数器值。当节点收到某个请求（或者回复）时，如果发现请求内嵌的最大计数器值大于节点自身的计数器值，则它立即把自己的计数器修改为该最大值。\nLamport 时间戳有时会与版本向量发生混淆。虽然存在一些相似之处，但它们的目的不同：版本向量用以区分两个操作是并发还是因果依赖，而 Lamport 时间戳则主要用于确保全序关系。 即使 Lamport 时间戳与因果序一致，但根据其全序关系却无法区分两个操作属于并发关系，还是因果依赖关系。Lamport 时间戳优于版本向量之处在于它更加紧凑和高效。\n全序关系广播 如果程序只运行在 CPU 单核上，可以非常简单地定义出操作的全序关系，即在单核上执行的顺序。但是，在分布式系统中，让所有的节点就全序关系达成一致就面临巨大挑战。\n如前所述，主从复制首先确定某个节点作为主节点，然后在主节点上顺序执行操作。接下来的主要挑战在于，如何扩展系统的吞吐量使之突破单主节点的限制，以及如何处理主节点失效时的故障切换。在分布式系统中，这些问题被称为全序关系广播或者原子广播。\n全序关系广播通常指节点之间交换消息的某种协议。 下面是个非正式的定义，它要求满足两个基本安全属性：\n 可靠发送：没有消息丢失，如果消息发送到了某一个节点，则一定要发送到所有节点。 严格有序：消息总是以相同的顺序发送给每个节点。  即使节点或网络出现了故障，全序关系广播算法的正确实现也必须保证上述两条。当然，网络中断时是不可能发送成功的，但算法要继续重试，直到最终网络修复，消息发送成功（且必须以正确的顺序发送）。\n使用场景  数据库复制：如果每条消息代表数据库写请求，并且每个副本都按相同的顺序处理这些写请求，那么所有副本可以保持一致（或许有些滞后）。该原则也被称为状态机复制。 可串行化事务：如果每条消息表示一个确定性事务井且作为存储过程来执行，且每个节点都遵从相同的执行顺序，那么可以保证数据库各分区以及各副本之间的一致性。 分布式锁：每个获取锁的请求都作为消息附加到日志中，所有消息按照日志中的顺序依次编号。序列号还可以作为令牌，它符合单调递增要求。 复制日志、事务日志、预写日志：传递消息就像追加方式更新日志。由于所有节点必须以相同的顺序发送消息，因此所有节点都可以读取日志并看到相同的消息序列。  全序关系广播实现线性化存储 如果有了全序关系广播，就可以在其上构建线性化的存储系统。例如，确保用户名唯一标识一个用户。\n可以通过使用全序关系广播以追加日志的方式来实现线性化的原子比较-设置操作，步骤如下所示：\n 在日志中追加一条消息，并指明想要的用户名。 读取日志，将其广播给所有节点，并等待回复。 检查是否有任何消息声称该用户名已被占用。 如果第一条这样的回复来自于当前节点，那么就成功获得该用户名，可以提交该获取声明（也许附加另一条消息到日志）并返回给客户端。反之，如果声称占用的第一条回复消息来自其他节点，则中止操作。  由于日志条目以相同的顺序发送到所有节点，而如果存在多个并发写入， 则所有节点将首先决定哪个请求在先。选择第一个写请求作为获胜者，并终止其他请求，以确保所有节点同意一个写请求最终要么提交成功要么终止。\n虽然此过程可确保线性化写入，但它却无法保证线性化读取，即从异步日志更新的存储中读取数据时，可能是旧值。 具体来说，这里只提供了顺序一致性，有时也称为时间线一致性，它弱于线性化保证。为了同时满足线性化读取，有以下几个方案：\n 可以采用追加的方式把读请求排序、广播，然后各个节点获取该日志，当本节点收到消息时才执行真正的读操作。消息在日志中的位置已经决定了读取发生的时间点。 如果可以用线性化的方式获取当前最新日志中消息的位置，则查询位置，等待直到该位置之前的所有条目都 经发送给你，接下来再执行读取。 可以从同步更新的副本上进行读取，这样确保总是读取最新值。  线性化存储实现全序关系广播 假设我们已经有了线性化的存储，那么如何在其上构建全序关系广播呢？最简单的方法是假设有一个线性化的寄存器来存储一个计数，然后使其支持原子自增-读取操作 或者原子比较-设置操作。\n算法思路很简单，对于每个要通过全序关系广播的消息，原子递增井读取该线性化的计数，然后将其作为序列号附加到消息中。接下来，将消息广播到所有节点（如果发生丢失， 重新发送），而接受者 严格按照序列化来发送回复消息。\n与 Lamport 时间戳不同，通过递增线性化寄存器获得的数字不会存在任何间隙。 因此，如果节点完成了消息 4 的发送，且接收到了序列化 6 的消息，那么在它对消息 6 回复之前必须等待消息 5。而 Lamport 时间戳则不是这样，而这也是区别全序关系广播与基于时间戳排序的关键。\n分布式事务与共识 有很多重要的场景都需要集群节点达成某种一致，例如：\n 主节点选举：对于主从复制的数据库，所有节点需要就谁来充当主节点达成一致。如果由于网络故障原因出现节点之间无法通信，就很容易出现争议。此时，共识对于避免错误的故障切换非常重要，后者会导致两个节点都自认为是主节点即脑裂。如果集群中存在两个这样的主节点，每个都在接受写请求，最终会导致数据产生分歧、不一致甚至数据丢失。 原子事务提交：对于支持跨节点或跨分区事务的数据库，会面临这样的问题：某个事务可能在一些节点上执行成功，但在其他节点却不幸发生了失败。为了维护事务的原子性，所有节点必须对事务的结果达成一致：要么全部成功提交，要么中止/回滚。这个共识的例子被称为原子提交问题。  原子提交与两阶段提交 单节点原子提交 对于在单个数据库节点上执行的事务，原子性通常由存储引擎来负责。 当客户端请求数据库节点提交事务时，数据库首先使事务的写入持久化（通常保存在预写日志中），然后把提交记录追加写入到磁盘的日志文件中。如果数据库在该过程中间发生了崩溃，那么当节点重启后，事务可从日志中恢复：如果在崩溃之前提交记录已成功写入磁盘，则认为事务己安全提交；否则，回滚该事务的所有写入。\n因此，在单节点上，事务提交非常依赖于数据持久写入磁盘的顺序关系：先写入数据，然后再提交记录。 事务提交（或中止）的关键点在于磁盘完成日志记录的时刻：在完成日志记录写之前如果发生了崩溃，则事务需要中止；如果在日志写入完成之后，即使发生崩溃，事务也会被安全提交。这就是在单一设备上上实现原子提交的核心思路。\n两阶段提交 两阶段提交（two-phase commit，2PC） 是一种在多节点之间实现事务原子提交的算法，用来确保所有节点要么 部提交，要么全部中止。\n2PC 中的提交/中止过程分为两个阶段（因此而得名），而不是单节点事务中的单个提交请求。2PC 的基本流程如下图所示：\n2PC 引入了单节点事务所没有的一个新组件——协调者（又称为事务管理器）。协调者通常实现为共享库，运行在请求事务相同进程中（也可以是单独的进程或服务）。\n通常，2PC 事务从应用程序在多个数据库节点上执行数据读/写开始。我们将这些数据库节点称为事务中的参与者。当应用程序准备提交事务时，协调者开始阶段 1：发送一个准备请求到所有节点，询问他们是否可以提交。协调者然后跟踪参与者的回应：\n 如果所有参与者回答是，表示他们已准备好提交，那么协调者接下来在阶段 2 会发出提交请求，提交开始实际执行。 如果有任何参与者回答否，则协调者在阶段 2 中向所有节点发送放弃请求。  系统承诺 该协议有两个关键的不归路：首先，当参与者投票是时，它做出了肯定提交的承诺（尽管还取决于其他的参与者的投票，协调者才能做出最后决定）。其次，协调者做出了提交（或者放弃）的决定，这个决定也是不可撤销。正是这两个承诺确保了 2PC 的原子性（而单节点原子提交其实是将两个事件合并，写入事务日志即提交） 。\n协调者故障  如果协调者本身发生了故障，那么此时该怎么处理呢？\n 2PC 能够顺利完成的唯一方法是等待协调者恢复。这就是为什么协调者必须在向参与者发送提交（或中止）请求之前要将决定写入磁盘的事务日志，协调者恢复之后通过读取事务日志来确定所有未决的事务状态。如果在协调 日志中没有完成提交记录就会中止。 此时 2PC 的提交点现在归结为协调者在常规单节点上的原子提交。\n三阶段提交 作为 2PC 的替代方案，目前也有三阶段提交算法。然而，3PC 假定一个有界的网络延迟和节点在规定时间内响应。考虑到目前大多数实际场景中存在无限网络延迟和进程暂停的情况，因此它无法保证原子性。\n通常，非阻塞原子提交依赖于一个完美的故障检测器，即有一个非常可靠的机制可以判断出节点是否已经崩溃。在无限延迟的网络环境中，超时机制并不是可靠的故障检测器，因为即使节点正常，请求也可能由于网络问题而最终超时。 正是由于这样的原因，尽管大家已经意识到上述协调者潜在的问题，但还在普遍使用 2PC。\n实际中的分布式事务 目前有两种截然不同的分布式事务概念：\n 数据库内部的分布式事务：某些分布式数据库（例如那些标配支持复制和分区的数据库）支持跨数据库节点 的内部事务。例如，VoltDB 和 MySQL Cluster 的 NDB 存储引擎就支持这样的内部分布式事务。此时，所有参与节点都运行着相同的数据库软件。 异构分布式事务：在异构分布式事务中，存在两种或两种以上不同的参与者实现技术。例如来自不同供应商的数据库，甚至是非数据库系统（如消息中间件）。即使是完全不同的系统，跨系统的分布式事务也必须确保原子提交。  Exactly-Once消息处理 异构的分布式事务旨在无缝集成多种不同的系统。例如，当且仅当数据库中处理消息事务成功提交，消息队列才会标记该消息已处理完毕。这个过程是通过自动提交消息和数据库写入来实现的。即使消息系统和数据库两种不同的技术运行在不同的节点上，采用分布式事务也能达到上述目标。\n如果消息发送或数据库事务任何一个发生失败，则两者必须中止，消息队列可以在稍后再次重传消息。因此，通过自动提交消息和消息处理的结果，可以确保消息可以有效处理有且仅有一次（成功之前有可能需要重试）。而如果事务最后发生终止，则放弃所有部分完成的结果。\n需要指出，只有在所有受影响的系统都使用相同的原子提交协议的前提下，这种分布式事务才是可行。\nXA事务 X/Open XA（eXtended Architecture，XA） 是异构环境下实施两阶段提交的一个工业标准，于 1991 年推出并得到广泛推广。目前，许多传统关系数据库（包括 PostgreSQL、MySQL、DB2、SQL Server、Oracle）和消息队列（包括 ActiveMQ、HornetQ、MSMQ、IBM MQ）都支持 XA。\nXA 并不是一个网络协议，而是一个与事务协调者进行通信的 API（它也支持其他语言的 API 绑定）。XA 假定应用程序通过网络或客户端的库函数与参与者（包括数据库、消息服务）节点进行通信。如果驱动程序支持 XA ，意味着应用可以调用 XA API 来确定操作是否是异构分布式事务的一部分。如果是，则发送必要的信息给数据库服务器。它还支持回调，这样协调者可以通过回调函数来通知所有参与者执行准备或者提交（或者中止）。\n分布式事务的限制 XA 事务解决了多个参与者之间如何达成一致这样一个非常现实而重要的问题，但它也引入了不少操作方面的限制：\n 如果协调者不支持数据复制，而是在单节点上运行，那么它就是整个系统的单点故障。而现实情况是，有许多协调者的实现默认情况下井非高可用，或者只支持最基本的复制。 许多服务器端应用程序都倾向于无状态模式（因为更受 HTTP 青睐），而所有的持久状态都保存在数据库中，这样应用服务器可以轻松地添加或删除实例。但是，当协调者就是应用服务器的一部分时，部署方式就发生了根本的变化。突然间，协调者的日志成为可靠系统的重要组成部分，它要求与数据库本身一样重要（需要协调者日志恢复那些有疑问的事务）。这样的应用服务器已经不再是无状态。 由于 XA 需要与各种数据系统保持兼容，所以它最终其实是多系统可兼容的最低标准。例如，它无法深入检测不同系统之间的死锁条件（因为这就将需要另一个标准化协议，使得多个系统交换事务所等待的锁信息），而且不适用于 SSI ，后者要求一个复杂的协议来识别不同系统间的写冲突。 对于数据库内部的分布式事务（而不是 XA），限制则少很多，例如 SSI 的分布式版本是可行的 。然而 2PC 要成功提交事务还是存在潜在的限制，它要求必须所有参与者都投票赞成， 如果有任何部分发生故障，整个事务只能失败。所以分布式事务有扩大事务失败的风险，这与我们构建容错系统的目标有些背道而驰。  支持容错的共识 共识问题通常形式化描述如下：一个或多个节点可以提议某些值，由共识算法来决定最终值。\n共识算法必须满足以下性质：\n 协商一致性：所有的节点都接受相同的决议。 诚实性：所有节点不能反悔，即对一项提议不能有两次决定。 合法性：如果决定了值 v，则 v 一定是由某个节点所提议的。 可终止性：节点如果不崩溃则最终一定可以达成决议。  协商一致性和诚实性属性定义了共识的核心思想：决定一致的结果，一旦决定，就不能改变。合法性属性主要是为了排除一些无意义的方案。可终止性则引入了容错的思想。 它重点强调一个共识算法不能原地空转，永远不做事情，换句话说，它必须取得实质性进展。即使某些节点出现了故障，其他节点也必须最终做出决定。\n可终止性属于一种活性，而另外三种则属于安全性方面的属性。\n共识算法与全序广播 最著名的容错式共识算法包括 VSR、Paxos、Raft、Zab。这些算法大部分其实并不是直接使用上述的形式化模型（提议井决定某个值，同时满足上面四个属性）。相反，他们是决定了一系列值，然后采用全序关系广播算法。\n全序关系广播的要点是，消息按照相同的顺序发送到所有节点，有且只有一次。如果仔细想想，这其实相当于进行了多轮的共识过程：在每一轮，节点提出他们接下来想要发送的消息，然后决定下一个消息的全局顺序。\n所以，全序关系广播相当于持续的多轮共识（每一轮共识的决定对应于一条消息）：\n 由于协商一致性，所有节点决定以相同的顺序发送相同的消息。 由于诚实性，消息不能重复。 由于合法性，消息不会被破坏，也不是凭空捏造的。 由于可终止性，消息不会丢失。  共识的局限性 共识算法对于分布式系统来说是一个巨大的突破：它为其他充满不确定性的系统带来了基础的安全属性（一致性，完整性和合法性），然而它们还能保持容错（只要多数节点正常工作且可达，就能取得进展）。它们提供了全序广播，因此它们也可以以一种容错的方式实现线性一致的原子操作。\n虽然有着上面这些好处，但这些好处的背后仍然存在一些代价：\n  在达成一致性决议之前，节点投票的过程是一个同步复制过程。 而数据库通常配置为异步复制，存在某些已提交的数据在故障切换时丢失的风险。\n  共识体系需要严格的多数节点才能运行。 如果由于网络故障切断了节点之间的连接，则只有多数节点所在的分区可以继续工作，剩下的少数节点分区则处于事实上的停顿状态\n  多数共识算法假定一组固定参与投票的节点集，这意味着不能动态添加或删除节点。\n  共识系统通常依靠超时机制来检测节点失效。 在网络延迟高度不确定的环境中，特别是那些跨区域分布的系统，经常由于网络延迟的原因，导致节点错误地认为主节点发生了故障。\n  共识算法往往对网络问题特别敏感。 例如，如果整个网络中存在一条网络连接持续不可靠， Raft 会进入一种奇怪的状态：它不断在两个节点之间反复切换主节点，当前主节点不断被赶下台，这最导致系统根本无法安心提供服务。\n  成员与协调服务 ZooKeeper 或 etcd 这样的项目通常被描述为 “分布式键值存储” 或 “协调与配置服务”。它们通常采用容错的全序广播算法在所有节点上复制这些数据从而实现高可靠。不仅如此，Zookeepr 还提供了以下这些特性：\n 线性化的原子操作：使用原子 CAS 操作可以实现锁：如果多个节点同时尝试执行相同的操作，只有一个节点会成功。共识协议保证了操作的原子性和线性一致性，即使节点发生故障或网络在任意时刻中断。分布式锁通常以租约（lease）的形式实现，租约有一个到期时间，以便在客户端失效的情况下最终能被释放。 操作全序：当某个资源受到锁或租约的保护时，你需要一个防护令牌来防止客户端在进程暂停的情况下彼此冲突。防护令牌是每次锁被获取时单调增加的数字。ZooKeeper 通过全序化所有操作来提供这个功能，它为每个操作提供一个单调递增的事务 ID（zxid）和版本号（cversion）。 故障检测：客户端在 ZooKeeper 服务器上维护一个长期会话，客户端和服务器周期性地交换心跳包来检查节点是否还活着。即使连接暂时中断，或者 ZooKeeper 节点失效，会话仍保持在活跃状态。但如果心跳停止的持续时间超出会话超时，ZooKeeper 会宣告该会话已死亡。当会话超时时（ZooKeeper 称这些节点为临时节点，即 ephemeral nodes），会话持有的任何锁都可以配置为自动释放。 更改通知：客户端不仅可以读取其他客户端创建的锁和值，还可以监听它们的变更。因此，客户端可以知道另一个客户端何时加入集群（基于新客户端写入 ZooKeeper 的值），或发生故障（因其会话超时，而其临时节点消失）。通过订阅通知，客户端不用再通过频繁轮询的方式来找出变更。  节点任务分配 ZooKeeper 和 Chubby 系统非常适合节点任务分配这一场景，例如：\n 如果系统有多个流程或服务的实例，并且需求其中的一个实例充当主节点；而如果主节点失效，由其他某个节点来接管。（例如主从复制数据库、作业调度系统等） 对于一些分区资源（可以是数据库，消息流，文件存储，分布式 actor system 等），需要决定将哪个分区分配给哪个节点。当有新节点加入集群时， 需要将某些现有分区从当前节点迁移到新节点， 从而实现负载动态平衡。 当节点移除或失败时，其他节点还需要接管失败节点。  上述场景中的任务，可以借助 ZooKeeper 中的原子操作，ephemeral nodes 和通知机制来实现。\n服务发现 ZooKeeper、Etcd、Consul 还经常用于服务发现。例如需要某项服务时，应该连接到哪个 IP 地址等。在典型的云环境中，虚拟机可能会起起停停，这种动态变化的节点无法提前知道服务节点的 IP 地址，因此，可以这样配置服务，每当节点启动时将其网络端口信息、向 ZooKeeper等服务注册，然后其他人只需向 ZooKeeper 的注册表中询问即可。\n成员服务 ZooKeeper 等还可以看作是成员服务范畴的一部分。成员服务用来确定当前哪些节点处于活动状态并属于集群的有效成员。 由于无限的网络延迟，无法可靠地检测一个节点究竟是否发生了故障。但是，可以将故障检测与共识绑定在一起，让所有节点就节点的存活达成一致意见。\n这里依然存在发生误判的可能性，即节点其实处于活动状态、却被错误地宣判为故障。 即使这样，系统就成员资格问题的决定是全体一致的，这是最重要的。例如，选举主节点的方式可能是简单地投票选择编号最小的节点， 一旦节点对于当前包含哪些成员出现了不同意见，那么共识过程就无法继续。\n","date":"2022-07-10T08:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%9D%E4%B8%80%E8%87%B4%E6%80%A7%E4%B8%8E%E5%85%B1%E8%AF%86/","title":"数据密集型应用系统设计 学习笔记（九）：一致性与共识"},{"content":"分布式系统的挑战 故障与部分失效 在分布式系统中，可能会出现系统的一部分工作正常，但其他某些部分出现难以预测的故障，我们称之为部分失效。问题的难点就在于这种部分失效是不确定的:如果涉及多个节点和网络，几乎肯定会碰到有时网络正常，有时则莫名的失败。\n正是由于这种不确定性和部分失效大大提高了分布式系统的复杂性。要使分布式系统可靠工作，就必然面临部分失效，这就需要依靠软件系统来提供容错机制。换句话说，我们需要在不可靠的组件之上构建可靠的系统。\n不可靠的网络 互联网和数据中心中的大多数内部网络都是异步分组网络（asynchronous packet networks）。在这种网络中，一个节点可以向另一个节点发送一个消息，但是网络不能保证它什么时候到达，甚至是能否到达。发送之后等待响应过程中，有很多事情可能会出错：\n  请求可能已经丢失（可能有人拔掉了网线）。\n  请求可能正在排队，稍后将交付（也许网络或接收方过载）。\n  远程节点可能已经失效（比如是崩溃或关机）。\n  远程节点可能暂时停止了响应（遇到长时间的垃圾回收）。\n  远程节点可能已经处理了请求，但是网络上的响应已经丢失（可能是网络交换机配置错误）。\n  远程节点可能已经处理了请求，但是响应被延迟处理（可能是网络或者你自己的机器过载）。\n  发送者甚至不清楚数据包是否完成了发送，只能选择让接收者来回复响应消息，但回复也有可能丟失或延迟。这些问题在一个异步网络中无法明确区分，发送者拥有的唯一信息是，尚未收到响应，但却无法判定具体原因。\n处理这个问题通常采用超时机制，即在等待一段时间之后，如果仍然没有收到回复则选择放弃，并且认为响应不会到达。 但是，即使判定超时，仍然并不清楚远程节点是否收到了请求（一种情况，请求仍然在某个地方排队，即使发送者放弃了，但最终请求会发送到接收者)。\n 那如何确定这个超时时间呢？\n  较长的超时值意味着更长时间的等待，才能宣告节点失效。而在此期间， 用户只能等待或者拿到错误信息。 较短的超时设置可以帮助快速检测故障，但可能会出现误判，例如实际上节点只是出现暂时的性能波动(由于节点或网络上的高负载峰值，结果却被错误地宣布为失效。  这并没有一个标准的答案，而需要结合具体的业务场景来进行分析。 通常通过实验的方式来一步步设置超时。先在多台机器上，多次测量网络往返时间，以确定延迟的大概范围，然后结合应用特点，在故障检测与过早超时风险之间选择一个合适的中间值。\n超时设置并不是一个不变的常量，而是需要持续测量响应时间及其变化，然后根据最新的响应时间分布来自动调整。 可以用 Phi Accrual 故障检测器完成，该检测器目前已在 Akka 和 Cassandra 中使用。TCP的重传超时也采用了类似的机制。\n不可靠的时钟 在分布式系统中， 时间总是件棘手的问题，由于跨节点通信不可能即时完成，消息由网络从一台机器到另一台机器总是需要花费时间。收到消息的时间应该晚于发送的时间，但是由于网络的不确定延迟，精确测量面临着很多挑战。这些情况使得多节点通信时很难确定事情发生的先后顺序。\n同时，网络上的每台机器都有自己的时钟硬件设备。这些设备并非绝对准确，即每台机器都维护自己本地的时间版本，可能比其他机器稍快或更慢。\n时钟 现代计算机内部至少有两种不同的时钟，一个是日历时钟（time-of-day clock）， 另一个是单调时钟（monotonic clock）。虽然它们都可以衡量时间，但要仔细区分二者，本质上他们是服务于不同的目的。\n日历时钟 日历时钟会根据某个日历返回当前的日期与时间。 例如，Linux 上的 clock_gettime(CLOCK_REALTIME) 会返回自纪元 1970 年 1 月 1 日（ UTC ）以来的秒数和毫秒数，不含闺秒。而有些系统则使用其他日期作为参考点。\n日历时钟通常与 NTP 服务器同步，这意味着来自一台机器的时间戳（理想情况下）与另一台机器上的时间戳相同。但是，如果本地时钟远远快于 NTP 服务器，则它可能会被强制重置，跳回到先前的某个时间点。这种跳跃经常忽略闰秒 ，导致其不太适合测量时间间隔。可以在一定程度上同步机器之间的时钟，最常用的方法也是网络时间协议 NTP （Network Time Protocl），它可以根据 组专门的时间服务器来调整本地时间，时间服务器则从精确更高的时间源获取高精度时间。\n单调时钟 单调时钟适用于测量持续时间（时间间隔），例如超时或服务的响应时间。Linux 上的 clock_gettime(CLOCK_MONOTONIC) 返回的即是单调时钟。单调时钟的名字来源于它们保证总是向前走（日历时钟会出现的回拨现象）。\n可以在一个时间点读取单调时钟的值，完成某项工作，然后再次检查时钟。时钟值之间的差值即两次检查之间的时间间隔。==注意，单调时钟的绝对值井没有任何意义，它可能是电脑启动以后经历的纳秒数或者其他含义。因此比较不同节点上的单调时钟值毫无意义，它们没有任何相同的基准==。\n在分布式系统中，可以采用单调时钟测量一段任务的持续时间（例如超时 ），它不假定节点间有任何的时钟同步，且可以容忍轻微测量误差。\n进程暂停 在分布式环境中，进程暂停有时会引起意想不到的时钟问题。\n假如有这样一个场景，主节点从其他节点获得一个租约，类似一个带有超时的锁。为了维持主节点的身份，节点必须在到期之前定期去更新租约 。如果节点发生了故障，则续约失败，这样另一个节点到期之后就可以接管。代码逻辑如下：\n1 2 3 4 5 6 7 8 9 10 11  while (true) { request = getIncomingRequest(); // 确保租约还剩下至少10秒  if (lease.expiryTimeMillis - System.currentTimeMillis() \u0026lt; 10000){ lease = lease.renew(); } if (lease.isValid()) { process(request); } }   上述代码依赖于同步的时钟，即租约到期时间由另一台机器所设置，并和本地时钟进行比较。如果时钟之间有超过几秒的差异，这段代码会出现些奇怪的事情。\n即使我们改为仅使用本地单调时钟，还有另一个问题：代码假定时间检查点 System.currentTimeMillis() 与请求处理 process(request) 间隔很短，通常代码运行足够快，所以设置 10 秒的缓冲区来确保在请求处理过程中租约不会过期。\n如果程序执行中出现了某些意外的暂停呢？假设线程在 lease.isValid() 消耗了了整 15 秒。那么当开始处理请求时，租约已经过期，而另一个节点已经接管了主节点。可惜我们无告有效通知线程暂停了这么长时间了，后续代码也不会注意到租约已经到期，除非运行到下一个循环迭代。不过，到那个时候它已经做了不安全的请求处理。\n 那什么原因会带来这么长时间的暂停呢？可能会存在以下事件\n  许多编程语言都有垃圾收集器（ GC ），有时运行期间会暂停所有正在运行的线程。 在虚拟化环境中，可能会暂停虚拟机（暂停所有执行进程并将内存状态保存到磁盘）然后继续（从内存中加载数据然后继续执行）。 运行在终端用户设备时，执行也可能发生暂停。例如用户关机脑或休眠。 当操作系统执行线程上下文切换时，或者虚拟机管理程序切换到另一个虚拟机时，正在运行的线程可能会在代码的任意位置被暂停。 如果应用程序执行同步磁盘访问，则线程可能暂停，直到缓慢的磁盘 I/O 操作完成。 如果操作系统配置了基于磁盘的内存交换分区，内存访问可能触发缺页中断，进而需要从磁盘中加载内存页。 当 I/O 进行时线程为暂停。 发送 SIGSTOP 信号来暂停 UNIX 进程，例如通过在 shell 中按下 Ctrl+Z。 这个信号立即阻止进程继续执行更多的 CPU 周期，直到 SIGCONT 恢复为止，此时它才会继续运行。  所有这些事件都可以随时抢占正在运行的线程，并在稍后的时间恢复运行，而线程甚至不会注意到这一点。这个问题类似于在单个机器上使多线程代码线程安全，因此你不能对时序做任何假设，因为随时可能发生上下文切换，或者出现并行运行。\n所以分布式系统中的每个节点都必须假定，执行过程中的任何时刻都可能被暂停相当长一段时间，即使是运行在某个函数中间 。 暂停期间，整个集群的其他部分都在照常运行，甚至会一致将暂停的节点宣告为故障节点。最终，暂停的节点可能会回来继续运行，除非再次检查时钟，否则它对刚刚过去的暂停一无所知。\n知识、真相与谎言 真相由多数决定 假定在一个发生了非对称故障的网络环境中，即某节点能够收到发送给它的消息，但是该节点发出的所有消息要么被丢弃，要么被推迟发送。即使该节点本身运行良好，可以接收来自其他节点的请求，但其他节点却无法顺利收到响应。当消息超时之后，由于都收不到回复，其他节点就会一致声明上述节点发生失效。\n综上所述，节点不能根据自己的信息来判断自身的状态。分布式系统不能完全依赖单个节点，因为节点可能随时失效，也可能暂停或者假死，甚至最终无法恢复。目前许多分布式算法都依赖于法定人数，即在节点之间进行投票。任何决策都需要来自多个节点的最小投票数，以减少对于某个特定节点的依赖。\n最常见的法定人数是取系统节点半数以上。如果某些节点发生故障，quorum 机制可以使系统继续工作（对于三个节点的系统，可以容忍一个节点失效）。由于系统只可能存在一个多数，绝不会有两个多数在同时做出相互冲突的决定，因此系统的决议是可靠的 （防止脑裂）。\n拜占庭故障 如果节点存在撒谎的情况（即故意发送错误的或破坏性的响应），那么分布式系统处理的难度就上了一个台阶。例如，节点明明没有收到某条消息，但却对外声称收到了。这种行为称为拜占庭故障，在这样不信任的环境中需要达成共识的问题也被称为拜占庭将军问题。\n 拜占庭将军问题 拜占庭将军问题是所谓“两将军问题”的更抽象标识，后者假定有两名将军需要就战斗计划达成一致。由于他们在两个不同的地点建立了营地， 中间只能通过信使进行沟通，而信使在传递消息时可能会出现延迟或丢失（就像网络中的信息包一样）。\n而在拜占庭版本中，有 n 位将军需要达成共识，并且其中存在一些叛徒试图阻挠共识的达成，即使大多数的将军都是忠诚的，发出了真实的信息，但是叛徒则试图通过发送虚假或不真实的信息来欺骗和混淆他人（同时努力隐藏自己）。而且大家事先并不知道叛徒是谁。\n 如果某个系统中即使发生部分节点故障，甚至不遵从协议，或者恶意攻击、干扰网络，但仍可继续正常运行，那么我们称之为拜占庭式容错系统。这种担忧在某些场景下是合理的：\n 在航空航天领域，计算机内存或 CPU 寄存器中的数据可能会被辐射而发生故障，导致以不可预知的方式响应其他节点。这种情况下如果将系统下线，代价将异常昂贵（例如，可能出现飞机撞毁或致使火箭与国际空间站相撞等），飞行控制系统必须做到容忍拜占庭故障 。 在有多个参与者的系统中，某些参与者可能会作弊或者欺骗他人。这时节点不能完全相信另一个节点所发送的消息，它可能就是恶意的。例如，像比特币和其他区块链一样的点对点网络就是让互不信任的当事方就某项交易达成一致，且不依赖于集中的机制。  大多数拜占庭容错算法要求系统超过三分之二的节点功能正常，例如有四个节点，则最多允许一台发生故障）。要采用这类算法对付 bug ，必须有四种不同的软件实现，然后希望该 bug 只出现在四个实现中的一个。但这并不现实，通常如果攻击者可以入侵一个节点，则很可能会攻陷几乎所有节点（由于运行相同的软件）。因此，传统的安全措施如认证、访问控制、加密、防火墙等仍是防范攻击的主要保护机制。\n理论系统模型与现实 目前分布式系统方面已有许多不错的具体算法（如共识算法 Raft、Paxos 等），而算法的实现不能过分依赖特定的硬件和软件配置。这就要求我们需要对预期的系统错误进行形式化描述。我们通过定义一些系统模型来形式化描述算法的前提条件。\n关于计时方面有三种常见的系统模型：\n 同步模型：假设网络延迟、进程暂停和和时钟误差都是受限的。这并不意味着完全同步的时钟或网络延迟为零。这只意味着你清楚了解网络延迟、进程暂停和时钟漂移将永远不会超过某个固定的上限。大多数实际系统的现实模型并不是同步模型，因为无限延迟和暂停确实会发生。 部分同步模型：意味着一个系统在大多数情况下像一个同步系统一样运行，但有时候会超出网络延迟、进程暂停和时钟漂移的界限。这是一个比较现实的模型，大多数情况下，网络和进程表现良好（否则无法持续提供服务），但是我们必须承认，在任何时刻都存在时序假设偶然被破坏的事实。而一旦发生这种情况，网络延迟、暂停和时钟错误可能会变得相当大。 异步模型：在这个模型中，一个算法不允许对时序做任何假设——事实上它甚至没有时钟（所以它不能使用超时）。一些算法被设计为可用于异步模型，但非常受限。  除了时机之外，我们还需要考虑节点失效。有以下三种最常见的节点失效系统模型：\n  崩溃-中止：在崩溃-中止模型中，算法假设一个节点只能以一种方式发生故障，即遭遇系统\n崩溃。这意味着节点可能在任何时候突然停止响应，且该节点以后永远消失，无法恢复。\n  崩溃-恢复：节点可能会在任何时候发生崩溃，且可能会在一段时间之后得到恢复并再次响应。在崩溃－恢复模型中，节点上持久性存储的数据会在崩溃之后得以保存，而内存中状态可能会丢失。\n  拜占庭（任意）故障：节点可以做任何事情，包括试图戏弄和欺骗其他节点。\n  对于真实系统的建模，最普遍的组合是崩溃-恢复模型结合部分同步模型。\n为了定义算法的正确性，我们可以描述它的一些属性信息。如果针对某个系统模型的算法在各种情况下都能满足定义好的属性要求，那么我们称这个算法是正确的。而其中最重要的两种属性是安全性与活性。安全性通常可以理解为没有发生意外，而活性则类似预期的事情最终一定会发生（如最终一致性）。\n对于分布式算法，要求在所有可能的系统模型下，都必须符合安全属性。也就是说，即使所有节点都发生崩溃，或者整个网络中断，算法确保不会返回错误的结果。 而对于活性，则存在一些必要条件。例如，只有在多数节点没有崩溃，以及网络最终可以恢复的前提下，我们才能保证最终可以收到响应。 部分同步模型的定义即要求任何网络中断只会持续一段有限的时间，然后得到了修复，系统最终返回到同步的一致状态。\n","date":"2022-07-10T07:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AB%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8C%91%E6%88%98/","title":"数据密集型应用系统设计 学习笔记（八）：分布式系统的挑战"},{"content":"事务 深入理解事务 ACID 事务所提供的安全保证即大家所熟知的 ACID ，分别代表原子性（Atomicity）， 一致性（Consistency），隔离性（Isolation）与持久性（Durability）。\n而不符合 ACID 标准的系统有时被称为 BASE ，取另外几个特性的首字母，即基本可用性（Basically Available），软状态（Soft State） 和最终一致性（ Eventual consistency）。\n原子性 ACID 中原子性所定义的特征是：在出错时中止事务，并将部分完成的写入全部丢弃。\n假如没有原子性保证，当多个更新操作中间发生了错误，就需要知道哪些更改已经生效，哪些没有生效，这个寻找过程会非常麻烦。或许应用程序可以重试，但情况类似，并且可能导致重复更新或者不正确的结果。原子性则大大简化了这个问题：如果事务已经中止，应用程序可以确定没有实质发生任何更改，所以可以安全地重试。\n一致性 ACID 中的一致性的主要是指对数据有特定的预期状态，任何数据更改必须满足这些状态约束（或者恒等条件）。\n例如一个账单系统，账户的贷款余额应和借款余额保持平衡。如果某事务从一个有效的状态开始，并且事务中任何更新操作都没有违背约束，那么最后的结果依然符合有效状态。\n这种一致性本质上要求应用层来维护状态一致（或者恒等），应用程序有责任正确地定义事务来保持一致性。这不是数据库可以保证的事情：即如果提供的数据修改违背了恒等条件，数据库很难检测进而阻止该操作（数据库可以完成针对某些特定类型的恒等约束检查，例如使用外键约束或唯一性约束。但通常主要靠应用程序来定义数据的有效／无效状态，数据库主要负责存储）。\n隔离性 ACID 语义中的隔离性意味着并发执行的多个事务相互隔离，它们不能互相交叉。\n例如，如果某个事务进行多次写入，则另一个事务应该观察到的是其全部完成（或者一个都没完成）的结果，而不应该看到中间的部分结果。\n持久性 持久性即保证事务提交成功，即使存在硬件故障或数据库崩溃， 事务所写入的任何数据也不会消失。\n对于单节点数据库，持久性通常意味着数据已被写入非易失性存储设备，如硬盘或 SSD。在写入执行过程中，通常还涉及预写日志等， 这样万一\t磁盘数据损坏可以进行恢复。而对于支持远程复制的数据库，持久性则意味着数据已成功复制到多个节点。为了实现持久性的保证，数据库必须等到这些写入或复制完成之后才能报告事务成功提交。\n弱隔离级别 可串行化的隔离会严重影响性能，而许多数据库却不愿意牺牲性能，因而更多倾向于采用较弱的隔离级别，它可以防止某些但并非全部的并发问题。\n读已提交 读已提交是最基本的的事务隔离级别，它只提供以下两个保证：\n 读数据库时，只能看到巳成功提交的数据（防止脏读）。 写数据库时，只会覆盖已成功提交的数据（防止脏写）。  某些数据库提供更弱的隔离级别，称为读未提交。它只防止脏写，而不防止脏读。\n脏读 假定某个事务已经完成部分数据写入，但事务尚未提交（或中止），此时如果可以看到尚未提交的数据的话，那就是脏读。\n脏写 如果两个事务同时尝试更新相同的对象，先前的写入是尚未提交的事务的一部分，如果被覆盖，那就是脏写。\n实现方法 数据库通常采用行级锁来防止脏写。当事务想修改某个对象时，它必须首先获得该对象的锁，然后一直持有锁直到事务提交（或中止）。 给定时刻，只有一个事务可以拿到特定对象的锁，如果有另一个事务尝试更新同一个对象，则必须等待，直到前面的事务完成了提交（或中止）后，才能获得锁并继续。\n但上述方法应用于脏写则不够理想。因为运行时间较长的写事务会导致许多只读的事务等待太长时间，这会严重影响只读事务的响应延迟，且可操作性差。\n因此，大多数数据库采用 MVCC 的方式来实现。对于每个待更新的对象，数据库都会维护其旧值和当前持锁事务将要设置的新值两个版本。在事务提交之前，所有其他读操作都读取旧值；仅当写事务提交之后，才会切换到读取新值。\n快照隔离级别与可重复读  快照级别隔离对于只读事务特别有效。但是，具体到实现，许多数据库却对它有不同的命名。 Oracle 称之为可串行化， PostgreSQL、MySQL 则称为可重复读。\n这种命名混淆的原因是 SQL 标准并没有定义快照级别隔离，而仍然是基于老的 System R 1975 年所定义的隔离级别，而当时还没有出现快照级别隔离。标准定义的是可重复读，这看起来比较接近于快照级别隔离，所以 PostgreSQL、MySQL 称它们的快照级别隔离为可重复读，这符合标准要求（即合规性）。\n 实现方法 为了实现快照隔离级别，数据库采用了一种防止脏读但却更为通用的机制。考虑到多个正在进行的事务可能会在不同的时间点查看数据库状态，所以数据库保留了对象多个不同的提交版本，这种技术因此也被称为多版本并发控制（Multi-Version Concurrency Control，MVCC）\n如果只是为了提供读已提交级别隔离，而不是完整的快照隔离级别，则只保留对象的两个版本就足够了：一个己提交的旧版本和尚未提交的新版本。所以，支持快照隔离级别的存储引擎往往直接采用 MVCC 来实现读已提交隔离。典型的做法是，在读已提交级别下，对每一个不同的查询单独创建一个快照；而快照隔离级别级别隔离则是使用一个快照来运行整个事务。\n接下来看看 PostgreSQL 是如何实现基于 MVCC 的快照隔离级别 （其他实现基本类似）。\n当事务开始时，首先赋予一个唯一的、单调递增的事务 ID（txid）。每当事务向数据库写入新内容时，所写的数据都会被标记写入者的事务 ID。\n表中的每行都有 created_by 字段，其中包含了创建该行的事务 ID 。还有一个 deleted_by 字段，初始为空。 如果事务要删除某行，该行实际上并未从数据库中删除，而只是将 deleted_by 字段设置为请求删除的事务 ID （标记删除） 。事后，当确定没有其他事务引用该标记删除的行时，数据库的垃圾回收进程才去真正删除井释放存储空间。这样一笔更新操作在内部会被转换为一个删除操作加一个创建操作。\n如上图，事务 13 从账户中扣除 100 美元，余额从 500 美元减为 400 美元。 accounts 表里会出现两行账户 2 。一行是余额为 500 但标记为删除（由事务 13 删除），另一个余额为 400 ，由事务 13 创建。\n一致性快照的可见规则 当事务读数据库时·，通过事务 ID 可以决定哪些对象可见，哪些不可见。要想对上层应用维护好快照的一致性，需要精心定义数据的可见性规则。例如：\n 每笔事务开始时，数据库列出所有当时尚在进行中的其他事务（即尚未提交或中止），然后忽略这些事务完成的部分写入（尽管之后可能会被提交），即不可见。 所有中止事务所做的修改全部不可见。 较晚事务ID（即晚于当前事务）所做的任何修改不可见，不管这些事务是否完成了提交。 除此之外，其他所有的写入都对应用查询可见。  换句话说，仅当以下两个条件都成立， 主数据对象对事务可见：\n  事务开始的时刻，创建该对象的事务已经完成了提交。\n  对象没有被标记为删除；或者即使标记了，但删除事务在当前事务开始时还没有完成提交。\n  防止丢失更新 写事务并发除了脏写以外，还会带来其他一些值得关注的冲突问题，最著名的就是更新丢失问题。\n更新丢失可能发生在这样一个操作场景中：应用程序从数据库读取某些值，根据应用逻辑做出修改，然后写回新值 （read-modify-write）。 当有两个事务在同样的数据对象上执行类似操作时，由于隔离性，第二个写操作并不包括第一个事务修改后的值，最终会导致第一事务的修改值可能会丢失。\n这种冲突还可能在其他不同的场景下发生，例如：\n 递增计数器，或更新账户余额（需要读取当前值，计算新值井写回更新后的值）。 对某复杂对象的一部分内容执行修改，例如对 JSON 文档中一个列表添加新元素（需要读取并解析文档，执行更改井写回修改后的文档）。 两个用户同时编辑 wiki 页面，且每个用户都尝试将整个页面发送到服务器，覆盖数据库中现有内容以使更改生效 。  并发写事务冲突是一个普遍问题，目前有多种可行的解决方案。\n原子写操作 许多数据库提供了原子更新操作，以避免在应用层代码完成读取-修改-写回操作，如果支持的话，通常这就是最好的解决方案。例如，以下指令在多数关系数据库中都是井发安全的：\n1  UPDATEcountersSETvalue=value+1WHEREkey=\u0026#39;foo\u0026#39;;  原子操作通常采用对读取对象加独占锁的方式来实现，这样在更新被提交之前不会让其他事务读取它。这种技术有时被称为游标稳定性。另一种实现方式是强制所有的原子操作都在单线程上执行。\n显式加锁 如果数据库不支持内置原子操作，另一种防止更新丢失的方法是由应用程序显式锁定待更新的对象。 然后，应用程序可以执行读取-修改-写回这样的操作序列。此时如果有其他事务尝试并发读取对象，则必须等待当前正在执行的序列全部完成。\n例如这样一个场景，在一个多人游戏中，几个玩家可以同时移动同 个数字。只靠原子操作可能还不够，因为应用程序还需要确保玩家的移动还需遵守其他游戏规则，这涉及应用层逻辑，不可能将其剥离转移给数据库层在查询时执行。此时，可以使用锁来防止两名玩家同时操作相同的棋子，如下列代码：\n1 2 3 4 5 6 7 8 9  BEGINTRANSACTION;SELECT*FROMfiguresWHEREname=\u0026#39;robot\u0026#39;ANDgame_id=222FORUPDATE;-- FOR UPDATE 指令指示数据库对返回的所有结果行要加锁。 -- 检查玩家的操作是否有效，然后更新先前SELECT返回棋子的位置。 UPDATEfiguresSETposition=\u0026#39;c4\u0026#39;WHEREid=1234;COMMIT;  原子比较和设置 原子操作和锁都是通过强制读取-修改-写回操作序列串行执行来防止丢失更新。另一种思路则是先让他们并发执行，但是如果事务管理器检测到了更新丢失的风险，则会中止当前事务，并强制回退到安全的读取-修改-写回方式。\n该方法的一个优点是数据库完全可以借助快照级别隔离来高效地执行检查。的确，PostgreSQL 的可重复读， Oracle 的可串行化以及 SQL Server 的快照级别隔离等，都可以自动检测何时发生了更新丢失，然后会中止违规的那个事务。但是，MySQL/InnoDB 的可重复读却并不支持检测更新丢失。有一些观点认为，数据库必须防止更新丢失，要不然就不能宣称符合快照级别隔离，如果基于这样的定义，那么 MySQL 就属于没有完全支持快照级别隔离。\n冲突解决和复制 对于支持多副本的数据库，防止丢失更新还需要考虑另一个维度 ：由多节点上的数据副本，不同的节点可能会并发修改数据，因此必须采取一些额外的措施来防止丢失更新。\n加锁和原子修改都有个前提即只有一个最新的数据副本。然而，对于多主节点或者无主节点的多副本数据库，由于支持多个井发写 ，且通常以异步方式来同步更新，所以会出现多个最新的数据副本。此时加锁和原子比较将不再适用。\n多副本数据库通常支持多个井发写，然后保留多个冲突版本，之后由应用层逻辑或依靠特定的数据结构来解决、合并多版本。\n如果操作可交换（顺序无关，在不同的副本上以不同的顺序执行时仍然得到相同的结果），则原子操作在多副本情况下也可以工作。例如，计数器递增或向集合中添加元素等都是典型的可交换操作。\n写倾斜与幻读 写倾斜 当两笔事务根据读取相同的一组记录进行条件判断通过后，更新了不同的记录对象，刚刚的写操作改变了决定的前提条件，结果可能违背了业务约束要求，此时的异常情况称为写倾斜。只有可串行化的隔离级别才能防止这种异常。\n写倾斜看起来很晦涩拗口，下面举几个常见场景：\n 会议室预订系统：要求在同一时间内，同一个会议室不能被预订两次。 用户名：网站通常要求每个用户有唯一的用户名，两个用户可能同时尝试创建相同的用户名。 防止双重开支：支付或积分相关的服务通常需要检查用户的花费不能超过其限额。如果有两笔交易同时进行，两个交易各自都不能超额，也不能加在一起后超额。  可以将写倾斜视为一种更广义的更新丢失问题。 即如果两个事务读取相同的一组对象，然后更新其中一部分：不同的事务可能更新不同的对象，则可能发生写倾斜。不同的事务如果更新的是同一个对象，则可能发生脏写或更新丢失。\n幻读 事务读取了某些符合查询条件的对象，同时另一个客户端执行写入，改变了先前的查询结果。 快照隔离可以防止只读查询的幻读，但写倾斜情况则需要特殊处理，例如采用区间范围锁。\n串行化 可串行化隔离通常被认为是最强的隔离级别。 它保证即使事务可能会井行执行，但最终的结果与每次一个即串行执行结果相同。 这意味着，如果事务在单独运行时表现正确，那么它们在并发运行时结果仍然正确，换句话说，数据库可以防止所有可能的竞争条件。\n目前大多数提供可串行化的数据库都使用了以下三种技术之一：\n 严格按照串行顺序执行 两阶段加锁 乐观井发控制技术，例如可串行化的快照隔离。  严格串行执行 解决并发问题最直接的方法是避免并发，即在一个线程上按顺序方式每次只执行一个事务。这样我们完全回避了诸如检测、防止事务冲突等问题，其对应的隔离级别是严格串行化的。\n当满足以下约束条件时，串行执行事务可以实现串行化隔离：\n 事务必须简短而高效，否则一个缓慢的事务将会影响到所有事务的执行性能。 仅限于活动数据集完全可以加载到内存的场景。有些很少访问的数据可能会被移动到磁盘，但万一单线程事务需要访问它，就会严重拖累性能 。 写入吞吐量必须足够低，才能在单个 CPU 核上处理，否则就需要采用分区，最好没有跨分区事务。 跨分区事务虽然也可以支持，但是占比必须很小。  两阶段加锁  虽然两阶段加锁（2PL）听起来和两阶段提交（two-phase commit，2PC）很相近，但它们是完全不同的东西。\n 近三十年来，可以说数据库只有一种被广泛使用的串行化算法，那就是两阶段加锁（two-phase locking，2PL）。\n两阶段加锁与前面提到的加锁方法类似，但锁的强制性更高。多个事务可以同时读取同一对象，但只要出现任何写操作（包括修改或删除），则必须加锁以独占访问：\n 如果事务 A 已经读取了某个对象，此时事务 B 想要写入该对象，那么就必须等到 事务 A 提交或中止之才能继续。以确保事务 B 不会在事务 A 执行的过程中间去修改对象。 如果事务 A 已经修改了对象， 此时事务 B 想要读取该对象， 则事务 B 必须等到事务 A 提交或者中止之后才能继续。对于 2PL ，不会出现读到旧值的情况。  因此 2PL 不仅在并发写操作之间互斥，读取也会和修改产生互斥。另一方面，因为 2PL 提供了串行化，所以它可以防止前面讨论的所有竞争条件，包括更新丢失和写倾斜。\n实现方法 目前， 2PL 已经用于 MySQL（InnoDB）和 SQL Server 中的可串行化， 以及 DB2 中的可重复读。\n数据库的每个对象都有一个读写锁来隔离读写操作，即锁可以处于共享模式或独占模式。 基本用法如下：\n 如果事务要读取对象 ，必须先以共享模式获得锁。 可以有多个事务同时获得一个对象的共享锁，但是如果某个事务已经获得了对象的独占锁，则所有其他事务必须等待。 如果事务要修改对象，必须以独占模式获取锁。 不允许多个事务同时持有该锁（包括共享或独占模式），换句话说，如果对象上已被加锁，则修改事务必须等待。 如果事务先是读取对象，然后尝试写入对象，则需要将共享锁升级为独占锁。 升级锁的流程等价于直接获得独占锁。 事务获得锁之后，一直持有锁直到事务结束（包括提交或中止）。 这也是名字两阶段的来由，在第一阶段（事务执行前）获取锁，第二阶段（事务结束后）释放锁。  由于使用了很多锁机制，所以很容易出现死锁 ，例如事务 A 可能在等待事务 B 释放它的锁， 事务 B 在等待事务 A 释放所持有的锁（循环等待）。数据库系统会自动检测事务之间的死锁情况（常见方法是超时或者回路检测），并强行中止其中的一个以打破僵局，这样另一个可以继续向前执行，而被中止的事务需要由应用层来重试。\n谓词锁 在前面的章节我们提到了幻读的问题，那么可串行化是如何解决幻读的呢？\n我们需要引入一种谓词锁。它的作用于之前描述的共享/独占锁， 区别在于，它并不属于某个特定的对象（行），而是作用于满足某些搜索条件的所有查询对象（区间）。\n谓词锁甚至可以保护数据库中那些尚不存在但可能马上会被插入的对象（幻读）。将两阶段加锁与谓词锁结合使用，数据库可以防止所有形式的写倾斜以及其他竞争条件，隔离变得真正可串行化。\n索引区间锁 但是由于谓词锁性能不佳，如果活动事务中存在过多的锁，那么检查匹配这些锁就变得非常耗时。因此，大多数使用 2PL 的数据库实际上实现的是索引区间锁（index-range locking，也称为next-key locking）。本质上它是对谓词锁的简化或者近似。\n简化谓词锁的方式是将其保护的对象扩大化到一整个索引上。 例如存在下面这样一个场景，我们有一个酒店的管理系统，假设谓词锁保护的查询条件是：房间 123 ，时间段是周六 。\n 如果索引建立在房间号上：此时数据库会将共享锁附加到房间号索引上，此时会锁定 123 号房间的所有记录。 如果索引建立在日期上：此时数据库会将共享锁附加到日期上，此时会锁定周六的所有记录。  无论哪种方式，查询条件的近似值都附加到某个索引上。 接下来，如果另一个事务想要插入、更新或删除同一个房间或重叠时间段的预订，则肯定需要更新这些索引，一定就会与共享锁冲突，因此会自动处于等待状态直到共享锁释放。\n这样就有效防止了 写倾斜和幻读问题。 的确，索引区间锁不像谓词锁那么精确，但由于开销低得多，可以认为是一种很好的折中方案。\n如果没有合适的索引可以施加区间锁，则数据库可以回退到对整个表施加共享锁。 这种方式的性能肯定不好，它甚至会阻止所有其他事务的写操作，但的确可以保证安全性。\n性能 两阶段加锁的主要缺点在于其性能，其事务吞吐量和查询响应时间相比于其他弱隔离级别下降非常多。\n其主要原因有以下几点：\n 锁的获取和释放本身的开销大：开销大的同时也降低了事务的并发性。两个并发事务如果做任何可能导致竞争条件的事情，其中一个必须等待对方完成。一旦出现多个事务同时访问同一对象，会形成一个等待队列，事务就必须等待队列前面所有其他事务完成之后才能继续。 数据库的访问延迟具有非常大的不确定性：某个事务本身很慢，或者是由于需要访问大量数据而获得了许多锁， 则它还会导致系统的其它部分都停顿下来。如果应用需要稳定的性能，这种不确定性就是致命的。 死锁变得更为频繁：如果事务由于死锁而被强行中止，应用层就必须从头重试，假如死锁过于频繁，则最后的性能和效率必然大打折扣。  可串行化的快照隔离 两阶段加锁虽然可以保证串行化，但性能差强人意且无法扩展（由于串行执行）。弱级别隔离虽然性能不错，但容易引发各种边界条件（如更新丢失，写倾斜 ，幻读等）。那有什么方法可以使性能和串行化隔离得到均衡吗？\n这时候就需要提到可串行化的快照隔离（Serializable Snapshot Isolation, SSI) 算法。它提供了完整的可串行性保证，而性能相比于快照隔离损失很小。目前， SSI 可用于单节点数据库（如 PostgreSQL 9.1 之后的可串行化隔离）或者分布式数据库（如 FoundationDB 采用了类似的算法）。\n实现 可串行化的快照隔离是一种乐观井发控制。如果可能发生潜在冲突，事务会继续执行而不是中止，寄希望一切相安无事。而当事务提交时（只有可串行化的事务被允许提交），数据库会检查是否确实发生了冲突，如果是的话，中止事务并接下来重试。\nSSI 基于快照隔离，也就是说，事务中的所有读取操作都是基于数据库的一致性快照。这是与早期的乐观并发控制主要区别。在快照隔离的基础上， SSI 新增加了相关算法来检测写入之间的串行化冲突从而决定中止哪些事务。\n性能 与两阶段加锁相比，可串行化快照隔离的一大优点是事务不需要等待其他事务所持有的锁。 这一点和快照隔离一样 ，读写通常不会互相阻塞。这样的设计使得查询延迟更加稳定、可预测。特别是，在一致性快照上执行只读查询不需要任何锁，这对于读密集的负载非常有吸引力。\n与串行执行相比，可串行化快照隔离可以突破单个 CPU 的限制。 FoundationDB 将冲突检测分布在多台机器上，从而提高总体吞吐量。即使数据可能跨多台机器进行分区，事务也可以在多个分区上读、写数据并保证可串行化隔离。\n但是，事务中止的比例会显著影响 SSI 的性能表现。 例如，一个运行很长时间的事务，读取和写入了大量数据，因而产生冲突并中止的概率就会增大，所以 SSI 要求读写型事务要简短（而长时间执行的只读事务则没有此限制）。但总体讲，相比于两阶段加锁与串行执行， SSI 更能容忍那些执行缓慢的事务。\n","date":"2022-07-10T06:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%83%E4%BA%8B%E5%8A%A1/","title":"数据密集型应用系统设计 学习笔记（七）：事务"},{"content":"分区 分区通常是这样定义的，即每一条数据（或者每条记录，每行或每个文档）只属于某个特定分区。\n采用数据分区的主要目的是提高可扩展性。不同的分区可以放在一个无共享集群的不同节点上。这样一个大数据集可以分散在更多的磁盘上，查询负载也随之分布到更多的处理器上\n对单个分区进行查询时，每个节点对自己所在分区可以独立执行查询操作，因此添加更多的节点可以提高查询吞吐量。 超大而复杂的查询尽管比较困难，但也可能做到跨节点的并行处理。\n分区与复制 分区通常与复制结合使用，即每个分区在多个节点都存有副本。这意味着某条记录属于特定的分区 ，而同样的内容会保存在不同的节点上以提高系统的容错性。\n一个节点上可能存储了多个分区。 如下图，每个分区都有自己的主副本，例如被分配给某节点，而从副本则分配在其他一些节点。一个节点可能即是某些分区的主副本，同时又是其他分区的从副本。\nK-V 数据的分区 分区的主要目标是将数据和查询负载均匀分布在所有节点上，如果节点平均分担负载，那么理论上 10 个节点应该能够处理 10 倍的数据量和 10 倍于单个节点的读写吞吐量。\n而如果分区不均匀，则会出现某些分区节点比其他分区承担更多的数据量或查询负载，称之为倾斜。倾斜会导致分区效率严重下降，在极端情况下，所有的负载可能会集中在一个分区节点上，这就意味着 10 个节点中 9 个空闲，系统的瓶颈在最繁忙的那个节点上。这种负载严重不成比例的分区即成为系统热点。\n避免热点最简单的方法是将记录随机分配到所有节点上。这种方法虽然可以比较均匀地分布数据，但是有个很大的缺点，即当我们试图读取特定的数据时，没有办法知道数据保存在哪个节点上，所以不得不井行查询所有节点。\n基于Key的区间分区 一种分区方式是为每个分区分配一段连续的 Key 或者 Key 区间范围（以最小值和最大值来标识）。如果知道 Key 区间的上下限，就可以轻松确定哪个分区包含这些 Key。 如果还知道哪个分区分配在哪个节点，就可以直接向该节点发出请求。\n每个分区内可以按照 Key 排序保存，这样可以轻松支持区间查询，即将关键字作为一个拼接起来的索引项从而一次查询得到多个相关记录。\n然而，基于 Key 的区间分区的缺点是某些访问模式会导致热点。如果 Key 是时间戳，则分区对应于一个时间范围，例如每天一个分区。这会导致该分区在写入时负载过高，而其他分区始终处于空闲状态。因此当 Key 为时间戳或者其他类似的属性时，我们需要对这些 Key 进行修改已保证不会产生大量倾斜。\n基于Key的哈希值分区 对于上述数据倾斜与热点问题，许多分布式系统采用了基于 Key 的哈希值来分区。一个好的哈希函数可以处理数据倾斜并使其均匀分布。\n一旦找到合适的哈希函数，就可以为每个分区分配一个哈希范围（而不是直接作用于 Key 范围），Key 根据其哈希值的范围划分到不同的分区中。如下图所示：\n这种方法可以很好地将 Key 均匀分配到多个分区中。分区边界可以是均匀间隔， 也可以是伪随机选择（如一致性哈希）。\n然而，通过 Key 的哈希值进行分区，就丧失了良好的区间查询特性。即使 Key 原本相邻，但经过哈希之后会分散在不同的分区中，区间查询就失去了原有的有序相邻的特性。\n基于哈希的分区方法可以减轻热点，但无法做到完全避免。 如在极端情况下，所有的读/写操作都是针对同一个Key，则最终所有请求都将被路由到同一个分区。如今大多数的系统仍然无法自动消除这种高度倾斜的负载，只能通过应用层来减轻倾斜程度。\n分区与二级索引 如果涉及到二级索引，分区的逻辑又会变得更加复杂，因为二级索引通常不能唯一标识一条记录，而是用来加速特定值的查询。\n二级索引面临的主要挑战是它们不能规整的映射到分区中。目前有两种主要的方法来支持对二级索引进行分区：基于文档的分区和基于词条的分区。\n  基于文档来分区二级索引（本地索引）：二级索引存储在与关键字相同的分区 ，这意味着写入时我们只需要更新一个分区，但缺点是读取二级索引时需要在所有分区上执行分散/聚集 。\n  基于词条来分区二级索引（全局索引）：它是基于索引的值而进行的独立分区。二级索引中的条目可能包含来自关键字的多个分区里的记录。在写入时，不得不更新二级索引的多个分区。但读取时，则可以从单个分区直接快速提取数据。\n  基于文档分区的二级索引 在这种索引方法中，每个分区完全独立，各自维护自己的二级索引，且只负责自己分区内的文档而不关心其他分区中的数据。每当需要写数据库时，包括添加，删除或更新文档等，只需要处理包含目标文档 ID 的那一个分区。因此文档分区索引被称为本地索引，而不是全局索引。\n如上图，由于数据可能分布在不同分区中，在读取时我们就需要将查询发送到所有的分区，然后合并所有返回的结果。这种查询分区数据库的方法被称为分散/聚集，其二级索引的查询代价高昂。即使采用了并行查询，也容易导致读延迟显著放大。\n基于词条分区的二级索引 我们对所有的数据构建全局索引，而不是每个分区维护自己的本地索引。而且，为避免成为瓶颈，不能将全局索引存储在一个节点上，否则就破坏了设计分区均衡的目标。所以，全局索引也必须进行分区，且可以与数据关键字采用不的分区策略。我们将这种索引方案称为词条分区，它以待查找的关键字本身作为索引。\n这种全局的词条分区相比于文档分区索引的主要优点是，它的读取更为高效，即它不需要采用分散/聚集对所有的分区都执行一遍查询，相反，客户端只需要向包含词条的那个分区发出读请求。\n然而全局索引的不利之处在于其写入速度较慢且非常复杂，主要因为单个文档更新时，里面可能会涉及多个二级索引，而二级索引的分区又可能完全不同甚至在不同的节点上，由此势必引人显著的写放大。\n分区再平衡 随着时间的推移，数据库可能总会出现某些变化：\n  查询压力增加，因此需要更多的 CPU来处理负载。\n  数据规模增加，因此需要更多的磁盘和内存来存储数据。\n  节点可能出现故障，因此需要其他机器来接管失效的节点。\n  所有这些变化都要求数据和请求可以从一个节点转移到另一个节点。这样一个迁移负载的过程称为再平衡（或动态平衡）。无论对于哪种分区方案，分区再平衡通常至少要满足 ：\n  平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。\n  再平衡执行过程中，数据库应该可以继续正常提供读写服务。\n  避免不必要的负载迁移，以加快动态再平衡，井尽量减少网络和磁盘 I/O 影响。\n  动态再平衡 固定数量的分区 创建远超实际节点数的分区数，然后为每个节点分配多个分区。 例如一个 10 节点的集群，数据库可以从一开始就逻辑划分为 1000 个分区，这样大约每个节点承担 100 个分区。如果集群中添加了新节点，该新节点可以从每个现有的节点上匀走几个分区，直到分区再次达到全局平衡。如果从集群中删除节点，则采取相反的均衡措施。具体逻辑如下图所示：\n原则上，也可以将集群中的不同的硬件配置因素考虑进来，即性能更强大的节点将分配更多的分区，从而分担更多的负载。目前 Riak、Elasticsearch、Couchbase、Voldemort 都支持这种动态平衡方法。\n动态创建分区 因此，一些数据库如 HBase 和 RethinkDB 等采用了动态创建分区。当分区的数据增长超过一个可配的参数阔值时，它就拆分为两个分区，每个分区承担一半的数据。相反，如果大量数据被删除，并且分区缩小到某个阈值以下，则将其与相邻分区进行合井。 该过程类似于 B 树的分裂操作。\n动态分区的一个优点是分区数量可以自动适配数据总量。如果只有少量的数据，少量的分区就足够了，这样就减轻了系统的开销；如果有大量数据，每个分区的大小则被限制一个可配置的最大值。 因此分区的数量与数据集的大小成正比关系。\n按节点比例分区 在前面两种方法中，分区的数量都与数据集大小有关，而与节点数无关。而 Cassandra 和 Ketama 采用了第三种方式，使分区数与集群节点数成正比关系。换句话说，每个节点具有固定数量的分区。\n此外，当节点数不变时，每个分区的大小与数据集大小保持正比的增长关系。当节点数增加时，分区则会调整变得更小。较大的数据量通常需要大量的节点来存储，因此这种方法使每个分区大小保持稳定。\n当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点。\n手动与自动再平衡 动态平衡的执行方式有以下几种：\n 手动再平衡：分区到节点的映射由管理员来显式配置。 全自动再平衡：由系统自动决定何时将分区从一个节点迁移到另一个节点，不需要任何管理员的介入。 半自动再平衡：自动生成分区分配的建议方案，但需要管理员的确认才能生效。 如 Couchbase，Riak 和 Vodemort。  全自动式再平衡会更加方便，它在正常维护之外所增加的操作很少。但是，也有可能出现结果难以预测的情况。再平衡总体讲是个比较昂贵的操作，它需要重新路由请求并将大量数据从一个节点迁移到另一个节点。万一执行过程中间出现异常，会使网络或节点的负载过重，并影响其他请求的性能。\n基于以上描述综合考虑，半自动再平衡可能是个更好的选择。虽然它比全自动再平衡响应慢，但它可以有效防止意外发生。\n请求路由 当客户端需要发送请求时，如何知道应该连接哪个节点呢？这其实属于一类典型的服务发现问题，这个问题有以下几种不同的处理策略：\n  允许客户端连接任何一个的节点，如果某节点恰好拥有所请求的分区，则直接处理该请求；否则，将请求转发到下一个合适的节点，接收答复，并将答复返回给客户端。\n  将所有客户端的请求都发送到路由层，由后者负责将请求转发到对应的分区节点上。路由层本身不处理任何请求，它仅充当一个分区感知的负载均衡器。\n  客户端感知分区和节点的分配关系。此时，客户端可以直接连接到目标节点，而不需要任何中介。\n  许多分布式数据系统依靠独立的协调服务（如 ZooKeeper ）跟踪集群范围内的元数据，如下图所示：\n每个节点都向 ZooKeeper 注册自己， ZooKeeper 维护了分区到节点的最终映射关系。其他参与者（路由层或分区感知的客户端 ）可以向 ZooKeeper 订阅此信息。一旦分区发生了改变，或者添加、删除节点， ZooKeeper 就会主动通知路由层，这样使路由信息保持最新状态。\n","date":"2022-07-10T05:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD%E5%88%86%E5%8C%BA/","title":"数据密集型应用系统设计 学习笔记（六）：分区"},{"content":"复制 复制主要指通过互联网络在多台机器上保存相同数据的副本。 通过数据复制方案，人们通常希望达到以下目的：\n  低延迟：使数据在地理位置上更接近用户，从而降低访问延迟\n  高可用：当部分组件出现故障，系统依然可以继续工作，从而提高可用性。\n  可拓展：扩展至多台机器以同时提供数据访问服务，从而提高读吞量。\n  主流的三种流行的复制数据变化的方法是：主从复制、多主节点复制和无主节点复制。\n主从复制 每个保存数据库完整数据集的节点称之为副本。而每当有数据写入时，所有的副本需要随之更新，否则就会出现数据不一致的问题。最常见的解决方案就是主从复制，如下图：\n其工作原理如下：\n 指定某一个节点为主节点。当用户写数据库时，必须将写请求首先发送给主节点，主节点首先将新数据写入本地存储。 其他节点则全部称为从节点。主节点把新数据写入本地存储后，然后将数据更改作为复制的日志或更改流发送给所有从节点。每个从节点获得更改日志之后将其应用到本地，且严格保持与主节点相同的写入顺序。 客户端从数据库中读数据时，可以在主节点或者从节点上执行查询。只有主节点才可以接受写请求 ，从客户端的角度来看，从节点都是只读的。  同步复制与异步复制 复制非常重要的一个设计选项是同步复制还是异步复制。\n 同步复制：  优点：一旦向用户确认，从节点可以明确保证完成了与主节点的更新同步，数据已经处于最新版本 。万 一主节点发生故障，总是可以在从节点继续访问最新的数据。 缺点：如果同步的从节点无法完成确认，写入就不能视为成功。 主节点会阻塞其后所有的写操作，直到同步副本确认完成。因此任何一个同步节点的中断都会导致整个系统更新停滞不前。   异步复制  优点：不管从节点上数据多么滞后，主节点总是可以继续响应写请求，系统的吞吐性能更好。 缺点：如果主节点发生失败且不可恢复 ，则所有尚未复制到从节点的写请求都会丢失。这意味着即使向客户端确认了写操作， 却还是无法保证数据的持久化。    在实际使用中有时还会采用半同步的设计方案。即某个从节点是同步的，而其他节点则是异步模式。万一同步的从节点变得不可用或性能下降， 则将另一个异步的从节点提升为同步模式。这样可以保证至少有两个节点（即主节点和一个同步从节点）拥有最新的数据副本。\n新增节点 如果需要增加副本数以提高容错能力，或者替换失败的副本，就需要考虑增加新的从节点。但如何确保新的从节点和主节点保持数据一致呢？\n主要操作步骤如下：\n 在某个时间点对主节点的数据副本产生一个一致性快照，这样避免长时间锁定整个数据库。 将此快照拷贝到新的从节点。 从节点连接到主节点并请求快照点之后所发生的数据更改日志。 获得日志之后，从节点来应用这些快照点之后所有数据变更。 不断重复上述过程。  节点失效的处理 从节点失效 从节点的本地磁盘上都保存了副本收到的数据变更日志。如果从节点发生崩溃，然后顺利重启，或者主从节点之间的网络发生暂时中断（闪断），则恢复比较容易，根据副本的复制日志，从节点可以知道在发生故障之前所处理的最后一笔事务，然后连接到主节点，并请求自那笔事务之后中断期间内所有的数据变更。在收到这些数据变更日志之后，将其应用到本地来追赶主节点。之后就和正常情况 样持续接收来自主节点数据流的变化。\n主节点失效 处理主节点故障的情况则比较棘手：选择某个从节点将其提升为主节点。这样之后的写请求会发送给新的主节点，然后其他从节点要接受来自新的主节点上的变更数据，这一过程称之为故障转移。\n故障转移可以手动进行，例如通知管理员主节点发生失效，采取必要的步骤来创建新的主节点。或者以自动方式进行切换。自动切换的步骤通常如下：\n 确认主节点失效。大多数系统都采用了基于超时的机制。节点间频繁地互相发生发送心跳存活消息，如果发现 某一个节点在一段比较长时间内（例如 30s）没有响应，即认为该节点发生失效。 选举新的主节点。可以通过选举的方式（超过多数的节点达成共识）来选举新的主节点，或者由之前选定的某控制节点来指定新的主节点。候选节点最好与原主节点的数据差异最小，这样可以最小化数据丢失的风险。 重新配置系统使新主节点生效。 客户端现在需要将写请求发送给新的主节点。如果原主节点之后重新上线，这时系统要确保原主节点降级为从节点，并让其认可新的主节点。  复制日志的实现 基于语句的复制 主节点记录所执行的每个写请求（操作语句）并将该操作语句作为日志发送给从节点。 对于关系数据库，这意味着每个 INSERT、UPDATE、 DELETE 语句都会转发给从节点，并且每个从节点都会分析井执行这些 SQL 语句。\n但这种复制方式有一些不适用的场景：\n 任何调用非确定性函数的语句，如 NOW() 获取当前时间，或 RAND() 获取一个随机数等，可能会在不同的副本上产生不同的值。 如果语句中使用了自增列，或者依赖于数据库的现有数据（例如，UPDATE ... WHERE \u0026lt;某些条件\u0026gt;），则所有副本必须按照完全相同的顺序执行，否则可能会带来不同的结果。进而，如果有多个同时并发执行的事务时， 会有很大的限制。 有副作用的语句（例如，触发器、存储过程、用户定义的函数等），可能会在不同的副本上产生不同的副作用。  基于预写日志传输 对于常见的磁盘数据结构，通常每个写操作都是以追加写的方式写入到日志中：\n 对于日志结构存储引擎，日志是主要的存储方式。日志段在后台压缩井支持垃圾回收。 对于采用覆盖写磁盘的 B 树 结构，每次修改会预先写入日志，如系统发生崩溃，通过索引更新的方式迅速恢复到此前一致状态。  所有对数据库写入的字节序列都被记入日志。因此可以使用完全相同的日志在另一个节点上构建副本：将日志入磁盘之后，主节点通过网络将其发送给从节点。 从节点收到日志进行处理，建立和主节点内容完全相同的数据副本。\n其主要缺点是日志描述的数据结果非常底层。 WAL 包含了哪些磁盘块的哪些字节发生改变，诸如此类的细节。这使得复制方案和存储引擎紧密耦合。如果数据库的存储格式从一个版本改为另一个版本，那么系统通常无法支持主从节点上运行不同版本的软件。\n基于行的逻辑日志复制 复制和存储采用不同的日志格式，这样使得复制与存储逻辑剥离。这种复制日志称为逻辑日志，以区分物理存储引擎的数据表示。\n关系数据库的逻辑日志通常是指一系列记录来描述数据表行级别的写请求：\n 对于行插入，日志包含所有相关列的新值。 对于行删除，日志里有足够的信息来唯一标识已删除的行，通常是靠主键，但如果表上没有定义主键，就需要记录所有列的旧值。 对于行更新，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少包含所有已更新列的新值）。  由于逻辑日志与存储引擎逻辑解耦，因此可以更容易地保持向后兼容，从而使主从节点能够运行不同版本的软件甚至是不同的存储引擎。对于外部应用程序来说，逻辑日志格式也更容易解析。\n基于触发器的复制 触发器支持注册自己的应用层代码，使得当数据库系统发生数据更改（写事务）时自动执行上述自定义代码。通过触发器技术，可以将数据更改记录到一个单独的表中，然后外部处理逻辑访问该表，实施必要的自定义应用层逻辑，例如将数据更改复制到另一个系统。\n基于触发器的复制通常比其他复制方式开销更高， 也比数据库内置复制更容易出错，或者暴露一些限制。然而，其高度灵活性仍有用武之地。\n复制滞后问题 写后读一致性 对于分布式数据库来说，提交新数据须发送到主节点，但是当用户读取数据 ，数据可能来自从节点，这就会带来一个问题。当用户在写入不久时查看数据，则新数据可能尚未到达从节点。对用户来讲， 看起来似乎是刚刚提交的数据丢失了，如下图所示：\n对于这种情况，就需要引入写后读一致性（也称读写一致性）。 其保证如果用户重新加载页面，他们总能看到自己最近提交的更新。但对其他用户则没有任何保证，这些用户的更新可能会在稍后才能刷新看到。\n单调读 如下图所示。假定用户 2345 从不同副本进行了多次完全相同的读取，第一次首先查询到了一个相对来说正常的节点，该节点接收到了用户 1234 发布的内容，并将其返回给了用户 2345。而当网页再次刷新，用户 2345 再次进行查询，此时又路由到了一个相对来说较为滞后的节点，这时由于其还未来得及同步用户 1234 的数据，就导致了其返回了一个空结果。对于用户 2345 来说，此时他看到了最新内容之后又看到了过期的内容，好像时间被回拨了一样。\n单调读可以确保不会发生这种异常。这是一个介于强一致性与最终一致性的保证 。其保证了当读取数据时，如果某个用户依次进行多次读取，则他绝不会看到回滚现象，即在读取较新值之后又发生读旧值的情况。\n实现单调读的一种方式是，确保每个用户总是从固定的同一副本执行读取（而不同的用户可以从不同的副本读取）。例如，基于用户 ID 的哈希的方法而不是随机选择副本。但如果该副本发生失效，则用户的查询必须重新路由到另一个副本。\n前缀一致读 如果数据库总是以相同的顺序写入，则读取总是看到一致的序列，不会发生这种反常。然而，在许多分布式数据库中，不同的分区独立运行，因此不存在全局写入顺序。这就导致当用户从数据库中读数据时，可能会看到数据库的某部分旧值和另一部分新值。\n为了防止这种异常问题，就需要引入一种保证：前缀一致读。其保证了对于一系列按照某个顺序发生的写请求，那么读取这些内容时也会按照当时写入的顺序。\n比较常见的实现方案是确保任何具有因果顺序关系的写入都交给一个分区来完成。但由于其性能不佳，如今通常使用一些新的算法来显式追踪事件因果关系。\n多主节点复制 主从复制并不是完美的，其存在单点问题：系统只有一个主节点，而所有的写入都必须经由主节点 。如果由于某种原因，例如与主节点之间的网络中断而导致主节点无法连接，主从复制方案就会影响所有的写入操作。\n如果要改进这个策略，就可以对主从复制模型进行自然的扩展。即配置多个主节点，每个主节点都可以接受写操作，处理写的每个主节点都必须将该数据更改转发到所有其他节点 。这就是多主节点复制。此时，每个主节点同时扮演其他主节点的从节点。\n处理写冲突 多主复制的最大问题是可能发生写冲突。而通常采用下面三种方法来处理。\n避免冲突 而处理冲突最理想的策略是避免发生冲突 ，即如果应用层可以保证对特定记录的写请求总是通过同一个主节点，这样就不会发生写冲突。\n例如，在一个应用系统中，用户需要更新自己的数据，那么我们确保特定用户的更新请求总是路由到特定的数据中心，并在该数据中心的主节点上进行读/写。但是，有时可能需要改变事先指定的主节点，例如由于该数据中心发生故障，不得不将流量重新路由到其他数据中心，此时，冲突避免方式不再有效，必须有措施来处理同时写入冲突的可能性。\n收敛于一致状态 当多个主节点写入数据发生冲突时，所有的复制模型至少应该确保数据在所有副本中最终状态一定是一致的。因此，数据库必须以一种收敛趋同的方式来解决冲突，这就意味着当所有更改最终被复制、同步之后，所有副本的最终值是相同。\n收敛的冲突解决有以下几种方式：\n 给每个写入分配唯一 ID。 例如时间戳、足够长的随机数，UUID 或者基于 K-V 的哈希值，挑选最高 ID 的写入作为胜利者，并将其他写入丢弃。 为每个副本分配一个唯一的 ID，并制定规则。 例如序号高的副本写入始终优先于序号低的副本。 以某种方式将这些值合并在一起。 例如，按字母顺序排序，然后拼接在一起。 利用预定义好的格式来记录和保留冲突相关的所有信息，然后依靠应用层的逻辑，事后解决冲突。  自定义冲突解决逻辑 解决冲突最合适的方式可能还是依靠应用层，所以大多数多主节点复制模型都有工具让用户编写应用代码来解决冲突。可以在写入时或在读取时执行这些代码逻辑：\n 在写入时执行：只要数据库系统在复制变更日志时检测到冲突，就会调用应用层的冲突处理程序。 在读取时执行：当检测到冲突时，所有冲突写入值都会暂时保存下来。当下一次读取数据时，会将数据的多个版本读返回给应用层。应用层可能会提示用户或自动解决冲突， 井将最后处理好的结果返回到数据库。  拓扑结构 拓扑结构描述了写请求从一个节点传播到其他节点的通信路径。 常见的拓扑结构如下图：\n 环形拓扑：每个节点接收来自前序节点的写入，井将这些写入（加上自己的）转发给后序节点。 星形拓扑：一个指定的根节点将写入转发给所有其他节点。 全链接拓扑：每个主节点将其写入同步到其他所有主节点。  在环形和星形拓扑中，写请求需要通过多个节点才能到达所有的副本，即中间节点需要转发从其他节点收到的数据变更。为防止无限循环，每个节点需要赋予一个唯一标识符，在复制日志中的每个写请求都标记了已通过的节点标识符。如果某个节点收到了包含自身标识符的数据更改，表明该请求已经被处理过，因此会忽略变更请求，避免重复转发。\n环形和星形拓扑的问题是，如果某一个节点发生了故障，在修复之前，会影响其他节点之间复制日志的转发。可以采用重新配置拓扑结构的方法暂时排除掉故障节点。\n全链接拓扑也存在一些问题。主要是存在某些网络链路比其他链路更快的情况（例如由于不同网络拥塞），从而导致复制日志之间的覆盖。为了使得日志消息正确有序，可以使用一种称为版本向量的技术。\n无主节点复制 无主节点复制模型采用了另一种方案，其选择放弃主节点，允许任何副本直接接受来自客户端的写请求。\n节点失效处理 对于有主复制模型来说，当节点失效后，其会通过主从切换、从节点追赶等方式来进行修复。但是对于无主模型来说，并没有这样的切换机制。\n对于无主模型，其解决节点失效的方法如下：当一个客户端从数据库中读取数据时，它不是向一个副本发送请求，而是并行地发送到多个副本。客户端可能会得到不同节点的不同响应，包括某些节点的新值和某些节点的旧值。可以采用版本号来确定哪个值更新。\n读修复与反熵 复制模型应确保所有数据最终复制到所有的副本。当一个失效的节点重新上线之后，它如何赶上中间错过的那些写请求呢？\nDynamo 风格的数据存储系统经常使用以下两种机制：\n 读修复：当客户端并行读取多个副本时，可以检测到过期的返回值，然后将新值写入到该副本。 反熵：后台进程不断查找副本之间数据的差异，将任何缺少的数据从一个副本复制到另一个副本。  读写的quorum 如果有 n 个副本，写入需要 w 个节点确认，读取必须至少查询 r 个节点，则只要满足 w + r \u0026gt; n ，读取的节点中一定会包含最新值。（成功写入的节点集合和读取的节点集合必然有重合 ，这样读取的节点中至少有一个具有最新值）。满足上述这些 r、w 值的读/写操作被称之为法定票数读或法定票数写。也可以认为 r 和 w 是用于判定读、写是否有效的最低票数。\n在 Dynamo 风格的数据库中，**参数 n、w 和 r 通常是可配置的。一个常见的选择是设置 n 为奇数，w = r = (n + 1) / 2 。**通常会根据具体的业务场景来灵活配置。\n仲裁条件 w + r \u0026gt; n 定义了系统可容忍的失效节点数，如下所示：\n 当 w \u0026lt; n ，如果一个节点不可用，仍然可以处理写入。 当 r \u0026lt; n ，如果一个节点不可用，仍然可以处理读取。 例如 n = 5，w = 3，r = 3。则可以容忍两个不可用的节点。 读取和写入操作总是并行发送到所有的 n 个副本。参数 r 和参数 w 只是决定要等待的节点数。即有多少个节点需要返回结果 ，我们才能判断出结果的正确性。  如果可用节点数小于所需的 w 或 r，则写入或读取就会返回错误。\n检查并发写 Dynamo 风格的数据库允许多个客户端对相同的主键同时发起写操作， 即使采用严格的 quorum 机制也可能会发生写冲突。这与多主节复制类似，此外，由于读时修复或者数据回传也会导致并发写冲突。\n由于网络延迟不稳定或者局部失效，请求在不同的节点上可能会呈现不同的顺序。如果节点每当收到新的写请求时就简单地覆盖原有的主键，那么这些节点将永久无法达成一致。\n下面就来介绍一下数据库内部如何进行冲突处理，保证副本收敛于相同的内容，从而达到最终一致性。\n最后写入获胜（LWW算法） 一种实现最终收敛的方方法是，每个副本总是保存最新值，允许覆盖并丢弃旧值。 那么，假定每个写请求都最终同步到所有副本，只要有一个明确的方法来确定哪个写入是最新的， 副本可以最终收敛到相同的值。但问题又来了，对于并发的写入，我们无法确定他们的顺序。\n即使无法确定写请求的自然顺序，我们也可以强制对其排序。附加为每一个写请求附加一个时间戳，然后选择最新的时间戳，丢弃较早时间戳的写入。这个冲突解决算能被称为最后写入获胜（LWW, last write wins）\nLWW 可以达成最终一致，但代价是牺牲数据持久性。如果同一个主键有多个并发写，即使这些并发写都向客户端报告成功（因为完成了写入 w 个副本），但最后只有一个写入值会存活下来，其他的将被系统默默丢弃。\n在某些场景下，由于 LWW 会导致数据的覆盖和丢失，所以它并不是一个好的选择。要确保 LWW 安全无副作用的唯一方法是：只写入一次然后写入值视为不可变，这样就避免了对同一主键的并发（覆盖）写。\nHappens-before关系和并发 如何判断两个操作是否是并发呢？通常如果两个操作同时发生，则称之为并发，然而事实上，操作是否在时间上重叠并不重要，由于分布式系统中复杂的时钟同步问题，现实当中，我们很难严格确定它们是否同时发生。 为了更好的定义并发性，我们并不依赖确切的发生时间，即不管物理的时机如何，如果两个操作并不需要意识到对方，我们即可称它们是并发操作。\n如果 B 知道 A，或者 B 依赖于 A，或者以某种方式在 A 的基础上构建，则称操作 A 在操作 B 之前发生。这是定义何为并发的关键。事实上，我们也可以简单地说，如果两个操作都不在另一个之前发生（或者两者都不知道对方） ，那么操作是并发的。\n确定前后关系 服务器判断操作是否并发的依据主要依靠对比版本号 ，而并不需要解释新旧值本身（值可以是任何数据结构）。算法的工作流程如下：\n 服务器为每个主键维护一个版本号，每当主键新值写入时递增版本号，井将新版本号与写入的值一起保存 。 当客户端读取主键时，服务器将返回所有（未被覆盖的）当前值以及最新的版本号。且要求写之前，客户必须先发送读请求 。 客户端写主键，写请求必须包含之前读到的版本号、读到的值和新值合并后的集合。写请求的响应可以像读操作一样会返回所有当前值，这样就可以一步步链接起多个写入的值。 当服务器收到带有特定版本号的写入时，覆盖该版本号或更低版本的所有值（因为知道这些值已经被合并到新传入的值集合中），但必须保存更高版本号的所有值（因为这些值与当前的写操作属于并发）。  当写请求包含了前一次读取的版本号时，意味着修改的是基于以前的状态。如果一个写请求没有包含版本号，它将与所有其他写入同时进行，不会覆盖任何已有值，其传入的值将包含在后续读请求的返回值列表当中。\n合并同时写入的值 如果多个操作并发发生，则客户端必须通过合并并发写入的值来继承旧值。\n版本矢量 所有副本的版本号集合称为版本矢量。 当读取数据时，数据库副本会返回版本矢量给客户端，而在随后写入时需要将版本信息包含在请求当中一起发送到数据库。版本矢量技术使数据库可以区分究竟应该覆盖写还是保留并发值。\n版本矢量可以保证从某一个副本读取值然后写入到另一个副本，而这些值可能会导致在其他副本上衍生出来新的“兄弟”值，但至少不会发生数据丢失且可以正确合并所有并发值。\n","date":"2022-07-10T04:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94%E5%A4%8D%E5%88%B6/","title":"数据密集型应用系统设计 学习笔记（五）：复制"},{"content":"编码与演化 数据编码格式 程序通常使用（ 至少）两种不同的数据表示形式：\n  在内存中，数据保存在对象、结构体、列表、数组、哈希表和树等结构中。 这些数据结构针对 CPU 高效访问和操作进行了优化（通常使用指针）。\n  将数据写入文件或通过网络发送时，必须将其编码为某种自包含的字节序列（例如 JSON 文档）。 由于指针对其他进程没有意义，所以这个字节序列表示看起来与内存中使用的数据结构大不一样。\n  因此，在这两种表示之间需要进行类型的转化。从内存中的表示到字节序列的转化称为编码（或序列化），相反的过程称为解码（或解析，反序列化） 。\n语言特定的格式 许多编程语言都内建了将内存对象编码为字节序列的支持。例如，Java 有 java.io.Serializable ，Ruby 有Marshal，Python 有 pickle 等等。许多第三方库也存在，例如 Java 的 Kryo。\n这些编码库非常方便，可以用很少的额外代码实现内存对象的保存与恢复。但是它们也有一些深层次的问题：\n 这类编码通常与特定的编程语言深度绑定，其他语言很难读取这种数据。 为了在相同的对象类型中恢复数据，解码过程需要能够实例化任意的类。这经常导致一些安全问题：如果攻击者可以让应用程序解码任意的字节序列，那么它们可以实例化任意的类，这通常意味着，它们可以做些可怕的 情，比如远程执行任意代码。 在这些库中，数据版本控制通常是事后才考虑的。 因为它们旨在快速简便地对数据进行编码，所以往往忽略了前向后向兼容性带来的麻烦问题。 效率（编码或解码所花费的CPU时间，以及编码结构的大小）往往也是事后才考虑的。 例如，Java 的内置序列化由于其糟糕的性能和臃肿的编码而臭名昭着  因此，除非临时使用，采用语言内置编码通常是一个坏主意。\nJSON、XML与二进制变体 当我们谈到可以被多种编程语言读写的标准编码时，JSON 和 XML 是最显眼的角逐者。它们广为人知，广受支持，也广受憎恶。 XML 经常收到批评：过于冗长与且过份复杂。 JSON 的流行则主要源于 Web 浏览器的内置支持，以及相对于 XML 的简单性。 CSV 是另一种流行的与语言无关的格式，尽管其功能相对较弱。\nJSON，XML 和 CSV 属于文本格式，因此具有较高的可读性。除了表面的语法问题之外，它们也存在一些微妙的问题：\n 数值编码多很多存在歧义的地方。XML 和 CSV 不能区分数字和字符串（除非引用一个外部模式）。 JSON 虽然区分字符串与数值，但不区分整数和浮点数，而且不能指定精度。 JSON 和 XML 对 Unicode 字符串有很好的支持，但是它们不支持二进制数据。 二进制字符串是很有用的功能，所以人们通过使用 Base64 将二进制数据编码为文本来绕过此限制。其特有的模式标识着这个值应当被解释为 Base64 编码。这种方案虽然管用，但会增加 33% 的数据大小。 XML 和 JSON 都有可选的模式支持。 这些模式语言相当强大，所以学习和实现起来都相当复杂。 XML 模式的使用相当普遍，但许多基于 JSON 的工具才不会去折腾模式。由于对数据的正确解读取决于模式中的信息，因此不使用 XML/JSON 模式的应用程序可能需要对相应的编码/解码逻辑进行硬编码。 CSV 没有任何模式，因此每行和每列的含义完全由应用程序自行定义。 如果应用程序变更添加了新的行或列，那么这种变更必须通过手工处理。 CSV 也是一个相当模糊的格式。尽管其转义规则已经被正式指定，但并不是所有的解析器都能够正确的实现它们。  Thrift与Protocol Buffers Apache Thrift 和 Protocol Buffers 是基于相同原理的二进制编码库。 Protocol Buffers 最初是在 Google 开发的，Thrift最初是在 Facebook 开发的，并且它们都是在 2007~2008 年开源的。\nThrift 和 Protocol Buffers 都需要一个模式来编码任何数据，因此需要通过接口定义语言 IDL 描述模式。如：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // Thrift struct Person { 1: required string userName, 2: optional i64 favoriteNumber, 3: optional list\u0026lt;string\u0026gt; interests } // Protocol Buffers message Person { required string user_name = 1; optional int64 favorite_number = 2; repeated string interests = 3; }   Avro Apache Avro 是另一种二进制编码格式，与 Protocol Buffers 和 Thrift 有着有趣的不同。由于 Thrift 不适合 Hadoop 的用例，因此它作为 Hadoop 的一个子项目在 2009 年开始启动。\nAvro 也使用模式来指定正在编码的数据的结构。 它有两种模式语言：一种（Avro IDL）用于人工编辑，一种（基于JSON）更易于机器读取。\n我们用 Avro IDL 编写的示例模式可能如下所示：\n1 2 3 4 5  record Person { string userName; union { null, long } favoriteNumber = null; array\u0026lt;string\u0026gt; interests; }   该模式的等价 JSON 表示如下：\n1 2 3 4 5 6 7 8 9  { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;userName\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;favoriteNumber\u0026#34;, \u0026#34;type\u0026#34;: [\u0026#34;null\u0026#34;, \u0026#34;long\u0026#34;], \u0026#34;default\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;interests\u0026#34;, \u0026#34;type\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: \u0026#34;string\u0026#34;}} ] }   数据流模式 进程间数据流动的常见方式有以下几种：\n 通过数据库 通过服务调用（RPC、REST） 通过异步消息传递  基于数据库的数据流 在数据库中，写入数据库的进程对数据进行编码，而读取数据库的进程对数据进行解码。可能只有一个进程访问数据库，在这种情况下， Reader 只是同一个进程的较新版本，此时，可以认为向数据库中存储内容，就是给未来的自己发送消息。\n基于服务的数据流 对于需要通过网络进行通信的进程，有许多不同的通信方式。最常见的是有两个角色：客户端和服务器。服务器通过网络公开 PI ，客户端可以连接到服务器以向该 API 发出请求。 服务器公开的 API 称为服务。\nWeb 以这种方式工作：客户向 Web 服务器发出请求，通过 GET 请求下载 HTML，CSS，JavaScript，图像等，并通过 POST 请求提交数据到服务器。 API 包含一组标准的协议和数据格式（HTTP，URL，SSL/TLS，HTML 等）。\n将大型应用程序按照功能区域分解为较小的服务，这样当一个服务需要来自另一个服务的某些功能或数据时，就会向另一个服务发出请求。这种构建应用程序的方式传统上被称为面向服务的体系结构（service-oriented architecture，SOA），最近被改进和更名为微服务架构。面向服务/微服务架构的一个关键设计目标是通过使服务独立部署和演化来使应用程序更易于更改和维护。\nWeb 服务（REST、SOAP） 当服务使用 HTTP 作为底层通信协议时，可称之为 Web 服务。这可能是一个小错误，因为 Web 服务不仅在 Web上使用，而且在几个不同的环境中使用。例如：\n 运行在用户设备上的客户端应用程序通过 HTTP 向服务发出请求。 这些请求通常通过公共互联网进行。 一种服务向同一组织拥有的另一项服务提出请求，这些服务通常位于同一数据中心内，作为面向服务/微型架构的一部分。 一种服务通过互联网向不同组织所拥有的服务提出请求。这用于不同组织后端系统之间的数据交换。此类别包括由在线服务提供的公共 API，或用于共享访问用户数据的 OAuth。  有两种流行的 Web 服务方法：REST 和 SOAP：\n REST：REST 不是一种协议，而是一个基于 HTTP 原则的设计理念。它强调简单的数据格式，使用 URL 来标识资源，并使用 HTTP 功能进行缓存控制 、身份验证和内容类型协商。根据 REST 原则所设计的 API 称为 RESTful。 SOAP：SOAP 是一种基于 XML 的协议，用于发出网络 API 请求。虽然它最常用于 HTTP ，但其目的是独立于HTTP ，并避免使用大多数 HTTP功能。相反，它带有庞大 而复杂的多种相关标准（Web 服务框架， Web Service Framework ，称为WS-*）和新增的各种功能。  RPC RPC（Remote Procedure Call ）即远程过程调用，其试图向远程网络服务发出请求，看起来与在同一进程中调用编程语言中的函数或方法相同（这种抽象称为位置透明）。 RPC 主要侧重于同一组织内多项服务之间的请求，通常发生在同一个数据中心内。\n其与本地函数调用存在以下差异：\n  本地函数调用是可预测的，并且成功或失败仅取决于受你控制的参数。而网络请求是不可预知的。由于网络问题，请求或响应可能会丢失，或者远程计算机可能很慢或不可用，这些问题完全不在你的控制范围之内。网络问题是常见的，所以你必须预测他们，例如通过重试失败的请求。\n  本地函数调用要么返回结果，要么抛出异常，或者永远不返回（因为进入无限循环或进程崩溃）。网络请求有另一个可能的结果。由于超时，它可能会返回没有结果。在这种情况下，如果你没有得到来自远程服务的响应，你无法知道请求是否通过。\n  如果重试失败的网络请求，可能会发生请求实际上已经完成，但是响应丢失。在这种情况下，重试将导致该操作被执行多次。 除非你在协议中引入去重机制（幂等）。本地函数调用没有这个问题。\n  每次调用本地函数时，通常需要大致相同的时间来执行。网络请求比函数调用要慢得多，而且其延迟也是非常可变的。好的时候它可能会在不到一毫秒的时间内完成，但是当网络拥塞或者远程服务超载时，可能需要几秒钟的时间完成一样的东西。\n  调用本地函数时，可以高效地将引用（指针）传递给本地内存中的对象。当你发出一个网络请求时，所有这些参数都需要被编码成可以通过网络发送的一系列字节。 如果参数是像数字或字符串这样的基本类型倒是没关系，但是对于较大的对象很快就会变成问题。\n  客户端和服务可以用不同的编程语言实现，所以 RPC 框架必须将数据类型从一种语言翻译成另一种语言。这可能会捅出大篓子，因为不是所有的语言都具有相同的类型 。\n  基于消息传递的数据流 它与 RPC 的相似之处在于，客户端的请求（通常称为消息）以低延迟传递到另一个进程。它与基于数据库的方式的相似之处在于，不是通过直接的网络连接发送消息，而是通过称为消息队列的中介发送的， 该中介会暂存消息。\n使用异步消息传递存在以下优点：\n 如果接收方不可用或过载，可以充当缓冲区，从而提高系统的可靠性。 它可以自动将消息重新发送到已经崩溃的进程，从而防止消息丢失。 避免发送方需要知道接收方的 IP 地址和端口号（这在虚拟机经常启启停停的云部署中特别有用）。 它允许将一条消息发送给多个接收方。 将发送方与接收方逻辑分离（发送方只是发布消息，不关心使用者）。  与 RPC 的差异在于：\n 消息传递通信通常是单向的：发送方通常不期望收到对其消息的回复 。进程可能发送一个响应，但这通常是在一个独立的通道上完成的。 这种通信模式是异步的 ：发送者不会等待消息被传递，而只是发送然后忘记它。  消息队列 详细的交付语义因实现和配置而异，但通常情况下，消息队列的使用方式如下：一个进程将消息发送到指定的队列或主题，代理确保将消息传递给那个队列或主题的一个或多个消费者或订阅者。在同一主题上可以有许多生产者和许多消费者。\n一个主题只提供单向数据流。 但是，消费者本身可能会将消息发布到另一个主题上（因此，可以将它们链接在一起，就像我们将在中看到的那样），或者发送给原始消息的发送者使用的回复队列（允许请求/响应数据流，类似于 RPC）。\n消息代理通常不会执行任何特定的数据模型 —— 消息只是包含一些元数据的字节序列，因此你可以使用任何编码格式。 如果编码是向后和向前兼容的，你可以灵活地对发布者和消费者的编码进行独立的修改，并以任意顺序进行部署。\n分布式Actor框架 Actor 模型是单个进程中并发的编程模型。逻辑被封装在 Actor 中，而不是直接处理线程（以及竞争条件，锁定和死锁的相关问题）。 每个 Actor 通常代表一个客户或实体，它可能有一些本地状态（不与其他任何角色共享），它通过发送和接收异步消息与其他角色通信。**不保证消息传送：在某些错误情况下，消息将丢失。**由于每个角色一次只能处理一条消息，因此不需要担心线程，每个角色可以由框架独立调度。\n在分布式 Actor 框架中，此编程模型用于跨多个节点伸缩应用程序。不管发送方和接收方是在同一个节点上还是在不同的节点上，都使用相同的消息传递机制。如果它们在不同的节点上，则该消息被透明地编码成字节序列，通过网络发送，并在另一侧解码。\n分布式的 Actor 框架实质上是将消息队列和 Actor 编程模型集成到一个框架中。 但是，如果要执行基于 Actor 的应用程序的滚动升级，则仍然需要担心向前和向后兼容性问题，因为消息可能会从运行新版本的节点发送到运行旧版本的节点，反之亦然。\n","date":"2022-07-10T03:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9B%E7%BC%96%E7%A0%81%E4%B8%8E%E6%BC%94%E5%8C%96/","title":"数据密集型应用系统设计 学习笔记（四）：编码与演化"},{"content":"存储与检索 核心存储结构 哈希 K-V 存储与大多数编程语言所内置的字典结构非常相似，通常采用哈希表（hash table） 来实现。那如果我们想将其拓展到磁盘存储索引，该如何去做呢？\n保存内存中的哈希表，把每个键一一映射到数据文件中特定的字节偏移，这样就可以找到每个值的位置。 每当在文件中追加新的数据，还要更新哈希表来反映刚刚写入数据的偏移量 （包括插入新的键和更新已有的键）。 查找某个值时，使用哈希表来找到文件中的偏移量，即存储位置，然后读取其内容。\n只要所有的 key 可以放入内存（因为哈希表需要保存在内存中），value 的数据量则可以超过内存大小，只需一次磁盘寻址，就可以将 value 从磁盘加载到内存。 如果那部分数据文件已经在文件系统的缓存中，则读取根本不需要任何的磁盘 I/O。\n如果只追加到一个文件，那么很有可能导致磁盘空间用尽。因此通常将日志分解成一定大小的段，当文件达到一定大小时就关闭它，井将后续写入到新的段文件中。\n此外，由于压缩往往使得段更小（假设键在段内被覆盖多次），也可以在执行压缩的同时将多个段合并在一起。\n每个段现在都有自己的内存哈希表，将键映射到文件的偏移量。为了找到键的值，首先检查最新的段的哈希表；如果键不存在，检查第二最新的段，以此类推。由于合并过程可以维持较少的段数，因此查找通常不需要检查很多哈希表。\n优点：\n 追加和分段合并主要是顺序写，它通常比随机写入快得多，特别是在 HDD 上。 如果段文件是追加的或不可变的，则并发和崩溃恢复要简单得多。 合并旧段可以避免随着时间的推移数据文件出现碎片化的问题  缺点：\n 哈希表必须全部放入内存，随着数据越来越多，哈希冲突解决的成本会越来越高。 区间查询效率不高。  SSTable与LSM-Tree 将日志结构的存储段中的 key-value 顺序按键排序。这种结构称为排序字符串表（SSTable）。它要求每个键在每个合并的段文件中只能出现一次。\nSSTable 相比哈希索引的日志段，具有以下优点：\n 合并段更加简单高效，即使文件大于可用内存。 在文件中查找特定的键时，不再需要在内存中保存所有键的索引。 由于读请求往往需要扫描请求范围内的多个 key-value 对，可以考虑将这些记录保存到一个块中并在写磁盘之前将其压缩。  SSTable的构建与维护 基本工作流程如下：\n  当写入时，将其添加到内存中的平衡树数据结构中（例如红黑树）。这个内存中的树有时被称为内存表。\n  当内存表大于某个阈值（通常为几兆字节）时，将其作为 SSTable 文件写入磁盘。由于树已经维护了按键排序的 key-value 对， 磁盘可以比较高效。新的 SSTable 文件成为数据库的最新部分。当 SSTable 写磁盘的同时，写入可以继续添加到一个新的内存表实例。\n  为了处理读请求，首先尝试在内存表中查找键，然后是最新的磁盘段文件，接下来是次新的磁盘段文件，以此类推，直到找到目标（或为空）。\n  后台进程周期性地执行段合并与压缩过程，以合并多个段文件，并丢弃那些已被覆盖或删除的值。\n  如果数据库崩溃，最近的写入（在内存表中但尚未写入磁盘）将会丢失。为了避免该问题，可以在磁盘上保留单独的日志（WAL 日志），每个写入都会立即追加到该日志。每当将内存表写入 SSTable 时，则相应的日志可以被丢弃。\n优化 LSM-Tree 的一些优化措施如下：\n 布隆过滤器：在确定键不存在之前，必须先检查内存表，然后将段一直回溯访问到最旧的段文件（可能必须从磁盘多次读取）。为了优化这种访问，存储引擎通常使用额外的布隆过滤器，其用于近似计算集合的内容。如果数据库中不存在某个键，它能够很快告诉你结果，从而节省了很多对于不存在的键的不必要的磁盘读取。 分层压缩与大小分级：在大小分级的压缩中，较新的和较小的 SSTable 被连续合并到较旧和较大的 SSTable 。在分层压缩中，键的范围分裂成多个更小的 SSTable，旧数据被移动到单独的层级，这样压缩可以逐步进行并节省磁盘空间。  B-Tree B-Tree 将数据库分解成固定大小的块或页（页是内部读/写的最小单元），传统页的大小为 4 KB。这种设计更接近底层硬件，因为磁盘也是以固定大小的块排列。\n每个页面都可以使用地址或位置进行标识，这样可以让一个页面引用另一个页面，类似指针，不过是指向磁盘地址，而不是内存。\n某一个页被指定为 B-Tree 的根，每当查找索引中的一个键时，总是从这里开始。该页面包含若干个键和对子页的引用。每个孩子都负责一个连续范围内的键，相邻引用之间的键可以指示这些范围之间的边界。\n例如上图即是一个查询，假定正在查找键 251，因此需要沿着 200~300 间的页引用，到达类似的页，它进一步将 200~300 范围分解成子范围。最终，我们到达一个包含单个键的页（叶子页），该页包含每个内联键的值或包含可以找到值的页的引用。\n如上图。如果要更新 B-Tree 中现有键的值，首先搜索包含该键的叶子页，更改该页的值，并将页写回到磁盘（对该页的任何引用仍然有效）。如果要添加新键，则需要找到其范围包含新键的页，并将其添加到该页。如果页中没有足够的可用空间来容纳新键，则将其分裂为两个半满的页，并且父页也需要更新以包含分裂之后的新的键范围。\n预写日志 为了使数据库能从崩溃中恢复，常见 B-Tree 实现需要支持磁盘上的额外的数据结构：预写日志（write-ahead log, WAL ）也称重做日志（Redo-log）。这是一个仅支持追加修改的文件，每个 B-tree 的修改必须先更新 WAL 然后再修改树本身的页。当数据库在崩溃后需要恢复 ，该日志用于将B-tree 恢复到最近一直的状态。\n优化 B-Tree 常见的一些优化措施如下：\n 一些数据库（如 LMDB）不使用覆盖页和维护 WAL 来进行崩溃恢复，而是使用写时拷贝方案 。 修改的页被写入不同的位置，树中父页的新版本被创建，并指向新的位置。 保存键的缩略信息，而不是完整的键，这样可以节省页空间。 特别是在树中间的页中，只需要提供足够的信息来描述键的起止范围。这样可以将更多的键压入到页中，让树具有更高的分支因子，从而减少层数。 许多 B-Tree 的实现尝试对树进行布局，以便相邻叶子页可以按顺序保存在磁盘上。 这主要由于页可以放在磁盘上的任何位置；没有要求相邻的页需要放在磁盘的相邻位置。如果查询需要按照顺序扫描大段的键范围，考虑到每个读取的页都可能需要磁盘 I/O，所以逐页的布局可能是低效的。 添加额外的指针到树中。 例如，每个叶子页面可能会向左和向右引用其同级的兄弟页，这样可以顺序扫描键，而不需要跳回到父页。 B-Tree 的变体如分形树借鉴了一些日志结构的想法也来减少磁盘寻道。  事务处理与分析处理 即使数据库开始被用于许多不同类型的数据，比如博客文章的评论，游戏中的动作，地址簿中的联系人等等，基本的访问模式仍然类似于处理商业交易。应用程序通常使用索引通过某个键查找少量记录。根据用户的输入插入或更新记录。由于这些应用程序是交互式的，这种访问模式被称为在线事务处理（OLTP, OnLine Transaction Processing）。\n数据库也开始越来越多地用于数据分析，这些数据分析具有非常不同的访问模式。通常，分析查询需要扫描大量记录，每个记录只读取几列，并计算汇总统计信息（如计数、总和或平均值），形成报告以帮助公司管理层做出更好的决策（商业智能）。这种访问模式被称为在线分析处理（OLAP, OnLine Analytice Processing）。\nOLTP 和 OLAP 之间的区别如下：\n   属性 事务处理 OLTP 分析处理 OLAP     主要读取模式 查询少量记录，按键读取 在大批量记录上聚合   主要写入模式 随机访问，写入要求低延时 批量导入（ETL）或者事件流   主要用户 终端用户，通过Web应用 内部数据分析师，用于决策支持   处理的数据 数据的最新状态（当前时间点） 随时间推移的历史事件   数据集尺寸 GB ~ TB TB ~ PB    数据仓库 起初，事务处理和分析查询使用了相同的数据库。 SQL 被证明是非常灵活的，可以同时胜任 OLTP 类型和OLAP 类型查询 。尽管如此，在二十世纪八十年代末和九十年代初期，企业有停止使用 OLTP 系统进行分析的趋势，转而在单独的数据库上运行分析。这个单独的数据库被称为数据仓库（data warehouse）。\n考虑到直接在 OLTP 数据库上运行临时的分析查询的开销特别大（会扫描大部分数据集），这可能会损害并发执行事务的性能。因此，数据仓库是一个独立的数据库，分析人员可以查询他们想要的内容而不影响 OLTP 操作。\n数据仓库包含公司各种 OLTP 系统中所有的只读数据副本。从 OLTP 数据库中提取数据（使用定期的数据转储或连续的更新流），转换成适合分析的模式，清理并加载到数据仓库中。将数据存入仓库的过程称为抽取-转换-加载（ETL）。具体流程如下图所示：\n雪花模型与星型模型 事实表的每一行代表在特定时间发生的事件。而它的一些列是属性，例如产品销售的价格和从供应商那里购买的成本（可以用来计算利润余额）。其他列可能会引用其他表的外键，称为维度表。由于事实表中的每一行都代表一个事件，维度通常代表事件的对象（who）、什么（what）、地点（where）、时间（when）、方法（how）以及原因（why）。\n星型模式这个名字的来源是当我们对表之间的关系进行可视化时，事实表在中间，被维度表包围；与这些表的连接就像星星的光芒。例如下图：\n这个模板的变体被称为雪花模式，维度被进一步分解为子维度。例如，品牌和产品类别可能有单独的表格，并且 dim_product 表格中的每一行都可以将品牌和类别作为外键引用，而不是将它们作为字符串存储在 dim_product 表格中。雪花模式比星形模式更规范化，但是星形模式通常是首选，因为它使用起来更简单。\n列式存储  在传统的按行存储的数据库中，当用户需要查询某个特定列的时候，即使我们已经建立了索引，但是存储引擎仍然会将这些行数据（可能有上百个字段）完整的从硬盘加载到内存中，并对它们进行解析、条件过滤，这需要花费大量的时间。在一些大数据的场景下，事实表中可能会有万亿行和数 PB 的数据，此时按行存储很明显是不可用的。\n 列式存储背后的想法很简单：不要将所有来自一行的值存储在一起，而是将来自每一列的所有值存储在一起。如果每个列存储在一个单独的文件中，查询只需要读取和解析查询中使用的那些列，这可以节省大量的工作，如下图所示：\n列式存储布局依赖于一组列文件，每个列文件以相同顺序的顺序存储行数据。 因此，如果需要重新组装完整的行，可以从每个单独的列文件中获取对应的那一行，并将它们放在一起形成完整的行。\n压缩 由于同一列中的数据类型相同，因此我们可以通过压缩数据来进一步降低对硬盘吞吐量的要求。在数据仓库中，通常使用位图编码来压缩数据：\n通常情况下，一列中不同值的数量要远远小于它的行数。我们假设某一列中有 n 种不同的值，此时我们就可以将其转换为 n 个位图，位图中对应位置用 0 和 1 标识该行是否存在这个值（即列中每个值对应一个位图，每行对应一个比特位）。\n考虑到 n 可能会很大，此时位图中存储的 1 可能会较为稀疏（浪费空间），这时就可以另外对位图进行游程编码，如图的下半部分所示。\n 内存带宽：对于需要扫描数百万行的数据仓库查询来说，一个巨大的瓶颈是从硬盘获取数据到内存的带宽。但是，这不是唯一的瓶颈。分析型数据库的开发人员还需要有效地利用主存储器到 CPU 缓存的带宽，避免 CPU 指令处理流水线中的分支预测错误和气泡，以及在现代 CPU 上使用单指令多数据（SIMD）指令。\n矢量化处理：列式存储布局可以更有效的利用 CPU 周期。查询引擎可以将大量压缩的列数据放在 CPU 的 L1 缓存中，然后在紧密的循环（即没有函数调用）中遍历。相比较于每个记录的处理都需要大量函数调用和条件判断的代码，CPU 执行这样一个循环要快得多。列压缩允许列中的更多行被放进相同数量的 L1 缓存。前面描述的按位 “与” 和 “或” 运算符可以被设计为直接在这样的压缩列数据块上操作。\n 排序 对于列式存储来说，按照插入顺序来存储数据是最简单的，因为我们只需要将插入行的数据追加到各个列文件中即可。但是考虑到查询、压缩的效率，通常我们会选择几个列作为排序列，所有列文件以该列为标准来排序数据。\n最常见的做法是依据我们的查询条件来指定，例如在经常以时间来筛选的场景，就可以将时间设定为第一个排序 key，这样在查询的时候，我们就能够缩小扫描的范围，只锁定对应时间周期的数据。接着，我们可以将数据的特征等信息作为第二、三个排序 key，这样就能够帮助我们更加精准的进行查询、分组、聚合。\n排序还有另一个好处，就是在排序完成后，相同的数据会连续存储。这时再采用一些压缩算法（例如前面提到的游程编码），就可以极大的节省空间。\n写入 对于列式存储来说，使用类似于 B+ 树那样原地更新的方法是不可行的，这不仅意味着我们需要先对数据进行解码，如果数据插入在中间位置，还需要在全部列文件中向后偏移该行以后的所有数据，效率十分低下。\n基于以上原因，列式存储通常采用类似 LSM 树那样的存储结构。即所有的写入首先会存储到内存中的 MemTable 中（红黑树、跳表、AVL 树等结构），当 MemTable 达到一定的大小后，此时就会将数据写入到磁盘中，与原有的列数据进行合并（保留最新的），并批量写入到新文件中。\n 在查询的时候其实也是分别查询磁盘结构和内存结构后合并查询结果，但是查询优化器向用户隐藏了这个细节。\n 聚合：物化视图与数据立方体 数据仓库的查询通常会涉及聚合函数，如 SQL 中的 COUNT、SUM、AVG、MIN 或 MAX。为了避免每次查询时都对原始数据进行一次聚合计算，通常会将一些频繁使用的聚合结果给缓存下来。\n创建这种缓存的一种方式是物化视图（Materialized View）。在关系数据模型中，它通常被定义为一个标准（虚拟）视图：**一个类似于表的对象，其内容是一些查询的结果。**不同的是，物化视图是查询结果的实际副本，会被写入硬盘，而虚拟视图只是编写查询的一个捷径。从虚拟视图读取时，SQL 引擎会将其展开到视图的底层查询中，然后再处理展开的查询。\n 当底层数据发生变化时，物化视图就需要更新，因为它是数据的非规范化副本。虽然数据库可以自动完成该操作，但是这样的更新使得写入的成本更高，这就是在 OLTP 数据库中不经常使用物化视图的原因，而在读取频繁的 OLAP 数据仓库中，它们的作用就体现了出来。\n 物化视图常见的一种特殊实现被称为数据立方体或 OLAP 立方体，它是按不同维度分组的聚合网格，如下图所示：\n数据立方体的优点是其提前计算好了一些聚合的结果，可以让一些查询变得非常快，但同时也因为它没有保留原始数据，不具有查询原始数据的灵活性，所以它通常作为提升查询性能的优化手段。\n","date":"2022-07-10T02:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89%E5%AD%98%E5%82%A8%E4%B8%8E%E6%A3%80%E7%B4%A2/","title":"数据密集型应用系统设计 学习笔记（三）：存储与检索"},{"content":"数据模型与查询语言 关系模型与文档模型 现在最著名的数据模型可能是 SQL ，它基于 Edgar Codd 在 1970 年提出的关系模型： 数据被组织成关系（在 SQL 中称为表），其中每个关系都是元组的无序集合（SQL 中称为行）。\n关系模型的目标就是将实现细节隐藏在更简洁的接口后面。\nNoSQL的诞生 NoSQL最常见的解释是 Non-Relationa，而如今又使用 Not Only SQL 来对其进行解释。NoSQL 其实并不代表具体的某些技术，它仅仅是一个概念，泛指非关系型的数据库，区别于关系数据库，它们不保证关系数据的ACID特性。\n采用 NoSQL 数据库有这样几个驱动因素 ，包括：\n 比关系数据库更好的扩展性需求，包括支持超大数据集或超高写入吞吐量。 普遍偏爱免费和开源软件而不是商业数据库产品。 关系模型不能很好地支持一些特定的查询操作。 对关系模式一些限制性感到沮丧，渴望更具动态和表达力数据模型。  对象-关系不匹配 如今大多数应用开发都采用面向对象的编程语言，由于兼容性问题，普遍对 SQL 数据模型存在抱怨：如果数据存储在关系表中，那么应用层代码中的对象与表、行和列的数据库模型之间需要一个笨拙的转换层。这种模型之间的脱离被称为阻抗失谐。\n像 ActiveRecord 和 Hibernate 这样的对象关系映射（ORM，Object-Relational Mapping） 框架减少了此转换 层的样板代码 ，但是他们并不能完全隐藏两个模型之间的差异。\n例如我们使用关系模型来描述一个简历：\n对于简历这种用户与经历这种一对多的关系，我们可以用多种方法表示：\n 传统的 SQL 模型，将职位、教育和联系信息放在单独的表中并使用外键引用 users 表。 SQL 标准增加了对结构化数据类型和 XML 数据的支持。这允许将多值数据存储在单行，井支持在这些文档中查询和索引。 将信息编码为 JSON、XML 文档，将其存储在数据库的文本列中，并由应用程序解释其结构和内容。  对于像简历这样的数据结构，它主要是一个自包含的文档，因此用 JSON 表示更加合适。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  { \u0026#34;user_id\u0026#34;: 251, \u0026#34;first_name\u0026#34;: \u0026#34;Bill\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Gates\u0026#34;, \u0026#34;summary\u0026#34;: \u0026#34;Co-chair of the Bill \u0026amp; Melinda Gates... Active blogger.\u0026#34;, \u0026#34;region_id\u0026#34;: \u0026#34;us:91\u0026#34;, \u0026#34;industry_id\u0026#34;: 131, \u0026#34;photo_url\u0026#34;: \u0026#34;/p/7/000/253/05b/308dd6e.jpg\u0026#34;, \u0026#34;positions\u0026#34;: [ { \u0026#34;job_title\u0026#34;: \u0026#34;Co-chair\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;Bill \u0026amp; Melinda Gates Foundation\u0026#34; }, { \u0026#34;job_title\u0026#34;: \u0026#34;Co-founder, Chairman\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;Microsoft\u0026#34; } ], \u0026#34;education\u0026#34;: [ { \u0026#34;school_name\u0026#34;: \u0026#34;Harvard University\u0026#34;, \u0026#34;start\u0026#34;: 1973, \u0026#34;end\u0026#34;: 1975 }, { \u0026#34;school_name\u0026#34;: \u0026#34;Lakeside School, Seattle\u0026#34;, \u0026#34;start\u0026#34;: null, \u0026#34;end\u0026#34;: null } ], \u0026#34;contact_info\u0026#34;: { \u0026#34;blog\u0026#34;: \u0026#34;http://thegatesnotes.com\u0026#34;, \u0026#34;twitter\u0026#34;: \u0026#34;http://twitter.com/BillGates\u0026#34; } }   JSON 模型减少了应用程序代码和存储层之间的阻抗失配，并且其比起关系模式的多表查询具有更好的局部性，其所有的相关信息都在一个地方，所以一次查询就足够了，因此大部分文档数据库都支持这种模型。\n数据模型 层次模型 层次模型与文档数据库使用的 JSON 模型有一些显著的相似之处。它将所有数据表示为嵌套在记录中的记录（树）。\n层次模型可以很好地支持一对多关系，但是它支持多对多关系则有些困难，而且不支持联结。开发人员必须决定是复制（反规范化）多份数据，还是手动解析记录之间的引用。\n为了解决层次模型的局限性，之后又提出了多种解决方案。其中最著名的是关系模型（relational model）和网络模型（network model）。\n网络模型 网络模型由一个称为数据系统语言会议（Conference on Data System Languages, CODASYL）的委员会进行标准化，井由多个不同的数据库厂商实施，它也被称为 CODASYL 模型。\n网络模型是层次模型的推广。在层次模型的树结构中，每个记录只有一个父节点；而在网络模型中， 一个记录可能有多个父结点，从而支持对多对一和多对多的关系进行建模。\n在网络模型中，记录之间的链接不是外键，而更像是编程语言中的指针（会存储在磁盘上），访问记录的唯一方法是选择一条始于根记录的路径，并沿着相关链接依次访问。这条链接链条也因此被称为访问路径。\n网络模型中的查询通过遍历记录列表，并沿着访问路径在数据库中移动游标来执行。如果记录有多个父结点，应用程序代码必须跟踪所有关系。因此其最大的问题在于它们使查询和更新数据库变得异常复杂而没有灵活性。\n关系模型 关系模型所做的则是定义了所有数据的格式：关系（表）只是元组（行）集合。 没有复杂的嵌套结构，也没有复杂的访问路径。可以读取表中的任何一行或者所有行，支持任意条件查询。可以指定某些列作为键并匹配这些列来读取特定行。可以在任何表中插入新行，而不必担心与其他表之间的外键关系。\n在关系数据库中，查询优化器自动决定以何种顺序执行查询，以及使用哪些索引。这些选择实际上等价于访问路径 但最大的区别在于它是由查询优化器自动生成的，而不是由应用开发人员所维护，因此不用过多地考虑它。\n数据查询语言 声明式/命令式查询语言 数据查询语言通常分为以下两类：命令式查询语言、声明式查询语言。\n 命令式查询语言：其会告诉计算机以特定顺序执行某些操作。 因此我们完全可以推理整个过程，逐行遍历代码、评估相关条件、更新对应的变量，并决定是否再循环一遍。 声明式查询语言：只需指定所需的数据模式，结果需满足什么条件，以及如何转换数据（例如，排序、分组和聚合），而不需指明如何实现这些目标。 数据库系统的查询优化器会决定采用哪些索引和联结，以及用何种顺序来执行查询的各个语句。  声明式查询语言很有吸引力，它比命令式 API 更加简洁和容易使用。但更重要的是，它对外隐藏了数据库引擎的很多实现细节，这样数据库系统能够在不改变查询语句的情况下提高性能。\n声明式语言通常适合于并行执行。 现在 CPU 提升速度的方式是增加核心数，而不是实现比之前更高的时钟频率。而命令式代码由于指定了特定的执行顺序，很难在多核和多台机器上并行。声明式语言则对于并行执行更加友好，它们仅指定了结果所满足的模式，而不指定如何得到结果的具体算法。\nMapReduce查询 MapReduce 是一种编程模型，用于在许多机器上批量处理海量数据，兴起于 Google。一些 NoSQL 存储系统（包括 MongoDB 和 CouchDB）支持有限的 MapReduce 方式在大量文档上执行只读查询。\nMapReduce 既不是声明式查询语言，也不是一个完全命令式的查询 API，而是介于两者之间：查询的逻辑用代码片段来表示，这些代码片段可以被处理框架重复地调用。它主要基于许多函数式编程语言中的 map 和 reduce 函数。\n 假设你是一名海洋生物学家，每当你看到海洋中的动物时，你都会在数据库中添加一条观察记录。现在你想生成一个报告，说明你每月看到多少鲨鱼\n 如果使用 SQL 来表示，代码如下：\n1 2 3 4 5 6  SELECTdate_trunc(\u0026#39;month\u0026#39;,observation_timestamp)ASobservation_month,sum(num_animals)AStotal_animalsFROMobservationsWHEREfamily=\u0026#39;Sharks\u0026#39;GROUPBYobservation_month;  而如果使用 MongoDB 的 MapReduce 功能，则可以按如下来表述：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  db.observations.mapReduce( function map() { var year = this.observationTimestamp.getFullYear(); var month = this.observationTimestamp.getMonth() + 1; emit(year + \u0026#34;-\u0026#34; + month, this.numAnimals); }, function reduce(key, values) { return Array.sum(values); }, { query: { family: \u0026#34;Sharks\u0026#34; }, out: \u0026#34;monthlySharkReport\u0026#34; } );   MapReduce 函数对于可执行的操作有所限制。它必须是纯函数，这意味着只能使用传递进去的数据作为输入，而不能执行额外的数据库查询，也不能有任何副作用。这些限制使得数据库能够在任何位置、以任意顺序来运行函数，并在失败时重新运行这些函数。\n图状数据模型 图由两种对象组成：顶点（也称为结点或实体）和边（也称为关系或弧）。很多数据可以建模为图。典型的例子包括：\n 社交网络：顶点是人，边指示哪些人彼此认识。 Web 图：顶点是网页，边表示与其他页面的 HTML 链接。 公路或铁路网：顶点是交叉路口，边表示他们之间的公路或铁路线。  如下图，即使用图来存储族谱信息：\n属性图 在属性图模型中，每个顶点包括：\n  唯一的标识符。\n  出边的集合。\n  入边的集合。\n  属性的集合（键值对） 。\n  每个边包括：\n  唯一的标识符。\n  边开始的顶点（尾部顶点） 。\n  边结束的顶点（头部顶点） 。\n  描述两个顶点间关系类型的标签。\n  属性的集合（键-值对）。\n  **可以将图存储看作由两个关系表组成，一个用于顶点， 另一个用于边。**如下列代码使用关系模式来标识属性图：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  CREATETABLEvertices(vertex_idINTEGERPRIMARYKEY,propertiesJSON);CREATETABLEedges(edge_idINTEGERPRIMARYKEY,tail_vertexINTEGERREFERENCESvertices(vertex_id),head_vertexINTEGERREFERENCESvertices(vertex_id),labelTEXT,propertiesJSON);CREATEINDEXedges_tailsONedges(tail_vertex);CREATEINDEXedges_headsONedges(head_vertex);  对于图模型还有一些值得注意的地方：\n 任何顶点都可以连接到其他任何顶点。没有模式限制哪种事物可以或不可以关联。 给定某个顶点，可以高效地得到它的所有人边和出边，从而遍历图，即沿着这些顶点链条一直向前或向后。 通过对不同类型的关系使用不同的标签，可以在单个图中存储多种不同类型的信息，同时仍然保持整洁的数据模型。  Cypher查询语言 Cypher 是一种用于属性图的声明式查询语言，最早为 Neo4j 图形数据库而创建。\n来看看一个创建的代码示例，每个顶点都有一个像 USA、Idaho 这样的名称，查询可以使用这些名称创建顶点之间的边，如使用箭头符号： (Idaho) -[:WITHIN]-\u0026gt; (USA) 创建一个标签为 WITHIN 边，其中 Idaho 为尾结点， USA 为头结点。\n1 2 3 4 5 6 7  CREATE (NAmerica:Location {name:\u0026#39;North America\u0026#39;, type:\u0026#39;continent\u0026#39;}), (USA:Location {name:\u0026#39;United States\u0026#39;, type:\u0026#39;country\u0026#39; }), (Idaho:Location {name:\u0026#39;Idaho\u0026#39;, type:\u0026#39;state\u0026#39; }), (Lucy:Person {name:\u0026#39;Lucy\u0026#39; }), (Idaho) -[:WITHIN]-\u0026gt; (USA) -[:WITHIN]-\u0026gt; (NAmerica), (Lucy) -[:BORN_IN]-\u0026gt; (Idaho)   接下来看一个使用 MATCH 语句的查询示例（查询从美国移民到欧洲的人员名单），其使用与上面相同的箭头语义：(person) -[:BORN_IN]-\u0026gt; () 即匹配所有顶点间带有标签 BORN_IN 的边，且尾部顶点对应于变量 person ，而头部顶点则没有要求。\n1 2 3 4  MATCH (person) -[:BORN_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (us:Location {name:\u0026#39;United States\u0026#39;}), (person) -[:LIVES_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (eu:Location {name:\u0026#39;Europe\u0026#39;}) RETURN person.name   该代码会匹配所有满足以下两个条件的任何顶点：\n  person 有一个到其他顶点的出边 BORN_IN 。从该顶点开始，可以沿着一系列出边 WITHIN，直到最终到达类型为 Location 的顶点，name 属性为 United States\nStates\n  person 顶点也有一个出边 LIVES_IN。沿着这条边，然后是一系列出边 WITHIN，最终到达类型为 Location 的顶点， name 属性为 Europe。\n  我们不需要关注它底层是如何进行查询的，对于声明式查询语言，通常在编写查询语句时，不需要指定执行细节,查询优化器会自动选择效率最高的执行策略，因此开发者可以专注于应用的其他部分。\nSQL中的图查询 既然可以使用关系数据库来表示图数据，那如果把图数据放在关系结构中，是否意味着也可以支持 SQL 查询呢？\n在关系数据库中，通常会预先知道查询中需要哪些 join 操作。而对于图查询，在找到要查询的顶点之前，可能需要遍历数量未知的边。也就是说，join 操作数量并不是预先确定的。\n在 SQL 查询过程中，这种可变的遍历路径可以使用递归公用表表达式（WITH RECURSIVE 语法）来表示。\n例如在上面 Cypher 中提到的用例，用 SQL 表示如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  WITHRECURSIVE-- in_usa 包含所有的美国境内的位置ID in_usa(vertex_id)AS(SELECTvertex_idFROMverticesWHEREproperties-\u0026gt;\u0026gt;\u0026#39;name\u0026#39;=\u0026#39;United States\u0026#39;UNIONSELECTedges.tail_vertexFROMedgesJOINin_usaONedges.head_vertex=in_usa.vertex_idWHEREedges.label=\u0026#39;within\u0026#39;),-- in_europe 包含所有的欧洲境内的位置ID in_europe(vertex_id)AS(SELECTvertex_idFROMverticesWHEREproperties-\u0026gt;\u0026gt;\u0026#39;name\u0026#39;=\u0026#39;Europe\u0026#39;UNIONSELECTedges.tail_vertexFROMedgesJOINin_europeONedges.head_vertex=in_europe.vertex_idWHEREedges.label=\u0026#39;within\u0026#39;),-- born_in_usa 包含了所有类型为Person，且出生在美国的顶点 born_in_usa(vertex_id)AS(SELECTedges.tail_vertexFROMedgesJOINin_usaONedges.head_vertex=in_usa.vertex_idWHEREedges.label=\u0026#39;born_in\u0026#39;),-- lives_in_europe 包含了所有类型为Person，且居住在欧洲的顶点。 lives_in_europe(vertex_id)AS(SELECTedges.tail_vertexFROMedgesJOINin_europeONedges.head_vertex=in_europe.vertex_idWHEREedges.label=\u0026#39;lives_in\u0026#39;)SELECTvertices.properties-\u0026gt;\u0026gt;\u0026#39;name\u0026#39;FROMverticesJOINborn_in_usaONvertices.vertex_id=born_in_usa.vertex_idJOINlives_in_europeONvertices.vertex_id=lives_in_europe.vertex_id;  下面来看看上述代码的逻辑：\n  首先，查找 name 属性为 United States 的顶点，将其作为 in_usa 顶点的集合的第一个元素。\n  从 in_usa 集合的顶点出发，沿着所有的 with_in 入边，将其尾顶点加入同一集合，不断递归直到所有 with_in 入边都被访问完毕。\n  同理，从 name 属性为 Europe 的顶点出发，建立 in_europe 顶点的集合。\n  对于 in_usa 集合中的每个顶点，根据 born_in 入边来查找出生在美国某个地方的人。\n  同样，对于 in_europe 集合中的每个顶点，根据 lives_in 入边来查找居住在欧洲的人。\n  最后，把在美国出生的人的集合与在欧洲居住的人的集合相交。\n  经过对比，如果相同的查询可以用 Cypher 查询语言写 4 行代码就可以完成，而使用 SQL 需要 29 行代码，这足以说明不同的数据模型适用于不同的场景。\n三元存储与SPARQL 三元存储模式几乎等同于属性图模型，只是使用不同的名词描述了相同的思想。在三元存储中，所有信息都以非常简单的三部分形式存储**（主体，谓语，客体）** 。\n三元组的主体相当于图中的顶点。而客体是以下两种之一：\n 原始数据类型中的值，如字符串或数字。 在这种情况下，三元组的谓语和客体分别相当于主体（顶点）属性中的键和值。 图中的另一个顶点。 此时，谓语是图中的边，主体是尾部顶点，而客体是头部顶点。  我们再次拿出 Cypher 中提到的例子，以 Turtle 三元组方式来表示这些数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  @prefix:\u0026lt;urn:example:\u0026gt;._:lucya:Person._:lucy:name\u0026#34;Lucy\u0026#34;._:lucy:bornIn_:idaho._:idahoa:Location._:idaho:name\u0026#34;Idaho\u0026#34;._:idaho:type\u0026#34;state\u0026#34;._:idaho:within_:usa._:usaa:Location_:usa:name\u0026#34;United States\u0026#34;_:usa:type\u0026#34;country\u0026#34;._:usa:within_:namerica._:namericaa:Location_:namerica:name\u0026#34;North America\u0026#34;_:namerica:type:\u0026#34;continent\u0026#34;  顶点的名字在定义文件以外没有任何意义，只是为了区分三元组的不同顶点。\n如果要定义相同主体的多个三元组，反复输入相同的单词就略显枯燥。可以使用分号来说明同一主体的多个对象信息。下面即用更简洁的语法来重写上面的逻辑：\n1 2 3 4 5  @prefix:\u0026lt;urn:example:\u0026gt;._:lucya:Person;:name\u0026#34;Lucy\u0026#34;;:bornIn_:idaho._:idahoa:Location;:name\u0026#34;Idaho\u0026#34;;:type\u0026#34;state\u0026#34;;:within_:usa_:usaa:Loaction;:name\u0026#34;United States\u0026#34;;:type\u0026#34;country\u0026#34;;:within_:namerica._:namericaa:Location;:name\u0026#34;North America\u0026#34;;:type\u0026#34;continent\u0026#34;.  RDF数据模型 语义网从本质上讲源于一个简单而合理的想法：网站通常将信息以文字和图片的方式发布给人类阅读，那为什么不把信息发布为机器可读的格式给计算机阅读呢？\n资源描述框架（Resource Description Framework, RDF） 就是这样一种机制，它让不同网站以一致的格式发布数据，这样来自不同网站的数据自动合并成一个数据网络，一种互联网级别包含所有数据的数据库。\nTurtle 语言代表了 RDF 数据的人类可读格式。有时候 RDF 也用 XML 格式编写，不过更冗长一些，人眼容易阅读 Turtle 这种格式，因此通常会借助一些自动化工具（如 Apache Jena）来快速转换各种 RDF 格式。\n下面即用 RDF/XML 语法重写上面的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  \u0026lt;rdf:RDF xmlns=\u0026#34;urn:example:\u0026#34; xmlns:rdf=\u0026#34;http://www.w3.org/1999/02/22-rdf-syntax-ns#\u0026#34;\u0026gt; \u0026lt;Location rdf:nodeID=\u0026#34;idaho\u0026#34;\u0026gt; \u0026lt;name\u0026gt;Idaho\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;state\u0026lt;/type\u0026gt; \u0026lt;within\u0026gt; \u0026lt;Location rdf:nodeID=\u0026#34;usa\u0026#34;\u0026gt; \u0026lt;name\u0026gt;United States\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;country\u0026lt;/type\u0026gt; \u0026lt;within\u0026gt; \u0026lt;Location rdf:nodeID=\u0026#34;namerica\u0026#34;\u0026gt; \u0026lt;name\u0026gt;North America\u0026lt;/name\u0026gt; \u0026lt;type\u0026gt;continent\u0026lt;/type\u0026gt; \u0026lt;/Location\u0026gt; \u0026lt;/within\u0026gt; \u0026lt;/Location\u0026gt; \u0026lt;/within\u0026gt; \u0026lt;/Location\u0026gt; \u0026lt;Person rdf:nodeID=\u0026#34;lucy\u0026#34;\u0026gt; \u0026lt;name\u0026gt;Lucy\u0026lt;/name\u0026gt; \u0026lt;bornIn rdf:nodeID=\u0026#34;idaho\u0026#34;/\u0026gt; \u0026lt;/Person\u0026gt; \u0026lt;/rdf:RDF\u0026gt;   因为旨在为全网数据交换而设计，RDF 存在一些特殊的约定：三元组的主体、谓语和客体通常是 URI。这种设计背后的原因是其假设你的数据需要和其他人的数据相结合，万一不同的人给同一个单词附加了不同的含义，采用 URI 则可以避免冲突。\nSPARQL查询语言 SPARQL 是一种采用 RDF 数据模型的三元存储查询语言，它比 Cypher 更早，并且由于 Cypher 的模式匹配是借用 SPARQL 的，所以二者看起来非常相似。\n让我们再次使用 Cypher 中的那个示例，如果用 SPARQL 执行相同的查询，其会比 Cypher 要简洁的多，代码如下:\n1 2 3 4 5 6  PREFIX : \u0026lt;urn:example:\u0026gt; SELECT ?personName WHERE { ?person :name ?personName. ?person :bornIn / :within* / :name \u0026#34;United States\u0026#34;. ?person :livesIn / :within* / :name \u0026#34;Europe\u0026#34;. }   SPARQL 是一种非常优秀的查询语言， 即使语义网从未实际出现，它也可以成为应用程序内部使用的强大查询工具。\nDatalog Datalog 是比 SPARQL 和 Cypher 更为古老的语言，在 20 世纪 80 年代被学者广泛研究。其数据模型类似于三元组模式，但进行了一点泛化。把三元组写成谓语（主语，客体），而不是**（主语，谓语，客体）**。\n我们尝试用 Datalog 来表示之前的查询，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12  within_recursive(Location, Name) :- name(Location, Name). /* Rule 1 */ within_recursive(Location, Name) :- within(Location, Via), /* Rule 2 */ within_recursive(Via, Name). migrated(Name, BornIn, LivingIn) :- name(Person, Name), /* Rule 3 */ born_in(Person, BornLoc), within_recursive(BornLoc, BornIn), lives_in(Person, LivingLoc), within_recursive(LivingLoc, LivingIn). ?- migrated(Who, \u0026#39;United States\u0026#39;, \u0026#39;Europe\u0026#39;). /* Who = \u0026#39;Lucy\u0026#39;. */   从上面的代码可以看出，Cypher 和 SPARQL 使用 SELECT 一次完成查询，而 Datalog 则每次查询一部分。我们定义了告诉数据库关于新谓语的规则。例如两个新的谓语，within_recursive 和 migrated。\n这些谓语并不是存储在数据库中的三元组，而是从数据或其他规则派生而来。规则可以引用其他规则，就像函数可以调用其他函数或者递归调用自己一样。像这样，复杂的查询可以通过每次完成一小块而逐步构建。\n","date":"2022-07-10T01:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9F%A5%E8%AF%A2%E8%AF%AD%E8%A8%80/","title":"数据密集型应用系统设计 学习笔记（二）：数据模型与查询语言"},{"content":"可靠性、可拓展性与可维护性 现如今大多应用程序都是 数据密集型（data-intensive） 的，而非 计算密集型（compute-intensive） 的。因此CPU很少成为这类应用的瓶颈，更大的问题通常来自数据量、数据复杂性、以及数据的变更速度。\n数据密集型应用通常也是基于标准模块构建而成，每个模块负责单一的常用功能。例如，许多应用系统都包含以下模块：\n  数据库（DataBase）：用以存储数据，这样之后应用可以再次访问。\n  缓存（Cache）：缓存那些复杂或操作代价昂贵的结果，加快读取速度。\n  索引（Search Indexe）：用户可以按关键字搜索数据井支持以各种方式对数据进行过滤\n  流处理（Stream Processing）：持续发送消息至另一个进程，处理采用异步方式。\n  批处理（Batch Processing）：定期处理大量的累积数据。\n  认识数据系统 影响数据系统设计的因素有很多，其中包括相关人员技能和经验水平、历史遗留问题、交付周期、对不同风险因素的容忍度、监管合规等。这些因素往往因时因地而异。本书将专注于对大多数软件系统都极为重要的三个问题：\n 可靠性（Reliability）：系统在困境（adversity，如硬件故障、软件故障、人为错误）中仍可正常工作（正确完成功能，并能达到期望的性能水准）。 可伸缩性（Scalability）：系统有合理的办法应对规模的增长（如数据量、流量、复杂性等）。 可维护性（Maintainability）：许多不同的人（工程师、运维）在不同的生命周期，都能高效地在系统上工作（使系统保持现有行为，并适应新的应用场景）。  可靠性 人们对于一个东西是否可靠，都有一个直观的想法。人们对可靠软件的典型期望包括：\n 应用程序表现出用户所期望的功能。 允许用户犯错，允许用户以出乎意料的方式使用软件。 在预期的负载和数据量下，性能满足要求。 系统能防止未经授权的访问和滥用。  如果所有这些在一起意味着“正确工作”，那么可以把可靠性粗略理解为即使出现问题，也能继续正确工作。\n尽管比起阻止错误，我们通常更倾向于容忍错误。可以恢复的故障种类有硬件故障、软件错误，人为失误三种。\n硬件故障 当考虑系统故障时，对于硬件故障总是很容易想到硬盘崩溃，内存故障，电网停电，甚至有人误拔掉了网线。\n那我们如何应对这些问题呢？\n 硬件冗余：例如对磁盘配置 RAID ，服务器配备双电源，甚至热插拔 CPU，数据中心添加备用电源、发电机等。 软件容错：例如当需要重启计算机时为操作系统打安全补丁，可以每次给一个节点打补丁然后重启，而不需要同时下线整个系统。  软件错误 这类错误难以预料，而且因为是跨节点相关的，所以比起不相关的硬件故障往往可能造成更多的系统失效。例子包括：\n 由于软件错误，导致当输入特定值时应用服务器总是崩溃。例如2012年6月30日的闰秒，由于Linux内核中的一个错误，许多应用同时挂掉。 失控进程会用尽一些共享资源，包括 CPU、内存、磁盘空间或网络带宽。 系统依赖的服务变慢，没有响应，或者开始返回错误的响应。 级联故障，一个组件中的小故障触发另一个组件中的故障，进而触发更多的故障。  虽然软件中的系统性故障没有速效药，但我们还是有很多小办法，如：包括认真检查依赖的假设条件与系统之间交互 、进行全面的测试、进程隔离，允许进程崩溃并自动重启、反复评估，监控井分析生产环节的行为表现等。\n人为失误 设计和构建软件系统总是由人类完成，也是由人来运维这些系统。即使有时意图是好的，但人却无发做到万无一失。经过统计，运维人员的配置错误是系统下线的首要原因。\n如果我们假定人是不可靠的，那么该如何保证系统的可靠性呢？我们可以尝试以下方法：\n 以最小出错的方式来设计系统。 想办法分离最容易出错的地方、容易引发故障的接口。 采用充分的测试，如从各单元测试到全系统集成测试以及手动测试。 当出现人为失失误时，提供快速的恢复机制以尽量减少故障影响。 设置详细而清晰的监控子系统，包括性能指标和错误率。 推行管理流程井加以培训 。  可拓展性 系统今天能可靠运行，并不意味未来也能可靠运行。**降级（degradation）**的一个常见原因是负载增加，例如：系统负载已经从一万个并发用户增长到十万个并发用户，或者从一百万增长到一千万。也许现在处理的数据量级要比过去大得多。\n负载 负载可以用一些称为**负载参数（load parameters）**的数字来描述。参数的最佳选择取决于系统架构，它可能是每秒向Web服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的用户数量、缓存命中率或其他东西。\n性能 常见的用于描述系统性能的指标如下：\n 吞吐量（throughput）：每秒可以处理的记录数量，或者在特定规模数据集上运行作业的总时间。通常用于评估批处理系统。 响应时间（response time）：即客户端发送请求到接收响应之间的时间。通常用于评估在线系统。   延迟（latency） 和 响应时间（response time） 经常用作同义词，但实际上它们并不一样。响应时间是客户所看到的，除了实际处理请求的时间（ 服务时间（service time） ）之外，还包括网络延迟和排队延迟。延迟是某个请求等待处理的持续时长，在此期间它处于 休眠（latent） 状态，并等待服务。\n 应对方法 拓展方式分为两种：\n 水平拓展：即将负载分布到多个更小的机器。 垂直拓展：即升级到更强大的机器。  跨多台机器分配负载也称为无共享（shared-nothing）架构。可以在单台机器上运行的系统通常更简单，但高端机器可能非常贵，所以非常密集的负载通常无法避免地需要水平伸缩。\n有些系统是弹性（elastic）的，这意味着可以在检测到负载增加时自动增加计算资源，而其他系统则是手动伸缩（人工分析容量并决定向系统添加更多的机器）。如果负载极难预测（highly unpredictable），则弹性系统可能很有用，但手动伸缩系统更简单，并且意外操作可能会更少。\n跨多台机器部署**无状态服务（stateless services）**非常简单，但将有状态的数据系统从单节点变为分布式配置则可能引入许多额外复杂度。出于这个原因，常识告诉我们应该将数据库放在单个节点上（纵向伸缩），直到伸缩成本或可用性需求迫使其改为分布式。\n扩展能力好的架构通常会做出某些假设，然后有针对性地优化设计。\n可维护性 软件的大部分开销并不在最初的开发阶段，而是在持续的维护阶段，包括修复漏洞、保持系统正常运行、调查失效、适配新的平台、为新的场景进行修改、偿还技术债、添加新的功能等等。\n因此在设计之初就尽量考虑尽可能减少维护期间的痛苦，从而避免自己的软件系统变成遗留系统。 为此，我们将特别关注软件系统的三个设计原则：\n  可运维性（Operability）：便于运维团队保持系统平稳运行。\n  简单性（Simplicity）：简化系统复杂性，使新工程师能够轻松理解系统。\n  可演化性（evolvability）：后续工程师能够轻松地对系统进行改进，井根据需求变化将其适配到非典型场\n景。\n  ","date":"2022-07-10T00:11:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%95%B0%E6%8D%AE%E5%AF%86%E9%9B%86%E5%9E%8B%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%8F%AF%E6%8B%93%E5%B1%95%E6%80%A7%E4%B8%8E%E5%8F%AF%E7%BB%B4%E6%8A%A4%E6%80%A7/","title":"数据密集型应用系统设计 学习笔记（一）：可靠性、可拓展性与可维护性"},{"content":"什么是 GoogleTest ？ 简介 GoogleTest（简称 GTest） 是 Google 开源的一个跨平台的（Liunx、Mac OS X、Windows等）的 C++ 单元测试框架，可以帮助程序员测试 C++ 程序的结果预期。不仅如此，它还提供了丰富的断言、致命和非致命判断、参数化、”死亡测试”等等。\nGoogleTest 官网：https://google.github.io/googletest/ ​ github 仓库：https://github.com/google/googletest\n单元测试 单元测试（unit testing），是指对软件中的最小可测试单元进行检查和验证。至于单元的大小或范围，并没有一个明确的标准，单元可以是一个函数、方法、类、功能模块或者子系统。 单元测试通常和白盒测试联系到一起，如果单从概念上来讲两者是有区别的，不过我们通常所说的单元测试和白盒测试都认为是和代码有关系的，所以在某些语境下也通常认为这两者是同一个东西。还有一种理解方式，单元测试和白盒测试就是对开发人员所编写的代码进行测试。\n优势  测试是独立的和可重复的。GoogleTest 使每个测试用例运行在不同的对象上，从而使测试之间相互独立。当测试失败时，GoogleTest 允许单独运行它以进行快速调试。 测试有良好的组织，可以反映被测试代码的结构。 GoogleTest 将相关测试划分到一个测试组内，组内的测试能共享数据，使测试易于维护。 测试是可移植的和可重复使用的。 与平台无关的代码，其测试代码也应该和平台无关，GoogleTest 能在不同的操作系统下工作，并且支持不同的编译器。 当测试用例执行失败时，提供尽可能多的有效信息，以便定位问题。 GoogleTest 不会在第一次测试失败时停止。相反，它只会停止当前测试并继续下一个测试。还可以设置报告非致命故障的测试，然后继续当前测试。因此，您可以在单个运行-编辑-编译周期中检测和修复多个错误。 测试框架应该将测试编写者从琐事中解放出来，让他们专注于测试内容。 GoogleTest 自动跟踪所有定义的测试，不需要用户为了运行它们而进行枚举。 测试高效、快速。GoogleTest 能在测试用例之间复用测试资源，只需支付一次设置/拆分成本，并且不会使测试相互依赖，这样的机制使单元测试更加高效。  环境搭建 安装 GoogleTest Bazel 首先创建一个工作区目录：\n1  $ mkdir my_workspace \u0026amp;\u0026amp; cd my_workspace   接着在工作区的根目录中创建一个 WORKSPACE 文件，并在其中引入外部依赖 GoogleTest，此时 Bazel 会自动去 Github 上拉取文件：\n1 2 3 4 5 6 7  load(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) http_archive( name = \u0026#34;com_google_googletest\u0026#34;, urls = [\u0026#34;https://github.com/google/googletest/archive/609281088cfefc76f9d0ce82e1ff6c30cc3591e5.zip\u0026#34;], strip_prefix = \u0026#34;googletest-609281088cfefc76f9d0ce82e1ff6c30cc3591e5\u0026#34;, )   接着选取一个目录作为包目录，在该目录下进行代码的编写，例如一个 hello_test.cc 文件：\n1 2 3 4 5 6 7 8 9  #include \u0026lt;gtest/gtest.h\u0026gt; // Demonstrate some basic assertions. TEST(HelloTest, BasicAssertions) { // Expect two strings not to be equal.  EXPECT_STRNE(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); // Expect equality.  EXPECT_EQ(7 * 6, 42); }   在同目录下的 BUILD 文件中说明目标编译的规则：\n1 2 3 4 5 6  cc_test( name = \u0026#34;hello_test\u0026#34;, size = \u0026#34;small\u0026#34;, srcs = [\u0026#34;hello_test.cc\u0026#34;], deps = [\u0026#34;@com_google_googletest//:gtest_main\u0026#34;], )   此时执行以下命令即可构建并运行测试程序：\n1  bazel test --test_output=all //:hello_test   Cmake 首先创建一个目录：\n1  $ mkdir my_project \u0026amp;\u0026amp; cd my_project   接着创建 CMakeLists.txt 文件，并声明对 GoogleTest 的依赖，此时 Cmake 会自动去下载对应的库：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  cmake_minimum_required(VERSION 3.14)project(my_project)# GoogleTest requires at least C++11 set(CMAKE_CXX_STANDARD 11)include(FetchContent)FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/609281088cfefc76f9d0ce82e1ff6c30cc3591e5.zip )# For Windows: Prevent overriding the parent project\u0026#39;s compiler/linker settings set(gtest_force_shared_crt ON CACHE BOOL \u0026#34;\u0026#34; FORCE)FetchContent_MakeAvailable(googletest)  此时我们就可以在代码中使用 GoogleTest，我们创建一个 hello_test.cc 文件：\n1 2 3 4 5 6 7 8 9  #include \u0026lt;gtest/gtest.h\u0026gt; // Demonstrate some basic assertions. TEST(HelloTest, BasicAssertions) { // Expect two strings not to be equal.  EXPECT_STRNE(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); // Expect equality.  EXPECT_EQ(7 * 6, 42); }   然后在 CMakeLists.txt 的末尾加上 hello_test.cc 的构建规则：\n1 2 3 4 5 6 7 8 9 10 11 12 13  enable_testing()add_executable( hello_test hello_test.cc )target_link_libraries( hello_test gtest_main )include(GoogleTest)gtest_discover_tests(hello_test)  运行下面的命令构建并允许测试程序：\n1 2 3 4 5  cmake -S . -B build cmake --build build cd build \u0026amp;\u0026amp; ctest   安装示例项目 从 GoogleTest 的 GitHub 中下载官方提供的示例项目：\n1  git clone https://github.com/google/googletest.git   示例项目位于 googletest/googletest/samples/ 目录，示例内容可参考官网的说明：samples\nGoogleTest 实战 断言 断言的概念 GoogleTest 中，是通过断言（Assertion）来判断代码实现的功能是否符合预期。断言的结果分为成功（success）、非致命错误（non-fatal failture）和致命错误（fatal failture）。\n 成功：断言成功，程序的行为符合预期，程序允许继续向下运行。可以在代码中调用 SUCCEED() 来表示这个结果。 非致命错误：断言失败，但是程序没有直接中止，而是继续向下运行。可以在代码中调用 FAIL() 来表示这个结果。 致命错误：中止当前函数或者程序崩溃。可以在代码中调用 ADD_FAILURE() 来表示这个结果。  GoogleTest 的断言是类似于函数调用的宏。通过对其行为进行断言来测试一个类或函数。当断言失败时，GoogleTest 会打印断言的源文件和行号位置以及失败消息。还可以提供自定义失败消息，该消息将附加到 GoogleTest 的消息中。\nEXPECT 与 ASSERT 宏的格式有两种：\n EXPECT_*：在失败时会产生非致命故障，不会中止当前功能。 ASSERT_*：在失败时会产生致命错误，并中止当前函数，同一用例后面的语句将不再执行。   通常 EXPECT_* 是首选，因为它们允许在测试中报告多个。但是如果在某条件不成立，程序就无法运行时，就应该使用 ASSERT_* 。\n另一方面，因为 ASSERT_* 是直接从当前函数返回的，可能会导致一些内存、文件资源没有释放，因此存在内存泄漏的问题。\n GoogleTest 提供了一组断言，用于以各种方式验证代码的行为。包括检查布尔条件、基于关系运算符比较值、验证字符串值、浮点值等等。甚至还有一些断言可以通过提供自定义谓词来验证更复杂的状态。有关 GoogleTest 提供的断言的完整列表，可以参考官方文档：Assertions。\n自定义失败信息 如果想要提供自定义的失败信息，只需要使用流操作符 \u0026laquo; 将这些信息流到断言宏中，例如：\n1 2 3 4 5  ASSERT_EQ(x.size(), y.size()) \u0026lt;\u0026lt; \u0026#34;Vectors x and y are of unequal length\u0026#34;; for (int i = 0; i \u0026lt; x.size(); ++i) { EXPECT_EQ(x[i], y[i]) \u0026lt;\u0026lt; \u0026#34;Vectors x and y differ at index \u0026#34; \u0026lt;\u0026lt; i; }   任何可以流向 std::ostream 的东西都可以流向断言宏，特别是 C 语言的字符串（char*）和 std::string 对象。如果一个宽字符串（wchar_t*，Windows 上 UNICODE 模式下的 TCHAR*，或 std::wstring）被流向一个断言，它将在打印时被转换成 UTF-8。\n功能测试 TEST 如果想要创建测试，可以使用宏函数 TEST，它具有以下特点：\n TEST 是一个没有返回值的宏函数。 我们可以在该函数中使用断言来检测代码是否有效，测试的结果由断言决定。如果测试中的任何断言失败（致命或非致命），或者如果测试崩溃，则整个测试失败。否则，它会成功。  函数定义如下：\n1 2 3  TEST(TestSuiteName, TestName) { ... test body ... }   TestSuiteName 对应测试用例集名称，TestName 是归属的测试用例名称。这两个名称都必须是有效的 C++ 标识符，并且它们不应包含任何下划线。测试的全名由其包含的测试用例集及其测试名称组成。来自不同测试用例集的测试可以具有相同的名称。\n这里以官方提供的 Sample1 中的阶乘函数为例：\nsample1.cc 中的阶乘函数定义如下：\n1 2 3 4 5 6 7 8  int Factorial(int n) { int result = 1; for (int i = 1; i \u0026lt;= n; i++) { result *= i; } return result; }   sample1_unittest.cc 中即为测试代码，这里为了能够更好的组织测试用例，将数据根据正负数划分为三个测试用例集，每一个用例集中都是相同类型的测试用例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // 测试负数 TEST(FactorialTest, Negative) { EXPECT_EQ(1, Factorial(-5)); EXPECT_EQ(1, Factorial(-1)); EXPECT_GT(Factorial(-10), 0); } // 测试0 TEST(FactorialTest, Zero) { EXPECT_EQ(1, Factorial(0)); } // 测试正数 TEST(FactorialTest, Positive) { EXPECT_EQ(1, Factorial(1)); EXPECT_EQ(2, Factorial(2)); EXPECT_EQ(6, Factorial(3)); EXPECT_EQ(40320, Factorial(8)); }   当我们执行测试时，输出如下：\n1 2 3 4 5 6 7 8 9 10  [==========] Running 6 tests from 2 test cases. [----------] Global test environment set-up. [----------] 3 tests from FactorialTest [ RUN ] FactorialTest.Negative [ OK ] FactorialTest.Negative (0 ms) [ RUN ] FactorialTest.Zero [ OK ] FactorialTest.Zero (0 ms) [ RUN ] FactorialTest.Positive [ OK ] FactorialTest.Positive (0 ms) [----------] 3 tests from FactorialTest (2 ms total)   这就表示我们的代码通过了所有的测试用例。\nTEST_F 如果发现自己编写了两个或多个对相似数据进行操作的测试，可以使用 test fixture 来为多个测试重用这些相同的配置。\n fixture，其语义是固定的设施，而 test fixture 在 GoogleTest 中的作用就是为每个 TEST 都执行一些同样的操作。\n 其对应的宏函数是 TEST_F，函数定义如下：\n1 2 3  TEST_F(TestFixtureName, TestName) { ... test body ... }   TestFixtureName 对应一个 test fixture 类的名称。因此我们需要自己去定义一个这样的类，并让它继承 testing::Test 类，然后根据我们的需要实现下面这两个虚函数：\n virtual void SetUp()：类似于构造函数，在 TEST_F 之前运行； virtual void TearDown()：类似于析构函数，在 TEST_F 之后运行。  此外 testing::Test 还提供了两个 static 函数：\n static void SetUpTestSuite()：在第一个 TEST 之前运行 static void TearDownTestSuite()：在最后一个 TEST 之后运行  除了这两种，还有一种全局事件，即继承testing::Environment，并实现下面两个虚函数：\n virtual void SetUp()：在所有用例之前运行； virtual void TearDown()：在所有用例之后运行。  这里以官方提供的 Sample3 中实现的 Queue 为例，其实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112  #ifndef GOOGLETEST_SAMPLES_SAMPLE3_INL_H_ #define GOOGLETEST_SAMPLES_SAMPLE3_INL_H_  #include \u0026lt;stddef.h\u0026gt; template \u0026lt;typename E\u0026gt; // E is the element type class Queue; template \u0026lt;typename E\u0026gt; // E is the element type class QueueNode { friend class Queue\u0026lt;E\u0026gt;; public: const E\u0026amp; element() const { return element_; } QueueNode* next() { return next_; } const QueueNode* next() const { return next_; } private: explicit QueueNode(const E\u0026amp; an_element) : element_(an_element), next_(nullptr) {} const QueueNode\u0026amp; operator=(const QueueNode\u0026amp;); QueueNode(const QueueNode\u0026amp;); E element_; QueueNode* next_; }; template \u0026lt;typename E\u0026gt; // E is the element type. class Queue { public: Queue() : head_(nullptr), last_(nullptr), size_(0) {} ~Queue() { Clear(); } void Clear() { if (size_ \u0026gt; 0) { // 1. Deletes every node.  QueueNode\u0026lt;E\u0026gt;* node = head_; QueueNode\u0026lt;E\u0026gt;* next = node-\u0026gt;next(); for (;;) { delete node; node = next; if (node == nullptr) break; next = node-\u0026gt;next(); } head_ = last_ = nullptr; size_ = 0; } } size_t Size() const { return size_; } QueueNode\u0026lt;E\u0026gt;* Head() { return head_; } const QueueNode\u0026lt;E\u0026gt;* Head() const { return head_; } QueueNode\u0026lt;E\u0026gt;* Last() { return last_; } const QueueNode\u0026lt;E\u0026gt;* Last() const { return last_; } void Enqueue(const E\u0026amp; element) { QueueNode\u0026lt;E\u0026gt;* new_node = new QueueNode\u0026lt;E\u0026gt;(element); if (size_ == 0) { head_ = last_ = new_node; size_ = 1; } else { last_-\u0026gt;next_ = new_node; last_ = new_node; size_++; } } E* Dequeue() { if (size_ == 0) { return nullptr; } const QueueNode\u0026lt;E\u0026gt;* const old_head = head_; head_ = head_-\u0026gt;next_; size_--; if (size_ == 0) { last_ = nullptr; } E* element = new E(old_head-\u0026gt;element()); delete old_head; return element; } template \u0026lt;typename F\u0026gt; Queue* Map(F function) const { Queue* new_queue = new Queue(); for (const QueueNode\u0026lt;E\u0026gt;* node = head_; node != nullptr; node = node-\u0026gt;next_) { new_queue-\u0026gt;Enqueue(function(node-\u0026gt;element())); } return new_queue; } private: QueueNode\u0026lt;E\u0026gt;* head_; // The first node of the queue.  QueueNode\u0026lt;E\u0026gt;* last_; // The last node of the queue.  size_t size_; // The number of elements in the queue.  Queue(const Queue\u0026amp;); const Queue\u0026amp; operator=(const Queue\u0026amp;); }; #endif // GOOGLETEST_SAMPLES_SAMPLE3_INL_H_   接着看看测试用例是如何编写的，首先声明了一个 test fixture 类，在这个类中实现了一些测试时用到的辅助函数，以及使用 SetUp 预置了一些测试数据。（除了有特殊需求，则不需要实现 TearDown，因为析构函数已经帮我们释放了资源）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  class QueueTestSmpl3 : public testing::Test { protected: void SetUp() override { q1_.Enqueue(1); q2_.Enqueue(2); q2_.Enqueue(3); } // 一个辅助函数  static int Double(int n) { return 2 * n; } // 测试 Queue::Map() 时用到的辅助函数  void MapTester(const Queue\u0026lt;int\u0026gt;* q) { const Queue\u0026lt;int\u0026gt;* const new_q = q-\u0026gt;Map(Double); ASSERT_EQ(q-\u0026gt;Size(), new_q-\u0026gt;Size()); for (const QueueNode\u0026lt;int\u0026gt;*n1 = q-\u0026gt;Head(), *n2 = new_q-\u0026gt;Head(); n1 != nullptr; n1 = n1-\u0026gt;next(), n2 = n2-\u0026gt;next()) { EXPECT_EQ(2 * n1-\u0026gt;element(), n2-\u0026gt;element()); } delete new_q; } Queue\u0026lt;int\u0026gt; q0_; Queue\u0026lt;int\u0026gt; q1_; Queue\u0026lt;int\u0026gt; q2_; };   接着看看它的 TEST_F ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // 测试默认构造函数 TEST_F(QueueTestSmpl3, DefaultConstructor) { EXPECT_EQ(0u, q0_.Size()); } // 测试出队 TEST_F(QueueTestSmpl3, Dequeue) { int* n = q0_.Dequeue(); EXPECT_TRUE(n == nullptr); n = q1_.Dequeue(); ASSERT_TRUE(n != nullptr); EXPECT_EQ(1, *n); EXPECT_EQ(0u, q1_.Size()); delete n; n = q2_.Dequeue(); ASSERT_TRUE(n != nullptr); EXPECT_EQ(2, *n); EXPECT_EQ(1u, q2_.Size()); delete n; } // 测试Map函数 TEST_F(QueueTestSmpl3, Map) { MapTester(\u0026amp;q0_); MapTester(\u0026amp;q1_); MapTester(\u0026amp;q2_); } } // namespace   这里以 DefaultConstructor 为例，来分析一下它的执行流程：\n QueueTestSmpl3 调用构造函数，构造对象。 QueueTestSmpl3 对象调用 SetUp 函数初始化测试配置。 DefaultConstructor 开始执行并结束测试。 QueueTestSmpl3 对象调用隐式生成的 TearDown 进行清理。 QueueTestSmpl3 调用析构函数，析构对象。  运行测试 调用测试 与其他 C++ 框架不同，TEST 和 TEST_F 会隐式向 GoogleTest 注册这些测试函数，因此我们不需要为了运行这些它们而进行枚举。\n定义完测试后，你可以用 RUN_ALL_TESTS 来运行它们，此时会运行所有的测试，如果全部成功则返回 0，反之则返回 1。其执行流程如下：\n 保存所有 GoogleTest 标志的状态。 为第一个测试创建一个 test fixture 对象。 通过 SetUp 初始化 test fixture 对象。 在 test fixture 对象上进行测试。 通过 TearDown 清理 test fixture。 删除 test fixture。 恢复所有 GoogleTest 标志的状态。 对下一个测试重复上述步骤，直到所有测试都运行完毕。  ==如果发生致命故障，则将跳过后续步骤。==\n编写 main 函数 用户不需要编写自己的 main 函数，编译器会自动将它链接到 gtest_main。如果用户有特殊需求，需要编写一个 main 函数，则需要在返回时调用 RUN_ALL_TESTS() 来运行所有测试，例如：\n1 2 3 4 5  int main(int argc, char **argv) { printf(\u0026#34;Running main() from %s\\n\u0026#34;, __FILE__); testing::InitGoogleTest(\u0026amp;argc, argv); return RUN_ALL_TESTS(); }    testing::InitGoogleTest 函数会解析 GoogleTest 标志的命令行，并删除所有识别的标志。这允许用户通过各种标志控制测试程序的行为。==您必须在调用 RUN_ALL_TESTS 之前调用此函数 ，否则标志将无法正确初始化==。\n ","date":"2022-07-03T18:55:13+08:00","permalink":"https://blog.orekilee.top/p/google-test/","title":"Google Test"},{"content":"CMake 简介 什么是 CMake？ CMake 是一个跨平台的安装（编译）工具，可以用简单的语句来描述所有平台的安装（编译过程）。他能够输出各种各样的makefile 或者 project 文件，CMake 的配置文件取名为 CMakeLists.txt。也就是在 CMakeLists.txt 这个文件中写 cmake 代码。 一句话：cmake 就是将多个 cpp、hpp 文件组合构建为一个大工程的语言。\n优缺点  优点：  开源，使用类BSD许可发布。 跨平台，并可以生成 native 编译配置文件，在 Linux/Unix 平台，生成 makefile；在苹果平台可以生成 Xcode；在Windows 平台，可以生成 MSVC 的工程文件。 能够管理大型项目。 简化编译构建过程和编译过程。cmake 的工具链非常简单：cmake + make。 高效率，因为 cmake 在工具链中没有 libtool。 可扩展，可以为 cmake 编写特定功能的模块，扩展 cmake 功能。   缺点：  cmake 只是看起来比较简单，而使用并不简单。 cmake 编写的过程实际上是编程的过程，每个项目使用一个 CMakeLists.txt（每个目录一个），使用的是 cmake 语法。 cmake 跟已有体系配合不是特别的理想，比如 pkgconfig。    编译流程 在 linux 下使用 CMake 生成 Makefile 并编译的流程如下：\n 编写 CMake 配置文件 CMakeLists.txt 。 在 CMakeLists.txt 文件所在目录创建一个 build 文件夹，然后进入目录。（这一步可以省略，但是生成的中间文件不易清理） 执行命令 cmake PATH 或者 ccmake PATH 生成 Makefile（ccmake 和 cmake 的区别在于前者提供了一个交互式的界面）。其中， PATH 是 CMakeLists.txt 所在的目录。 使用 make 命令进行编译，使用 make install 进行安装。  CMake 实战 环境搭建 安装 CMake 这里以 CentOS 8 为例：\n1 2  #低版本 yum -y install cmake   如需安装高版本，执行下面的命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  #安装gcc的软件依赖 yum install -y gcc gcc-c++ make automake #安装openssly依赖 yum install -y openssl openssl-devel #下载Cmake，可以在 https://cmake.org/files/ 找到需要的版本 wget https://cmake.org/files/v3.22/cmake-3.22.0-linux-x86_64.tar.gz #解压 tar -zxvf cmake-3.22.0-linux-x86_64.tar.gz #进入文件夹 cd cmake-3.22.0-linux-x86_64.tar.gz #打开环境变量文件 vim /etc/profile #添加环境变量 export PATH=$PATH:$CMAKE_HOME/bin\t#在PATH加上$CMAKE_HOME/bin\t export CMAKE_HOME=/home/lee/tool/cmake-3.22.0-linux-x86_64/\t#具体的路径 #使配置生效 source /etc/profile #查看是否安装成功 cmake -version   安装示例项目 本文用到的所有示例都来自于 GitHub 上的 cmake-examples 和 cmake-demo，下面将结合实例来进行讲解。\n1 2  git clone https://github.com/ttroy50/cmake-examples.git git clone https://github.com/wzpan/cmake-demo.git   编译单个源文件  本节对应的源代码路径如下：/cmake-examples/01-basic/A-hello-cmake/\n 首先查看一下本示例的目录结构：\n1 2 3 4  . ├── CMakeLists.txt ├── main.cpp └── README.adoc   在本目录下有三个文件，分别是源文件 main.cpp，cmake 构建规则 CMakeLists.txt 以及说明文件 README.adoc，下面来看看他们的具体内容。\n1 2 3 4 5 6 7  #include \u0026lt;iostream\u0026gt; int main(int argc, char *argv[]) { std::cout \u0026lt;\u0026lt; \u0026#34;Hello CMake!\u0026#34; \u0026lt;\u0026lt; std::endl; return 0; }   源文件是一个简单的 Hello World。\n1 2 3 4 5  cmake_minimum_required(VERSION 3.5)\t# 设CMake最小版本号 project(hello_cmake)\t# 设置工程名 add_executable(hello_cmake main.cpp)\t# 生成可执行文件   CMakeLists 中主要包含了三个命令：\n cmake_minimum_required(VERSION 3.5)：指定运行此配置文件所需的 CMake 的最低版本。 project (hello_cmake)：设置项目的名称，同时会自动生成 PROJECT_NAME 变量，使用 ${PROJECT_NAME} 即可访问到 hello_cmake。 add_executable(hello_cmake main.cpp)：第一个参数是可执行文件名，第二个参数是要编译的源文件列表。z这里将名为 main.cpp 的源文件编译成一个名称为 hello_cmake 的可执行文件。  接着我们可以开始构建项目，构建的方法有以下两种：\n 内部构建：直接在源文件目录构建项目，会导致临时文件和源代码放在一起，不好清理。 外部构建：创建一个可以位于文件系统上任何位置的构建文件夹。 所有临时构建和目标文件都位于此目录中，以保持源代码树的整洁。  这里以外部构建为例，此时我们需要新建一个构建文件夹 build，并在该目录下运行 cmake 命令进行构建：\n1 2 3 4 5 6 7 8  #创建并进入build目录 mkdir build \u0026amp;\u0026amp; cd build #构建当前目录 cmake .. #使用cmake生成的makefile编译得到可执行文件 make   此时在当前目录下，就会生成可执行文件 hello_cmake。将其运行查看是否成功编译：\n1 2  ./hello_cmake Hello CMake!   编译多个源文件 单个目录下的多个源文件  本节对应的源代码路径如下：/cmake-demo/Demo2\n 首先查看一下本示例的目录结构：\n1 2 3 4 5  . ├── CMakeLists.txt ├── main.cc ├── MathFunctions.cc └── MathFunctions.h   与上个示例不同，本示例在单个目录下有着多个源文件，此时 CMakeLists 如下：\n1 2 3 4 5 6 7 8 9 10 11 12  # CMake 最低版本号要求 cmake_minimum_required (VERSION 2.8)# 项目信息 project (Demo2)# 查找目录下的所有源文件 # 并将名称保存到 DIR_SRCS 变量 aux_source_directory(. DIR_SRCS)# 指定生成目标 add_executable(${PROJECT_NAME} ${DIR_SRCS})  在本示例中，为了避免一个个将所有源文件输入，使用了 aux_source_directory 命令。\n aux_source_directory ：第一个参数是目录的路径，第二个参数是变量名。当我们使用这个命令时，就会将指定目录下的所有源文件保存到指定的变量名中。  如果不想使用这种方法，而是向一条条枚举每个变量，可以使用 set 来手动将源文件保存到变量名中：\n1 2 3 4 5 6 7 8 9 10 11 12 13  # CMake 最低版本号要求 cmake_minimum_required (VERSION 2.8)# 项目信息 project (Demo2)set(DIR_SRCS MathFunctions.cc main.cc )# 指定生成目标 add_executable(${PROJECT_NAME} ${DIR_SRCS})  多个目录下的多个源文件  本节对应的源代码路径如下：/cmake-demo/Demo3\n 首先查看一下本示例的目录结构：\n1 2 3 4 5 6 7  . ├── CMakeLists.txt ├── main.cc └── math ├── CMakeLists.txt ├── MathFunctions.cc └── MathFunctions.h   与上个示例不同，本示例在多个目录下有着多个源文件。在这种情况下，我们需要在每个目录中都编写一个 CMakeLists.txt。这里为了方便，我们可以将 math 里的文件编译为一个静态库再有 main 函数调用。\n首先看看 math 目录下的 CMakeLists.txt，这里主要做的事是将当前目录下的文件编译为一个静态库：\n1 2 3 4 5 6  # 查找当前目录下的所有源文件 # 并将名称保存到 DIR_LIB_SRCS 变量 aux_source_directory(. DIR_LIB_SRCS)# 指定生成 MathFunctions 链接库 add_library (MathFunctions ${DIR_LIB_SRCS})   add_library：用于从某些源文件创建一个库，默认生成在构建文件夹。第一个参数为库名（不需要 lib 前缀，会自动添加），第二个参数用于指定 SHARED（动态库），STATIC（静态库）（如果不写，则通过全局的BUILD_SHARED_LIBS 的 FALSE 或 TRUE 来指定）。第三个参数即为源文件列表。  接着看看根目录的 CMakeLists.txt：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # CMake 最低版本号要求 cmake_minimum_required (VERSION 2.8)# 项目信息 project (Demo3)# 查找目录下的所有源文件 # 并将名称保存到 DIR_SRCS 变量 aux_source_directory(. DIR_SRCS)# 添加 math 子目录 add_subdirectory(math)# 指定生成目标 add_executable(Demo ${DIR_SRCS})# 添加链接库 target_link_libraries(Demo MathFunctions)   add_subdirectory：用于表示该项目包含一个子目录，此时会去处理子目录下的 CMakeLists.txt 与源文件。 target_link_libraries：该命令用于指明可执行文件 Demo 需要链接 MathFunctions 库。第一个参数为可执行文件名，第二个参数为访问权限（PUBLIC、PRIVATE、INTERFACE，默认为 PUBLIC），第三个参数为库名（这两个参数可以为多个）。  导入外部库 本地导入（find_package）  本节对应的源代码路径如下：/cmake-examples/01-basic/H-third-party-library\n 首先查看一下本示例的目录结构：\n1 2 3 4  . ├── CMakeLists.txt ├── main.cpp └── README.adoc   这里主要演示如何导入一个本地的第三方库（这里以 boost 为例），接着看看 MakeLists.txt：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  cmake_minimum_required(VERSION 3.5)# Set the project name project (third_party_include)# find a boost install with the libraries filesystem and system #使用库文件系统和系统查找boost install find_package(Boost 1.46.1 REQUIRED COMPONENTS filesystem system)#这是第三方库，而不是自己生成的静态动态库 # check if boost was found if(Boost_FOUND) message (\u0026#34;boost found\u0026#34;)else() message (FATAL_ERROR \u0026#34;Cannot find Boost\u0026#34;)endif()# Add an executable add_executable(third_party_include main.cpp)# link against the boost libraries target_link_libraries( third_party_include PRIVATE Boost::filesystem )  这里使用 find_package 命令来在本地搜索对应的第三方库，Boost 代表需要查询的库名称；1.46.1 代表需要库的最低版本；REQUIRED 表示该库是必须的，如果找不到会报错；COMPONENTS 用于检测该库的对应组件是否存在，如果不存在则认为找到的库不满足条件。\n外部导入（FetchContent） FetchContent 是 3.11.0 版本开始提供的功能，只需要一个 URL 或者 Git 仓库即可引入一个库，这里以 GoogleTest 库为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  cmake_minimum_required(VERSION 3.14) project(my_project) # GoogleTest requires at least C++11 set(CMAKE_CXX_STANDARD 11) include(FetchContent) FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/609281088cfefc76f9d0ce82e1ff6c30cc3591e5.zip ) # For Windows: Prevent overriding the parent project\u0026#39;s compiler/linker settings set(gtest_force_shared_crt ON CACHE BOOL \u0026#34;\u0026#34; FORCE) FetchContent_MakeAvailable(googletest)   使用方法：\n include(FetchContent) ：表示引入 FetchContent。 FetchContent_Declare(第三方库) ：获取第三方库，可以是一个 URL 或者一个 Git 仓库。 FetchContent_MakeAvailable(第三方库) ：将这个第三方库引入项目。 target_link_libraries(主项目 PRIVATE 子模块::子模块) ：链接这个第三方库。  测试与安装 CMake 也可以指定安装规则，以及添加测试。这两个功能分别可以通过在产生 Makefile 后使用 make install 和 make test 来执行。\n 本节对应的源代码路径如下：/cmake-demo/Demo8\n 首先查看一下本示例的目录结构：\n1 2 3 4 5 6 7 8 9  . ├── CMakeLists.txt ├── config.h.in ├── License.txt ├── main.cc └── math ├── CMakeLists.txt ├── MathFunctions.cc └── MathFunctions.h   自定义安装规则 首先查看 math 目录下的 CMakeLists.txt：\n1 2 3 4 5 6 7 8 9 10  # 查找当前目录下的所有源文件 # 并将名称保存到 DIR_LIB_SRCS 变量 aux_source_directory(. DIR_LIB_SRCS)# 指定生成 MathFunctions 链接库 add_library (MathFunctions ${DIR_LIB_SRCS})# 指定 MathFunctions 库的安装路径 install (TARGETS MathFunctions DESTINATION lib)install (FILES MathFunctions.h DESTINATION include)  这里使用 install 命令表明了将静态库 MathFunctions 安装到 /usr/local/lib 目录下，将头文件 MathFunctions.h 安装到 /usr/local/include 目录下。\n接着查看根目录的 install 内容：\n1 2 3 4  # 指定安装路径 install (TARGETS Demo DESTINATION bin)install (FILES \u0026#34;${PROJECT_BINARY_DIR}/config.h\u0026#34; DESTINATION include)  这里将可执行程序 Demo 安装到了 /usr/local/lib 目录下，再将 config.h 安装到 /usr/local/lib 目录下。\n /usr/local/ 是默认安装的根目录，可以通过修改 CMAKE_INSTALL_PREFIX 变量的值来指定这些文件应该拷贝到哪个根目录\n 测试 CMake 提供了一个称为 CTest 的测试工具。我们要做的只是在项目根目录的 CMakeLists 文件中调用一系列的 add_test 命令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  # 启用测试 enable_testing()# 测试程序是否成功运行 add_test (test_run Demo 5 2)# 测试帮助信息是否可以正常提示 add_test (test_usage Demo)set_tests_properties (test_usage PROPERTIES PASS_REGULAR_EXPRESSION \u0026#34;Usage: .* base exponent\u0026#34;)# 测试 5 的平方 # add_test (test_5_2 Demo 5 2) # set_tests_properties (test_5_2 # PROPERTIES PASS_REGULAR_EXPRESSION \u0026#34;is 25\u0026#34;) # 测试 10 的 5 次方 # add_test (test_10_5 Demo 10 5) # set_tests_properties (test_10_5 # PROPERTIES PASS_REGULAR_EXPRESSION \u0026#34;is 100000\u0026#34;) # 测试 2 的 10 次方 # add_test (test_2_10 Demo 2 10) # set_tests_properties (test_2_10 # PROPERTIES PASS_REGULAR_EXPRESSION \u0026#34;is 1024\u0026#34;) # 定义一个宏，用来简化测试工作 macro (do_test arg1 arg2 result) add_test (test_${arg1}_${arg2} Demo ${arg1} ${arg2}) set_tests_properties (test_${arg1}_${arg2} PROPERTIES PASS_REGULAR_EXPRESSION ${result})endmacro (do_test)# 利用 do_test 宏，测试一系列数据 do_test (5 2 \u0026#34;is 25\u0026#34;)do_test (10 5 \u0026#34;is 100000\u0026#34;)do_test (2 10 \u0026#34;is 1024\u0026#34;)   enable_testing：用于启动测试。 add_test：用于添加测试，第一个参数为测试名，第二个参数为可执行程序，剩下的为可执行程序的参数。 set_tests_properties：测试的提示信息。 macro：宏，用于编写一个重复性操作来简化测试用例的编写。 do_test：编写的测试宏。  生成安装包 如果想要生成安装包，则需要使用 CPack，它是由 CMake 提供的一个工具，专门用于打包。此时需要在 CMakeLists.txt 中添加以下内容：\n1 2 3 4 5 6 7  # 构建一个 CPack 安装包 include (InstallRequiredSystemLibraries)set (CPACK_RESOURCE_FILE_LICENSE \u0026#34;${CMAKE_CURRENT_SOURCE_DIR}/License.txt\u0026#34;)set (CPACK_PACKAGE_VERSION_MAJOR \u0026#34;${Demo_VERSION_MAJOR}\u0026#34;)set (CPACK_PACKAGE_VERSION_MINOR \u0026#34;${Demo_VERSION_MINOR}\u0026#34;)include (CPack)   include (InstallRequiredSystemLibraries)：导入 InstallRequiredSystemLibraries 模块。 设置一些 CPack 相关变量。 include (CPack)：导入 CPack 模块。  接着执行 cmake 和 make 构建工程，此时再执行 cpack 命令即可生成安装包：\n1 2 3 4 5  #生成二进制安装包 cpack -C CPackConfig.cmake #生成源码安装包 cpack -C CPackSourceConfig.cmake   当命令执行成功后，就会在当前目录下生成 *.sh、*.tar.gz、*.tar.Z 这三个格式的安装包。\n","date":"2022-07-02T18:53:13+08:00","permalink":"https://blog.orekilee.top/p/cmake/","title":"CMake"},{"content":"什么是 Bazel？ 简介 Bazel 是一个开源构建和测试工具，类似于 Make、Maven 和 Gradle。它使用人类可读的简要构建语言。Bazel 支持多种语言的项目，并针对多个平台构建输出。Bazel 支持跨多个代码库和大量用户的大型代码库。\n特点  高级构建语言：使用人类可读的抽象语言在高度语义上描述项目的构建属性。与其他工具不同，Bazel 会根据库、二进制文件、脚本和数据集的概念进行操作，避免了编写对编译器和连接器等工具进行单独调用的复杂性。 快速而又可靠：借助高级的本地和分布式缓存来记录所有已经完成的工作，并跟踪对文件内容和构建命令所做的更改。这样，Bazel 就能知道何时需要重新构建某些内容，并且仅重建必要的内容。如需进一步加快构建速度，您可以将项目设置为以高度并行且增量的方式进行构建。 跨平台：可以在 Linux、macOS 和 Windows 上运行。Bazel 可以在同一个项目中为多个平台（包括桌面端、服务器和移动端）构建二进制文件和可部署软件包。 可伸缩：在处理具有 10 万多个源文件的 build 时，Bazel 也能够保持敏捷性。Bazel 还可以帮助你扩展你的组织、代码库和持续集成系统，它可与数以万计的代码库和用户群进行协作。 可拓展：支持如 C++、Java、Android、iOS、Go和其他各种语言平台，并且可以使用 Bazel 熟悉的扩展语言轻松添加对新语言和平台的支持，分享和重用由不断增长的 Bazel 社区编写的语言规则。  使用流程 如果需要使用 Bazel 构建或测试项目，通常需要执行以下操作：\n 设置 Bazel。下载并安装 Bazel。 设置项目的 WORKSPACE。这是 Bazel 用于查找构建输入、BUILD 文件的目录，以及存储构建输出的目录。 编写 BUILD 文件。告诉 Bazel 应该构建什么以及如何去构建。 通过命令行运行 Bazel。运行后 Bazel 会将结果输出至 WORKSPACE 中。  构建流程 当运行构建或测试时，Bazel 会执行以下操作：\n 加载与目标相关的 BUILD 文件。 分析输入及其依赖项，应用指定的构建规则，并生成操作图。 对输入执行构建操作，直到生成最终构建输出。  基本概念 WORKSPACE（工作区） 工作区是一个目录，它包含了构建目标所需要的源码文件。每个工作区中都有一个名为 WORKSPACE 的文本文件，该文件可能为空，也可能包含构建输出所需的外部依赖的引用。包含名为 WORKSPACE 的文件的目录被视为工作区的根。\n代码以代码库形式组织。包含 WORKSPACE 文件的目录是主代码库的根目录，也称为 @。其他（外部）代码库是使用工作区规则在 WORKSPACE 文件中定义的。\nPACKAGE（包） 包是工作区中主要的代码组织单元，其中包含一系列相关的文以及描述这些文件之间关系的 BUILD 文件。\n包是工作区的子目录，它的根目录必须包含文件 BUILD（或 BUILD.bazel）。除了那些具有 BUILD 文件的子包以外，其它子目录属于包的一部分。\nTARGET（目标） 包是目标的容器，所以目标在包的 BUILD 文件中定义。 目标主要分为以下几种类型：\n  File（文件）：\n 源文件（Source File）：由相关人员编写的，并签入到代码库中。 派生文件（Derived files）：又叫输出文件，是构建工具依据规则自动生成的文件。    Rule（规则）：每个规则实例均指定一组输入文件和输出文件之间的关系。 规则的输入可以是源文件，但也可以是其他规则的输出。\n  在BUILD中声明规则的语法时：\n1 2 3 4  规则类型( name = \u0026#34;...\u0026#34;, #必须 其它属性 = ...\t#可选 )     规则的类型一般以编程语言为前缀，例如 cc，java，后缀通常有：\n  *_binary：用于构建目标语言的可执行文件\n  *_test：用于自动化测试，其目标是可执行文件，如果测试通过应该返回零。\n  *_library：用于构建目标语言的库\n      PACKAGE GROUP（包组）：即一组包，主要用于限制某些特定规则的可见性。由 package_group 函数定义。它们有三个属性：它们包含的包列表、名称以及它们包含的其他包组。引用它们的唯一方法来自规则的 visibility 属性或 package 函数的 default_visibility 属性，且它们不会生成或使用文件。\n  规则生成的文件与规则本身始终属于同一个包，即无法将文件生成到其他包中。但是规则的输入可以来自其他包。\nLABEL（标签） 目标的名称又被称为标签。每个标签都唯一标识一个目标。规范格式的典型标签如下所示：\n1  @myrepo//my/app/main:app_binary   以上面的标签为例，它由三部分组成\n  代码库名：标签的第一部分是代码库名称，例如上面的 @myrepo//。如果标签引用了其使用的同一个代码库，其代码库标识符可以缩写为 //，例如：\n1  //my/app/main:app_binary     包名：标签的第二部分是包名称 my/app/main，这是相对于代码库根目录的包路径。如果标签引用了其使用的同一个包，则可以省略包名称（还可以选择是否使用英文冒号进行分隔），例如：\n1 2 3  #两者等价 app_binary :app_binary     目标名：标签后面的冒号部分 app_binary 是目标名称。当与软件包路径的最后一个组成部分匹配时，可以省略冒号和冒号，例如：\n1 2 3  #两者等价 //my/app/lib //my/app/lib:lib   在 BUILD 文件中，引用当前包中定义的规则时，冒号不能省略。而引用当前包中文件时，冒号可以省略。 但是，从其它包引用时、从命令行引用时，都必须使用完整的标签。\n  BUILD（构建文件） 每个包中都包含一个 BUILD 文件，而 BUILD 文件中的构建规则的每个实例又称为目标，并指向一组特定的源文件和依赖项。 目标还可以指向其他目标。\nBUILD 文件包含几种不同类型的 Bazel 指令。最重要的类型是构建规则，告知 Bazel 如何构建所需的输出，例如可执行二进制文件或库。BUILD 文件中的语句会被从上而下的逐条解释，某些语句的顺序很重要， 例如变量必须先定义后使用，但是规则声明的顺序无所谓。\n为了促进代码和数据之间的完全分离，BUILD 文件不能包含函数定义、for 语句或 if 语句，但可以在 .bzl 文件中声明函数、控制结构，并在BUILD文件中用 load 语句加载。例如：\n1  load(\u0026#34;//foo/bar:file.bzl\u0026#34;, \u0026#34;some_library\u0026#34;)   上面的语句加载 foo/bar/file.bzl 并添加其中定义的符号 some_libraray 到当前环境中，load 语句可以用来加载规则、函数、常量（字符串、列表等）。（==参数必须是字符串字面量（无变量），并且 load 语句必须出现在顶层，它们不能位于函数正文中。==）\nDEPENDENCY（依赖项） 如果在构建或执行时 A 需要目标 B，目标 A 会依赖于目标 B。依存关系会使目标产生有向无环图（DAG），这就是所谓的依赖图。\n直接依赖项是指依赖项图表中长度为 1 的路径可以访问的其他目标。 传递依赖项是指它通过图中任意长度的路径来依赖的那些目标。\n大多数构建规则都具有三个用于指定不同类型通用依赖项的属性：srcs、deps 和 data。\n srcs：直接输出源文件的规则所使用的文件。 deps：指向独立编译的模块的规则，这些模块提供头文件、符号、库、数据等。 data：构建目标可能需要一些数据文件才能正常运行。这些数据文件不是源代码：它们不会影响目标的构建方式。  实战 环境搭建 安装 Bazel 这里以 CentOS 8 为例：\n  首先从 Fedora COPR 下载相应的 .repo 文件并将其复制到 /etc/yum.repos.d/ 中：\n1  wget --no-check-certificate -P /etc/yum.repos.d/ https://copr.fedorainfracloud.org/coprs/vbatts/bazel/repo/epel-7/vbatts-bazel-epel-7.repo     使用 yum 安装 Bazel：\n1  yum -y install bazel4     查看 Bazel 版本号，看看是否安装成功：\n1  bazel --version     安装示例项目 从 Bazel 的 GitHub 中下载官方提供的示例项目：\n1  git clone https://github.com/bazelbuild/examples   示例项目位于 examples/cpp-tutorial 目录，结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  examples └── cpp-tutorial ├──stage1 │ ├── main │ │ ├── BUILD │ │ └── hello-world.cc │ └── WORKSPACE ├──stage2 │ ├── main │ │ ├── BUILD │ │ ├── hello-world.cc │ │ ├── hello-greet.cc │ │ └── hello-greet.h │ └── WORKSPACE └──stage3 ├── main │ ├── BUILD │ ├── hello-world.cc │ ├── hello-greet.cc │ └── hello-greet.h ├── lib │ ├── BUILD │ ├── hello-time.cc │ └── hello-time.h └── WORKSPACE   示例主要分为三个阶段：\n stage1：构建位于单个软件包中的单个目标。 stage2：将项目拆分为多个目标，但将其保留在单个软件包中。 stage3：将项目拆分为多个软件包，并使用多个目标进行构建。  C++ 项目构建示例 基础知识 构建项目 想要构建项目，只需要在命令行输入 bazel build 命令，并在其后面指定标签名，例如：\n1  bazel build 标签名   当构建执行成功时，就会在终端输出提示信息，其中包括每个目标构建输出的位置、构建时间、关键路径等，具体的输出信息可以参考下面的几个示例。\n当构建完成后，可以使用 bazel clean 命令清除构建结果。\n查看依赖项图表 成功的构建会将其所有依赖项明确列在 BUILD 文件中。Bazel 会使用这些语句创建项目的依赖项图，以实现准确的增量构建。\n如需直观呈现示例项目的依赖项，您可以通过在工作区根目录下运行以下命令，生成依赖项图的文本表示形式：\n1 2  bazel query --notool_deps --noimplicit_deps \u0026#34;依赖项属性(目标)\u0026#34; \\  --output graph   上述命令指示 Bazel 查找目标的所有依赖项（不包括主机和隐式依赖项），并将输出格式化为图表。接着将文本粘贴到 WebGraphviz 中即可查看。\n示例一：编译单个目标 首先我们需要查看示例一的 BUILD 文件，规则可参考该链接 cc_library 规则。文件内容如下：\n1 2 3 4 5 6  load(\u0026#34;@rules_cc//cc:defs.bzl\u0026#34;, \u0026#34;cc_binary\u0026#34;) cc_binary( name = \u0026#34;hello-world\u0026#34;, srcs = [\u0026#34;hello-world.cc\u0026#34;], )   可以看到，依赖关系非常简单，只需要编译一个 TARGET，并且这个 TARGET 也只依赖于 hello-world.cc。直接运行构建命令：\n1  bazel build //main:hello-world   此时 Bazel 生成以下输出信息：\n1 2 3 4 5 6 7 8  Starting local Bazel server and connecting to it... INFO: Analyzed target //main:hello-world (15 packages loaded, 57 targets configured). INFO: Found 1 target... Target //main:hello-world up-to-date: bazel-bin/main/hello-world INFO: Elapsed time: 14.070s, Critical Path: 1.19s INFO: 6 processes: 4 internal, 2 processwrapper-sandbox. INFO: Build completed successfully, 6 total actions   执行生成的二进制文件，看看是否能够正常运行：\n1 2 3 4 5  bazel-bin/main/hello-world #输出 Hello world Sun Jun 5 18:14:07 2022   接着生成示例项目的依赖项图表：\n1 2  bazel query --notool_deps --noimplicit_deps \u0026#34;deps(//main:hello-world)\u0026#34; \\  --output graph   此时输出依赖项文本如下：\n1 2 3 4 5 6  digraph mygraph { node [shape=box]; \u0026#34;//main:hello-world\u0026#34; \u0026#34;//main:hello-world\u0026#34; -\u0026gt; \u0026#34;//main:hello-world.cc\u0026#34; \u0026#34;//main:hello-world.cc\u0026#34; }   将其复制到 WebGraphviz 中，得到的依赖项图表如下：\n如上图，示例一仅具有一个目标，该目标会构建没有其他依赖项的单个源文件。\n示例二：编译多个目标 接着我们查看示例二的 BUILD 文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  load(\u0026#34;@rules_cc//cc:defs.bzl\u0026#34;, \u0026#34;cc_binary\u0026#34;, \u0026#34;cc_library\u0026#34;) cc_library( name = \u0026#34;hello-greet\u0026#34;, srcs = [\u0026#34;hello-greet.cc\u0026#34;], hdrs = [\u0026#34;hello-greet.h\u0026#34;], ) cc_binary( name = \u0026#34;hello-world\u0026#34;, srcs = [\u0026#34;hello-world.cc\u0026#34;], deps = [ \u0026#34;:hello-greet\u0026#34;, ], )   在这个 BUILD 文件中存在两个 TARGET，分别是库文件 hello-greet 与 可执行文件 hello-world。由于 hello-world 目标中的 deps 属性说明该目标的构建依赖于 hello-greet，因此在构建时会先构建 hello-greet 库，再根据这个库构建 hello-world。\n执行下面的命令开始构建：\n1  bazel build //main:hello-world   输出结果如下：\n1 2 3 4 5 6 7 8  Starting local Bazel server and connecting to it... INFO: Analyzed target //main:hello-world (15 packages loaded, 60 targets configured). INFO: Found 1 target... Target //main:hello-world up-to-date: bazel-bin/main/hello-world INFO: Elapsed time: 11.865s, Critical Path: 0.25s INFO: 7 processes: 4 internal, 3 processwrapper-sandbox. INFO: Build completed successfully, 7 total actions   执行生成的二进制文件，看看是否能够正常运行：\n1 2 3 4 5  bazel-bin/main/hello-world #输出 Hello world Sun Jun 5 20:01:37 2022   接着生成示例项目的依赖项图表：\n如上图，示例二构建包含两个目标的项目。hello-world 目标会构建一个源文件，并依赖于另一个目标 hello-greet，后者会构建两个额外的源文件。\n示例三：编译多个包 由于示例三中需要对多个包进行编译，所以我们首先使用 tree 命令来看看它们的目录结构：\n1 2 3 4 5 6 7 8 9 10 11 12  . ├── lib │ ├── BUILD │ ├── hello-time.cc │ └── hello-time.h ├── main │ ├── BUILD │ ├── hello-greet.cc │ ├── hello-greet.h │ └── hello-world.cc ├── README.md └── WORKSPACE   可以看到，在工作区中包含了两个子目录 lib 和 main，并且这两个子目录中都有 BUILD 文件，因此它们都是包。\n接着查看这两个包中的 BUILD 文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  #lib/BUILD load(\u0026#34;@rules_cc//cc:defs.bzl\u0026#34;, \u0026#34;cc_library\u0026#34;) cc_library( name = \u0026#34;hello-time\u0026#34;, srcs = [\u0026#34;hello-time.cc\u0026#34;], hdrs = [\u0026#34;hello-time.h\u0026#34;], visibility = [\u0026#34;//main:__pkg__\u0026#34;], ) #main/BUILD load(\u0026#34;@rules_cc//cc:defs.bzl\u0026#34;, \u0026#34;cc_binary\u0026#34;, \u0026#34;cc_library\u0026#34;) cc_library( name = \u0026#34;hello-greet\u0026#34;, srcs = [\u0026#34;hello-greet.cc\u0026#34;], hdrs = [\u0026#34;hello-greet.h\u0026#34;], ) cc_binary( name = \u0026#34;hello-world\u0026#34;, srcs = [\u0026#34;hello-world.cc\u0026#34;], deps = [ \u0026#34;:hello-greet\u0026#34;, \u0026#34;//lib:hello-time\u0026#34;, ], )   我们发现，这两个包中存在跨包依赖。main 包中的 hello-world 不仅依赖于本包的 hello-greet，还依赖于 lib 包中的 hello-time。因此我们需要确保 lib 包中的 hello-time 对 main 包中的 hello-world 可见（默认情况下仅对同一 BUILD 文件中的其他目标可见，即可见性为 //visibility:private），这时就需要在 hello-time 目标中加上 visibility 属性，并将可见性设置为 //main:__pkg__，即对 main 包可见。\n执行下面的命令开始构建：\n1  bazel build //main:hello-world   输出结果如下：\n1 2 3 4 5 6 7 8  Starting local Bazel server and connecting to it... INFO: Analyzed target //main:hello-world (16 packages loaded, 63 targets configured). INFO: Found 1 target... Target //main:hello-world up-to-date: bazel-bin/main/hello-world INFO: Elapsed time: 8.273s, Critical Path: 0.26s INFO: 8 processes: 4 internal, 4 processwrapper-sandbox. INFO: Build completed successfully, 8 total actions   执行生成的二进制文件，看看是否能够正常运行：\n1 2 3 4 5  bazel-bin/main/hello-world #输出 Hello world Sun Jun 5 20:26:44 2022   接着生成示例项目的依赖项图表：\nC++ 常见构建方法 在目标中包含多个文件  Glob 是一个辅助函数，可用于查找符合特定路径模式的所有文件，并返回一个包含可变排序的新路径列表。Glob 仅在自己的包中搜索文件，并且仅查找源文件（而不是生成的文件或其他目标）。\n 如果想在单个目标中包含多个文件，可以使用 glob 函数来进行文件匹配，例如：\n1 2 3 4 5  cc_library( name = \u0026#34;build-all-the-files\u0026#34;, srcs = glob([\u0026#34;*.cc\u0026#34;]), hdrs = glob([\u0026#34;*.h\u0026#34;]), )   使用此目标时，Bazel 将在其找到的包含此目标的 BUILD 文件所在的目录（不包括子目录）中构建找到的所有 .cc 和 .h 文件。\n使用传递性包含 如果文件包含一个头文件，那么任何以该文件为源的规则（即在 srcs、hdrs 或 textual_hdrs 属性中有该文件）都应该依赖于所包含的头文件的库规则。反过来说，只有直接依赖项才需要指定为依赖项。\n假设 sandwich.h 包含 bread.h，bread.h 包含 flour.h。sandwich.h 不包括 flour.h，因此 BUILD 文件将如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  cc_library( name = \u0026#34;sandwich\u0026#34;, srcs = [\u0026#34;sandwich.cc\u0026#34;], hdrs = [\u0026#34;sandwich.h\u0026#34;], deps = [\u0026#34;:bread\u0026#34;], ) cc_library( name = \u0026#34;bread\u0026#34;, srcs = [\u0026#34;bread.cc\u0026#34;], hdrs = [\u0026#34;bread.h\u0026#34;], deps = [\u0026#34;:flour\u0026#34;], ) cc_library( name = \u0026#34;flour\u0026#34;, srcs = [\u0026#34;flour.cc\u0026#34;], hdrs = [\u0026#34;flour.h\u0026#34;], )   此时 sandwich 库依赖于 bread 库，又 bread 依赖于 flour 库。\n添加包含路径 如果不想在工作区的根目录中设置包含（include）路径。而现有的库可能已经有了一个 include 目录，但是它却与工作区中的路径不一致，例如下面目录结构：\n1 2 3 4 5 6 7 8  └── my-project ├── legacy │ └── some_lib │ ├── BUILD │ ├── include │ │ └── some_lib.h │ └── some_lib.cc └── WORKSPACE   Bazel 应该包含 some_lib.h 作为 legacy/some_lib/include/some_lib.h，但假设 some_lib.cc 包含 some_lib.h。要使该包含路径有效，legacy/some_lib/BUILD 需要在 copts 属性中指定 some_lib/include 目录是包含目录：\n1 2 3 4 5 6  cc_library( name = \u0026#34;some_lib\u0026#34;, srcs = [\u0026#34;some_lib.cc\u0026#34;], hdrs = [\u0026#34;include/some_lib.h\u0026#34;], copts = [\u0026#34;-Ilegacy/some_lib/include\u0026#34;], )   这对于外部依赖项尤其有用，因为其头文件必须包含 / 前缀。\n包括外部库  gtest（Google Test）是一个跨平台的（Liunx、Mac OS X、Windows等）C++单元测试框架，由 Google发布。gtest 是为在不同平台上为编写 C++ 测试而生成的。它提供了丰富的断言、致命和非致命判断、参数化、死亡测试等等。\n 当我们需要引入一个外部库时，可以使用 WORKSPACE 文件中的某个代码库函数下载，并代码库中提供它，这里以 gtest 为例：\n1 2 3 4 5 6 7 8  load(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) http_archive( name = \u0026#34;gtest\u0026#34;, url = \u0026#34;https://github.com/google/googletest/archive/release-1.10.0.zip\u0026#34;, sha256 = \u0026#34;94c634d499558a76fa649edb13721dce6e98fb1e7018dfaeba3cd7a083945e91\u0026#34;, build_file = \u0026#34;@//:gtest.BUILD\u0026#34;, )   接着创建 gtest 的 BUILD 文件，由于 gtest 具有几项特殊要求，使得其 cc_library 规则更加复杂：\n googletest-release-1.10.0/src/gtest-all.cc #include 对 googletest-release-1.10.0/src/ 中的所有其他文件执行排除操作：将其从编译中排除，以防止出现重复符号的链接错误。 它使用相对于 googletest-release-1.10.0/include/ 目录 (gtest/gtest.h) 的头文件，因此必须将该目录添加到包含路径。 它需要关联 pthread，因此需要添加为 linkopt。  规则将如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  cc_library( name = \u0026#34;main\u0026#34;, srcs = glob( [\u0026#34;googletest-release-1.10.0/src/*.cc\u0026#34;], exclude = [\u0026#34;googletest-release-1.10.0/src/gtest-all.cc\u0026#34;] ), hdrs = glob([ \u0026#34;googletest-release-1.10.0/include/**/*.h\u0026#34;, \u0026#34;googletest-release-1.10.0/src/*.h\u0026#34; ]), copts = [ \u0026#34;-Iexternal/gtest/googletest-release-1.10.0/include\u0026#34;, \u0026#34;-Iexternal/gtest/googletest-release-1.10.0\u0026#34; ], linkopts = [\u0026#34;-pthread\u0026#34;], visibility = [\u0026#34;//visibility:public\u0026#34;], )   由于所有内容都以 googletest-release-1.10.0 为前缀，是归档结构的副产物。可以通过添加 strip_prefix 属性让 http_archive 去除此前缀：\n1 2 3 4 5 6 7 8 9  load(\u0026#34;@bazel_tools//tools/build_defs/repo:http.bzl\u0026#34;, \u0026#34;http_archive\u0026#34;) http_archive( name = \u0026#34;gtest\u0026#34;, url = \u0026#34;https://github.com/google/googletest/archive/release-1.10.0.zip\u0026#34;, sha256 = \u0026#34;94c634d499558a76fa649edb13721dce6e98fb1e7018dfaeba3cd7a083945e91\u0026#34;, build_file = \u0026#34;@//:gtest.BUILD\u0026#34;, strip_prefix = \u0026#34;googletest-release-1.10.0\u0026#34;, )   此时 gtest.BUILD 可以简化为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  cc_library( name = \u0026#34;main\u0026#34;, srcs = glob( [\u0026#34;src/*.cc\u0026#34;], exclude = [\u0026#34;src/gtest-all.cc\u0026#34;] ), hdrs = glob([ \u0026#34;include/**/*.h\u0026#34;, \u0026#34;src/*.h\u0026#34; ]), copts = [\u0026#34;-Iexternal/gtest/include\u0026#34;], linkopts = [\u0026#34;-pthread\u0026#34;], visibility = [\u0026#34;//visibility:public\u0026#34;], )   编写一个使用 gtest 的测试代码，看看是否能够成功构建：\n1 2 3 4 5 6  #include \u0026#34;gtest/gtest.h\u0026#34;#include \u0026#34;main/hello-greet.h\u0026#34; TEST(HelloTest, GetGreet) { EXPECT_EQ(get_greet(\u0026#34;Bazel\u0026#34;), \u0026#34;Hello Bazel\u0026#34;); }   创建测试文件的 BUILD：\n1 2 3 4 5 6 7 8 9  cc_test( name = \u0026#34;hello-test\u0026#34;, srcs = [\u0026#34;hello-test.cc\u0026#34;], copts = [\u0026#34;-Iexternal/gtest/include\u0026#34;], deps = [ \u0026#34;@gtest//:main\u0026#34;, \u0026#34;//main:hello-greet\u0026#34;, ], )   使用 bazel test 进行测试：\n1 2 3 4 5 6 7 8 9 10  bazel test test:hello-test #输出 INFO: Found 1 test target... Target //test:hello-test up-to-date: bazel-bin/test/hello-test INFO: Elapsed time: 4.497s, Critical Path: 2.53s //test:hello-test PASSED in 0.3s Executed 1 out of 1 tests: 1 test passes.   添加对预编译库的依赖关系 如果想使用只包含已编译版本的库（例如头文件和 .so 文件），就需要将其封装在 cc_library 规则中，例如：\n1 2 3 4 5  cc_library( name = \u0026#34;mylib\u0026#34;, srcs = [\u0026#34;mylib.so\u0026#34;], hdrs = [\u0026#34;mylib.h\u0026#34;], )   ","date":"2022-07-01T18:54:13+08:00","permalink":"https://blog.orekilee.top/p/bazel/","title":"Bazel"},{"content":"Redis 多机服务 主从同步(复制） 主从同步是Redis高可用服务的基石，其将主要存储数据的服务器成为主服务器(master)，把对主服务器进行复制的服务器成为从服务器(slave)。 并且从节点还可以是其他服务器的主节点，并且拥有属于自己的从节点 通过主从模式来进行读写的分离，主服务器进行写操作，然后将数据同步给从服务器，让从服务器来进行读操作，通过这种模式来分摊主服务器的压力。\nRedis的复制功能主要分为同步(sync)与命令传播(command propagate) 两个操作\n同步 同步操作用于将从服务器的数据库状态更新至主服务器当前的数据库状态。\n在从服务器对主服务器进行复制之前，需要先将从服务器的数据库状态更新至主服务器的服务器状态\n命令传播 命令传播操作用于在主服务器的数据库状态发生变化(执行写命令)，导致主从服务器的数据库状态不一致时，让主从服务器的数据库重新回到一致状态。 当客户端对主服务器进行写操作后，此时主从服务器的数据库状态就会不一致。\n为了能够再次让主从服务器的数据库状态恢复一致，此时主服务器会将同一命令发送给从服务器，当从服务器执行完改命令时，数据库状态再次恢复一致。 优缺点 优点\n 性能方面：可以实现读写的分离，由主服务器来进行写操作，并将写的结果同步至从服务器，由从服务器来进行读操作，这样就能将压力分摊到各个服务器上 高可用：当主服务器宕机之后，可以通过故障转移机制将从节点提升为主节点，快速的进行服务器的宕机恢复。 防止数据丢失：当主服务器的磁盘损坏或者数据丢失后，因为从服务器还保留相关的数据，不至于导致数据全部丢失  缺点\n 由于主从同步需要人工管理，主节点崩溃后需要人工进行从节点的提升才能恢复Redis的正常使用  从上面可以看到，主从同步并没有一个自动的管理机制，当出现主服务器宕机的情况，需要人工干预来进行恢复，但是如果主从服务器数量庞大，又或是因为高并发导致的大量崩溃，这时需要的时间和难度都是非常大的，于是Redis中又引入了哨兵模式(Sentinel) 来作为解决方案，将管理由人工转向哨兵，使得Redis具有自动容灾恢复的能力\n哨兵 哨兵是Redis高可用性的解决方案，通过一个或者多个哨兵组成的哨兵系统，可以监控任意多个主服务器以及它们的从服务器。当某个被监视的主服务器进入下线状态时，哨兵就会自动将下线主服务器的某个从服务器升级为新的主服务器，然后由新的主服务器继续处理命令请求\n总结下来就是哨兵模式可以用来监控主从同步服务器节点，并在主从服务器出现问题的时候实现自动容灾恢复 下线判断与选举 由于一个主服务器可能会同时被多个哨兵进行同时进行监控，所以当一个哨兵主观的将其判定为下线时，为了确保这个主服务器真的下线了，它会向同样监视这一主服务器的其他哨兵进行询问，看看它们是否也认为该服务器下线了，当积累到一定数量的下线判断时，此时就会客观认为主服务器下线，开始进行故障转移。\n但是故障转移只能由一个哨兵来进行，所以此时所有监控该服务器的哨兵会进行协商，选举出一个领头哨兵来进行故障转移。\n每一个哨兵都会向其他哨兵发送一个带有自己运行ID的命令，如果接收到该命令的哨兵还没有进行投票，就会将该ID设置为它的头领ID，并返回一个确认恢复。通过这种方法每一个哨兵都可以直到有多少个人为其投票，并选出一个票数最高的作为头领哨兵\n故障转移 故障转移分为以下三个步骤\n 在已下线主服务器的从服务器中挑选一个出来作为新的主服务器 在已下线主服务器的从服务器改为复制新的主服务器 将已下线主服务器设置为新的主服务器的从服务器，当这个旧的主服务器重新上线时，他就会成为新的主服务器的从服务器   集群 集群(Cluster)是Redis多机运行中最完美的方案 ，它的出现甚至可以让我们抛弃掉主从同步和哨兵来实现Redis多机的运行。\n集群是无代理模式去中心化的运行模式，客户端发送的绝大多数命令会直接交给相关节点执行，大部分情况下请求命令不需要转发，或者仅仅只需要转发一次就能完成请求和响应。所以集群中的单个节点的性能与单机Redis服务器的性能非常接近，并且通过水平拓展能够使得性能进行翻倍，所以集群的性能非常的高 由于主从同步只能有一个主节点，而集群可以拥有无数个主从节点，有着更强大的平行拓展能力。 所以在理论情况下，如果水平拓展一倍的主节点，相当于请求处理的性能也提高了一倍，也就是说通过平行拓展N倍的主从节点，就会比单机服务来说性能提升了N倍。\n握手 每个节点其实就是运行在集群模式下的Redis服务器，而这些节点在一开始时都是互相独立的，它们都处于一个只包含自己的集群中，要组建一个真正可以工作的集群，我们就必须要将各个独立的节点通过握手的方式连接起来。 分片 集群通过分片的方式来保存数据库中的键值对。 集群的整个数据库被分为个16384个槽，并且将一个或者多个槽指派给某个节点，让这个节点来负责管理这个槽中的数据以及相关命令，通过这种方法就能很好的进行压力的分摊。\n节点之间会互相转递指派槽的信息 对于发送来的命令，会通过其所在的槽来分配至对应的节点，如果分配错误，也会通过转向操作来转交给至正确的节点 ","date":"2022-05-26T15:07:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E5%A4%9A%E6%9C%BA%E6%9C%8D%E5%8A%A1/","title":"Redis 多机服务"},{"content":"Redis 事务 MySQL 事务 ：ACID、并发带来的问题、事务的隔离级别、事务的实现 在之前的MySQL系列博客中我已经讲过了一些事务的内容，但是Redis与传统的关系型数据库不同，因此下面我会在讲解Redis事务的同时与SQL数据库的事务进行比较。\n为了能帮助大家更好的理解，首先给出Redis事务的所有接口，并结合案例来讲解其具体使用方法\n   命令 作用     MUTLI 标记一个事务块的开始。   EXEC 执行所有事务块内的命令   DISCARD 取消事务，放弃执行事务块内的所有命令   WATCH 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断   UNWATCH 取消 WATCH 命令对所有 key 的监视    事务的实现 Redis的事务与传统的SQL事务不同，它的本质是一组命令的集合， 一个事务中的所有命令都会被序列化，在事务执行过程的中，会按照顺序执行它的事务主要存在以下三个阶段\n 事务开始(multi) 命令入队 事务执行(exec)  下面就结合一个具体案例，来讲解一下它的实现原理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  127.0.0.1:6379\u0026gt; MULTI\t# 开始事务 OK 127.0.0.1:6379\u0026gt; SET CITY1 \u0026#34;beijing\u0026#34;\t# 插入三个城市 QUEUED 127.0.0.1:6379\u0026gt; SET CITY2 \u0026#34;shanghai\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; SET CITY3 \u0026#34;shenzhen\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; GET CITY2\t# 获取城市的名字 QUEUED 127.0.0.1:6379\u0026gt; GET CITY3 QUEUED 127.0.0.1:6379\u0026gt; EXEC\t# 执行事务 1) OK 2) OK 3) OK 4) \u0026#34;shanghai\u0026#34; 5) \u0026#34;shenzhen\u0026#34;    事务开始\n 当我们执行MULTI命令时即代表着事务的开启，此时会将客户端从非事务状态切换到事务状态，通过为客户端状态中的flags加上REDIS_MULTI标识实现\n1 2  # 打开事务标识 client.flags |= REDIS_MULTI    命令入队\n 当我们切换至事务状态后，Redis服务器会根据我们命令来决定执行命令还是将命令放入队列中\n 如果客户端发送的命令是事务相关即EXEC、DISCARD、WATCH、UNWATCH、MULTI等，服务器会立刻执行命令 如果客户端发送的是上面以外的命令，这时候服务器就会将命令放入事务队列中，并向客户端返回QUEUED，告知客户端命令入队  如下图  事务队列\n 在客户端的事务状态中维护者一个事务队列，以及队列长度的计数器\n1 2 3 4 5 6 7  typedef struct multiState { multiCmd* commands;\t//事务队列，FIFO \tint count;\t//命令计数器 \t} multiState   事务队列其实就是multiCmd类型的数组，其中每一个multiCmd节点都包含着每条命令的具体信息，如指向具体实现的命令指针、命令的参数、命令的数量\n1 2 3 4 5 6 7 8 9  typedef struct multiCmd { robj** argv;\t//参数 \tint argc;\t//参数的数量 \tstruct redisCommand* cmd;\t//指向具体实现的命令指针 \t} multiCmd   假设此时客户端执行以下命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  redis\u0026gt; MULTI OK redis\u0026gt; SET \u0026#34;name\u0026#34; \u0026#34;Practical Common Lisp\u0026#34; QUEUED redis\u0026gt; GET \u0026#34;name\u0026#34; QUEUED redis\u0026gt; SET \u0026#34;author\u0026#34; \u0026#34;Peter Seibel\u0026#34; QUEUED redis\u0026gt; GET \u0026#34;author\u0026#34; QUEUED   此时底层的事务队列如下  执行事务\n 当客户端向服务器发送EXEC命令时，服务器会立即遍历客户端的事务队列，按照FIFO(先进先出)的顺序执行队列中的所有命令，执行完毕后将命令所得的结果全部返回给客户端 讲完了原理，下面就来讲讲Redis的ACID与传统SQL的有什么不同\nACID  原子性：原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。\n 在Redis中，单条命令能够保证原子性，但是事务并不能保证原子性。下面我分别以编译、运行两个阶段的异常举例，来验证这个结论\n编译时异常\n首先我们来验证编译时异常是否能够保证原子性，我们故意产生语法错误，来验证编译异常时事务是否能够执行\n1 2 3 4 5 6 7 8 9 10 11 12  127.0.0.1:6379\u0026gt; MULTI\t#开启事务 OK\t127.0.0.1:6379\u0026gt; SET CITY1 \u0026#34;beijing\u0026#34;\tQUEUED 127.0.0.1:6379\u0026gt; SET CITY2 \u0026#34;shanghai\u0026#34; QUEUED 127.0.0.1:6379\u0026gt; SET CITY3\t# 故意不给value,使其语法错误 (error) ERR wrong number of arguments for \u0026#39;set\u0026#39; command\t# 编译报错 127.0.0.1:6379\u0026gt; EXEC\t# 执行事务 (error) EXECABORT Transaction discarded because of previous errors.\t# 事务中存在错误命令，执行失败 127.0.0.1:6379\u0026gt; KEYS *\t# 所有命令都没有执行 (empty array)   从上面可以看到，在编译时异常时Redis是能够保证原子性的。\n运行时异常\n接着来看看运行时异常，我们故意对一个字符串使用计数操作，看看报错后事务是否能够执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  127.0.0.1:6379\u0026gt; MULTI\t# 开启事务 OK 127.0.0.1:6379\u0026gt; SET key1 \u0026#34;HELLO\u0026#34; # 插入一个字符串 QUEUED 127.0.0.1:6379\u0026gt; INCRBY key1 10 # 对这个字符串+10,必定执行失败 QUEUED 127.0.0.1:6379\u0026gt; SET key2 \u0026#34;WORLD\u0026#34; # 其他命令 QUEUED 127.0.0.1:6379\u0026gt; GET key2 QUEUED 127.0.0.1:6379\u0026gt; EXEC\t#执行事务 1) OK 2) (error) ERR value is not an integer or out of range\t# 运行错误 3) OK 4) \u0026#34;WORLD\u0026#34; 127.0.0.1:6379\u0026gt; GET key1\t# 其他命令执行成功 \u0026#34;HELLO\u0026#34; 127.0.0.1:6379\u0026gt; GET key2\t# 其他命令执行成功 \u0026#34;WORLD\u0026#34;   从上面我们可以看到，即使事务中有一条命令在执行期间出现了错误，整个事务也会继续执行下去，并且之前执行的命令也不会有任何影响。总结一下两种情况\n 编译时异常（代码有问题、命令有错）：事务中所有的命令都不会被执行！ 运行时错误（命令存在语法性错误）：其他命令可以正常执行的，错误命令抛出异常！  这也就是Redis事务与传统SQL数据库事务最大的区别，即Redis不支持事务回滚机制(rollback) 在官方文档中作者是这样描述的，不支持事务回滚的原因是因为这种复杂的功能和Redis追求的简单高效不相符。并且这种运行时错误通常由编程错误产生，通常只会出现在开发环境中，而并不会在生产环境中发生，就没有必要为Redis开发事务回滚功能\n 基于以上几点，我们得出结论，Redis的事务不能保证原子性 一致性：一致性即事务操作前与操作后的状态始终一致\n  隔离性：隔离性指的是即使数据库中有多个事务并发执行，各个事务之间也不会互相影响。\n 由于Redis使用单线程来执行事务以及事务队列中的命令，并且在执行事务的期间不会对事务进行终端，因此Redis的事务总是以串行的方式运行的，因此也不存在隔离级别这个概念 持久性：持久性指的是事务一旦提交，其结果就是永久性的。\n 从上面也可以看出，除了隔离性以及原子性以外，其余部分都与传统SQL数据库区别不大。\nWATCH乐观锁 并发编程中常见的锁机制：乐观锁、悲观锁、CAS、自旋锁、互斥锁、读写锁 如果不了解乐观锁及其实现原理的小伙伴可以看看我的往期博客，在这里就不再重复，我就简单的说明一下\n 悲观锁做事比较悲观，它始终认为共享资源在我们使用的时候会被其他线程修改，容易导致线程安全的问题，因此在访问共享数据之前就要先加锁，阻塞其他线程的访问 乐观锁则于悲观锁相反，它则比较乐观。它始终认为多线程同时修改共享资源的概率较低，所以乐观锁会直接对共享资源进行修改，但是在更新修改结果之前它会验证这段时间有没有其他线程对资源进行修改，如果没有则提交更新，如果有的话则放弃本次操作。  WATCH命令就是一个乐观锁，当它会监视任意数量的key，当执行事务时，如果这些key中有任何一个被修改，服务器都会拒绝执行事务，并向客户端返回事务执行失败的空回复\n下面就结合具体场景来演示一下\n假设此时小明的账户中有1000元，小王的账户有500元，此时小明想转250元给小王 1 2 3 4 5 6 7 8 9 10  127.0.0.1:6379\u0026gt; SET xiaoming 1000 OK 127.0.0.1:6379\u0026gt; SET xiaowang 500 OK 127.0.0.1:6379\u0026gt; MULTI OK 127.0.0.1:6379\u0026gt; DECRBY xiaoming 250\t# 转账 QUEUED 127.0.0.1:6379\u0026gt; INCRBY xiaowang 250 QUEUED   但是在转账的途中，正好小明花呗的自动还款时间到了，扣费900元，并先他一步提交\n1 2 3  # 另外一个线程中，抢先扣费 127.0.0.1:6379\u0026gt; DECRBY xiaoming 900 (integer) 100   当我们再次执行事务的时候，按道理来说金额不够就应该转账失败，但是此时小明的余额却变成了负数，这就出现了问题，用户可以无限的进行套现，这也就是我们通常所说的事务并发执行的问题。\n1 2 3  127.0.0.1:6379\u0026gt; EXEC 1) (integer) -150 2) (integer) 750   为了保证安全，通常我们会使用WATCH当作乐观锁操作，对key进行监控，当另一个线程修改被监控的key时，就会让事务失败。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  127.0.0.1:6379\u0026gt; WATCH xiaoming\t# 监控小明的账户 OK 127.0.0.1:6379\u0026gt; MULTI\t# 开启事务，转账 OK 127.0.0.1:6379\u0026gt; DECRBY xiaoming 250 QUEUED 127.0.0.1:6379\u0026gt; INCRBY xiaowang 250 # 另一个线程修改小明余额 127.0.0.1:6379\u0026gt; DECRBY xiaoming 900 (integer) 100 # 继续运行事务 127.0.0.1:6379\u0026gt; EXEC (nil)\t# key被修改，事务执行失败 127.0.0.1:6379\u0026gt; GET xiaoming\t# 可以看到，事务并没有执行 \u0026#34;100\u0026#34;   通过这种方法，就能够确保我们并发执行事务的安全，当我们确认当前操作不会导致恶劣影响的时候，就可以通过UNWATCH取消监控，然后WATCH来获取修改后的新余额来继续监控、执行事务。\n当你看到这里的时候，是不是感觉似曾相识？没错，这就是之前博客中我提到的CAS以及版本号机制，也是乐观锁的常见实现方法\n在Redis中，服务器通过一个字典来标记所有正在监控key的客户端 当某一个key被修改时，就会将所有监控它的客户端的REDIS_DIRTY_CAS标识打开，来标记数据已经被修改，当客户端执行EXEC命令时如果发现标识被修改，则说明此时可能会存在安全问题，于是拒绝执行事务 总结一下就是，我们通过CAS机制判断REDIS_DIRTY_CAS是否被打开来决定事务的执行，并通过WATCH实现版本号机制以及服务器对客户端的统一管理\n总结  Redis单条命令保证原子性，但是事务不能保证原子性 Redis事务中没有隔离级别的概念 Redis事务的本质就是一组命令的集合，命令通过事务队列以FIFO的方式顺序执行 WATCH命令即乐观锁，服务器通过字典将所有监控客户端与被监控key进行关联，并通过REDIS_DIRTY_CAS来判断key是否被修改，从而决定是否执行事务  ","date":"2022-05-26T15:06:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E4%BA%8B%E5%8A%A1/","title":"Redis 事务"},{"content":"Redis 缓存 缓存一致性 对于缓存和数据库的更新操作，主要分为以下两种\n 先删除缓存，再更新数据库 先更新数据库，再删除缓存   首先可能会带来疑惑的点是，为什么这里是删除缓存而不是更新缓存？\n 按照常理来说，更新的效率通常都会比删除高，因为我们在删除了缓存后当有读操作到来时，当其查询缓存不存在时，就会去查询数据库，并将读取到的值写入到缓存中，这样的效率明显比更新低。\n但是我们还需要考虑一个问题，即缓存的使用率问题。如果在短时间内对数据库进行了10000次更新操作，那么缓存也必定会进行10000次的更新操作，那这个缓存它真的有用到那么多次吗？如果它仅仅是一个冷门数据，可能在这期间内只进行了仅仅几次的查询操作，那我们的这些更新操作不是会显得很多余吗？\n所以，我们才会去使用删除。因为在我们删除缓存后，只有在其真正使用到这个数据的时候，才会将其写入缓存，因此我们就不用每次都对缓存进行更新操作，从而保证效率。\n先删除缓存，再更新数据库 对于这种情况，能够保证缓存的一致性吗？\n答案是否定的，例如下面这种场景：  线程A写入数据，此时先删除缓存 线程B读取数据，查询缓存不存在，直接去查询数据库 线程B将查询到的旧值写入至缓存中 线程A将新数据更新至数据库中  对于上述这种情况，线程B在线程A更新数据库之前就提前读取了数据库，从而读取到了旧值，而后线程B将读取到的旧值再次写入缓存中，就出现了缓存不一致的情况。\n 那么这个问题如何解决呢？\n 这时候就需要引入延时双删的机制\n延时双删 为了避免在更新数据库的时候，其他的线程读取到了数据库中的旧值并将其写入缓存这种情况，我们会在数据库更新完后等待一段时间，再次删除缓存，来保证下一个到来的线程能够将正确的缓存更新回去。 流程如下\n 线程A写入数据，此时先删除缓存 线程B读取数据，查询缓存不存在，直接去查询数据库 线程B将查询到的旧值写入至缓存中 线程A将新数据更新至数据库中，休眠一段时间 线程A将缓存再次删除，来确保缓存的一致性 其他线程查询数据库，将正确的值更新至缓存中  那么，为了保证我们能够将错误的缓存删除，所以我们的sleep时间只需要大于线程读写缓存的时间即可\n先更新数据库，再删除缓存 那么如果我们先更新数据库，再更新缓存呢？ 对于这种操作，缓存不一致的情况就更加明显了。由于磁盘I/O速度慢，在更新数据库、删除缓存这段操作之前，其他线程读取到的都是原本缓存中的旧值。甚至可能会由于缓存删除失败(如缓存服务当前不可用的情况)从而导致严重的缓存不一致问题。\n那么如何解决这个问题呢？可以使用以下几种方法\n修改缓存过期时间 这是解决这个问题最简单的方法，同时也是治标不治本的方法。\n我们可以将缓存过期时间变短，使其每隔一段时间就会去数据库中加载数据，对于更新不频繁的数据来说，就可以很好的解决不一致的问题，但若是更新特别频繁的热点数据，这个方法则失去了作用。\n由于这个方法的适用面小，且实时性和一致性不高，所以我们通常都会选择使用消息队列来解决这个问题。\n消息队列 我们可以引入一个消息队列来解决这个问题，在更新数据库后，我们往消息队列中写入数据，等到消费者从消息队列中取出数据时，再将缓存删除。借助消息队列的消息重试机制来保证我们一定能够成功删除缓存，从而确保缓存的一致性。 但是这种方法也存在几个问题\n 引入消息队列后可能会因为消息的处理导致一定程度的延迟，从而引起短期内的消息不一致 引入消息队列后导致问题整体复杂化  所以我们只有在对实时性和一致性要求不高的情况下才会选择这种做法\n缓存淘汰策略 redis中缓存的数据是有过期时间的，当缓存数据失效时，redis会删除过期数据以节省内存，那redis是怎样怎样的策略来删除过期数据的呢？\n过期键删除策略 过期删除策略通常有以下三种\n 定时删除：在为键设置过期时间的同时创建一个定时器，当过期时间到来时就会触发定时器中的处理函数，立即执行过期键的删除操作 定期删除：每隔一段时间就对数据库进行一次检查，删除其中的过期键。检查的数据库数量及删除的过期键数量由算法决定 惰性删除：不会主动去删除过期键。每次获取键时都会判断获取的键是否过期，如果过期则删除，没过期则返回  其中前两种为主动删除策略，最后一种为被动删除策略。下面就来谈谈这三种策略的优缺点以及Redis中究竟使用的哪一种\n定时删除 定时删除策略对于内存来说十分友好，通过定时器能够保证过期键能够在第一时间被删除，而不会一直占用内存。\n但是同样的，它对CPU时间非常不友好。在过期键比较多的时候，维护大量的定时器会给CPU带来巨大的压力，即使过期键少的时候，它也会将宝贵的CPU时间用在维护定时器，以及删除和当前任务无关的过期键上，对服务器的响应时间与吞吐量造成了一定的影响。\n惰性删除 从开始的描述可以看出，惰性删除对于CPU时间来说是最为友好的，因为我们只会在取出键的时候才会对其进行删除操作，这也就保证了我们不会在执行其他任务的时候又背地里去删除无关的过期键，合理的利用了CPU时间。\n但是！！！也正是因为这个原因，使得它对内存极度不友好。如果一个键已经过期，而只要我们不去获取这个键，就不会触发过期检查，那也就意味着他会一直占用这一块内存而不释放。\n这意味着什么呢？如果我们有非常多的过期键，而这些过期键又恰好因为版本迭代、项目组交替，在后续版本中并没有对其进行访问，那么它可能永远也不会被删除。我们可以将这种情况当成内存泄漏中的一种，对于Redis这种内存数据库来说，这种情况造成的后果十分严重\n定期删除 定期删除策略其实是上述两种策略的折中选择。\n定期删除策略相对于定时删除策略来说，由于其每隔一段时间才进行一次删除操作，通过限制了删除操作的时常和频率，大大减少了删除操作对CPU时间的影响。\n相比于惰性删除，并且由于定期删除过期键，有效地减少了过期键带来的空间浪费。即兼顾了CPU，又避免了内存浪费，是两者的折中选择。\n但是上述这些优点的前提，就是我们必须要确定一个合理的删除操作的时长和频率。\n 如果删除操作过于频繁，则又退化成了定时删除策略，浪费了大量的CPU时间 如果删除操作执行过少，则又会像惰性删除一样，出现大量的内存浪费问题。  Redis的选择 下面给出三种的效率对比\n CPU：惰性删除 \u0026gt; 定期删除 \u0026gt; 定时删除 内存利用率：定时删除 \u0026gt; 定期删除 \u0026gt; 惰性删除\n  定时删除占用太多CPU时间，影响服务器的吞吐量和性能，但是很好的避免了内存浪费 惰性删除浪费太多内存，有内存泄漏的风险，但是却保证了CPU的效率 定期删除属于前两种的折中，既保证了CPU时间的合理利用，又避免了内存的浪费  为了能够在合理利用CPU时间与避免浪费内存空间之间取得平衡，Redis同时使用了惰性删除和定期删除。\n这样的搭配虽然保证了Redis强大的吞吐量以及响应速度，但是却存在因为没有定时删除机制，所以存在着内存浪费问题。\n由于Redis中通常存储的数据量十分庞大，这就导致了定期删除每次只能抽取其中的一部分进行删除，倘若有一部分过期键一直没有被抽取到，并且我们也一直没有访问它来触发惰性删除，这个过期键就会一直存在内存中，如果不进行处理，就可能导致内存耗尽。\n为了解决这个问题，Redis又引入了内存淘汰机制\n内存淘汰机制 当Redis的内存占用过高时，如果内存不足以容纳新写入的数据，就会通过某种机制来删除一部分键，来减少当前占用的内存，这就是内存淘汰机制。\n当前Redis提供了8种内存淘汰策略，除却之前的6种，还有两种Redis4.0后新增的LFU模式：volatile-lfu以及allkeys-lfu\n   名称 作用     volatile-lru 在已设置过期时间的key中，挑选最近最少使用的key淘汰   volatile-lfu 在已设置过期时间的key中，挑选最不经常使用的key淘汰   volatile-ttl 在已设置过期时间的key中，挑选将要过期的key淘汰   volatile-random 在已设置过期时间的key中，随机挑选key淘汰   allkeys-lru 在所有key中，挑选最近最少使用的key淘汰   allkeys-lfu 在所有key中， 挑选最不经常使用的key淘汰   allkeys-random 在所有key中，随机挑选key淘汰   no-eviction 当内存不足以写入新数据时，写入操作会报错，并且不会淘汰数据(不常用)    乍一看策略很多很难记，其实总共就是四种不同的淘汰策略，以及两种key的选择范围\n选择范围\n allkeys：淘汰的范围为所有的key volatile：淘汰的范围为已设置过期时间的key  淘汰策略\n LRU：Least recently used，即淘汰最近最少使用的key LFU：Least Frequently Used，即淘汰最不经常使用的key TTL：Time To Live，即淘汰生命时间最短，即将要过期的key Random：随机淘汰  其中LRU和LFU较为常用，如果有想了解其算法原理的，可以看看我的往期博客 高级数据结构与算法 | LRU缓存机制（Least Recently Used） 高级数据结构与算法 | LFU缓存机制（Least Frequently Used）\n缓存常见问题 缓存雪崩 缓存雪崩指的是在短时间内，有大量缓存的键同时过期，由于缓存过期，导致此时所有的请求就直接查询数据库，而数据库很难抵挡这样巨大的压力，严重情况下就会导致数据库被大流量打死，直接宕机。\n下面是正常的查询流程以及缓存雪崩后的查询流程 缓存雪崩的解决方法有以下几种\n 随机化过期时间，为了避免缓存同时过期，在设置缓存时在原有时间上添加随机时间，使失效时间分散开来 加锁排队，加锁排队可以起到缓冲的作用，防止大量请求同时操作数据库，但是也正因为如此也减少了吞吐量，导致响应时间变慢，用户体验变差。 设置二级缓存，即加入一个本地缓存作为备案，当Redis缓存失效后就暂时使用本地缓存进行代替，避免直接访问数据库。 设置热点数据永不过期，有更新操作时直接更新缓存即可  缓存击穿 缓存击穿与缓存雪崩很像，不过一个是针对大量缓存一个是针对热点缓存。\n缓存击穿即当某个热点缓存突然失效，而正好对其有着大量的请求，此时这些请求就会直接向数据库进行查询，导致数据库面临巨大的压力\n缓存击穿的解决方法有以下几种\n 设置热点数据永不过期，有更新操作时直接更新缓存即可 加锁排队，通过加锁来减少同一时间的访问量，缓解压力  缓存穿透 缓存穿透是指查询的数据在缓存中和数据库中都不存在，此时请求就会直接绕过缓存抵达数据库，导致数据库压力过大。(由于主键通常都是从1开始自增，此时大量查询负数或者特别大的数据就会导致缓存穿透)。 出于容错考虑，由于这些数据在数据库中不存在，所以不会将结果保存到缓存中。而又因为缓存中没有这些数据，所以每次请求都会绕过缓存，直接向数据库查询，这就是缓存穿透。 缓存穿透的解决方法有以下几种\n 参数校验，对于那些不合法的请求就直接返回空结果，不进行查询 布隆过滤器，可以根据布隆过滤器来判断数据在不在数据库，虽然布隆过滤器查询存在不一定准确，但是如果布隆过滤器中查不到，则一定说明不存在，就不会进入数据库查询 缓存空结果，将每次查询的结果进行缓存，即使查询不到的也缓存一个空结果，当有非法请求时就直接返回空结果  缓存预热 与上面三种不同，缓存预热并不是一个需要解决的问题，而是一种优化的策略，通过这种策略能够更快的响应用户的查询。\n缓存预热指的是在启动系统的时候，提前将查询的结果预存到缓存中，这样用户查询时就可以直接从缓存中读取，减少了用户的等待时间 缓存预热的实现方法有以下三种\n 把需要缓存的函数写入到系统的构造函数中，这样系统就会在启动的时候自动的加载数据并缓存数据 把需要缓存的函数挂载到前端页面或者后端的接口上，手动触发缓存预热 设置定时任务，定时自动进行缓存预热  ","date":"2022-05-26T15:05:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E7%BC%93%E5%AD%98/","title":"Redis 缓存"},{"content":"Redis 持久化策略 什么是持久化 由于内存具有易失性，无法进行断电存储，所以在重启之后数据就会丢失，但是硬盘具有永久存储的特性，所以持久化就是将数据从内存中保存到硬盘的过程，目的就是为了防止数据的丢失。 同时持久化也是Redis比起Memcached的优势，Memcached并不支持持久化\nRedis的持久化分为以下三种\n RDB（Redis DataBase）持久化 AOF（Append Only File）持久化 混合持久化  RDB持久化 RDB持久化其实就是以快照的方式进行持久化的存储。 对于一个Redis服务器来说，它的所有非空数据库以及数据库中的所有键值对就是当前数据库的状态。所以只需要将数据库的状态保存在硬盘当中，即使服务器停机或者断电，只要硬盘中存储的状态还在，就可以通过它来还原数据库原来的状态。\n为了保证文件的安全以及容量更小，RDB持续化所生成的RDB文件是一个经过压缩的二进制文件，通过这个文件就可以还原数据库的状态。\nSAVA与BGSAVA RDB持久化根据执行持久化的对象不同又分为SAVA和BGSAVA两种方式\nSAVA即让Redis服务进程来执行持久化，所以直到RDB持久化结束之前，Redis服务进程会一直处于阻塞状态，无法处理任何命令。\nBGSAVE则会通过fork()来创建一个子进程，然后让子进程来接管RDB持久化，而父进程继续处理命令请求\n由于SAVA的会导致主进程的阻塞，所以使用时基本不会考虑，所以通常我们都会默认使用BGSAVA来进行，下面我指的也都是BGSAVA\nRDB持久化的优缺点 优点：\n RDB文件是经过压缩的二进制文件，占用内存更小更紧凑，适合作为备份文件 RDB容灾恢复更有用，因为其更加紧凑，可以更快的传输到远程服务器进行数据恢复。 RDB可以提高Redis的运行速度，因为使用BGSAVA持久化时会fork出子进程进行持久化的I/O操作，主进程不会受到干扰。 比起AOF格式的文件，RDB文件这种直接恢复状态的重启更快 缺点： 由于RDB是以快照形式进行保存的，并且快照之间存在一定的时间间隔，如果Redis服务被终止，则会导致丢失一段时间的数据 RDB的BGSAVA需要fork()出子进程来进行持久化，但是如果CPU性能不佳且数据量很大的时候，fork()的时间就会增加，导致Redis可能会停止服务一段时间。  AOF持久化 AOF持久化其实就是保存Redis服务器所执行的命令来保存数据库的状态，将命令追加到AOF文件的末尾（Append Only File），AOF的核心其实就是将所有执行过的命令重新执行，来恢复状态\n什么意思呢？例如我们执行了几条命令，此时AOF持久化会将这些命令以请求协议格式追加到AOF文件末尾 当服务器启动时，就会读取AOF文件中的所有命令，将其在服务器上重新执行一次，来恢复服务器的状态。 AOF重写 随着服务器存储的数据越来越多，此时AOF保存的命令也越来越多，文件的体积就会变得非常大，这样就可能导致对Redis服务器以及宿主机造成影响，并且随着文件的增大，使用AOF来进行数据还原需要的时间也就更多。\n并且还存在一个问题，就是命令中存在着大量冗余和多余的命令。\n1 2 3 4 5 6  127.0.0.1:6379\u0026gt;lpushlist252134(integer)5127.0.0.1:6379\u0026gt;lpoplist2\u0026#34;4\u0026#34;127.0.0.1:6379\u0026gt;rpoplist2\u0026#34;5\u0026#34;  例如上面这些命令，我存储了5 2 1 3 4五个数据，之后我又删除了4和5，所以最终剩下的只有1 2 3。 但是如果将所有的命令都保存进去，在恢复状态的时候又会重新模拟一次中间的删除流程，这些步骤是不需要的，我们只需要最终的结果。\n所以AOF引入了重写的机制，即只保存能够获取最终结果的命令 重写的流程很简单，就是去直接读取当前数据库中的键值状态，然后构造出对应的命令来进行保存 例如上面那个，就直接进行一次lpush 1 2 3，就可以直接省去了中间的操作。\n并且和RDB的BGSAVA一样，为了不阻塞主进程，所有的重写操作都会通过创建子进程来进行，并且由于子进程创建时会通过写时拷贝机制带有服务器数据的副本，所以也不需要对数据进行加锁就可以保证安全，提高了效率 此时子进程进行AOF的重写，父进程则继续处理接受的请求。 但是这时又引入了一个问题，如果父进程接受了新的命令，这些命令可能就会对数据库的状态进行修改，这样就会导致重写后的AOF文件所保存的状态和当前的数据库状态不一致。\n为了解决这个问题，服务器新增了一个AOF重写缓冲区，将两个AOF的过程给分割开 服务器流程\n 执行客户端发送来的新命令 将执行后的写命令追加到AOF缓冲区中 将执行后的写命令追加到AOF重写缓冲区中  这样AOF缓冲区的数据会被定期写入和同步到AOF文件中，不会影响整个AOF的流程。 而在执行重写后所执行的命令也都会保存到AOF重写缓冲区中。\n所以在子进程重写结束后，其会通过信号的方式来通知父进程，此时服务器就会进行以下的操作来完成重写的AOF文件的更换\n 将当前AOF重写缓冲区中的命令保存到一个新的AOF文件中，此时的新AOF文件的状态与当前数据库保存一致，并且比起旧的AOF更加简洁 此时对新的AOF文件进行改名，原子性的覆盖掉原先的AOF文件，完成AOF重写文件与旧文件的更替  AOF持久化的优缺点 优点\n AOF持久化保存的数据更加完整，其设定了三种保存策略：每次操作保存、每秒钟保存、跟随系统的持久化策略保存，其中每秒保存一次为AOF的默认策略。通过这种方法，使得即使发生了意外情况，最多也只会丢失1秒的数据，而不像RDB会丢失一段时间的数据 AOF如其名，采用了命令追加的方式写入到文件中，所以不会出现文件损坏的问题 AOF持久化将命令以协议格式写入文件，非常容易理解和解析，即使不小心使用flushall进行删库，并且状态被保存了下来。也可以通过删除AOF文件中的flushall命令来消除那次操作。（RDB快照则没办法避免）  缺点\n 在数据量相同的时候，AOF文件要大于RDB文件 在Redis负载较高的时候，RDB的性能比AOF更好 RDB使用快照的形式来持久化整个Redis数据，而AOF是将每次执行的命令追加到AOF文件中，所以RDB比AOF更健壮  混合持久化 混合持久化是在Redis4.0之后新增的一种方式，混合持久化结合了RDB和AOF的优点，在写入文件的时候，会先把当前的数据以RDB的形式写入文件的开头，再将后续的操作命令以AOF的格式写入文件中，这样既能保证Redis重启时的速度（RDB快照状态恢复），又能减少数据丢失的风险（AOF丢失时间短）\n混合持久化的优缺点 优点\n 结合了RDB和AOF的优点，开头为RDB格式的数据，可以快速启动（快照），并且之后为AOF格式，减少了大量数据丢失的风险 缺点 AOF文件可读性变差，因为AOF文件前面增加了RDB格式的内容 兼容性差，由于混合持久化是在4.0之后才引入的，如果开启之后则混合持久化AOF文件就不能使用在之前的版本  ","date":"2022-05-26T15:04:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E6%8C%81%E4%B9%85%E5%8C%96%E7%AD%96%E7%95%A5/","title":"Redis 持久化策略"},{"content":"数据类型 基础数据类型 String 字符串类型（SDS）即简单动态字符串，它是以键值对key-value的形式进行存储的，根据 key 来存储和获取value值\n依据不同情况，字符串在底层会使用 int 、 raw 或者 embstr 三种不同的编码格式\n 如果数据为可以使用long类型来保存的整数，则使用int 如果数据为可以使用long double类型来保存的浮点数，则使用embstr或者raw 如果数据为字符串，或者长度过大没办法用long来表示的整数，以及长度过大无法用long double表示的浮点数，则使用embstr或者raw。当数据小于39字节时，使用embstr，当大于39字节时使用raw   基本用法 1 2 3 4 5 6  127.0.0.1:6379\u0026gt;sethelloworld//设置key-valueOK127.0.0.1:6379\u0026gt;gethello//根据key获取value\u0026#34;world\u0026#34;127.0.0.1:6379\u0026gt;strlenhello//计算value长度(integer)5  使用场景  存放用户（登录）信息； 存放文章详情和列表信息； 存放和累计网页的统计信息。  Hash 字典类型 (Hash) 又被成为散列类型或者是哈希表类型，它是将⼀个键值 (key) 和⼀个特殊的“哈希表”关联起来。 哈希类型的底层数据结构可以是压缩列表（ZipList)或者字典（Dict）\n 当哈希对象的所有键值对的键和值的字符串长度都小于64字节，并且保存的键值对数量小于512个时，使用压缩列表 如果不满足上述条件中的任意一个，都会使用字典   基本用法 1 2 3 4 5 6  127.0.0.1:6379\u0026gt;hsethash1nameleeage20//设置key-value的映射(integer)2127.0.0.1:6379\u0026gt;hgethash1name//获取key为name的value\u0026#34;lee\u0026#34;127.0.0.1:6379\u0026gt;hgethash1age//获取key为age的value\u0026#34;20\u0026#34;  使用场景  存储用户信息或者某个物品的信息，无需序列化，直接建立映射  List 列表类型 (List) 是⼀个使用线性结构存储的结构，它的元素插入会按照先后顺序存储到链表结构中。 列表类型的底层数据结构可以是压缩列表（ZipList)或者链表（LinkedList）\n 当列表对象的所有字符串元素长度都小于64字节，并且保存的元素数量小于512个时，使用压缩列表 如果不满足上述条件中的任意一个，都会使用链表   基本用法 1 2 3 4 5 6  127.0.0.1:6379\u0026gt;lpushlist112345//依次头插12345，此时数据为54321(integer)5127.0.0.1:6379\u0026gt;rpoplist1//尾删\u0026#34;1\u0026#34;127.0.0.1:6379\u0026gt;lpoplist1//头删\u0026#34;5\u0026#34;  使用场景  消息队列：列表类型可以使用 rpush 实现先进先出的功能，同时又可以使用 lpop 轻松的弹出（查询并删除）第⼀个元素，所以列表类型可以用来实现消息队列； 文章列表：对于博客站点来说，当用户和文章都越来越多时，为了加快程序的响应速度，我们可以把用户自己的文章存入到 List 中，因为 List 是有序的结构，所以这样不仅可以完美的实现分页功能，而且加速了程序的响应速度。  Set 集合类型 (Set) 是⼀个无序并唯⼀的键值集合。\n集合类型的底层数据结构可以是整数集合（IntSet)或者字典（Dict）\n 当集合对象的所有元素都是整数值，并且保存的元素数量小于512个时，使用整数集合 如果不满足上述条件中的任意一个，都会使用字典   基本用法 1 2 3 4 5 6 7  127.0.0.1:6379\u0026gt;saddtestSetv1v2v3v4v2v4v1(integer)4127.0.0.1:6379\u0026gt;smemberstestSet//去重且无序1)\u0026#34;v2\u0026#34;2)\u0026#34;v1\u0026#34;3)\u0026#34;v4\u0026#34;4)\u0026#34;v3\u0026#34;  使用场景  微博关注我的人和我关注的人都适合用集合存储，可以保证人员不会重复； 中奖人信息也适合用集合类型存储，这样可以保证⼀个人不会重复中奖。  Zset 有序集合类型 (SortedSet) 相比于集合类型多了⼀个排序属性 score（分值），所以对于有序集合ZSet 来说，每个存储元素相当于有两个值组成的，⼀个是有序结合的元素值，⼀个是分值。有序集合的存储元素值也是不能重复的，但分值是可以重复的。\n有序集合类型的底层数据结构可以是压缩列表（ZipList)或者跳表（SkipList ）\n 当有序集合对象的所有元素成员的长度都小于64字节，并且保存的元素数量小于128个时，使用压缩列表 如果不满足上述条件中的任意一个，都会使用跳表（这里的跳表是结合字典的） 这里不是直接使用跳表，而是搭配字典一起使用 之所以这样设置是因为考虑到如果直接使用跳跃表，如果需要查找成员的分值时只能通过遍历来进行查找，而这样的效率是O(logN) 而字典虽然建立映射后可以O(1)的查找到分值，但是哈希只能通过key值进行查找，并不支持范围查询。 所以将两者进行结合，使用字典建立起元素与分值的映射，使用字典来进行成员分数的查找，而使用跳跃表来进行范围型操作，这样就很好的解决了这个问题。  1 2 3 4 5  typedef struct zset { zskiplist *zsl; dict *dict; } zset;   基本用法 1 2 3 4 5 6 7  127.0.0.1:6379\u0026gt;zaddzset13v18v22v36v4#插入时以分值-值的形式插入(integer)4127.0.0.1:6379\u0026gt;zrangezset10-1#查找结果按照升序排序1)\u0026#34;v3\u0026#34;2)\u0026#34;v1\u0026#34;3)\u0026#34;v4\u0026#34;4)\u0026#34;v2\u0026#34;  使用场景  学生成绩排名； 粉丝列表，根据关注的先后时间排序。  特殊数据类型 Geospatial(地理空间) 在使用一些小程序的时候，里面通常都会通过定位使用者的位置，来显示附近的人、外卖距离、剩余路径等功能，在Redis3.2中也引入了推算地理信息的数据结构，即Geospatial介绍 把某个具体的位置信息（经度，纬度，名称）添加到指定的key中，数据将会用一个sorted set存储，以便稍后能使用 GEORADIUS和 GEORADIUSBYMEMBER命令来根据半径来查询位置信息。\n这个命令(指GEOADD)的参数使用标准的x,y形式，所以经度（longitude）必须放在纬度（latitude）之前，对于可被索引的坐标位置是有一定限制条件的：非常靠近极点的位置是不能被索引的， 在EPSG:900913 / EPSG:3785 / OSGEO:41001指定如下：\n 有效的经度是-180度到180度 有效的纬度是-85.05112878度到85.05112878度  上面是官方文档的描述，其实Geo并没有我们想象中的复杂，它的本质就是一个Zset，通过将坐标以Geohash的方式进行处理，将经度纬度错位后形成一个52位整数，所以我们同样能够使用Zset提供的接口来操作Geo。\n用法    命令 作用     GEOADD key 经度 纬度 地点名称 将指定的地理空间位置（经度、纬度、名称）添加到指定的key中   GEODIST key 地点1 地点2 返回两个给定位置之间的距离   GEOPOS key 地点 从key里返回所有给定位置元素的位置（经度和纬度）   GEOHASH key 地点 返回一个或多个位置元素的 Geohash 表示   GEORADIUS key 经度 纬度 半径 以给定的经纬度为中心， 找出某一半径内的元素   GEORADIUSBYMEMBER key 地点 半径 找出位于指定范围内的元素，中心点是由给定的位置元素决定   Zset的接口同样适用于Geo，因此需要删除、查询全部时就可以使用Zset的接口。     下面示范一下这些命令的使用方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  # 为了方便示范，下面加入一些城市的地理信息——GEOADD  127.0.0.1:6379\u0026gt; GEOADD CHINA 108.94683 34.29296 \u0026#34;xian\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 116.405285 39.904989 \u0026#34;beijing\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 121.472644 31.231706 \u0026#34;shanghai\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 113.280637 23.125178 \u0026#34;guangzhou\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 114.085947 22.547 \u0026#34;shenzhen\u0026#34; (integer) 1 127.0.0.1:6379\u0026gt; GEOADD CHINA 110.33119 20.031971 \u0026#34;hainan\u0026#34; (integer) 1 # 获取北京和上海的距离——GEODIST  127.0.0.1:6379\u0026gt; GEODIST CHINA beijing shanghai km \u0026#34;1067.5980\u0026#34; # 获取西安的坐标——GEOPOS  127.0.0.1:6379\u0026gt; GEOPOS CHINA xian 1) 1) \u0026#34;108.94683212041854858\u0026#34; 2) \u0026#34;34.29296115814533863\u0026#34; # 以经度120 纬度35位置为中心，获取半径1000千米内的城市——GEORADIUS  127.0.0.1:6379\u0026gt; GEORADIUS CHINA 120 35 1000 km 1) \u0026#34;beijing\u0026#34; 2) \u0026#34;shanghai\u0026#34; # 获取在广州半径500千米内的城市——GEORADIUSBYMEMBER  127.0.0.1:6379\u0026gt; GEORADIUSBYMEMBER CHINA guangzhou 500 km 1) \u0026#34;shenzhen\u0026#34; 2) \u0026#34;guangzhou\u0026#34; 3) \u0026#34;hainan\u0026#34; # 将广州和深圳的坐标转换为11为的GEO哈希值——GEOHASH 127.0.0.1:6379\u0026gt; GEOHASH CHINA guangzhou shenzhen 1) \u0026#34;ws0e9cb3yj0\u0026#34; 2) \u0026#34;ws10k0dcg10\u0026#34;   Hyperloglog(基数统计) 在我们为网站统计访问量、日活量时，由于我们统计的是用户数量而非访问次数，因此即使一个用户多次访问也只会统计一次，这种不重复的数据通常被称为基数。\n在传统的做法中，我们通常会采用set来保存用户的ID来进行计数，因为其本身存在着去重的功能，但是由于我们所需要的是对用户进行计数，如果通过将所有用户的ID保存的方法来完成，当用户量大的时候就会对内存产生巨大的压力，并且效率也大大降低。\n为了解决这个问题，Redis在2.8.9版本添加了HyperLogLog结构。\n介绍 Redis HyperLogLog是用来做基数统计的算法，HyperLogLog的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。在Redis 里面，每个HyperLogLog键只需要花费12KB内存，就可以计算接近2^64个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。\n但是，由于HyperLogLog使用的是概率算法，通过存储元素的hash值的第一个1的位置，来计算元素数量，所以HyperLogLog 不会存储元素本身，在数据量大的时候也可能会存在一定的误差。但是在基数统计这一方面，它的效果是其他结构无法比拟的。\n用法    命令 作用     PFADD key value 添加指定的值到Hyperloglog中   PFCONUT key 返回给定Hyperloglog的基数估算值   PFMERGE destkey sourcekey 将目标Hyperloglog合并到源Hyperloglog中    使用示范\n1 2 3 4 5 6 7 8 9 10 11 12 13  127.0.0.1:6379\u0026gt; PFADD NUMS1 1 2\t3 4\t#向NUMS1插入1-4 (integer) 1 127.0.0.1:6379\u0026gt; PFADD NUMS1 1\t#数据已存在，不再插入 (integer) 0 127.0.0.1:6379\u0026gt; PFCOUNT NUMS1\t#查看当前基数数量 (integer) 4 127.0.0.1:6379\u0026gt; PFADD NUMS2 3 4 5 6\t#向NUMS2插入3-6 (integer) 1 127.0.0.1:6379\u0026gt; PFMERGE NUMS1 NUMS2\t#将NUMS2合并到NUMS1中 OK 127.0.0.1:6379\u0026gt; PFCOUNT NUMS1\t#此时NUMS1中记录了1-6,六个元素 (integer) 6\t  Bitmap(位图) 介绍 位图其实就是哈希的变形，他通过哈希映射来处理数据，位图本身并不存储数据，而是存储标记。通过一个比特位，即0/1来标记一个数据的两种状态位图通常情况下用在数据量庞大，且数据不重复的情景下标记某个数据的两种状态。我们可以使用位图来记录当前用户的登陆情况、或者实现打卡、签到等功能。\n用法    命令 作用     GETBIT key offset value(0/1) 设置Bitmap中偏移量为offset的位置的值   SETBIT key offset value 返回Bitmap中偏移量为offset的位置的值   BITCOUNT key 计算位图中有多少个1    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  127.0.0.1:6379\u0026gt; SETBIT TEST 1 1\t#将位图中第1，3，5位设置为1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT TEST 3 1 (integer) 0 127.0.0.1:6379\u0026gt; SETBIT TEST 5 1 (integer) 0 127.0.0.1:6379\u0026gt; GETBIT TEST 1 #查看位图中1，2，3位的值 (integer) 1 127.0.0.1:6379\u0026gt; GETBIT TEST 2 (integer) 0 127.0.0.1:6379\u0026gt; GETBIT TEST 3 (integer) 1 127.0.0.1:6379\u0026gt; BITCOUNT TET\t#统计位图中1的数量，由于我们只设置了1，3，5位，因此为3 (integer) 3   ","date":"2022-05-26T15:03:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","title":"Redis 数据类型"},{"content":"数据结构 SDS 结构分析 由于C字符串存在大量问题，所以在Redis中，并没有使用C风格字符串，而是自己构建了一个简单动态字符串即SDS（simple dynamic string）\n1 2 3 4 5 6 7 8  struct sdshdr { // buf 中已占用空间的长度  int len; // buf 中剩余可用空间的长度  int free; // 数据空间  char buf[]; };   为解决C字符串缓冲区溢出问题以及长度计算问题，SDS中引入了len来统计当前已使用空间长度，free来计算剩余的空间长度\nC字符串的主要缺陷就是因为它没有记录自己的长度，而如果在需要了解长度时，就只能通过O（N）的效率进行一次遍历 并且因为C字符串没有统计剩余空间的字段，也没有容量字段，所以很容易就会因为strcat等函数造成缓冲区的溢出，为弥补这一缺陷，redis在sds中增加了free字段 通过标记剩余空间，当对SDS进行插入操作时，就会提前判断当前剩余空间是否足够，如果不足则会先进行空间的拓展，再进行插入，这样就解决了缓冲区溢出的问题\n内存策略 由于Redis作为一个高效的内存数据库，用于速度要求严苛，插入删除频繁 的场景，为了提高内存分配的效率，防止大量使用内存重分配而调用系统函数导致的性能损失问题（用户态和内核态的切换），Redis主要依靠空间预分配和惰性空间释放来解决这个问题\n空间预分配 为减少空间分配的次数，当需要进行空间拓展时，不仅仅会为SDS分配修改所必须要的空间，并且会为SDS预分配额外的未使用空间。\n预分配未使用空间的策略如下\n 当SDS修改后的长度小于1MB时，将会预分配大小和当前len一样的空间(free = len)，也就是使空间增长一倍，来减少因为初始时申请大空间导致的连续分配问题 当SDS修改后的长度大于等于1MB时，每次分配都会分配1MB的空间，防止空间的浪费。  惰性空间释放 当我们对SDS进行删除操作时，并不会立即回收删除后空余的空间，而是将空余空间以free字段记录下来，以备后面使用。 这样做的目的在于防止因为空间缩短后因为再度插入导致的空间拓展问题。 并且如果有需求需要真正释放空间，Redis也提供了对应的API，所以不必担心会因为惰性的空间释放而导致的内存浪费问题。\n总结 比起 C 字符串， SDS 具有以下优点：\n 常数复杂度获取字符串长度。（len字段） 杜绝缓冲区溢出。（free字段） 减少修改字符串长度时所需的内存重分配次数。（空间预分配，惰性空间释放） 二进制安全。（以二进制形式处理） 兼容部分 C 字符串函数。（底层基于C字符串，以空字符结尾）  链表 结构分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  typedef struct listNode { // 前置节点  struct listNode *prev; // 后置节点  struct listNode *next; // 节点的值  void *value; } listNode; /* * 双端链表迭代器 */ typedef struct listIter { // 当前迭代到的节点  listNode *next; // 迭代的方向  int direction; } listIter; /* * 双端链表结构 */ typedef struct list { // 表头节点  listNode *head; // 表尾节点  listNode *tail; // 节点值复制函数  void *(*dup)(void *ptr); // 节点值释放函数  void (*free)(void *ptr); // 节点值对比函数  int (*match)(void *ptr, void *key); // 链表所包含的节点数量  unsigned long len; } list;   从上面的结构可以看出，Redis的链表是一个带头尾的双端无环链表，并且通过len字段记录了链表节点的长度 同时为了实现多态与泛型，链表中还提供了dup，free，match属性来设置相关的函数，使得链表支持不同类型的值的存储\n总结  链表被广泛用于实现 Redis 的各种功能， 比如列表键， 发布与订阅， 慢查询， 监视器， 等等。 每个链表节点由一个 listNode 结构来表示， 每个节点都有一个指向前置节点和后置节点的指针， 所以 Redis 的链表实现是双端链表。 每个链表使用一个 list 结构来表示， 这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。 因为链表表头节点的前置节点和表尾节点的后置节点都指向 NULL ， 所以 Redis 的链表实现是无环链表。 通过为链表设置不同的类型特定函数， Redis 的链表可以用于保存各种不同类型的值。  字典 结构分析 Redis的字典底层采用了哈希表来进行实现。\n首先看看字典底层哈希表的结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  typedef struct dictht { // 哈希表数组  dictEntry **table; // 哈希表大小  unsigned long size; // 哈希表大小掩码，用于计算索引值  // 总是等于 size - 1  unsigned long sizemask; // 该哈希表已有节点的数量  unsigned long used; } dictht;   哈希表中记录了当前的总长度，已有节点，以及当前索引大小（用于哈希函数来计算节点位置)\n为解决哈希冲突，Redis字典采用了链地址法来构造了哈希桶的结构，也就是哈希数组中的每个元素都是一个链表。 下面来看看哈希节点的结构\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  typedef struct dictEntry { // 键  void *key; // 值  union { void *val; uint64_t u64; int64_t s64; } v; // 指向下个哈希表节点，形成链表  struct dictEntry *next; } dictEntry;   可以看到，为保证键值对适用于多重类型，key值使用的时void的形式，而value使用了64位有符号整型和64位无符号整型，void指针的一个联合体，每个节点使用next来链接成一个链表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  typedef struct dictType { // 计算哈希值的函数  unsigned int (*hashFunction)(const void *key); // 复制键的函数  void *(*keyDup)(void *privdata, const void *key); // 复制值的函数  void *(*valDup)(void *privdata, const void *obj); // 对比键的函数  int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数  void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数  void (*valDestructor)(void *privdata, void *obj); } dictType;   为保证字典具有多态及泛型，dictType中提供了如哈希函数以及K-V的各种操作函数，使得字典适用于多重情景\nrehash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  /* * 字典 */ typedef struct dict { // 类型特定函数  dictType *type; // 私有数据  void *privdata; // 哈希表  dictht ht[2]; // rehash 索引  // 当 rehash 不在进行时，值为 -1  int rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 目前正在运行的安全迭代器的数量  int iterators; /* number of iterators currently running */ } dict;   从字典的结构中，我们可以看到里面同时存放了两个哈希表，以及一个rehashidx属性。 这就牵扯到了字典的核心之一，rehash。\nRedis作为一个插入频繁且对效率要求高的数据库，当插入的数据过多时，就会因为哈希表中的负载因子过高而导致查询或者插入的效率降低，此时就需要通过rehash来进行重新扩容并重新映射。 但是如果只是用一个哈希表，映射时就会导致数据库暂时不可用，作为一个使用频繁的数据库，短期的停机几乎是不可容许的问题，所以Redis设计时采用了双哈希的结构，并采用了渐进式rehash的方法来解决这个问题。 rehash的步骤如下\n 为ht[1]的哈希表分配空间 将ht[0]中的键值对重新映射到ht[1]上 当ht[0]的数据迁移完成，此时ht[0]为一个空表，此时释放ht[0]，并让ht[1]成为新的ht[0]，再为ht[1]创建一个新的空白哈希表，为下一次的rehash做准备   渐进式rehash 由于数据库中可能存在大量的数据，而rehash的时候又过长，为了避免因为rehash造成的服务器停机，rehash的过程并不是一次完成的，而是一个多次的，渐进式的过程。 在渐进式rehash的时候，由于数据不断的进行迁移，无法确定数据处于哪一个表上， 此时如果进行插入、删除、查找的操作时就会在两个表上进行，如果在一个表中没找到对应数据，就会到另一个表中继续查找。\n并且如果此时新插入节点，都会统一的防止在新表ht[1]中，防止对ht[0]的rehash造成干扰，保证ht[0]节点的只减少不增加\n总结  字典被广泛用于实现 Redis 的各种功能， 其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现， 每个字典带有两个哈希表， 一个用于平时使用， 另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突， 被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时， 程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面， 并且这个 rehash 过程并不是一次性地完成的， 而是渐进式地完成的。  跳表 跳表是一个较为少见的数据结构，如果不了解的可以看看我之前的博客 看了这篇博客，还敢说你不懂跳表吗？ 由于跳表的实现简单且性能可与平衡树相媲美，对于大量插入删除的数据库来说，跳表只需要进行简单的链表插入和索引的选拔，而不像平衡树一样需要进行整体平衡的维持。并且由于在范围查找上的效率远远强于平衡树，所以Redis底层选取跳表来作为有序集合的底层之一。\n结构分析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  typedef struct zskiplistNode { // 成员对象  robj *obj; // 分值  double score; // 后退指针  struct zskiplistNode *backward; // 层  struct zskiplistLevel { // 前进指针  struct zskiplistNode *forward; // 跨度  unsigned int span; } level[]; } zskiplistNode;   跳跃表的查询从最顶层出发，通过前进指针来往后查找，通过比较节点的分数来判断当前节点是否与索引匹配，如果查找不到则进入下层继续查找，并记录下跨越的层数span来进行排位。 同时为了处理特殊情况，还准备了一个后退指针来进行从表尾到表头的遍历，但是与前进不同，后退指针并不存在跳跃，而是只能一个一个向后查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  /* * 跳跃表 */ typedef struct zskiplist { // 表头节点和表尾节点  struct zskiplistNode *header, *tail; // 表中节点的数量  unsigned long length; // 表中层数最大的节点的层数  int level; } zskiplist;   跳跃表通过保存表头和表尾节点，来快速访问表头和表尾。并且保存了节点的数量来实现O(1)的长度计算 为了避免因为层数过高导致的大量空间损失，Redis跳跃表的节点高度最高位32层。\n总结  跳跃表是有序集合的底层实现之一， 除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成， 其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度）， 而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中， 多个节点可以包含相同的分值， 但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序， 当分值相同时， 节点按照成员对象的大小进行排序。  整数集合 整数集合时集合键的底层实现之一，当集合中的元素全部都是整数值的时候，并且集合中元素不多时，Redis就会使用整数集合来作为集合键的底层结构。整数集合具有去重且排序的特性\n结构分析 1 2 3 4 5 6 7 8 9 10 11  typedef struct intset { // 编码方式  uint32_t encoding; // 集合包含的元素数量  uint32_t length; // 保存元素的数组  int8_t contents[]; } intset;   在上面的结构中，虽然content数组的类型是一个8bit的整型，但是数据真正存储的方式并不是这个类型，而是根据encoding来决定具体的类型，8bit只是作为一个基本单位来进行使用。\n例如此时encoding设置为INTSET_ENC_INT16时，数组存储的格式就有每个元素16bit 如果encoding设置为INTSET_ENC_INT64时，数组存储的格式就有每个元素64bit 升级 升级的流程\n 根据插入的类型来决定集合升级的类型，拓展数组的整体空间并为新节点分配空间 对集合中所有数据类型进行升级，在保持有序的前提下将所有升级后的元素移动到合适的位置上 将新节点插入到集合的对应位置  新元素的插入位置 由于会引起升级的元素的类型都必顶比数组中的所有数据都大，所以也就决定了其要么比所有数据都大，要么比所有数据都小（负数），所以插入位置只能是首部和尾部\n 当新元素比所有数据都大时在尾部插入 当新元素比所有数据都小时在首部插入  如果插入一个32位的数据，则引起全体升级\n分配底层空间\n整体升级并挪动位置\n插入元素\n降级 在整数列表中升级是一个不可逆的过程，即使将所有高类型的数据删除了，也不会进行降级。 理由是防止因为降级后再次升级带来的大量数据挪动的问题，在保证了效率的同时，也带来了一定程度上的空间浪费（非必要时尽量不要升级） 升级带来的好处\n 灵活 ： 由于C语言是一个静态类型语言，为了避免出现类型错误通常会将不同类型分开，例如如果要存储16bit、32bit、64bit的整型就需要3个数组，但是使用整数集合就可以通过升级的策略来进行元素类型的自适应，就可以任意的将各种类型的整数插入进去，而不必担心类型错误 节约内存：与上面类似，如果我们只有一个64bit的数组，而里面存储了不少16bit、32bit的元素，则会造成空间的大量浪费。而使用整数集合则可以同时保存多个类型，只需要确保升级的操作只在必要时进行，就不会造成空间的浪费  总结  整数集合是集合键的底层实现之一。 整数集合的底层实现为数组， 这个数组以有序、无重复的方式保存集合元素， 在有需要时， 程序会根据新添加元素的类型， 改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性， 并且尽可能地节约了内存。 整数集合只支持升级操作， 不支持降级操作。  压缩列表 压缩列表（ziplist）是列表键和哈希键的底层实现之一。\n当一个列表键只包含少量列表项， 并且每个列表项要么就是小整数值， 要么就是长度比较短的字符串， 那么 Redis 就会使用压缩列表来做列表键的底层实现。主要核心就是为了节约空间\n结构分析 例如这张图，可以看出当前包含三个节点，总空间为0x50(十进制80），到尾部的偏移量为0x3c(十进制60)，节点数量为0x3(十进制3)\n每个压缩列表节点可以由一个整数或者一个字节数组组成\n整数的类型可以是以下六种之一：\n 4位的介于0-12的无符号整数 1字节的有符号整数 3字节的有符号整数 int16_t int32_t int64_t  字节数组可以是以下三种之一：\n 长度小于等于63（2^6 -1)字节的字节数组 长度小于等于16383（2^14 -1)字节的字节数组 长度小于等于4294967295（2^32 -1)字节的字节数组  而压缩列表节点又有三个属性组成，分别是previous_entry_length，encoding，content。\nprevious_entry_length 这个属性记录了压缩列表前一个节点的长度，该属性根据前一个节点的大小不同可以是1个字节或者5个字节。\n 如果前一个节点的长度小于254个字节，那么previous_entry_length的大小为1个字节，即前一个节点的长度可以使用1个字节表示 如果前一个节点的长度大于等于254个字节，那么previous_entry_length的大小为5个字节，第一个字节会被设置为0xFE(十进制的254），之后的四个字节则用于保存前一个节点的长度。  小于254字节时的表示 大于等于254字节时的表示 为什么要这样设计呢？ 由于压缩列表中的数据以一种不规则的方式进行紧邻，无法通过后退指针来找到上一个元素，而通过保存上一个节点的长度，用当前的地址减去这个长度，就可以很容易的获取到了上一个节点的位置，通过一个一个节点向前回溯，来达到从表尾往表头遍历的操作\nencoding encoding通过以下规则来记录content的类型\n 一字节、两字节或者五字节长， 值的最高位为 00 、 01 或者 10 的是字节数组编码： 这种编码表示节点的 content 属性保存着字节数组， 数组的长度由编码除去最高两位之后的其他位记录； 一字节长， 值的最高位以 11 开头的是整数编码： 这种编码表示节点的 content 属性保存着整数值， 整数值的类型和长度由编码除去最高两位之后的其他位记录；   content content属性负责保存节点的值，值的具体类型由上一个字段encoding来决定。\n例如存储字节数组，00表示类型为字节数组，01011表示长度为11 存储整数值，表示存储的为整数，类型为int16_t 连锁更新 当添加或删除节点时，可能就会因为previous_entry_length的变化导致发生连锁的更新操作。\n假设e1的previous_entry_length只有1个字节，而新插入的节点大小超过了254字节，此时由于e1 的previous_entry_length无法该长度，就会将previous_entry_length的长度更新为5字节。 但是问题来了，假设e1原本的大小为252字节，当previous_entry_length更新后它的大小则超过了254，此时又会引发对e2的更新。 顺着这个思路，一直更新下去 同理，删除也会引发连锁的更新 从上图可以看出来，在最坏情况下，会从插入位置一直连锁更新到末尾，即执行了N次空间重分配， 而每次空间重分配的最坏复杂度为 O(N) ， 所以连锁更新的最坏复杂度为 O(N^2) 。\n即使存在这种情况，但是并不影响我们使用压缩列表\n 压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点， 连锁更新才有可能被引发， 这种情况就和连中彩票一样，很少见 即使出现连锁更新， 但只要被更新的节点数量不多， 就不会对性能造成任何影响： 比如说， 对三五个节点进行连锁更新是绝对不会影响性能的；  总结  压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表， 或者从压缩列表中删除节点， 可能会引发连锁更新操作， 但这种操作出现的几率并不高。  ","date":"2022-05-26T15:02:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"Redis 数据结构"},{"content":"Redis 基本概念 什么是 Redis? Redis 是一个使用 ANSI C 编写的开源、支持网络、基于内存、分布式、可选持久性的键值对存储数据库。它可以作为内存数据库、缓存和消息中间件，其中缓存是它最主要的使用场景。\n什么是缓存？ 缓存概念 缓存是⼀个高速数据交换的存储器，使用它可以快速的访问和操作数据。\n举个通俗的例子。 小明经营着一家饭店，在刚开张的时候由于名气不足，客源少，生意并不是很忙，平时没事的时候就闲着，有客人来了再进厨房安排做菜。随着饭店的日益发展，此时的饭店已经不同往日，有着大量的稳定客源，并且在某些节假日的时候甚至爆满。按照以前的做法，那肯定是行不通了，在用餐高峰期的时候因为备餐慢导致了客户的长时间等待，使得饭店的屡遭投诉。 为解决这一问题，小明想到了一个办法，可以在空闲的时候，提前将热门的菜做完后放入保温柜，等用餐高峰期时再拿出来加热后就可以直接上菜，就规避了短时间内大量客源而导致的备餐慢的问题，通过这一方法，即使在高峰期，也能很好的应对。\n这就是缓存的本质，将热点资源（高频读、低频写）提前放入离用户最近、访问速度更快的地方，以提高访问速度。\n缓存 VS 数据库 相比于数据库而言，缓存的操作性能更高\n 缓存⼀般都是通过 key-value 查询数据，因为不像数据库⼀样还有查询的条件等因素，所以查询的性能⼀般会比数据库高； 缓存的数据是存储在内存中的，而数据库的数据是存储在磁盘中的，因为内存的操作性能远远大于磁盘，因此缓存的查询效率会高很多； 缓存更容易做分布式部署（当⼀台服务器变成多台相连的服务器集群），而数据库⼀般比较难实现分布式部署，因此缓存的负载和性能更容易平行扩展和增加。  本地缓存 VS 分布式缓存 根据缓存是否与应用进程属于同一进程（单机与多机)，又分为本地缓存和分布式缓存\n本地缓存 本地缓存也叫做单机缓存，即将服务部署到一台服务器上，所以本地缓存只适用于当前系统 举个例子，这个就如同每个学校的校规，根据学校的宗旨以教学理念不同，每个学校的校规都不一样，对于A学校的学生来说，B学校的校规毫无意义，也就是说每个学校的校规只适用与那个学校。\n所以本地缓存只适用于当前系统\n优缺点\n 访问速度快，但无法进行大数据存储 集群的数据更新问题 数据随应用进程的重启而丢失  分布式缓存 分布式缓存也叫做多机缓存，即将服务部署到多台服务器上，并且通过负载分发将用户的请求按照⼀定的规则分发到不同服务器。 而分布式缓存就如同教育局定下来的教学规范，无论是任何学校都必须遵守这个规范。\n所以分布式缓存适用与所有的系统。\n优缺点\n 支持大数据量存储，不受应用进程重启影响 数据集中存储，保证数据一致性 数据读写分离，高性能，高可用 数据跨网络传输，性能低于本地缓存  Memcached VS Redis 在市面上流行的分布式缓存中间件有两种，分别是Redis和Memcached，我们该如何对他们进行一个选择呢？\n  存储方式\n  Memcached把所有数据存在内存当中，数据大小不能超过内存大小，并且断电后数据会丢失。（不支持持久化，导致容灾能力弱）\n  Redis有部分存储在硬盘中，保证了数据的持久性。（持久化策略）\n    数据类型\n  Memcached对数据类型的支持较为简单，有时需要将数据拿到客户端来进行类似的修改再set回去，增加了网络IO的次数和数据体积\n  Redis具有复杂的数据类型，并且这些复杂类型的操作和get/set一样高效\n    存储值大小\n  Redis最大可以达到512mb\n  Memcached最大只有1mb\n    性能\n  Redis使用单核，在存储小数据时Redis有着明显的优势\n  Memcached使用多核，虽然在存储小数据的时候性能不及Redis，但是在存储大数据的时候Memcached要远远强于Redis\n    虽然从上面的结论以及当前流行程度来看，Redis都遥遥领先，但是在某些场景下，Memcached的作用也会高于Redis(例如海量数据查询），所以还需要根据具体使用场景来进行选择\n  适用场景\n  Redis除了作为NoSQL数据库使用外，还能用做消息队列、数据堆栈和数据缓存等；\n  Memcached适合于缓存SQL语句、数据集、用户临时性数据、延迟查询数据和session等。\n    ","date":"2022-05-26T15:01:13+08:00","permalink":"https://blog.orekilee.top/p/redis-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"Redis 基本概念"},{"content":"一致性哈希 分布式存储 如果我们需要存储QQ号与个人信息，建立起\u0026lt;QQ, 个人信息\u0026gt;的KV模型。\n假设QQ有10亿用户，并且每个用户的个人信息占据了100M，如果要存储这些数据，所需要的空间就得(100亿* 100M) = 10WT，这么庞大的数据是不可能在单机环境下存储的，只能采用分布式的方法，用多个机器进行存储，但是即使使用多机，这些数据也至少要10w台机器(假设每台服务器存1T)才能存储。\n并且我们还需要考虑，如何将这10w台机器与我们的数据建立起映射关系呢？ 换句话来说就是，我们如何确定哪些数据应该放在哪个机器呢？这时就需要用到哈希算法。\n简单哈希 我们可以采用除留余数法来完成一个映射，key值为qq号，余数为机器数量，得到的结果就是应该存储的机器的编号。这样我们将数据放入指定机器中，使用时再根据机器号进入对应的机器进行增删查改即可。\n1  机器号 = hash(QQ号) % 机器数量   但是这个方法存在着一个致命的缺陷，随着用户量不断增多或者用户信息增加，10w台机器就会不够用，此时就需要将机器扩容至15w台。 当进行扩容后，由于机器数量发生变化，数据的映射关系也会变化，我们就需要进行rehash来将数据重新映射到正确的位置上。\n但是问题来了，这10w台机器的数据如果需要进行重新映射，花费的时间几乎是不可想象的，我们不可能说为了迁移数据而让服务器宕机数月之久，所以这种方法是不可能行得通的。\n一致性哈希 为了弥补上一种方法的，就引入了一致性哈希算法。\n上面一种方法的主要缺陷就是由于扩容后rehash带来的数据大量迁移问题。 为了解决上述问题，一致性哈希将哈希构造成一个0~2^32-1的环形结构，并将余数从原来的机器数量修改值为整型最大值(也可以是比这个更大的)。因为这个数据足够大，所以不需要考虑因为机器数增加导致的rehash问题。\n1  机器号 = hash(QQ号) % 2^32   我们将环中的某一区间去映射到某台服务器，让这台服务器负责这个区间的管理，这样就能让这10w台服务器来切分这个闭环结构 当我们要查询某个数据的时候，根据哈希函数算出的映射位置来找到包含该位置的那个区间所对应的服务器，然后在那个服务器中进行操作即可 如果原先的服务器不够用了，此时增加5w个服务器，也不需要像之前一样对所有机器的数据进行迁移，我们只需要迁移负载重的机器即可 例如此时NodeC中存储了25000-50000的数据，此时往其中增加一个新服务器NodeE，让其负责映射闭环中25000-37500的数据。 此时我们需要做的就是将NodeC中25000~37500的那一部分重新迁移到NodeE上，并改变两个服务器的映射范围，就完成了数据的迁移。从这里我们可以看到，一致性哈希将服务器数据的整体迁移变成了高负载服务器的部分迁移，大大提高了效率以及稳定性。\n总结： 一致性哈希就是一个大范围的闭环，由于除数过大，我们也不需要因为由于除数增加导致全体rehash。并且映射关系变味了数据区间——机器，如果要增加机器，就只需要改变映射范围，并将区间中的小部分数据进行迁移，大大的提高了效率。\n虚拟节点 但是，一致性哈希也存在缺陷，就是在节点过少的时候可能会因为节点分布不均匀而导致数据倾斜问题。例如当前只有两个服务器 此时就会出现这种情况，部分节点数据过少，而部分节点数据过多，此时的数据大量集中在NodeA上，数据大量倾斜。\n既然节点较少，那就可以考虑在不增加服务器的基础上多增加几个节点，所以为了解决这问题，一致性哈希又引入了虚拟节点。对每个服务节点进行多次哈希映射，每个映射的位置都会放置该服务节点，成为虚拟节点。例如上图，就分别将NodeA和NodeB分成了三个虚拟节点。我们不需要改变数据定位的算法，只需要将虚拟节点与服务节点进行映射，将定位到虚拟节点NodeX #1、#2、#3的节点再定位回服务节点即可。\n通过这种方法就保证了即使服务节点少，也能做到相对均匀的数据分布\n","date":"2022-05-24T17:20:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/","title":"一致性哈希"},{"content":"一致性模型 什么是一致性模型? 在分布式系统中，C（一致性） 和 A（可用性）始终存在矛盾。若想保证可用性，就必须通过复制、分片等方式冗余存储。而一旦进行复制，又来带多副本数据一致性的问题——一个副本的数据更新之后，其他副本必须要保持同步，否则数据不一致就可能导致业务出现问题。\n因此，每次更新数据对所有副本进行修改的时间以及方式决定了复制代价的大小。全局同步与性能实际上是矛盾的，而为了提高性能，往往会采用放宽一致性要求的方法。因此，我们需要用一致性模型来理解和推理在分布式系统中数据复制需要考虑的问题和基本假设。\n 那么什么是一致性模型呢？\n 一致性模型本质上是进程与数据存储的约定：如果进程遵循某些规则，那么进程对数据的读写操作都是可预期的。\n下图即为 Jepsen 概括的常见的一致性模型：\n 不可用（Unavailable）：粉色代表网络分区后完全不可用。当出现网络隔离等问题的时候，为了保证数据的一致性，不提供服务。熟悉 CAP 理论的同学应该清楚，这就是典型的 CP 系统了。 严格可用 （Sticky Available）：黄色代表严格可用。即使一些节点出现问题，在一些还没出现故障的节点，仍然保证可用，但需要保证 client 的操作是一致的。 完全可用（Highly Available）：蓝色代表完全可用。就是网络全挂掉，在没有出现问题的节点上面，仍然可用。  一致性模型主要可以分为两类：能够保证所有进程对数据的读写顺序都保持一致的一致性模型称为强一致性模型，而不能保证的一致性模型称为弱一致性模型。\n强一致性模型 强一致性包含线性一致性和顺序一致性，其中前者对安全性的约束更强，也是分布式系统中能保证的最好的一致性。\n线性一致性（Linearizable Consistency） 线性一致性是最严格的且可实现的单对象单操作一致性模型。在这种模型下，写入的值在调用和完成之间的某个时间点可以被其他节点读取出来。且所有节点读到数据都是原子的，即不会读到数据转换的过程和中间未完成的状态。\n要想达到线性一致，需要满足以下条件：\n 任何一次读都能读取到某个数据最近的一次写的数据。 所有进程看到的操作顺序都跟全局时钟下的顺序一致。 所有进程都按照全局时钟的时间戳来区分事件的先后，那么必然所有进程看到的数据读写操作顺序一定是一样的  我们发现，这个要求十分苛刻，难以在现实中实现。因为各种物理限制使分布式数据不可能一瞬间去同步这种变化。（通信是必然有延迟的，一旦有延迟，时钟的同步就没法做到一致。）\n顺序一致性（Sequential Consistency） 由于线性一致的代价高昂，因此人们想到，既然全局时钟导致严格一致性很难实现，那么我们能否放弃了全局时钟的约束，改为分布式逻辑时钟实现呢？\nLamport 在 1979 年就提出的顺序一致性正是基于上述原理。顺序一致性中所有的进程以相同的顺序看到所有的修改。读操作未必能及时得到此前其他进程对同一数据的写更新，但是每个进程读到的该数据的不同值的顺序是一致的。\n其需要满足以下条件：\n 任何一次读写操作都是按照某种特定的顺序。 所有进程看到的读写操作顺序都保持一致。  我们发现他们都能够保证所有进程对数据的读写顺序保持一致。那么它与线性一致性有什么不同呢？\n尽管顺序一致性通过逻辑时钟保证所有进程保持一致的读写操作顺序，但这些读写操作的顺序跟实际上发生的顺序并不一定一致。而线性一致性是严格保证跟实际发生的顺序一致的。\n我们以下图为例：\n 图 a 满足了顺序一致性，但未满足线性一致性。从全局时钟来看，p2 的 read(x,0) 在 p1 的 write(x,4) 之后，但是 p1 却读出了旧的数据。因此不满足线性一致性。但是两个进程各自的读写顺序却是合理的，进程之间也没有产生冲突，因此从这两个进程的视角来看，执行流程是这样的 write(y,2)、read(x,0)、 write(x,4)、read(y,2)。此时满足顺序一致性。 图 b 满足线性一致性。因为每个读操作都读到了该变量的最新写的结果，同时两个进程看到的操作顺序与全局时钟的顺序一样。 图 c 不满足顺序一致性。因为从进程 P1 的角度看，它对变量 y 的读操作返回了结果 0。那么就是说，P1 进程的对变量 y 的读操作在 P2 进程对变量 y 的写操作之前，x 变量也如此。此时两个进程存在冲突，因此这个顺序不满足顺序一致性。  弱一致性模型 弱一致性是指系统在数据成功写入之后，不承诺立即可以读到最新写入的值，也不会具体承诺多久读到，但是会尽可能保证在某个时间级别之后，可以让数据达到一致性状态。其中包含因果一致性和最终一致性、客户端一致性。\n因果一致性（Causal Consistency）  什么是因果关系呢？\n 如果事件 B 是由事件 A 引起的或者受事件 A 的影响，那么这两个事件就具有因果关系。\n因果一致性是一种弱化的顺序一致性模型，它仅要求有因果关系的操作顺序是一致的，没有因果关系的操作顺序是随机的。\n因果一致性的条件包括：\n 所有进程必须以相同的顺序看到具有因果关系的读写操作。 不同进程可以以不同的顺序看到并发的读写操作。   我们如何确定是否具有因果关系呢？如何传播这些因果关系呢?\n 即通过逻辑时钟来保证两个写入是有因果关系的。而实现这个逻辑时钟的一种主要方式就是向量时钟。向量时钟算法利用了向量这种数据结构，将全局各个进程的逻辑时间戳广播给所有进程，每个进程发送事件时都会将当前进程已知的所有进程时间写入到一个向量中，而后进行传播。\n最终一致性（Eventual Consistency） 最终一致性是更加弱化的一致性模型，它被表述为副本之间的数据复制完全是异步的，如果数据停止修改，那么副本之间最终会完全一致。而这个最终可能是数毫秒到数天，乃至数月，甚至是“永远”。\n对于最终一致性，我们主要关注以下两点：\n 最终是多久。通常来说，实际运行的系统需要能够保证提供一个有下限的时间范围。 并发冲突如何解决。一段时间内可能数据可能多次更新，到底以哪个数据为准？通常采用最后写入成功或向量时钟等策略。  因为在数据写入与读取完全不考虑别的约束条件，因此最终一致性具有最高的并发度，经常被应用于对性能要求高的场景中。\n客户端一致性（Client-centric Consistency）  在最终一致性的模型中，如果客户端在数据不同步的时间窗口内访问不同的副本的同一个数据，会出现读取同一个数据却得到不同的值的情况。为了解决这个问题，有人提出了以客户端为中心的一致性模型。\n 客户端一致性是站在一个客户端的角度来观察系统的一致性。其保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证。\n分布式数据库中，一个节点很可能同时连接到多个副本中，复制的延迟性会造成它从不同副本读取数据是不一致的。而客户端一致性就是为了定义并解决这个问题而存在的，这其中包含了写跟随读、管道随机访问存储两大类别。\n写跟随读（Writes-Follow-Reads Consistency） 写跟随读的另一个名字是回话因果（session causal）。可以看到它与因果一致的区别是，它只针对一个客户端。 即对于一个客户端，如果一次读取到了写入的值 V1，那么这次读取之后写入了 V2。从其他节点看，写入顺序一定是 V1、V2。\n管道随机访问存储（PRAM，Pipeline Random Access Memory） 管道随机访问存储的名字来源于共享内存访问模型。其对于单个进程的写操作都被观察到是顺序的，但不同的进程写会观察到不同的顺序。\n其可拆解为以下三种一致性：\n 单调读一致性（Monotonic-read Consistency）：它强调一个值被读取出来，那么后续任何读取都会读到该值，或该值之后的值，而不会读取到旧值。 单调写一致性（Monotonic-write Consistency）：如果从一个节点写入两个值，它们的执行顺序是 V1、V2。那么从任何节点观察它们的执行顺序都应该是 V1、V2。 读你所写一致性（Read-your-writes Consistency）：一个节点写入数据后，在该节点或其他节点上是一定能读取到这个数据的。  能够同时满足以上三种一致性的即为满足 PRAM。\n PRAM、因果一致、线性一致到底有什么区别呢？\n  图 a 满足了顺序一致性与因果一致性。图上的进程都满足相同的顺序与因果关系，因此满足顺序一致性与因果一致性。 图 b 满足了因果一致性，但未满足顺序一致性。对于进程 p3 和 p4 其看到的 p1 和 p2 的执行顺序不一致，因此不满足顺序一致性。但是由于 p1 与 p2 的写入没有任何因果关系，所以此时满足因果一致性。 图 c 满足了 PRAM，但未满足因果一致性。由于 p2 的 r(x,4) 依赖于 p1 的 w(x,4)，此时两者存在因果关系。然而对于进程 p3 和 p4 而言，其看到的 p1 和 p2 执行顺序不同，因此此时并不满足因果一致性。但此时我们再来分析它们的观察顺序，此时 p4 观察的到顺序是 w(x.7)、w(x,2)、w(x,4)。而 p3 观察到的是 w(x,2)、w(x,4)、w(x,7)。尽管此时它们对不同进程写操作观察的顺序不同，但是对于同一个进程的写操作观察顺序是一致的，因此其满足 PRAM 一致性。  下图即为上面讨论的几种一致性模型的关系：\n事务隔离性 在一开始那张一致性模型图中，其实是有两个分支的，一个对应的就是数据库里面的隔离性（Isolation），另一个其实对应的是分布式系统的一致性（Consistency）。\n事务隔离是描述并行事务之间的行为，而一致性是描述非并行事务之间的行为。其实广义的事务隔离应该是经典隔离理论与一致性模型的一种混合。\n潜在问题 如下即数据库实现中遇到的各种各样的 isolation 问题。\n P0 Dirty Write（脏写）：一个事务修改了另一个尚未提交的事务已经修改的值。 P1 Dirty Read（脏读）：一个事务读取到了另一个执行到一半的事务中修改的值。 P2 Non-Repeatable Read（不可重复读）：一个事务读取过程中读到了另一个事务更新后的结果。 P3 Phantom（幻读）：某一事务 A 先挑选出了符合一定条件的数据，之后另一个事务 B 修改了符合该条件的数据，此时 A 再进行的操作都是基于旧的数据，从而产生不一致。 P4 Lost Update（丢失更新）：更新被另一个事务覆盖。 P4C Cursor Lost Update（游标丢失更新）：与 Lost Update 类似，只是发生于 cursor 的操作过程之中。 A5A Read Skew（读倾斜）：由于事务的交叉导致读取到了不一致的数据。 A5B Write Skew（写倾斜）：两个事务同时读取到了一致的数据，然后分别进行了满足条件的修改，但最终结果破坏了一致性。  隔离级别 对于分布式数据库来说，原始的隔离级别并没有舍弃，而是引入了一致性模型后，扩宽数据库隔离级别的内涵。其中共有如下数种隔离级别：\n Read Uncommitted（读未提交）：事务执行过程中能够读到未提交的修改。 Read Committed（读已提交）：事务执行过程中能够读到已提交的修改。 Monotonic Atomic View（单调原子视图）：在 Read Committed 的基础上加上了原子性的约束，观测到其他事务的修改时会观察到完整的修改。 Cursor Stability（稳定游标）：使用 cursor 读取某个数据时，这个不能被其他事务修改直至 cursor 释放或事务结束。 Snapshot Isolation（快照隔离级别）：即使其他事务修改了数据，重复读取都会读到一样的数据。 Repeatable Read（可重复读）：每个事务在独立、一致的 snapshot 上进行操作，直至提交后其他事务才可见。 Serializable（串行化）：事务按照一定的次序顺序执行。  对应的可能发生的问题如下\n    P0 P1 P4C P4 P2 P3 A5A A5B     Read Uncommitted NP P P P P P P P   Read Committed NP NP P P P P P P   Cursor Stability NP NP NP SP SP P P SP   Repeatable Read NP NP NP NP NP P NP NP   Snapshot NP NP NP NP NP SP NP P   Serializable NP NP NP NP NP NP NP NP     P（Possible）：会发生。 SP（Sometimes Possible）：有时候可能发生。 NP（Not Possible）：不可能发生。  ","date":"2022-05-24T16:40:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B/","title":"一致性模型"},{"content":"一致性协议：Gossip Gossip协议（Gossip Protocol）又称Epidemic协议（Epidemic Protocol），是基于谣言传播方式的节点或者进程之间信息交换的协议，在分布式系统中被广泛使用，比如我们可以使用Gossip协议来确保网络中所有节点的数据一致。\n 说到社交网络，就不得不提著名的六度分隔理论。1967年，哈佛大学的心理学教授Stanley Milgram想要描绘一个连结人与社区的人际连系网。做过一次连锁信实验，结果发现了“六度分隔”现象。简单地说：“你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过六个人你就能够认识任何一个陌生人。\n数学解释该理论：若每个人平均认识260人，其六度就是260↑6 =1,188,137,600,000。消除一些节点重复，那也几乎覆盖了整个地球人口若干多多倍，这也是Gossip协议的雏形。\n Gossip基于六度分隔理论，每个节点像谣言传播一样，随机的将信息传播到其他节点上，不断重复这个过程，直到将信息传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致。\nGossip主要有以下三个功能：\n 直接邮寄（Direct Mail） 反熵（Anti-entropy） 谣言传播（Rumor mongering）  直接邮寄 直接发送需要更新的数据到其他节点，当数据发送失败时，将数据缓存到队列中，然后进行重传。\n从上面可以看出，这种方法实现简单切数据同步及时，但是可能会因为重试的缓存队列满了而丢数据，从而无法实现最终一致性。\n 那么我们如何实现最终一致性呢？\n 这时候就需要借助到了Gossip协议中的反熵。\n反熵 熵指混乱程度，反熵就是消除不同节点间数据的差异，提升节点间数据的相似度。\n反熵的过程如下：\n 集群中的节点每隔一段时间就随机选取某个其他节点 互相交换数据来消除两个节点之间的差异 实现数据的最终一致性  反熵主要通过三种方式进行：\n  推（Push）\n 节点A将数据（key,value,version）推送给节点B，节点B将A中比自己新的数据更新过来。     拉（Pull）\n 节点A仅将数据（key,version）推送给 B，B将本地比A新的数据（key, value, version）推送给A，A更新本地数据。     推拉（Push/Pull）\n 同时执行上述两个步骤，同时修复两个节点的数据。      从上面的三种反熵方式可以看出，反熵是需要节点两两交换和比对自己所有的数据，这样来看的话，通讯成本是很高的，而在实际场景下这种频繁的交换会大大影响性能。\n那有没有办法来减少反熵的次数呢？\n 我们可以通过引入如校验和、奇偶校验、CRC校验和格雷码校验等机制来降低需要对比的数据量和通讯信息。\n 执行反熵时，相关节点都是已知的，且节点数量不能太多。如果节点动态变化或节点数过多，反熵就不合适。\n那在这种场景下，有没有办法来解决动态、多节点的最终一致性呢？\n 答案是有的，那这时候就要用到Gossip协议中的谣言传播。\n谣言传播 谣言传播，就像是一个谣言的产生流程一样，每个人都会向自己身边的人传播，知道谣言散布各地。\n在分布式系统中，当一个节点有了新数据后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新数据，直到所有节点都存储了该数据，可以理解为之前讲的反熵中的推的方式。\n从上面可以看出，谣言传播仍然具有以下缺点：\n 时间随机：所有节点达到一致性是一个随机性的概率。可以使用闭环反熵修复。 消息冗余：同一节点会多次接收同一消息，增加了消息处理的压力，每一次通信都会对网络带宽、CPU等资源造成负载，进而影响达到最终一致性的时间。 拜占庭问题：如果有恶意节点出现，那么其他节点也会出问题。所以需要先修复故障节点。  闭环反熵 对于谣言传播，所有节点达到一致性是一个随机性的概率，其达到最终一致性的时间并不可控，这并不满足我们的期望，我们更希望能在一个确定的时间范围内实现数据副本的最终一致性，因此Gossip协议又引入了闭环反熵。\n它按照一定顺序来修复节点的数据差异，先随机选择一个节点，顺着这个节点往下循环修复。每个节点都会对比自身与下一个节点，将本节点存在而下个节点不存在的缺失数据发送给下一个节点来进行修复，如下图：\n与上面的反熵不同，闭环反熵不再是一个节点不断随机选择另一个节点，来修复副本上的熵，而是设计了一个闭环的流程，一次修复所有节点的副本数据不一致。通过这种方法我们就能够将最终一致性的时间范围明确下来，使其可控。\n总结 上面说了那么多缺点，下面也来讲讲Gossip的几个优点\n 拓展性：网络可以允许节点的动态增加和减少，新增加的节点的状态最终会与其他节点一致。 容错：网络中任何节点的宕机和重启都不会影响 Gossip 消息的传播，Gossip 协议具有天然的分布式系统容错特性。 去中心化：Gossip协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。 实现简单：Gossip 协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了 logN。 高性能：Gossip 协议的过程极其简单，实现起来几乎没有太多复杂性。  Gossip的三种功能其实都是为了实现反熵，第一种用消息队列，第二种用推拉消息，第三种用散播谣言，下面给出三个功能的使用场景\n   功能 应用场景     直接邮寄 实际场景，直接邮寄一定要实现，性能损耗最低。通过发送更新数据或缓存重传就能修复数据的不一致。   反熵 在存储组件中，节点都是已知的，采用反熵修复数据副本的不一致。   谣言传播 集群节点变化时，或节点较多时，采用谣言传播方式，来同步更新多节点的数据，来实现最终一致性。    ","date":"2022-05-24T16:30:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEgossip/","title":"一致性协议：Gossip"},{"content":"一致性协议：ZAB ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。 在ZooKeeper中，主要依赖ZAB协议来实现分布式数据一致性，基于该协议，ZooKeeper实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。\nZAB协议包括了两种基本的模式，分别是崩溃恢复和消息广播。\n消息广播 为了保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致，ZAB协议中引入了消息广播模式。\n在上面我们提到了，ZooKeeper集群中只有Leader服务器能够执行写操作，为了保证集群的数据一致性，我们需要将Leader节点更新的数据同步到Follower与Observer服务器中，所以当Leader服务器接收到客户端发送的写请求后，会自动生成对应的提案并发起一轮消息广播。\n消息广播的执行流程如下：\n 接受到客户端发送的事务请求，Leader服务器为其生成对应的事务提议。 Leader为每一个Follower和Observer都准备了一个FIFO的队列，并把提议发送到队列上。 当Follower接收到事务提议后，都会先将其以事务日志的形式写入本地磁盘中，然后再写入成功后反馈给Leader服务器一个ACK。 当Leader接收到半数以上Follower节点的ACK，它就会认为大部分节点都同意议题，准备开始提交。 Leader向所有节点发送提交事务的Commit请求，完成事务。  为了防止因为网络等原因导致的Follower、Observer节点处理请求的顺序不同而导致的数据不一致问题，保证消息广播过程中消息接收与发送的顺序性，消息广播中引入了FIFO队列和事务ID来解决这个问题。\n 在消息广播的过程中，Leader服务器会为每一个Follower、Observer服务器都各自分配一个单独的队列，然后将需要广播的事务提议放到这些队列中，并根据FIFO策略进行消息发送。由于ZAB由于协议是通过TCP协议来进行网络通信的，这样不仅保证了消息的发送顺序性，也保证了接受顺序性。 在广播事务提议之前，Leader服务器会先给这个提议分配一个全局单调递增的唯一事务ID（ZXID）。为了保证每一个消息严格的因果关系，必须将每一个事务提议按照其ZXID的先后顺序来进行排序与处理。  如果你了解过二阶段提交（2PC）协议，你会发现其实消息广播的过程实际上就是一个简化版本的二阶段提交过程，他将二阶段提交中的中断逻辑删除，Leader服务器不需要等待集群中的全部Follower服务器都响应反馈，只需要得到过半Follower的ACK就开始执行事务的提交。这种简化版的2PC虽然提高了效率，但是无法处理Leader服务器崩溃退出而导致的数据不一致问题，因此ZooKeeper中又添加了崩溃恢复模式来解决这个问题。\n崩溃恢复 当Leader服务器出现崩溃退出或机器重启，亦或是集群中不存在半数以上的服务器与Leader服务器保持正常通信时，在重新开始新的一轮原子广播事务操作之前，此时所有节点都会使用崩溃恢复协议来使彼此达到一个一致的状态。\n 崩溃恢复过程需要确保那些已经在Leader服务器上提交的事务最终被所有的事务提交。\n 假设一个事务中Leader服务器（server2）上被提交了，并且已经得到了过半Follower服务器的ACK反馈，但是在它将Commit消息发送给所有的Follower机器之前，Leader服务器就挂掉了，如下图：\n从上图可以看到，部分的节点收到了commit请求并进行了提交，而有一部分Leader还没来得及发送就已经崩溃了。针对这种情况，崩溃恢复必须要确保该事务最终能够在所有的服务器上都被提交成功，否则将会出现数据不一致的情况。所以在重新选举的时候，必定会选取ZXID最大的节点来确保其保留了最新的事件。\n 崩溃恢复过程需要确保丢弃那些只在Leader服务器上被提出的事务。\n 如果Leader服务器在提交了一个事务之后，还没来得及广播发送commit就已经崩溃推出了，从而导致集群中的其他服务器都没有收到这个事务提议。当原先的Leader节点故障恢复后，再次以Follower的角色加入集群后，此时就因为只有它完成了事务提交，而产生了数据不一致的情况，如下图：\n针对这种情况，我们需要让server2在故障恢复后能够丢弃这些只在它这个节点上提出的事务，来确保数据一致。\n为了能够满足上述的两个要求，所以ZooKeeper让Leader选举算法保证新选举出来的Leader服务器拥有集群中所有机器最高的事务编号（ZXID最大），那么这就肯定能够保证新选举出来的Leader一定具有所有已经提交的提案，此时新的Leader就会将事务日志中尚未提交的消息同步到各个服务器中。\n","date":"2022-05-24T16:20:23+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEzab/","title":"一致性协议：ZAB"},{"content":"一致性协议：Bully Bully 是最常用的一种领导选举算法，它使用节点 ID 的大小来选举新领导者。在所有活跃的节点中，选取节点 ID 最大或者最小的节点为主节点。\n核心算法  每个节点都会获得分配给它的唯一 ID。在选举期间，ID 最大的节点成为领导者。因为 ID 最大的节点“逼迫”其他节点接受它成为领导者，它也被称为君主制领导人选举：类似于各国王室中的领导人继承顺位，由顺位最高的皇室成员来继承皇位。如果某个节点意识到系统中没有领导者，则开始选举，或者先前的领导者已经停止响应请求。\n 算法核心如下：\n 集群中每个活着的节点查找比自己 ID 大的节点，如果不存在则向其他节点发送 Victory 消息，表明自己为领导节点。 如果存在比自己 ID 大的节点，则向这些节点发送 Election 消息，并等待响应。 如果在给定的时间内，没有收到这些节点回复的消息，则自己成为领导节点，并向比自己 ID 小的节点发送 Victory 消息。 节点收到比自己 ID 小的节点发送的 Election 消息，则回复 Alive 消息。  上图举例说明了 Bully 领导者选举算法，其中：\n 节点3 注意到先前的领导者 6 已经崩溃，并且通过向比自己 ID 更大的节点发送选举消息来开始新的选举。 4 和 5 以 Alive 响应，因为它们的 ID 比 3 更大。 3 通知在这一轮中作出响应的最大 ID 节点是5。 5 被选为新领导人，它广播选举信息，通知排名较低的节点选举结果。  这种算法的一个明显问题是：它违反了“安全性”原则（即一次最多只能选出一位领导人）。在存在网络分区的情况下，在节点被分成两个或多个独立工作的子集的情况下，每个子集都会选举其领导者。（脑裂）\n该算法的另一个问题是：**它对 ID较大的节点有强烈的偏好，但是如果它们不稳定，会严重威胁选举的稳定性，并可能导致不稳定节点永久性地连任。**即不稳定的高排名节点提出自己作为领导者，不久之后失败，但是在新一轮选举中又赢得选举，然后再次失败，选举过程就会如此重复而不能结束。这种情况，可以通过监控节点的存活性指标，并在选举期间根据这些指标来评价节点的活性，从而解决该问题。\n算法改进 Bully 算法虽然经典，但由于其相对简单，在实际应用中往往不能得到良好的效果。因此在分布式数据库中，我们会看到如下所述的多种演进版本来解决真实环境中的一些问题，但需要注意的是，其核心依然是经典的 Bully 算法。\n故障转移节点列表 **我们可以使用多个备选节点作为在发生领导节点崩溃后的故障转移目标，从而缩短重选时间。**每个当选的领导者都提供一个故障转移节点列表。当集群中的节点检测到领导者异常时，它通过向该领导节点提供的候选列表中排名最高的候选人发送信息，开始新一轮选举。如果其中一位候选人当选，它就会成为新的领导人，而无须经历完整的选举。\n如果已经检测到领导者故障的进程本身是列表中排名最高的进程，它可以立即通知其他节点自己就是新的领导者。\n上图显示了采用这种优化方式的过程，其中：\n 6 是具有指定候选列表 {5，4} 的领导者，它崩溃退出，3 注意到该故障，并与列表中具有最高等级的备选节点5 联系； 5 响应 3，表示它是 Alive 的，从而防止 3 与备选列表中的其他节点联系； 5 通知其他节点它是新的领导者。  如果备选列表中，第一个节点是活跃的，我们在选举期间需要的步骤就会更少。\n节点分角色 另一种算法试图通过将节点分成候选和普通两个子集来降低消息数量，其中只有一个候选节点可以最终成为领导者。普通节点联系候选节点、从它们之中选择优先级最高的节点作为领导者，然后将选举结果通知其余节点。\n为了解决并发选举的问题，该算法引入了一个随机的启动延迟，从而使不同节点产生了不同的启动时间，最终导致其中一个节点在其他节点之前发起了选举。该延迟时间通常大于消息在节点间往返时间。具有较高优先级的节点具有较低的延迟，较低优先级节点延迟往往很大。\n上图显示了选举过程的步骤，其中：\n 节点 4 来自普通的集合，它发现了崩溃的领导者 6，于是通过联系候选集合中的所有剩余节点来开始新一轮选举； 候选节点响应并告知 4 它们仍然活着； 4 通知所有节点新的领导者是 2。  该算法减小了领导选举中参与节点的数量，从而加快了在大型集群中该算法收敛的速度。\n邀请算法 **邀请算法允许节点“邀请”其他进程加入它们的组，而不是进行组间优先级排序。**该算法允许定义多个领导者，从而形成每个组都有其自己的领导者的局面。每个节点开始时都是一个新组的领导者，其中唯一的成员就是该节点本身。\n组领导者联系不属于它们组内的其他节点，并邀请它们加入该组。如果受邀节点本身是领导者，则合并两个组；否则，受邀节点回复它所在组的组长 ID，允许两个组长直接取得联系并合并组，这样大大减少了合并的操作步骤。\n上图显示了邀请算法的执行步骤，其中：\n 四个节点形成四个独立组，每个节点都是所在组的领导，1 邀请 2 加入其组，3 邀请 4 加入其组； 2 加入节点 1的组，并且 4 加入节点3的组，1 为第一组组长，联系人另一组组长 3，剩余组成员（在本例中为 4个）获知了新的组长 1； 合并两个组，并且 1 成为扩展组的领导者。  由于组被合并，不管是发起合并的组长成为新的领导，还是另一个组长成为新的领导。为了将合并组所需的消息数量保持在最小，一般选择具有较大 ID 的组长的领导者成为新组的领导者，这样，只有来自较小 ID 组的节点需要更新领导者。\n与所讨论的其他算法类似，该算法采用“分而治之”的方法来收敛领导选举。邀请算法允许创建节点组并合并它们，而不必从头开始触发新的选举，这样就减少了完成选举所需的消息数量。\n环形算法 **在环形算法中，系统中的所有节点形成环，并且每个节点都知道该环形拓扑结构，了解其前后邻居。**当节点检测到领导者失败时，它开始新的选举，选举消息在整个环中转发，方式为：每个节点联系它的后继节点（环中离它最近的下一节点）。如果该节点不可用，则跳过该节点，并尝试联系环中其后的节点，直到最终它们中的一个有回应。\n节点联系它们的兄弟节点，收集所有活跃的节点从而形成可用的节点集。在将该节点集传递到下一个节点之前，该节点将自己添加到集合中。\n该算法通过完全遍历该环来进行。当消息返回到开始选举的节点时，从活跃集合中选择排名最高的节点作为领导者。\n如上图所示，你可以看到这样一个遍历的例子：\n 先前的领导 6 失败了，环中每个节点都从自己的角度保存了一份当前环的拓扑结构； 以 3 为例，说明查找新领导的流程，3 通过开始遍历来发起选举轮次，在每一步中，节点都按照既定路线进行遍历操作，5 不能到 6，所以跳过，直接到 1； 由于 5 是具有最高等级的节点，3 发起另一轮消息，分发关于新领导者的信息。  该算法的一个优化方法是每个节点只发布它认为排名最高的节点，而不是一组活跃的节点，以节省空间：因为 Max 最大值函数是遵循交换率的，也就是知道一个最大值就足够了。当算法返回到已经开始选举的节点时，最后就得到了 ID 最大的节点。\n另外由于环可以被拆分为两个或更多个部分，每个部分就会选举自己的领导者，这种算法也不具备“安全性”。 如前所述，要使具有领导的系统正常运行，我们需要知道当前领导的状态。因此，为了系统整体的稳定性，领导者必须保证是一直活跃的，并且能够履行其职责。\n脑裂的解决方案 在上文讨论的所有算法都容易出现脑裂的问题，即最终可能会在独立的两个子网中出现两个领导者，而这两个领导并不知道对方的存在。\n为了避免脑裂问题，我们一般需要引入法定人数来选举领导。比如 Elasticsearch 选举集群领导，就使用 Bully 算法结合最小法定人数来解决脑裂问题。\n如上图所示，目前有 2 个网络、5 个节点，假定最小法定人数是3。A 目前作为集群的领导，A、B 在一个网络，C、D 和 E 在另外一个网络，两个网络被连接在一起。\n当这个连接失败后，A、B 还能连接彼此，但与 C、D 和 E 失去了联系。同样， C、D 和 E 也能知道彼此，但无法连接到 A 和B。\n此时，C、D 和 E 无法连接原有的领导 A。同时它们三个满足最小法定人数3，故开始进行新一轮的选举。假设 C 被选举为新的领导，这三个节点就可以正常进行工作了。\n而在另外一个网络中，虽然 A 是曾经的领导，但是这个网络内节点数量是 2，小于最小法定人数。故 A 会主动放弃其领导角色，从而导致该网络中的节点被标记为不可用，从而拒绝提供服务。这样就有效地避免了脑裂带来的问题。\n","date":"2022-05-24T16:20:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEbully/","title":"一致性协议：Bully"},{"content":"一致性协议：Raft 由于Paxos算法相对来说较为复杂且难以理解，因此在后来又出现了一种用于替代Paxos的算法——Raft\nRaft 算法是一种简单易懂的共识算法，所谓共识，就是多个节点对某个事情达成一致的看法，即使是在部分节点故障、网络延时、网络分割的情况下。它依靠状态机和主从同步的方式，在各个节点之间实现数据的一致性。\nRaft算法的核心主要为以下两部分，下面将进行具体讲解\n 主节点选举（Leader Election） 数据同步（Log Replication）  状态机 在Raft中节点中存在三种状态，状态之间可以相互进行转换\n  Leader（主节点）\n  Follower（从节点）\n  Candidate（竞选节点）\n  同时，每个节点上会存放一个倒计时器（Election Timeout），时间随机在150ms到300ms之间。Leader节点会周期性的向所有Follower发送一个心跳包（Heartbeat），收到心跳包的节点会将其计时器清零后重新计时，如果在倒计时结束前没有收到Leader的心跳包，此时Follower就会变为Candidate，开始进入竞选状态。\n具体的状态转移如下图\n执行流程 主节点选举  介绍了状态机后，下面就来看看主节点的选举流程\n 第一步，在一开始时，由于没有Leader，所以此时所有节点的身份都是Follower，每一个节点上都有着自己的计数器，当计时器达到了超时时间后，该节点就会转换为Candidate，开始选举过程。\n第二步，成为Candidate的节点首先会给自己投一张票，然后向集群中的其他所有节点发起投票请求。\n第三步，收到投票请求并且还未投票的Follower节点会向发起者回复投票反馈，如果收到了超过半数的回复，此时Candidate选举成功，状态转变为Leader。\n第四步，Leader节点会立刻向其他节点发出通告，告知其他节点它已成功选举成Leader，收到通知的节点会转换为Follower。并且Leader会周期性的发送心跳包给所有Follower来表明它还存活，当Follower接收到心跳包时，就会重置计时器。\n一旦Leader节点挂掉，它就无法发出心跳包来重置Follower的倒计时器，那么当Follower的超时时间到达后，其就会转换成Candidate节点，再次重复以上过程。\n上面介绍的是单节点的选举，倘若同时有多个Follower同时倒计时结束后成为Candidate，同时开始选举，并且在选举过程中所获得的票数相同，此时就陷入了僵局，谁都无法成为Leader。\n在重新选举时，由于每个节点设置的超时时间都是随机的，因此下一次同时出现多个Candidate并获得同样票数导致选举失败的情况的概率则非常低。\n数据同步 Raft中数据同步的方式与之前写过的2PC（二阶段提交协议）有一些相似，也是分为投票和提交两个阶段，具体流程如下。\n第一步，客户端将修改传入Leader中（此时修改并没有提交，只是写入日志中）。\n第二步，Leader将修改复制到集群内的所有Follower节点上，如果复制失败，会不断进行重试直至成功。\n第三步，Follow节点们成功接收到复制的数据后，会反馈结果给Leader节点，如果Leader节点接收到超过半数的Follower反馈，则表明复制成功，于是提交自己的数据，并且通知客户端数据提交成功。\n第四步，提交成功后，Leader会通知所有的Follower让它们也提交修改，此时所有节点的值达成一致，完成数据同步流程。\n为了便于理解，可以结合下面的Raft原理动画一同学习。\nRaft原理动画\n","date":"2022-05-24T15:59:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEraft/","title":"一致性协议：Raft"},{"content":"一致性协议：Paxos 拜占庭将军问题  拜占庭位于如今的土耳其的伊斯坦布尔，是东罗马帝国的首都。由于当时拜占庭罗马帝国国土辽阔，为了达到防御目的，每个军队都分隔很远，将军与将军之间只能靠信差传消息。在战争的时候，拜占庭军队内所有将军和副官必须达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，在军队内有可能存有叛徒和敌军的间谍，叛徒可以任意行动以达到以下目标：欺骗某些将军采取进攻行动；促成一个不是所有将军都同意的决定，如当将军们不希望进攻时促成进攻行动；或者迷惑某些将军，使他们无法做出决定。如果叛徒达到了这些目的之一，则任何攻击行动的结果都是注定要失败的，只有完全达成一致的努力才能获得胜利。\n这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，拜占庭问题就此形成 。\n 拜占庭将军问题的核心在于在缺少可信任的中央节点和可信任的通道的情况下，分布在不同地方的各个节点应如何达成共识。我们可以将这个问题延伸到分布式计算领域中。\n在分布式计算领域中，试图在异步系统和不可靠的通道上达到一致性状态是不可能的，因此在对一致性的研究过程中，都往往假设信道是可靠的，而事实上，大多数系统都是部署在同一个局域网中，因此消息被篡改的情况非常罕见；另一方面，由于机器硬件和网络原因导致的消息不完整问题，也仅仅只需要一套简单的校验算法即可避免。\n那么当我们假设拜占庭问题不存在（所有消息都是完整的，没有被篡改），那么这种情况下需要什么算法来保证一致性呢？拜占庭将军问题的提出者Lamport提出了一种非拜占庭将军问题的一致性解决方案——Paxos算法\nPaxos 问题描述 与拜占庭将军问题一样，Lamport同样使用故事的方式来提出Paxos问题\n 希腊岛屿Paxon上的议员在议会大厅中表决通过法律，并通过服务员传递纸条的方式交流信息，每个议员会将通过的法律记录在自己的账目上。问题在于执法者和服务员都不可靠，他们随时会因为各种事情离开议会大厅，并随时可能有新的议员进入议会大厅进行法律表决，使用何种方式能够使得这个表决过程正常进行，且通过的法律不发生矛盾。\n 不难看出故事中的议会大厅就是分布式系统，议员对应节点或进程，服务员传递纸条的过程就是消息传递的过程，法律即是我们需要保证一致性的值。议员和服务员的进出对应着节点/网络的失效和加入，议员的账目对应节点中的持久化存储设备。上面表决过程的正常进行可以表述为进展需求：当大部分议员在议会大厅呆了足够长时间，且期间没有议员进入或者退出，那么提出的法案应该被通过并被记录在每个议员的账目上。\nPaxos算法需要解决的问题就是如何在上述分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。\n为了能够使得表决过程正常进行，且通过的法律不发生矛盾，那么我们需要保证以下几点\n 在这些被提出的提案中，最后只能有一个被选定 如果没有提案被提出，那么就不会有被选定的提案 当一个提案被选定后，节点们应该可以获取被选定的提案信息  在Paxos算法中，主要有以下三种角色，并且一个节点可能同时担任几种角色\n 提议者（Proposer）：负责提出提议； 接受者（Acceptor）：对每个提议进行投票； 告知者（Learner）：被告知投票的结果，不参与投票过程。  同时，假设不同参与者之间通过收发消息来进行通信，那么我们需要具备以下条件\n 每个参与者可能会因为出错而导致停止、重启等情况， 虽然消息在传输过程中可能会出现不可预知的延迟、重复、丢失等情况，但是消息不会被损坏或篡改（即不存在拜占庭问题）  执行过程 首先我们规定一个提议包含两个字段：[n, v]，其中 n 为序号（具有唯一性），v 为提议值。\nPaxos执行过程主要分为两个阶段，与之前讲过的2PC（二阶段提交协议）类似。\n Prepare阶段 Accept阶段  如下图\nPrepare阶段  首先，每个Proposer都会向所有的Acceptor发送一个Prepare请求。 当Acceptor接收到一个Prepare请求，提议内容为[n1,v1]，由于之前还未接受过Prepare请求，那么它会返回一个[no previous]的Prepare响应，并且设置当前接受的提议为[n1,v1]，同时保证以后不会再接收序号小于n1的提议。 如果Acceptor再次收到一个Prepare请求，提议内容为[n2,v2]，此时会有两种情况。  如果n2 \u0026lt; n1，此时Acceptor就会直接抛弃该请求 如果n2 \u0026gt;= n1，此时Acceptor就会发送一个[n1,v1]的Prepare响应，设置当前接受到的提议为[n2,v2]，同时保证与上一步逻辑相同，保证以后不会再接受序号小于n2的提议。    Accept阶段  当一个Proposer接收到超过一半的Acceptor的Prepare响应时，此时它就会发送一个针对[n,v]提案的Accept请求给Acceptor。 如果一个Acceptor收到一个编号为n的提案的Accept请求，此时有两种情况。  如果该Acceptor没有对编号大于n的Prepare请求做出过响应，它就会接受该提案，并发送Learn提议给Learner 如果该Acceptor接受过编号大于n的Prepare请求，那么它就会拒绝、不回应或回复error。（如果一个Proposer没有收到过半的回应，那他就会重新进入第一阶段，递增提案号后重新提出Prepare请求）    在上述过程中，每一个Proposer都有可能会产生多个提案。但只要每个Proposer都遵循如上述算法运行，就一定能保证算法执行的正确性。\nLearner获取提案 在Accept阶段之后Acceptor选定提案后，根据具体的应用场景不同，Learner主要采用以下三种方案来学习选定的提案\n   方案 优点 缺点     Acceptor直接将提议发给所有的Learner Learner能够快速获取被选定的提议 通信次数过多，每个Acceptor都要和Learn产生通信（M * N）   Acceptor接受提议后将提议发给主Learner，主Learner再通知其他Learner 通信次数减少（M + N - 1） 单点问题（主Learner出现故障就会崩溃）   Acceptor将提议发一个Learner集合，Learener集合再发给其他Learener 解决了单点问题，集合中Learner个数越多就越可靠 网络通信复杂度变高    当Learner们发现有大多数的Acceptor接受了某一个提议，那么该提议的提议值则就是Paxos最终选择出来的结果。\n活锁问题 在Paxos算法实际运作的时候还存在这样一种极端的情况——当有两个Proposer依次提出了一系列编号递增的提案，此时就会导致陷入死循环，无法完成第二阶段，也就是无法选定一个提案，如以下场景。\n Proposer P1发出编号为n1的Prepare请求，收到过半响应，完成了阶段一的流程。 同时，Proposer P2发出编号为n2的Prepare请求（n2 \u0026gt; n1），也收到了过半的响应，完成了阶段一的流程，并且Acceptor承诺不再接受编号小于n2的提案。 P1进入第二阶段时，由于Acceptor不接受小于n2的提案，所以P1重新回到第一阶段，递增提案号为n3后重新发出Prepare请求 紧接着，P2进入第二阶段，由于Acceptor不接受小于n3的提案，此时它也重新回到第一阶段，递增提案号后重新发出Prepare请求 于是P1、P2陷入了死循环，谁都无法完成阶段二，这也就导致了没有value能被选定。  为了保证Paxos算法的可持续性，以避免陷入上述提到的死循环，就必须选择一个主Proposer，并规定只有主Proposer才能够提出提案。这样一来，只要主Proposer和过半的Acceptor能够正常进行网络通信，那么肯定会有一个提案被批准（第二阶段的accept），则可以解决死循环导致的活锁问题。\n","date":"2022-05-24T15:58:13+08:00","permalink":"https://blog.orekilee.top/p/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEpaxos/","title":"一致性协议：Paxos"},{"content":"分布式锁 随着互联网技术的不断发展、数据量的大幅增加、业务逻辑的复杂化导致传统的集中式系统已经无法应用于当前的业务场景，因此分布式系统被应用在越来越多的地方，但是在分布式系统中，由于网络、机器（如网络延迟、分区，机器宕机）等情况导致场景更加复杂，充满了不可靠的情况。为了保证一致性，在这种情况下我们就需要用到分布式锁。\n 那么分布式锁需要具备哪些条件呢？\n  获取、释放锁的性能要高 判断锁的获取操作必须要是原子的（防止同一个锁被多个节点获取） 网络或者机器出现问题导致无法继续工作时，必须要释放锁（防止死锁） 可重入的，一个线程可以多次获取同一把锁（防止死锁） 阻塞锁（依据业务需求）  但是目前并没有能够满足上面所有要求的完美结局方案，对于分布式锁，我们通常使用以下三种方法来实现\n 数据库 Redis(缓存) Zookeeper  数据库 唯一索引 我们可以利用数据库中的唯一索引来实现。由于唯一索引能够保证记录只被插入一次，因此我们可以利用其判断当前是否处于锁定状态。所以当想要获取锁的时候，就向数据库中插入一条记录，而释放锁的时候就删除这条记录即可。\n但是该方法存在以下问题\n 锁没有失效时间，如果解锁失败的话其他进程无法再获得该锁（死锁） 非阻塞锁，插入失败就直接报错，没有办法进入队列重试 不可重入，同一线程在没有释放锁之前无法重复获得该锁  对于数据库来说我们还可以选择使用排他锁、乐观锁等方法来实现分布式锁，但是由于这些方法对原表有侵入、占用数据库连接等情况，一般情况下都不做考虑，因此这里也就不详细描述。\nRedis SETNX、EXPIRE 我们可以利用setnx(set if not exist)命令来实现锁。只有在缓存中不存在Key的时候才会set并返回true，而Key已存在的时候就直接返回false。同时为了防止获取锁失败而导致的死锁情况，我们可以利用expire命令对这个key设置一个超时时间。\n为了防止我们setnx成功之后线程发生异常中断导致我们来不及设置expire而导致死锁，我们通常会使用以下命令来设置\n1 2 3 4 5 6 7 8  SETkeyrandom_valueNXEX30000/* EX seconds – 设置键key的过期时间，单位时秒 PX milliseconds – 设置键key的过期时间，单位时毫秒 NX – 只有键key不存在的时候才会设置key的值 XX – 只有键key存在的时候才会设置key的值 */  该命令仅在Key不存在（NX选项）时才插入，并且设置到期时间为30000毫秒（PX选项），value设置为随机值，该值在所有客户端和所有锁定请求中必须唯一（防止被他人误删）。\n当我们想要释放锁时，为了保证安全（防止误删除另一个客户端创建的锁），仅当密钥存在且存储在密钥上的值恰好是期望的值时，才删除该密钥，下面是以lua脚本完成的删除逻辑\n1 2 3 4 5  if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end   这种方法虽然实现起来非常简单，但是其存在着单点问题，它加锁时只作用于一个Redis节点上，如果该节点出现故障故障，即使使用哨兵来保证高可用，也会出现锁丢失的情况，如下场景\n 在Redis的Master节点拿到锁，此时锁还没有同步到Slave节点 此时Master发生故障，哨兵主导进行故障转移，Slave节点升级为Master节点 由于锁没来得及同步，因此导致锁丢失  考虑到这种情况，Redis作者antirez基于分布式环境下提出了一种更高级的分布式锁的实现方式：Redlock。\nRedLock算法 Redlock 是 Redis 的作者 antirez 给出的集群模式的 Redis 分布式锁，它基于 N 个完全独立（不存在主从复制或者其他集群协调机制）的 Redis节点（通常情况下 N 设置成 5，为了资源的合理利用通常为奇数）。\n算法的流程如下\n 获取当前时间 客户端依次尝试从5个(奇数)相互独立的Redis实例中使用相同的key和具有唯一性的value获取锁 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从**大多数（N / 2 + 1）**实例上获取了锁，才认为获取锁成功； 如果获取了锁，则重新计算有效期时间，即原有效时间减去获取锁消耗的时间 如果客户端获取锁失败，则释放所有实例上的锁  虽然RedLock算法比上面的单点Redis锁更可靠，但是由于分布式的复杂性，实现起来的条件也更加的苛刻。\n 由于必须获取**（N / 2 + 1）**个节点上的锁，所以可能会出现锁冲突的情况（即每个人都获取了一些锁，但是没有人获取一半以上的锁）。针对这个问题，其借鉴了Raft算法的思路，即产生冲突后为每个节点设置一个随机开始时间，在时间到后重新尝试获取锁，但是这也导致了获取锁的成本增加。 如果5个节点有2个宕机，锁的可用性会极大降低，因为必须等待这两个宕机节点的结果超时才能返回。另外只剩3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，加锁难度也加大了。 如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况。  介于以上情况，我们还可以选择更加可靠的方法，即Zookeeper实现的分布式锁。\nZookeeper Zookeeper是一个为分布式应用提供一致性服务的软件，它内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名。\n由于Zookeeper同样没有实现锁API，所以我们利用其数据节点来表示锁，数据节点分为以下三种类型\n 永久节点：节点创建后永久存在，不会因为会话的消失而消失 临时节点：与永久节点相反，当客户端结束会话后立即删除 顺序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。  实现原理 除了上面介绍的节点外，我们还需要用到Watcher（监视器）来注册对节点状态的监听\n Watcher：注册一个该节点的监视器，当节点状态发生变化时，Watcher就会触发，此时Zookeeper将会向客户端发送一条通知（Watcher只能被触发一次）  根据上述特性，我们就可以使用临时顺序节点与Watcher来实现分布式锁\n 创建一个锁目录/lock 当需要获取锁时，就在lock目录下创建临时顺序节点 获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则到lock目录下注册一个子节点变更的Watcher监听，获得子节点的变更通知后重复此步骤直至获得锁 执行完业务逻辑后，主动删除自己的节点来释放锁。此时会触发其他节点的Watcher，让其他人获取锁。   那如果出现网络中断或者机器宕机，锁还能释放吗？\n 这里需要注意的是，我们使用的是临时节点，所以当客户端因为某种原因无法继续工作时，就会导致会话的中断，临时节点就会被Zookeeprer自动删除。这也就是Zookeeper相较于Redis更加可靠的原因。\n羊群效应 上面这个实现方法，大体上能够满足一般的分布式集群竞争锁的需求，并且能够保证一定的性能。但是随着机器规模的扩大，其效率会越来越低。\n为什么呢？我们思考一下锁的释放流程\n 在我们获取锁失败后，会注册一个对lock目录的Watcher监控，当有节点变更消息时，就会通知给所有注册了的机器。然而这个通知除了使序号最小的节点获取锁外，对其他的节点没有产生任何实际作用。\n 性能瓶颈的原因就是上面这个问题，大量的Watcher通知和子节点列表获取两个操作重复运行，并且绝大多数运行结果都是判断出自己并非是序号最小的节点，从而继续等待下一次的通知，浪费了大量的资源。\n如果集群规模较大，不仅会对Zookeeper服务器造成巨大的性能影响和网络冲击，更严重的时候甚至会因为多个节点对应的客户端同时释放锁导致大量的节点消失，从而短时间内向剩余客户端发送大量的事件通知——这就是所谓的羊群效应。\n改进方法 羊群效应出现根源在于其没有找到事件的真正关注点，对于分布式锁的竞争过程来说，它的核心逻辑就是判断自己是否是所有子节点中序号最小的。那么问题就简单了，我们只需要关注比自己序号小的那一个相关节点的变更情况就可以了，而不再需要关注全局的子列表变更情况。\n于是，改进后的获取流程如下\n 创建一个锁目录/lock 当需要获取锁时，就在lock目录下创建临时顺序节点 获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则到Watcher自己的次小节点（防止羊群效应） 执行完业务逻辑后，主动删除自己的节点来释放锁，此时会触发顺序的下一个节点的Watcher  总结    方案 优点 缺点 应用场景     数据库 直接使用数据库，操作简单 分布式系统的性能瓶颈大部分都在数据库，而使用数据库锁加大了负担 业务逻辑简单，对性能要求不高   Redis 性能较高，且实现起来方便 锁超时机制不可靠，当线程获取锁时，可能因为处理时间过长导致锁超时失效 追求高性能，允许偶发的锁失效问题   ZooKeeper 不依赖超时时间释放锁，可靠性高 由于频繁的创建和删除节点，性能比不上Redis锁 系统要求高可靠性    ","date":"2022-05-24T15:54:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","title":"分布式锁"},{"content":"分布式ID ID是数据的唯一标识，传统的做法是使用数据库的自增ID，但是随着业务规模的不断发展，数据量将越来越大，于是需要进行分库分表，而分表后，每个表中的ID都会按自己的节奏进行自增，很有可能出现ID冲突的情况。这时就需要一个单独的机制来负责生成一个全局唯一的ID。这个ID也可以叫做分布式ID。\n对于一个合格的分布式ID，它应该满足以下几项条件\n 全局唯一 高可用 高性能 趋势递增 方便接入  下面就介绍几种常见的分布式ID生成方案\n数据库 自增ID 我们可以利用数据库的auto_increment自增ID来生成分布式ID，只需要一个数据库实例即可完成。\n建表如下\n1 2 3 4 5 6 7 8  CREATEDATABASE`SEQID`;CREATETABLESEQID.SEQUENCE_ID(idbigint(20)unsignedNOTNULLauto_increment,stubchar(10)NOTNULLdefault\u0026#39;\u0026#39;,PRIMARYKEY(id),UNIQUEKEYstub(stub))ENGINE=MyISAM;  我们通过往该表中插入数据，并获取到自增的主键id。为了考虑到并发的安全 ，我们可以通过事务来完成这个操作。\n1 2 3 4  begin;insertintoSEQUENCE_ID(value)VALUES(\u0026#39;values\u0026#39;);selectlast_insert_id();commit;  我们可以看出，这种依赖单点的方法虽然实现起来简单，但是这种依赖单点的方法并不可靠，因为Mysql并不能很好的支持高并发，当请求ID量特别大的时候就会因为单点的宕机而影响到整个业务系统。\n多主模式 既然单点不可靠，那么我们可以考虑使用集群的方式来解决这个问题。考虑到单个主节点可能会因为宕机而没有将数据同步到从节点，可能会导致ID重复的情况，因此我们可以考虑使用多主集群。\n为了防止多个主节点生成重复的ID，我们通常会通过控制初始值和步长来避免这个问题。\n例如在有两个主节点的情况下，我们就可以通过这种方式来错开ID。\n1 2 3 4 5 6 7  -- 节点1 set@@auto_increment_offset=1;-- 起始值 set@@auto_increment_increment=2;-- 步长 -- 节点2 set@@auto_increment_offset=2;-- 起始值 set@@auto_increment_increment=2;-- 步长   但是，这种方法的可拓展性不强，倘若我们要往集群中增加新的主节点时，我们就需要重新去设置步长，并且还需要根据前两个节点的自增ID的大小来考虑我们新节点的起始值，而这些操作只能人工来进行。如果在修改步长的时候出现了重复的ID，此时就还需要进行停机修改。\n号段模式 前面两种方法的局限性在于其每次获取ID时都需要直接访问数据库，效率较低，如果能够一次获取大量的ID，并将其缓存在本地，那样就可以大大的提升ID获取的效率，这也是号段模式的核心思想。号段模式每次从数据库中批量的获取一段自增ID，即取出一个范围的ID交给号段服务维护。例如（1，2000]即代表着2000个ID。我们的业务系统只需要到号段服务中申请ID，不需要每次都去请求数据库，直到所有的ID都用完后才会去申请下一个号段。\n数据库表修改如下\n1 2 3 4 5 6 7 8  CREATETABLEid_generator(idint(10)NOTNULL,max_idbigint(20)NOTNULLCOMMENT\u0026#39;当前最大id\u0026#39;,stepint(20)NOTNULLCOMMENT\u0026#39;号段的步长\u0026#39;,biz_typeint(20)NOTNULLCOMMENT\u0026#39;业务类型\u0026#39;,versionint(20)NOTNULLCOMMENT\u0026#39;版本号\u0026#39;,PRIMARYKEY(`id`))  号段服务不再强依赖数据库，即使数据库不可用，号段服务也可以继续工作直到申请的ID全部使用完。但是如果此时号段服务重启，就会导致剩余的ID丢失。\n为了保证号段服务的高可用，我们同样需要建立一个集群，在请求方从号段服务获取ID时，就会随机的选取一个节点来获取，而这种并发场景下我们同样需要考虑到并发安全的问题，因此我们上面的表中也提供了一个版本号的字段version，我们可以使用乐观锁来进行并发的控制。\n1  updateid_generatorsetcurrent_max_id=#{newMaxId},version=version+1whereversion=#{version}  我们可以使用上面的SQL来获取新号段，当update更新成功就说明号段获取成功了。\n雪花算法 雪花算法（Snowflake） 是twitter开源的一个分布式ID的生成算法，它的核心思想是：生成一个long类型的ID，一个long大小8字节，一个字节8个比特位，因此它使用64个比特位来确定一个分布式ID。其中41bit代表时间戳，10bit标识一台机器，剩下12bit则用来标识每个id 第1个bit位为符号位，因为生成的id通常为正数所以固定为0 2~42位为时间戳部分，其精确至毫秒。同时为了更加合理的利用，其并不会存储当前的时间，而是使用时间戳的差值（当前时间 - 固定的开始时间），这样就可以保证ID从更小的起点开始生成。 43~52位为工作机器的id，这里的计算会更加灵活，可以根据机房数量、机器数量来自行均衡，保证能够利用到更多的机器。 53~64位为序列编号，在同一毫秒中的同一台机器上我们可以生成4096个ID  考虑到不同的业务场景以及各个公司的特性，大多数公司并不会去直接使用snowflake，而是会对其进行改造，让其更贴合自身的使用场景。如百度的uid-generator、美团的Leaf、滴滴的TinyId等。\n下面是github上开源的一个java实现的snowflake\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101  /** * twitter的snowflake算法 -- java实现 * github链接：https://github.com/beyondfengyu/SnowFlake * @author beyond * @date 2016/11/26 */ public class SnowFlake { /** * 起始的时间戳 */ private final static long START_STMP = 1480166465631L; /** * 每一部分占用的位数 */ private final static long SEQUENCE_BIT = 12; //序列号占用的位数  private final static long MACHINE_BIT = 5; //机器标识占用的位数  private final static long DATACENTER_BIT = 5;//数据中心占用的位数  /** * 每一部分的最大值 */ private final static long MAX_DATACENTER_NUM = -1L ^ (-1L \u0026lt;\u0026lt; DATACENTER_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L \u0026lt;\u0026lt; MACHINE_BIT); private final static long MAX_SEQUENCE = -1L ^ (-1L \u0026lt;\u0026lt; SEQUENCE_BIT); /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATACENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT; private final static long TIMESTMP_LEFT = DATACENTER_LEFT + DATACENTER_BIT; private long datacenterId; //数据中心  private long machineId; //机器标识  private long sequence = 0L; //序列号  private long lastStmp = -1L;//上一次时间戳  public SnowFlake(long datacenterId, long machineId) { if (datacenterId \u0026gt; MAX_DATACENTER_NUM || datacenterId \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;datacenterId can\u0026#39;t be greater than MAX_DATACENTER_NUM or less than 0\u0026#34;); } if (machineId \u0026gt; MAX_MACHINE_NUM || machineId \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;machineId can\u0026#39;t be greater than MAX_MACHINE_NUM or less than 0\u0026#34;); } this.datacenterId = datacenterId; this.machineId = machineId; } /** * 产生下一个ID * * @return */ public synchronized long nextId() { long currStmp = getNewstmp(); if (currStmp \u0026lt; lastStmp) { throw new RuntimeException(\u0026#34;Clock moved backwards. Refusing to generate id\u0026#34;); } if (currStmp == lastStmp) { //相同毫秒内，序列号自增  sequence = (sequence + 1) \u0026amp; MAX_SEQUENCE; //同一毫秒的序列数已经达到最大  if (sequence == 0L) { currStmp = getNextMill(); } } else { //不同毫秒内，序列号置为0  sequence = 0L; } lastStmp = currStmp; return (currStmp - START_STMP) \u0026lt;\u0026lt; TIMESTMP_LEFT //时间戳部分  | datacenterId \u0026lt;\u0026lt; DATACENTER_LEFT //数据中心部分  | machineId \u0026lt;\u0026lt; MACHINE_LEFT //机器标识部分  | sequence; //序列号部分  } private long getNextMill() { long mill = getNewstmp(); while (mill \u0026lt;= lastStmp) { mill = getNewstmp(); } return mill; } private long getNewstmp() { return System.currentTimeMillis(); } public static void main(String[] args) { SnowFlake snowFlake = new SnowFlake(2, 3); for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; 12); i++) { System.out.println(snowFlake.nextId()); } } }   Redis 我们可以利用Redis中的incr命令来原子的获取自增ID\n1 2 3 4  127.0.0.1:6379\u0026gt;setseq_id1//初始化自增IDOK127.0.0.1:6379\u0026gt;incrseq_id//自增并返回结果(integer)2  使用Redis实现起来特别简单且高效，但是我们还需要考虑到持久化时带来的一些问题\n RDB持久化：由于其是定期保存一次数据库的快照，为了保证效率他也存在着一定的时间间隔。倘若在我们刚保存一次快照后，连续获取了几次ID，而此时还没来得及做下一次持久化就宕机了。当我们通过持久化重启Redis后，这段时间生成的ID就会被重复使用。 AOF持久化：AOF相当于是逻辑日志，其会通过保存我们执行过的命令来进行持久化。它并不像RDB出现一段时间的数据丢失而导致的ID重复的情况，但是在它恢复的过程中需要重新执行保存的命令，因此随着ID数量的增多，它重启恢复数据的时间也会越来越慢。  总结     优点 缺点     数据库自增ID 实现简单，ID单调自增，数值类型查询效率高 单点问题，在高并发时可能会有宕机的风险   数据库多主集群 解决了单点问题，一定程度上提高了稳定性 可拓展性不强，随着业务规模的不断扩大， 集群也会随之增加，但是新主节点的加入较为麻烦，需要人工操作   号段模式 不强依赖数据库，提高了效率 当为了保证高可用而使用多主集群时，仍然需要去修改起始值和步长   雪花算法 1.可以根据业务特性自由分配比特位，较为灵活。2.不依赖第三方系统，独立部署 强依赖机器时钟，如果出现时钟回拨则会导致系统不可用   Redis 实现起来简单且高效 持久化恢复存在问题，如RDB重复ID，AOF速度慢    ","date":"2022-05-24T15:50:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8Fid/","title":"分布式ID"},{"content":"分布式事务 分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于分布式系统的不同节点之上。\n简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性\n2PC（二阶段提交协议） 在分布式系统中，每一个机器节点虽然都能够明确地知道自己在进行事务操作过程中的结果是成功或失败，但是却无法直接获取到其他分布式节点的操作结果。\n为了保证事务处理的ACID特性，就需要引入一个称为协调者的组件来统一调度所有分布式节点的执行逻辑，这些被调度的分布式节点则被称为参与者。协调者负责调度参与者的行为，最终决定这些参与者是否要把事务真正进行提交，基于这个思想，衍生出了二阶段提交和三阶段提交两种协议。\n2PC是Two-Phase Commit的缩写，即二阶段提交，是为了使基于分布式系统架构下的所有节点在进行事务处理过程中能够保持原子性和一致性而设计的一种算法。目前绝大多数关系型数据库都是使用二阶段提交协议来完成分布式事务处理的。\n执行流程 顾名思义，二阶段提交协议就是将事务的提交过程分成了两个阶段来进行处理，在第一阶段的主要内容就是进行投票，来表明是否有继续执行事务的必要。\n阶段一：提交事务请求（投票阶段）\n 事务询问：协调者向所有参与者发出事务询问，询问是否可以执行事务操作，并等待各参与者的响应。 执行事务：各个参与者节点执行事务操作，并将undo和redo信息记入事务日志中。 各参与者向协调者反馈事务询问的响应：如果参与者成功执行事务，则反馈YES响应，表示事务执行成功。如果参与者执行事务失败，则反馈NO响应，表示事务执行失败。  在第二阶段中，会根据第一阶段参与者的反馈来决定是否能够提交事务，要么全都成功，要么全都失败。\n阶段二：执行事务提交（执行阶段）\n 执行事务提交  发送提交请求：协调者向所有参与者发起提交请求。 事务提交：参与者在收到提交请求后，会正式执行事务提交操作。 反馈事务提交结果：参与者在完成事务提交之后向协调者发送ACK消息。 完成事务：协调者接收到所有参与者的ACK后完成事务。   中断事务  发送回滚请求：协调者向所有参与者发起回滚请求。 事务回滚：参与者在收到回滚请求后，利用阶段一记录的undo信息来执行事务回滚操作。 反馈事务回滚结果：参与者在完成事务回滚之后向协调者发送ACK消息。 中断事务：协调者接收到所有参与者的ACK后完成事务中断。    优缺点  优点  原理简单 实现方便   缺点  同步阻塞：在执行过程中，所有参与该事务操作的逻辑都会处于阻塞状态，也就是说各个参与者在等待其他参与者响应的过程中将无法执行其他操作。 单点问题：在上述过程中，协调者起到了核心的调度作用。一旦协调者出现了问题，那么整个提交流程将无法运转，甚至如果在二阶段的提交流程中出现了问题，将导致其他参与者都处于锁定事务资源的状态中，无法完成事务。 数据不一致：倘若在第二阶段的提交过程中，协调者向参与者发送提交请求，而由于网络原因或者是协调者本身的原因，导致只有部分参与者收到了提交请求，此时就导致了只有接收到请求的参与者进行了事务提交，而产生数据不一致的问题。 过于保守：二阶段提交协议没有设计较为完整的容错机制，任意一个节点的失败都会导致整个事务的失败    二阶段提交协议存在着上述几种缺陷，因此研究者在二阶段协议的基础上进行了改进\n3PC（三阶段提交协议） 3PC是Three-Phase Commit的缩写，即三阶段提交，是2PC的改进版，其将二阶段提交协议的提交事务请求过程一分为二，形成了由CanCommit、PreCommit、DoCommint三个阶段组成的事务处理协议，其协议设计如下图所示 执行流程 阶段一：CanCommit\n 事务询问 各参与者向协调者反馈事务询问的响应  阶段二：PreCommit\n 执行事务预提交  发送预提交请求 事务预提交 各参与者向协调者反馈事务执行的结果   中断事务  发送中断请求 中断事务    阶段三：DoCommit\n 执行提交  发送提交请求 事务提交 反馈事务提交结果 完成事务   中断事务  发送中断请求 事务回滚 反馈事务回滚结果 中断事务    需要注意的是，在阶段三中可能会出现以下两种问题\n 协调者出现问题 协调者和参与者之间的网络出现故障  无论出现上述那种问题，最终都会导致参与者无法及时的接收到来自协调者的DoCommit或是Abort请求，针对这种异常情况，参与者都会在等待超时后继续进行事务提交。\n优缺点  优点：相较于二阶段提交协议，降低了参与者的阻塞范围，并且能够在出现单点故障后继续达成一致。 缺点：三阶段提交协议在去除阻塞的同时也引入了新的问题，那就是参与者在收到预提交的消息时，如果出现了网络分区的情况，协调者与参与者无法进行正常的网络通信，但是参与者依旧会进行事务的提交，从而导致数据的不一致。  本地消息表（异步确保） 本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。\n 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。 之后将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。  ","date":"2022-05-24T15:48:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","title":"分布式事务"},{"content":"分布式时钟 逻辑时钟与物理时钟 对于分布式系统来说，时钟分为逻辑时钟与物理时钟两种。物理时钟对应的是我们真实世界的时间，一般由操作系统提供，而逻辑时钟则一般被实现为一个单调递增的计数器。\n 为什么在分布式系统中不直接使用物理时钟，而是使用逻辑时钟呢？\n 对于分布式系统而言，即使我们可以使用一些工具去同步集群内的时间，但是使所有节点的时间保持一致这个目标依旧是很难达成的。而如果我们使用不同节点产生的不一致的物理时间来进行一致性计算，就会导致结果出现很大的偏差，因此分布式系统就通过另外的方法来记录事件的顺序关系，也就是上面提到的逻辑时间。\n如何实现逻辑时钟? Lamport timestamps  Leslie Lamport 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为 Lamport 时间戳（Lamport timestamps）。\n 分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport 时间戳原理如下图：\n 每个事件对应一个 Lamport 时间戳，初始值为0 如果事件在节点内发生，时间戳加1 如果事件属于发送事件，时间戳加1并在消息中带上该时间戳 如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1  假设有事件 a、b，C(a)、C(b )分别表示事件 a、b 对应的 Lamport 时间戳，如果 a 发生在b 之前，记作 a⇒b，则有C(a)\u0026lt;C(b)，例如图1中有 C1→B1，那么 C(C1)\u0026lt;C(B1)。通过该定义，事件集中 Lamport 时间戳不等的事件可进行比较，我们获得事件的偏序关系(partial order)。注意：如果C(a)\u0026lt;C(b)，并不能说明a⇒b，也就是说C(a)\u0026lt;C(b是a⇒b的必要不充分条件\n如果 C(a) = C(b)，那a、b事件的顺序又是怎样的？值得注意的是当 C(a) = C(b)的时候，它们肯定没有因果关系，所以它们之间的先后顺序其实并不会影响结果，我们这里只需要给出一种确定的方式来定义它们之间的先后就能得到全序关系。注意：Lamport逻辑时钟只保证因果关系（偏序）的正确性，不保证绝对时序的正确性。\n通过以上定义，我们可以对所有事件排序，获得事件的全序关系。以上图例子，我们可以进行排序： C1 ⇒ B1 ⇒ B2 ⇒ A1 ⇒ B3 ⇒ A2 ⇒ C2 ⇒ B4 ⇒ C3 ⇒ A3 ⇒ B5 ⇒ C4 ⇒ C5 ⇒ A4\n观察上面的全序关系你可以发现，从时间轴来看 B5 是早于 A3 发生的，但是在全序关系里面我们根据上面的定义给出的却是 A3 早于 B5，可以发现 Lamport 逻辑时钟是一个正确的算法，即有因果关系的事件时序不会错，但并不是一个公平的算法，即没有因果关系的事件时序不一定符合实际情况。\nVector clock 因此 Vector clock 就在 Lamport timestamps 的基础上加以改进，它通过 vector 结构记录了本节点与其他节点的 Lamport timestamps。其原理如下图\n从上图可以看出，Vector clock 与 Lamport timestamps 的规则几乎一模一样。\n假设有事件 a、b 分别在节点 P、Q 上发生，Vector clock 分别为 Ta、Tb，如果 Tb[Q] \u0026gt; Ta[Q] 并且 Tb[P] \u0026gt;= Ta[P]，则a发生于b之前，记作a⇒b。到目前为止还和 Lamport timestamps 差别不大，那 Vector clock 怎么判别同时发生关系呢？\n如果 Tb[Q] \u0026gt; Ta[Q] 并且 Tb[P] \u0026lt; Ta[P]，则认为 a、b 同时发生，记作 a \u0026lt;=\u0026gt; b。例如图 2 中节点 B 上的第4 个事件 (A:2，B:4，C:1) 与节点 C 上的第 2 个事件 (B:3，C:2) 没有因果关系、属于同时发生事件。\nVersion vector 基于 Vector clock 我们可以获得任意两个事件的顺序关系，或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突。这时，就诞生了 Version vector。\n分布式系统中为了保证可用性，数据往往存在多个副本，而这多个副本又可能会被同时更新，这就会导致副本之间产生数据不一致的情况。Version vector 的目的就是为了发现这些数据冲突，其实现与 Vector clock类似，下图则是其具体运作流程。\n client端写入数据，该请求被Sx处理并创建相应的 vector ([Sx, 1])，记为数据 D1 第 2 次请求也被 Sx 处理，数据修改为 D2，vector 修改为 ([Sx, 2]) 第 3、第 4 次请求分别被 Sy、Sz 处理，client 端先读取到 D2，然后 D3、D4 被写入 Sy、Sz 第 5 次更新时 client 端读取到 D2、D3和D4 3个数据版本，通过类似 Vector clock 判断同时发生关系的方法可判断 D3、D4 存在数据冲突，最终通过一定方法解决数据冲突并写入 D5  Vector clock只用于发现数据冲突，不能解决数据冲突。解决数据冲突因场景而异，具体方法有以最后更新为准(last write win)，或将冲突的数据交给client由client端决定如何处理，或通过quorum决议事先避免数据冲突的情况发生。\n 由于记录了所有数据在所有节点上的逻辑时钟信息，Vector clock 和 Version vector 在实际应用中可能面临的一个问题是 vector 过大，用于数据管理的元数据甚至大于数据本身。\n 解决该问题的方法是使用 server id 取代 client id 创建 vector (因为 server 的数量相对 client 稳定)，或设定最大的 size、如果超过该 size 值则淘汰最旧的 vector 信息。\n","date":"2022-05-24T15:45:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%B6%E9%92%9F/","title":"分布式时钟"},{"content":"分布式基础理论：CAP与BASE CAP 一个分布式系统不可能同时满足一致性（Consistency）、可用性（Availability）、分区容错性（Partition Tolerance）这三个基本需求，最多只能满足其中的两项，不可能三者兼顾。 一致性：在分布式系统中的所有数据副本，在同一时刻是否一致（等同于所有节点访问同一份最新的数据副本） 可用性：分布式系统在面对各种异常时可以提供正常服务的能力（非故障的节点在有限的时间内返回合理的响应） 分区容错性：分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。  如下图\n 为什么三者不可兼顾呢？\n 我们首先就需要了解以下网络分区的概念。\n在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络等）中，由于一些特殊的原因导致这些子网络之间出现网络不连通的状况，但各个子网络的内部网络是正常的，从而就导致了整个系统的网络环境被切分成了若干个孤立的区域。\n节点被划分为多个区域后，每个区域内部可以通信，但是区域之间无法通信。\n对于一个分布式系统而言，我们的组件必然要被部署到不同的节点上，也必然会出现子网络。我们无法保证网络始终可靠，那么网络分区则是一个必定会产生的异常情况。\n当发生网络分区的时候，如果我们要继续提供服务，那么分区容错性也就是我们必然需要面对和解决的问题，因此分区容错性P是必定要满足的。 那么一致性C和可用性A可以兼顾吗？\n 答案必定是否定的，为什么呢？倘若分布式系统中出现了网络分区的情况，此时某一个节点在进行写操作，为了保证一致性，那么就必须要禁止其他节点的读写操作以防止数据冲突，而此时就导致其他的节点无法正常工作，即与可用性发生冲突。而如果让其他节点都正常进行读写操作的话，那就无法保证数据的一致，影响了数据的一致性，因此，我们只能满足可用性A和一致性C二者其一用一句话总结就是，CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性A或者一致性C。 因此在上面我给出的图中，CA是不可能的选项，在实际场景中，我们会根据具体的需求来选择CP和AP。\nBASE BASE理论是基本可用（Basically Available） 、软状态（Soft-state）和最终一致性（Eventually Consistent） 三个短语的缩写。其是对CAP中的一致性和可用性进行一个权衡的结果。\n 基本可用（Basically Available）：基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性（响应时间上的损失、系统功能上的损失）。但是，这绝不等价于系统不可用。 软状态（Soft-state）：允许系统中的数据存在中间状态（CAP理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性（Eventually Consistent）：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。  BASE理论的核心思想就是我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。什么意思呢？其实就是要牺牲数据的一致性（不保证强一致性，只保证最终一致性）来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。\n总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统事务的ACID特性使相反的，它完全不同于ACID的强一致性模型，而是提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性与BASE理论往往又会结合在一起使用。\n","date":"2022-05-24T15:43:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BAcap%E4%B8%8Ebase/","title":"分布式基础理论：CAP与BASE"},{"content":"状态一致性 什么是状态一致性   有状态的流处理，内部每个算子任务都可以有自己的状态。\n  对于流处理器内部（没有接入sink）来说，所谓的状态一致性，其实就是我们所说的计算结果要保证准确，一条数据不应该丢失，也不应该重复计算。\n  在遇到故障时可以恢复状态，恢复以后的重新计算，结果应该也是完全正常的。\n  状态一致性种类   最多一次（At-Most-Once）\n  任务发生故障时最简单的措施就是既不恢复丢失的状态，也不重放丢失的事件，所以至多一次是最简单的一种情况。\n  它保证了每个事件至多被处理一次。\n    至少一次（At-Least-Once）\n 对于大多数现实应用而言，用户的期望是不丢事件，这类保障被称为至少一次。 它意味着所有事件最终都会处理，虽然有些可能会处理多次。    精确一次（Exactly-Once）\n 精确一次是最严格，最难实现的一类保障。 它不但能够保证事件没有丢失，而且每个事件对于内部状态的更新都只有一次。 Flink利用Checkpoints机制来保证精确一次语义。    端到端（end-to-end）状态一致性 端到端的保障指的是在整个数据处理管道上结果都是正确的。在每个组件都提供自身的保障情况下，整个处理管道上端到端的保障会受制于保障最弱的那个组件。\n 那么端到端的精确一次在各部分又是如何实现的呢？\n  内部：Checkpoints机制，在发生故障的时候能够恢复各个环节的数据。 Source：可设置数据读取的偏移量，当发生故障的时候重置偏移量到故障之前的位置。 Sink：从故障恢复时，数据不会重复写入外部系统。  其中前两种在上文已经介绍过了，下面就介绍一下Sink如何提供端到端的精确一次性保障。\nSink端到端状态一致性的保证 应用若是想提供端到端的精确一次性保障，就需要一些特殊的Sink连接器，根据情况不同，这些连接器可以使用两种技术来实现精确一次保障：\n  幂等性写（idempotent write）\n- 幂等操作的含义就是可以多次执行，但是只会引起一次改变。 - 例如我们将相同的键值对插入一个哈希结构中就是一个幂等操作， 因为由于该键值对已存在后，无论插入多少次都不会改变结果。 - 由于可以在不改变结果的前提下多次执行，因此幂等性写操作在一定程度上减轻Flink检查点机制所带来的重复结果的影响\n  事务性写（transactional write）\n 事务性写其实就是原子性写，即只有在上次成功的检查点之前计算的结果才会被写入外部Sink系统。 事务性写虽然不会像幂等性写那样出现重放过程中的不一致现象，但是会增加一定延迟，因为结果只有在检查点完成后才对外可见。 实现思想：构建的事务对应着Checkpoints，待Checkpoints真正完成的时候，才把所有对应的结果写入Sink系统中。 实现方式：  预写日志（Write Ahead Log，WAL） 两阶段提交（Two Phase Commit，2PC）      预写日志 把结果数据先当成状态保存，然后在收到Checkpoints完成的通知时，一次性写入Sink系统。 简单易于实现，由于数据提前在状态后端做了缓存，所以无论什么Sink系统都能用这种方式一批搞定。 但同时它也存在问题，写入数据时出现故障则会导致一部分数据成功一部分失败。 DataStream API提供了一个模板类GenericWriteAheadSink，来实现这种事务性Sink。  两阶段提交 对于每个Checkpoints，Sink任务会启动一个事务，并将接下来所有接收的数据添加到事务里。 然后将这些数据写入外部 Sink，但不提交它们，这时只是“预提交”。 当它收到Checkpoints完成的通知时，它才正式提交事务，实现结果的真正写入。 这种方式真正实现了精确一次，它需要一个提供事务支持的外部Sink系统，Flink提供了TwoPhaseCommitSinkFunction接口。 对外部Sink系统的要求  外部Sink系统必须提供事务支持，或者Sink任务必须能够模拟外部系统上的事务。 在Checkpoints的隔离期间里，必须能够开启一个事务并接受数据写入。 在收到Checkpoints完成的通知之前，事务必须是“等待提交”的状态。在故障恢复的情况下，这可能需要一些时间。如果这个时候 Sink系统关闭事务（例如超时了），那么未提交的数据就会丢失。 Sink任务必须能够在进程失败后恢复事务。 提交事务必须是幂等操作。    Flink+Kafka端到端状态一致性的保证   内部：利用Checkpoints机制把状态保存，当发生故障的时候可以恢复状态，从而保证内部的状态一致性。\n  source 端：Kafka Consumer作为Source，可以将偏移量保存下来，当发生故障时可以从发生故障前的偏移量重新消费数据，从而保证一致性。\n  sink端：Kafka Producer作为Sink，采用两阶段提交Sink，需要实现一个TwoPhaseCOmmitSinkFunction。\n  ","date":"2022-05-24T14:40:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/","title":"Flink 状态一致性"},{"content":"Flink 容错机制 Checkpoints（检查点） Flink中基于异步轻量级的分布式快照技术提供了Checkpoints容错机制，Checkpoints可以将同一时间点作业/算子的状态数据全局统一快照处理，包括前面提到的算子状态和键值分区状态。当发生了故障后，Flink会将所有任务的状态恢复至最后一次Checkpoint中的状态，并从那里重新开始执行。\n 那么Checkpoints的生成策略是什么样的呢？它会在什么时候进行快照的生成呢？\n 其实就是在所有任务都处理完同一个输入数据流的时候，这时就会对当前全部任务的状态进行一个拷贝，生成Checkpoints。\n为了方便理解，这里先简单的用一个朴素算法来解释这一生成过程（Flink的Checkpoints算法实际要更加复杂，在下面会详细讲解）\n 暂停接受所有输入流。 等待已经流入系统的数据被完全处理，即所有任务已经处理完所有的输入数据。 将所有任务的状态拷贝到远程持久化，生成Checkpoints。在所有任务完成自己的拷贝工作后，Checkpoints生成完毕。 恢复所有数据流的接收  恢复流程 为了方便进行实例的讲解，假设当前有一个Source任务，负责从一个递增的数字流（1、2、3、4……）中读取数据，读取到的数据会分为奇数流和偶数流，求和算子的两个任务会分别对它们进行求和。在当前任务中，数据源算子的任务会将输入流的当前偏移量存为状态，求和算子的任务会将当前和存为状态。\n如上图，在当前生成的Checkpoints中保存的输入偏移为5，偶数求和为6，奇数求和为9。\n假设在下一轮计算中，任务sum_odd计算出现了问题，任务sum_odd的时候产生了问题，导致结果出现错误。由于出现问题，为了防止从头开始重复计算，此时会通过Checkpoints来进行快照的恢复。\nCheckpoints恢复应用需要以下三个步骤\n 重启整个应用 利用最新的检查点重置任务状态 恢复所有任务的处理   第一步我们需要先重启整个应用，恢复到最原始的状态。   紧接着从检查点的快照信息中读取出输入源的偏移量以及算子计算的结果，进行状态的恢复   状态恢复完成后，继续Checkpoints恢复的位置开始继续处理。  从检查点恢复后，它的内部状态会和生成检查点的时候完全一致，并且会紧接着重新处理那些从之前检查点完成开始，到发生系统故障之间已经处理过的数据。虽然这意味着Flink会重复处理部分消息，但上述机制仍然可以实现精确一次的状态一致性，因为所有的算子都会恢复到那些数据处理之前的时间点。\n但这个机制仍然面临一些问题，因为Checkpoints和恢复机制仅能重置应用内部的状态，而应用所使用的Sink可能在恢复期间将结果向下游系统（如事件日志系统、文件系统或数据库）重复发送多次。为了解决这个问题，对于某些存储系统，Flink提供的Sink函数支持精确一次输出 （在检查点完成后才会把写出的记录正式提交）。另一种方法则是适用于大多数存储系统的幂等更新。\n生成策略 Flink中的Checkpoints是基于Chandy-Lamport分布式快照算法 实现的，该算法不会暂停整个应用，而是会将生成Checkpoints的过程和处理过程分离，这样在部分任务持久化状态的过程中，其他任务还可以继续执行。\n在介绍生成策略之前，首先需要介绍一下Checkpoints barrier（屏障） 这一种特殊记录。\n如上图，与水位线相同，Flink会在Source中间隔性地生成barrier，通过barrier把一条流上的数据划分到不同的Checkpoints中，在barrier之前到来的数据导致的状态更改，都会被包含在当前所属的Checkpoints中；而基于barrier之后的数据导致的所有更改，就会被包含在之后的Checkpoints中。\n 假设当前有两个Source任务，各自消费一个递增的数字流（1、2、3、4……），读取到的数据会分为奇数流和偶数流，求和算子的两个任务会分别对它们进行求和，并将结果值更新至下游Sink。   此时JobManager向每一个Source任务发送一个新的Checkpoints编号，以此启动Checkpoints生成流程。   在Source任务收到消息后，会暂停发出记录，紧接着利用状态后端生成本地状态的Checkpoints，并把barrier连同编号广播给所有传出的数据流分区。 状态后端在状态存入Checkpoints后通知Source任务，并向JobManager发送确认消息。 在所有barrier发出后，Source将恢复正常工作。   Source任务会广播barrier至所有与之相连的任务，确保这些任务能从它们的每个输入都收到一个barrier 在等待过程中，对于barrier未到达的分区，数据会继续正常处理。而barrier已经到达的分区，它们新到来的记录会被缓冲起来，不能处理。这个等待所有barrier到来的过程被称为barrier对齐   任务中收齐全部输入分区发送的barrier后，就会通知状态后端开始生成Checkpoints，同时继续把Checkpoints barrier广播转发到下游相连的任务。   任务在发出所有的Checkpoints barrier后就会开始处理缓冲的记录。等到所有缓冲记录处理完后，任务就会继续处理Source。   Sink任务在收到分隔符后会依次进行barrier对齐，然后将自身状态写入Checkpoints，最终向JobManager发送确认信息。 JobManager在接收到所有任务返回的Checkpoints确认信息后，就说明此次Checkpoints生成结束。  Savepoints（保存点）  由于Cheakpoints是周期性自动生成的，但有些时候我们需要手动的去进行镜像保存功能，于是Flink同时还为我们提供了Savepoints来完成这个功能，Savepoints不仅可以做到故障恢复，还可以用于手动备份、版本迁移、暂停或重启应用等。 Savepoints是Checkpoints的一种特殊实现，底层也是使用Checkpoint机制，因此Savepoints可以认为是具有一些额外元数据的Checkpoints。 Savepoints的生成和清理都无法由Flink自动进行，因此都需要用户自己来显式触发。  ","date":"2022-05-24T14:38:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/","title":"Flink 容错机制"},{"content":"Flink 状态管理 通常意义上，函数里所有需要任务去维护并用来计算结果的数据都属于任务的状态，可以把状态想象成任务的业务逻辑所需要访问的本地或实例变量。\n如上图，任务首先会接受一些输入数据。在处理这些数据的过程中，任务对其状态进行读取或更新，并根据状态的输入数据计算结果。我们以一个持续计算接收到多少条记录的简单任务为例。当任务收到一个新的记录后，首先会访问状态获取当前统计的记录数目，然后把数目增加并更新状态，最后将更新后的状态数目发送出去。\nFlink会负责进行状态的管理，包括状态一致性、故障处理以及高效存取相关的问题都由Flink负责搞定，这样开发人员就可以专注于自己的应用逻辑。\n在Flink中，状态都是和特定operator（算子）相关联，为了让Flink的Runtime（运行）层知道算子有哪些状态，算子需要自己对其进行注册。根据作用域的不同，状态可以分为以下两类\n operator state（算子状态） keyed state（键值分区状态）  算子状态 算子状态的作用域是某个算子任务，这意味着所有在同一个并行任务之内的记录都能访问到相同的状态**（每一个并行的子任务都共享一个状态）。算子状态不能通过其他任务访问，无论该任务是否来自相同算子（相同算子的不同任务之间也不能访问）**。\nFlink为算子状态提供了三种数据结构\n 列表状态（list state）：将状态表示为一组数据的列表。（每一个并行的子任务共享一个状态） 联合列表状态（union list state）：同样将状态表示为数据的列表，但在进行故障恢复或者从某个保存点（savepoint）启动应用的时候，状态恢复的方式和普通的列表状态有所不同。（把之前的每一个状态广播到对应的每一个算子中） 广播状态（broadcast state）：专门为那些需要保证算子的每个任务状态都相同的场景而设计。（把同一个状态广播给所有算子子任务）  键值分区状态 键值分区状态会按照算子输入记录所定义的键值来进行维护或访问。Flink为每个键值都维护了一个状态实例，该实例总是位于那个处理对应键值记录的算子任务上。当任务在处理一个记录时，会自动把状态的访问范围限制为当前记录的键值，因此所有键值相同的记录都能访问到一样的状态。\nFlink为键值分区状态提供以下几种数据结构\n 单值状态（value state）：每个键对应存储一个任意类型的值。 列表状态（list state）：每个键对应存储一个值的列表。 映射状态（map state）：每个键对应存储一个键值映射。 聚合状态（Reducing state \u0026amp; Aggregating State）：每个键对应存储一个用于聚合操作的列表  状态后端（State Backends） 有状态算子的任务通常会对每一条到来的记录读写状态，因此高效的状态访问对于记录处理的低延迟而言至关重要。为了保证快速访问状态，每个并行任务都会把状态维护在本地。至于状态具体的存储、访问和维护，则是由一个名为状态后端的可拔插（pluggable） 组件来决定。状态后端主要负责两件事情：本地状态管理和将状态以检查点的形式写入远程存储。\n目前，Flink提供了三种状态后端，状态后端的选择会影响有状态应用的鲁棒性及性能。\n  MemoryStateBackend\n MemoryStateBackend将状态以常规对象的方式存储在TaskManager进程的JVM堆，并在生成Checkpoints时会将状态发送至JobManager并保存到它的堆内存中。 如果状态过大，则可能导致JVM上的任务由于OutOfMemoryError而终止，并且可能由于堆中放置了过多常驻内存的对象而引发垃圾回收停顿问题。 由于内存具有易失性，所以一旦JobManager出现故障就会导致状态丢失，因此MemoryStateBackend通常用于开发和调试。 内存访问速度快，延迟低，但容错性也低。    FsStateBackend\n 与MemoryStateBackend一样将本地状态存储在TaskManager进程的JVM堆里，不同的是将Checkpoints存到了远程持久化文件系统（FileSystem）中。 受到TaskManager内存大小的限制，并且也可能导致垃圾回收停顿问题。 FsStateBackend既让本地访问享有内存的速度，又可以支持故障容错。    RocksDBStateBackend\n  RocksDBStateBackend会将全部状态序列化后存到本地RocksDB实例中\n  由于磁盘I/O以及序列化/反序列化对象的性能开销，相较于内存中维护状态而言， 读写性能会偏低。\n  RocksDB的支持并不直接包含在Flink中，需要额外引入依赖\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-statebackend-rocksdb_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;       有状态算子的扩缩容 流式应用的一项基本需求是根据输入数据到达速率的变化调整算子的并行度。对于无状态的算子扩缩容很容易，但是对于有状态算子来说，这就变的复杂了很多。因为我们需要把状态重新分组，分配到与之前数量不等的并行任务上。\n针对不同类型状态的算子，Flink提供了四种扩缩容模式\n 键值分区状态 算子列表状态 算子联合列表状态 算子广播状态  键值分区状态 带有键值分区状态的算子在扩缩容时会根据新的任务数量对键值重新分区，但为了降低状态在不同任务之间迁移的必要成本，Flink不会对单独的键值实施再分配，而是会把所有键值分为不同的键值组（Key group）。每个键值组都包含了部分键值，Flink以此为单位把键值分配给不同任务。\n算子列表状态 带有算子列表状态的算子在扩缩容时会对列表中的条目进行重新分配。理论上，所有并行算子任务的列表条目会被统一收集起来，随后均匀分配到更少或更多的任务之上。如果列表条目的数量小于算子新设置的并行度，部分任务在启动时的状态就可能为空。\n算子联合列表状态 带有算子联合列表状态的算子会在扩缩容时把状态列表的全部条目广播到全部任务上，随后由任务自己决定哪些条目应该保留，哪些应该丢弃。\n算子广播状态 带有算子广播状态的算子在扩缩容时会把状态拷贝到全部新任务上，这样做的原因是广播状态能确保所有任务的状态相同。在缩容的情况下，由于状态经过复制不会丢失，我们可以简单的停掉多出的任务。\n","date":"2022-05-24T14:35:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/","title":"Flink 状态管理"},{"content":"Flink 流处理 Dataflow编程 顾名思义，Dataflow程序描述了数据如何在不同操作之间流动。Dataflow程序通常表现为有向无环图（DAG），图中顶点称为算子（Operator），表示计算。而边表示数据依赖关系。\n算子是Dataflow程序的基本功能单元，他们从输入获取数据，对其进行计算，然后产生数据并发往输出以供后续处理。而所有Flink程序都由三部分算子组成。\n Source（数据源）：负责获取输入数据。 Transformation（数据处理）：对数据进行处理加工，通常对应着多个算子。 Sink（数据汇）：负责输出数据。  执行图 类似上图的Dataflow图被称为逻辑图，因为它们表达了高层视角下的计算逻辑。为了执行Dataflow程序，需要将逻辑图转化为物理Dataflow图（执行图），后者会指定程序的执行细节。\n在Flink中，执行图按层级顺序分为以下四层\n StreamingGraph  是根据用户通过Stream API编写的代码生成的初始流程图，用于表示程序的拓扑结构。   JobGraph  StreamGraph经过优化后生成了JobGraph，提交给JobManager的数据结构。主要的优化为将多个符合条件的节点链接在一起作为一个节点（任务链Operator Chains）后放在一个作业中执行，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。   ExecutionGraph  JobManager根据JobGraph生成ExecutionGraph，ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。   物理执行图  JobManager根据ExecutionGraph对任务进行调度后，在各个TaskManager上部署作业后形成的“图”，并不是一个具体的数据结构。    并行度 Flink程序的执行具有并行、分布式的特性。\n在执行过程中，一个Stream包含一个或多个分区（partition），而每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务中不同的线程、不同的物理机或不同的容器中彼此互不依赖地执行。\n一个特定算子的子任务的个数被称之为并行度（paralelism）。一般情况下一个流程序的并行度可以认为就是其所有算子中最大的并行度，一个程序中不同的算子可以具有不同的并行度。\n数据传输策略 Stream在算子之间传输数据的形式可以是one-to-one（forwarding）的模式也可以是Redistributing的模式，具体是哪一种需要取决于算子的种类。\n One-to-one  Stream维护着分区以及元素的顺序（比如在Source和map operator之间），那意味着map算子的子任务看到的元素的个数以及顺序跟Source算子的子任务生产的元素的个数、顺序相同，map、filter、flatmap等算子都是one-to-one的对应关系。 类似于Spark中的窄依赖。   Redistributing  Stream的分区会发生改变（map()跟keyBy/window之间或者keyBy/windows跟Sink之间）。每一个算子的子任务依据所选择的Transformation发送数据到不同的目标任务。 例如keyBy()基于hashCode重分区、broadcast和rebalance会随机重新分区，这些算子都会引起redistribute过程，而redistribute过程就类似于Spark中的shuffle过程。 类似于Spark中的宽依赖。    任务链 Flink 采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过本地转发（local forward） 的方式进行连接。\n相同并行度的 one-to-one 操作 （两个条件缺一不可），Flink 这样相连的算子链接在一起形成一个 task，原来的算子成为里面的 subtask。每个 task 由一个线程执行。将算子链接成 task 是个有用的优化：它减少线程间切换、缓冲的开销，并且减少延迟的同时增加整体吞吐量。\n时间语义 对于流式数据处理，最大的特点就是数据上具有时间的属性特征，Flink根据时间产生的位置不同，将时间区分为如下三种时间概念\n 事件时间（Event Time）：数据流事件实际发生的时间。 接入时间（Ingestion Time）：数据进入Flink系统的时间。 处理时间（Processing Time）：当前流处理算子所在机器上的本地时钟时间。  Flink中默认使用的是处理时间，但是在大多数情况下都会使用事件时间（即实际事件的发生点，也符合事件发生进而分析的逻辑），一般只有在Event Time无法使用的情况下才会使用接入时间和处理时间，因此我们可以通过调用执行环境的setStreamTimeCharacteristic方法来指定时间语义\n1 2 3 4 5  //创建执行环境\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //设置指定的时间语义，如下面的设置为EventTime env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);    处理时间与事件时间的选择\n 在大部分场景由于我们需要依据事件发生的顺序来进行逻辑处理，因此都会使用事件时间。但是在一些特殊场景下，考虑到事件数据数据乱序到达以及延迟到达等问题，为了保证实时性和低延迟，处理时间就会派上用场。\n例如下面几种场景：\n 更重视处理速度而非准确性的应用。 需要周期性实时报告结果而无论其准确性（如实时监控仪表盘）。  因此，对比处理时间和事件时间得出结论：\n 处理时间提供了低延迟，但是它的结果依赖处理速度，因此具有不确定性。 事件时间则与之相反，能够保证结果的准确性，并允许你处理延迟甚至无序的事件。  水位线（Watermarks） 在理想状态下，事件数据都是按照事件产生的时间顺序传输至Flink系统中。但事实上，由于网络或者分布式系统等外部因素的影响下，事件数据往往不能及时传输，导致系统的不稳定而造成数据乱序到达或者延迟到达等情况。\n一旦出现这种问题，如果我们严格按照Event Time来决定窗口的运行，我们既不能保证属于该窗口的数据已经全部到达，也不能无休止的等待延迟到达的数据，因此我们需要一种机制来控制数据处理的进度，这就是水位线（Watermarks）机制。\n水位线是一个全局的进度指标，它能够衡量数据处理进度==（表达数据到达的完整性）**，保证事件数据全部到达Flink系统，即使数据乱序或者延迟到达，也能够像预期一样计算出正确和连续的结果。\n 那么它是如何做到的呢？\n  Flink会使用最新的事件时间减去固定时间间隔作为水位线，该时间时间为用户外部配置的支持最大延迟到达的时间长度。 当一个算子接收到一个时间为T的水位线，就可以认为不会再收到任何时间戳小于或等于T的事件了（迟到事件或异常事件） 水位线其实就相当于一个提示算子的信号，当水位线时间戳大于时间窗口的结束时间，且窗口中含有事件数据时，此时算子就会认为某个特定时间区间的时间戳已经全部到齐，立即开始触发窗口计算或对接收的数据进行排序。  从上面我们可以看出，水位线其实就是在结果的准确性和延迟之间做出取舍，它虽然保证了低延迟，但是伴随而来的却是低可信度。倘若我们要保证后续的延迟事件不丢失，就必须额外增加一些代码来处理他们，但是如果采用这种保守的机制，虽然可信度低高了，但是延迟又会继续增加，因此延迟和可信无法做到两全其美，需要我们依据具体场景来自己平衡。\n","date":"2022-05-24T14:29:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E6%B5%81%E5%A4%84%E7%90%86/","title":"Flink 流处理"},{"content":"Flink 架构 架构体系 在Flink整个软件架构体系中，遵循了分层的架构设计理念，在降低系统耦合度的同时也为上层用户构建Flink应用提供了丰富且友好的借口。从上图可以看出Flink的架构体系基本上可以分为以下三层\n API \u0026amp; Libraries层 Runtime核心层 物理部署层  API \u0026amp; Libraries层 作为分布式数据处理框架，Flink同时提供了支持流计算和批计算的借口，同时在此基础之上抽象出不同的应用类型的组件库，如基于流处理的CEP（复杂事件处理库）、SQL\u0026amp;Table库和基于批处理的FlinkML（机器学习库）、Gelly（图处理库）等。\nAPI层包括构建流计算应用的DataStream API和批计算应用的DataSet API，两者都提供给用户丰富的数据处理高级API，例如Map、FlatMap操作等，同时也提供比较低级的Process Function API，用户可以直接操作状态和时间等底层数据。\nRuntime核心层 该层主要负责对上层不同接口提供基础服务，也是Flink分布式计算框架的核心实现层，支持分布式Stream作业的执行、JobGraph到ExecutionGraph的映射转换、任务调度等。\n物理部署层 该层主要涉及Flink的部署模式，目前Flink支持多种部署模式：本地、集群（Standalone/YARN）、云（GCE/EC2）、Kubenetes。Flink能够通过该层能够支持不同的部署，用户可以根据需要选择使用对应的部署模式\n运行时组件 Flink系统主要由以下四个组件组成\n JobManager（任务管理器） TaskManager（作业管理器） ResourceManger（资源管理器） Dispatcher（分发器）  Flink本身是用Java和Scala实现的，因此所有组件都基于JVM（Java虚拟机） 运行。\nJobManager Flink遵循Master-Slave（主从）架构设计原则，JobManager为Master节点，TaskManager为Slave节点，并且所有组件之间的通信都借助Akka，包括任务的状态以及CheckPoint（检查点）触发等信息。\n 作为主进程（Master Process），JobManager控制着单个应用程序的执行，也就是每个应用都由一个不同的JobManager管理。 JobManager可以接受需要执行的应用，该应用会包含一个所谓的Job Graph（任务图），即逻辑Dataflow Graph（数据流图），以及一个打包了全部所需类、库以及其他资源的JAR文件。 JobManager将JobGraph转化为名为Execution Graph（执行图）的物理Dataflow Graph，其中包含了所有可以并发实行的任务。 JobManager会从ResourceManager申请执行任务的必要资源——TaskManager slot，一旦它收到了足够数量的TaskManager slot，它就会将Execution Graph中的任务分发给TaskManager来执行。在执行过程中，JobManager还要负责所有需要集中协调的操作，如创建CheakPoint等。  TaskManager  TaskManager是Flink的工作进程（Worker Process），在Flink的搭建过程中要启动多个TaskManager。每个TaskManager提供一定数量的slot（处理槽），slot的数量限制了TaskManager可执行的任务数。 TaskManager在启动之后会向ResourceManager注册它的slot，当接收到ResourceManager的指示时，TaskManager会向JobManager提供一个或者多个slot。之后JobManager就可以向slot中分配任务来执行。 在执行过程中，运行同一应用的不同任务的TaskManager之间会产生数据交换。  ResourceManger  Flink为不同的环境和资源提供者（如YARN、Kubernetes、Stand-alone）提供了不同的ResourceManger。 ResourceManger负责管理Flink的处理资源单元——TaskManager Slot。 当JobManager申请TaskManager slot时，ResourceManger会指示一个拥有空闲slot的TaskManager将其slot提供给JobManager。如果ResourceManger的slot数无法满足JobManager的请求，则ResourceManger可以与资源提供者通信，让他们提供额外的容器来启动更多的TaskManager进程。同时，ResourceManger还负责终止空闲进程的TaskManager以释放计算资源。  Dispatcher   Dispatcher在会跨多个作业运行，它提供了一个REST接口来让我们提交需要执行的应用，一旦某个应用提交执行，则Dispatcher会启动一个JobManager并将应用转交给它。\n  REST接口意味着Dispatcher这一集群的HTTP入口可以受到防火墙的保护。\n  Dispatcher同时还会启动一个Web UI，用来展示和监控有关作业执行的信息。\n  Dispatcher并不是必需的组件，某些应用提交执行的方式可能用不到Dispatcher。\n   ","date":"2022-05-24T14:23:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E6%9E%B6%E6%9E%84/","title":"Flink 架构"},{"content":"Flink介绍 概述 Apache Flink是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。\nApache Flink功能强大，支持开发和运行多种不同种类的应用程序。它的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。Flink不仅可以运行在包括 YARN、 Mesos、Kubernetes在内的多种资源管理框架上，还支持在裸机集群上独立部署。在启用高可用选项的情况下，它不存在单点失效问题。事实证明，Flink已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。世界各地有很多要求严苛的流处理应用都运行在Flink之上。\n接下来，我们来介绍一下Flink中的几个重要概念。\n批与流   批处理的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。\n  流处理的特点是无界、实时, 无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。\n  在Spark的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。而在Flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。\n 无界流：有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流：有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。  Flink 擅长处理无界和有界数据集，精确的时间控制和状态化使得Flink的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。\n部署应用到任何地方 Apache Flink是一个分布式系统，它需要计算资源来执行应用程序。Flink集成了所有常见的集群资源管理器，例如Hadoop YARN、 Apache Mesos和Kubernetes，但同时也可以作为独立集群运行。\nFlink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。\n部署Flink应用程序时，Flink会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。\n利用内存性能 有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。\n分层AP Flink 根据抽象程度分层，提供了三种不同的 API。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。\n ProcessFunction：可以处理一或两条输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的某一时刻触发回调函数。因此，你可以利用 ProcessFunction 实现许多有状态的事件驱动应用所需要的基于单个事件的复杂业务逻辑。 DataStream API：为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如map()、reduce()、aggregate() 等函数。你可以通过扩展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。 SQL \u0026amp; Table API：Flink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API和SQL借助了 Apache Calcite来进行查询的解析，校验以及优化。它们可以与DataStream和DataSet API无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。Flink 的关系型 API 旨在简化数据分析、数据流水线和 ETL 应用的定义。  特点 Apache Flink是一个集合众多具有竞争力特性于一身的第三代流处理引擎，它的以下特点使得它能够在同类系统中脱颖而出。\n 同时支持高吞吐、低延迟、高性能。  Flink是目前开源社区中唯一一套集高吞吐、低延迟、高性能三者于一身的分布式流式处理框架。像Apache Spark也只能兼顾高吞吐和高性能特性，主要因为在Spark Streaming流式计算中无法做到低延迟保障；而流式计算框架Apache Storm只能支持低延迟和高性能特性，但是无法满足高吞吐的要求。   同时支持事件时间和处理时间语义。  在流式计算领域中，窗口计算的地位举足轻重，但目前大多数框架窗口计算采用的都是处理时间，也就是事件传输到计算框架处理时系统主机的当前时间。Flink能够支持基于事件时间语义进行窗口计算，也就是使用事件产生的时间，这种基于事件驱动的机制使得事件即使乱序到达，流系统也能够计算出精确的结果，保证了事件原本的时序性。   支持有状态计算，并提供精确一次的状态一致性保障。  所谓状态就是在流式计算过程中将算子的中间结果数据保存着内存或者文件系统中，等下一个事件进入算子后可以从之前的状态中获取中间结果中计算当前的结果，从而不须每次都基于全部的原始数据来统计结果，这种方式极大地提升了系统的性能，并降低了数据计算过程的资源消耗。   基于轻量级分布式快照实现的容错机制。  Flink能够分布式运行在上千个节点上，将一个大型计算任务的流程拆解成小的计算过程，然后将Task分布到并行节点上进行处理。在任务执行过程中，能够自动发现事件处理过程中的错误而导致的数据不一致问题，在这种情况下，通过基于分布式快照技术的Checkpoints，将执行过程中的状态信息进行持久化存储，一旦任务出现异常终止，Flink就能够从Checkpoints中进行任务的自动恢复，以确保数据中处理过程中的一致性。   保证了高可用，动态扩展，实现7 * 24小时全天候运行。  支持高可用性配置（无单点失效），和Kubernetes、YARN、Apache Mesos紧密集成，快速故障恢复，动态扩缩容作业等。基于上述特点，它可以7 X 24小时运行流式应用，几乎无须停机。当需要动态更新或者快速恢复时，Flink通过Savepoints技术将任务执行的快照保存在存储介质上，当任务重启的时候可以直接从事先保存的Savepoints恢复原有的计算状态，使得任务继续按照停机之前的状态运行。   支持高度灵活的窗口操作。  Flink将窗口划分为基于Time、Count、Session，以及Data-driven等类型的窗口操作，窗口可以用灵活的触发条件定制化来达到对复杂流传输模式的支持，用户可以定义不同的窗口触发机制来满足不同的需求。    应用场景 在实际生产的过程中，大量数据在不断地产生，例如金融交易数据、互联网订单数据、GPS定位数据、传感器信号、移动终端产生的数据、通信信号数据等，以及我们熟悉的网络流量监控、服务器产生的日志数据，这些数据最大的共同点就是实时从不同的数据源中产生，然后再传输到下游的分析系统。\n针对这些数据类型主要包括以下场景，Flink对这些场景都有非常好的支持。\n  实时智能推荐\n 利用Flink流计算帮助用户构建更加实时的智能推荐系统，对用户行为指标进行实时计算，对模型进行实时更新，对用户指标进行实时预测，并将预测的信息推送给Web/App端，帮助用户获取想要的商品信息，另一方面也帮助企业提高销售额，创造更大的商业价值。    复杂事件处理\n 例如工业领域的复杂事件处理，这些业务类型的数据量非常大，且对数据的时效性要求较高。我们可以使用Flink提供的CEP（复杂事件处理）进行事件模式的抽取，同时应用Flink的SQL进行事件数据的转换，在流式系统中构建实时规则引擎。    实时欺诈检测\n 在金融领域的业务中，常常出现各种类型的欺诈行为。运用Flink流式计算技术能够在毫秒内就完成对欺诈判断行为指标的计算，然后实时对交易流水进行规则判断或者模型预测，这样一旦检测出交易中存在欺诈嫌疑，则直接对交易进行实时拦截，避免因为处理不及时而导致的经济损失    实时数仓与ETL\n 结合离线数仓，通过利用流计算等诸多优势和SQL灵活的加工能力，对流式数据进行实时清洗、归并、结构化处理，为离线数仓进行补充和优化。另一方面结合实时数据ETL处理能力，利用有状态流式计算技术，可以尽可能降低企业由于在离线数据计算过程中调度逻辑的复杂度，高效快速地处理企业需要的统计结果，帮助企业更好的应用实时数据所分析出来的结果。    流数据分析\n 实时计算各类数据指标，并利用实时结果及时调整在线系统相关策略，在各类投放、无线智能推送领域有大量的应用。流式计算技术将数据分析场景实时化，帮助企业做到实时化分析Web应用或者App应用的各种指标。    实时报表分析\n 实时报表分析说近年来很多公司采用的报表统计方案之一，其中最主要的应用便是实时大屏展示。利用流式计算实时得出的结果直接被推送到前段应用，实时显示出重要的指标变换，最典型的案例就是淘宝的双十一实时战报。    Flink VS Spark Streaming   数据模型\n  Flink基本数据模型是数据流，以及事件序列。\n  Spark采用RDD模型，Spark Streaming的DStream实际上也就是一组组小批\n数据RDD的集合。\n    运行时架构\n  Flink是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节\n点进行处理。\n  Spark是批计算，将DAG划分为不同的Stage，一个完成后才可以计算下一个。\n    ","date":"2022-05-24T14:22:13+08:00","permalink":"https://blog.orekilee.top/p/flink-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"Flink 基本概念"},{"content":"DBImpl模块 Open 数据库 Open 操作主要用于创建新的 LevelDB 数据库或打开一个已存在的数据库。Open 操作的主要函数共需传递 3 个参数：两个输入参数 options 与 dbname，一个输出参数 dbptr。\n首先我们来看看它的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DB::Open(const Options\u0026amp; options, const std::string\u0026amp; dbname, DB** dbptr) { *dbptr = nullptr; //初始化dbimpl  DBImpl* impl = new DBImpl(options, dbname); impl-\u0026gt;mutex_.Lock(); VersionEdit edit; //尝试恢复之前已经存在的数据库文件中的数据  bool save_manifest = false; Status s = impl-\u0026gt;Recover(\u0026amp;edit, \u0026amp;save_manifest); //判断Memtable是否为空  if (s.ok() \u0026amp;\u0026amp; impl-\u0026gt;mem_ == nullptr) { //创建新的Log和MemTable  uint64_t new_log_number = impl-\u0026gt;versions_-\u0026gt;NewFileNumber(); WritableFile* lfile; s = options.env-\u0026gt;NewWritableFile(LogFileName(dbname, new_log_number), \u0026amp;lfile); if (s.ok()) { edit.SetLogNumber(new_log_number); impl-\u0026gt;logfile_ = lfile; impl-\u0026gt;logfile_number_ = new_log_number; impl-\u0026gt;log_ = new log::Writer(lfile); impl-\u0026gt;mem_ = new MemTable(impl-\u0026gt;internal_comparator_); impl-\u0026gt;mem_-\u0026gt;Ref(); } } //判断是否需要保存Manifest文件  if (s.ok() \u0026amp;\u0026amp; save_manifest) { edit.SetPrevLogNumber(0); edit.SetLogNumber(impl-\u0026gt;logfile_number_); //生成新的版本  s = impl-\u0026gt;versions_-\u0026gt;LogAndApply(\u0026amp;edit, \u0026amp;impl-\u0026gt;mutex_); } if (s.ok()) { //请理无用的文件  impl-\u0026gt;RemoveObsoleteFiles(); //尝试进行Compaction  impl-\u0026gt;MaybeScheduleCompaction(); } impl-\u0026gt;mutex_.Unlock(); if (s.ok()) { assert(impl-\u0026gt;mem_ != nullptr); *dbptr = impl; } else { delete impl; } return s; }   具体的实现流程如下图所示：\n 初始化一个 DBImpl 的对象 impl，将相关的参数选项 options 与数据库名称 dbname 作为构造函数的参数。 调用 DBImpl 对象的 Recover 函数，尝试恢复之前存在的数据库文件数据。 进行 Recover 操作后，判断 impl 对象中的 MemTable 对象指针 mem_ 是否为空，如果为空，则进入第 4 步，不为空则进入第 5 步。 创建新的 Log 文件以及对应的 MemTable 对象。这一步主要分别实例化 log::Writer 和 MemTable 两个对象，并赋值给 impl 中对应的成员变量，后续通过 impl 中的成员变量操作 Log 文件和 MemTable。 判断是否需要保存 Manifest 相关信息，如果需要，则保存相关信息。 判断前面步骤是否都成功了，如果成功，则调用 DeleteObsoleteFiles 函数对一些过时文件进行删除，且调用 MaybeScheduleCompaction 函数尝试进行数据文件的 Compaction 操作。  Get Get 主要用于从 LevelDB 中获取对应的键-值对数据，它是单个数据读取的主要接口。Get 的主要参数为数据读参数选项 options、键 key，以及一个用于返回数据值的 string 类型指针 value。其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Get(const ReadOptions\u0026amp; options, const Slice\u0026amp; key, std::string* value) { Status s; MutexLock l(\u0026amp;mutex_); SequenceNumber snapshot; //获取序列号并赋值给snapshot  if (options.snapshot != nullptr) { snapshot = static_cast\u0026lt;const SnapshotImpl*\u0026gt;(options.snapshot)-\u0026gt;sequence_number(); } else { snapshot = versions_-\u0026gt;LastSequence(); } MemTable* mem = mem_; MemTable* imm = imm_; Version* current = versions_-\u0026gt;current(); mem-\u0026gt;Ref(); if (imm != nullptr) imm-\u0026gt;Ref(); current-\u0026gt;Ref(); bool have_stat_update = false; Version::GetStats stats; { mutex_.Unlock(); //首先查找memtable  LookupKey lkey(key, snapshot); if (mem-\u0026gt;Get(lkey, value, \u0026amp;s)) { //如果查找不到，接着查找immutable  } else if (imm != nullptr \u0026amp;\u0026amp; imm-\u0026gt;Get(lkey, value, \u0026amp;s)) { //如果还是没找到，则继续查找SSTable  } else { s = current-\u0026gt;Get(options, lkey, value, \u0026amp;stats); have_stat_update = true; } mutex_.Lock(); } if (have_stat_update \u0026amp;\u0026amp; current-\u0026gt;UpdateStats(stats)) { MaybeScheduleCompaction(); } mem-\u0026gt;Unref(); if (imm != nullptr) imm-\u0026gt;Unref(); current-\u0026gt;Unref(); return s; }   具体的实现流程如下图所示：\nGet 在查询读取数据时，依次从 MemTable、Immutable MemTable 以及当前保存的 SSTable 文件中进行查找。如果在 MemTabel 中找到，立即返回对应的数值，如果没有找到，再从 Immutable MemTable 中查找。而如果Immutable MemTable 中还是没有找到，则会从持久化的文件 SSTable 中查找，直到找出该键对应的数值为止。\n SequenceNumber 有什么用呢？\n其主要作用是对 DB 的整个存储空间进行时间刻度上的序列备份，即要从 DB 中获取某一个数据，不仅需要其对应的键 key，而且需要其对应的时间序列号。对数据库进行写操作会改变序列号，每进行一次写操作，则序列号加 1。\n Put、Delete、Write Put 主要有3个参数：写操作参数 opt、操作数据的 key 与操作数据新值 value。其代码如下：\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Put(const WriteOptions\u0026amp; o, const Slice\u0026amp; key, const Slice\u0026amp; val) { return DB::Put(o, key, val); } Status DB::Put(const WriteOptions\u0026amp; opt, const Slice\u0026amp; key, const Slice\u0026amp; value) { WriteBatch batch; batch.Put(key, value); return Write(opt, \u0026amp;batch); }   从上面的代码可以看出， Put 其实也是将单条数据的操作变更为一个批量操作，然后调用 Write 进行实现。\nDelete 不会直接删除数据，而是在对应位置插入一个 key 的删除标志，然后在后续的 Compaction 过程中才最终去除这条 key-value 记录。其代码如下:\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Delete(const WriteOptions\u0026amp; options, const Slice\u0026amp; key) { return DB::Delete(options, key); } Status DB::Delete(const WriteOptions\u0026amp; opt, const Slice\u0026amp; key) { WriteBatch batch; batch.Delete(key); return Write(opt, \u0026amp;batch); }   从上面的代码可以看出 Delete 的本质其实也是一个 Write 操作。\n在介绍 Write 之前，首先介绍其封装的消息结构 Writer 与任务队列 writes_。\nWriter 用于保存基本信息，如批量操作 batch、状态信息 status、是否同步 sync、是否完成 done 以及用于多线程操作的条件变量cv 。\n1 2 3 4 5 6 7 8 9 10 11 12  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  struct DBImpl::Writer { explicit Writer(port::Mutex* mu) : batch(nullptr), sync(false), done(false), cv(mu) {} Status status; //状态  WriteBatch* batch; //批量写入对象  bool sync;//表示是否已经同步了  bool done;//表示是否已经处理完成  port::CondVar cv;//这个是条件变量 };   接着看看任务队列 writers_，该队列对象中的元素节点为 Writer 对象指针。可见 writes_ 与写操作的缓存空间有关，批量操作请求均存储在这个队列中，按顺序执行，已完成的出队，而未执行的则在这个队列中处于等待状态。\n1 2  // https://weread.qq.com/web/reader/9f932e70727ac58e9f9d8cck636320102206364d3f0ffdc std::deque\u0026lt;Writer*\u0026gt; writers_ GUARDED_BY(mutex_);   Write 主要有两个参数：WriteOptions 对象与 WriteBatch 对象。WriteOptions 主要包含一些关于写操作的参数选项，而WriteBatch对象，相当于一个缓冲区，用于定义、保存一系列的批量操作。其代码实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Write(const WriteOptions\u0026amp; options, WriteBatch* updates) { //实例化一个Writer对象b并插入writers_队列中等待执行  Writer w(\u0026amp;mutex_); w.batch = updates; w.sync = options.sync; w.done = false; MutexLock l(\u0026amp;mutex_); writers_.push_back(\u0026amp;w); while (!w.done \u0026amp;\u0026amp; \u0026amp;w != writers_.front()) { w.cv.Wait(); } if (w.done) { return w.status; } Status status = MakeRoomForWrite(updates == nullptr); uint64_t last_sequence = versions_-\u0026gt;LastSequence(); Writer* last_writer = \u0026amp;w; if (status.ok() \u0026amp;\u0026amp; updates != nullptr) { //合并写入操作  WriteBatch* write_batch = BuildBatchGroup(\u0026amp;last_writer); WriteBatchInternal::SetSequence(write_batch, last_sequence + 1); last_sequence += WriteBatchInternal::Count(write_batch); { mutex_.Unlock(); //将更新写入日志文件中，并且将日志文件写入磁盘中  status = log_-\u0026gt;AddRecord(WriteBatchInternal::Contents(write_batch)); bool sync_error = false; if (status.ok() \u0026amp;\u0026amp; options.sync) { status = logfile_-\u0026gt;Sync(); if (!status.ok()) { sync_error = true; } } //将更新写入Memtable中  if (status.ok()) { status = WriteBatchInternal::InsertInto(write_batch, mem_); } mutex_.Lock(); if (sync_error) { RecordBackgroundError(status); } } if (write_batch == tmp_batch_) tmp_batch_-\u0026gt;Clear(); versions_-\u0026gt;SetLastSequence(last_sequence); } //由于和并写入操作一次可能会处理多个writer_队列中的元素，因此将所有已经处理的元素状态进行变更，并且发送signal信号  while (true) { Writer* ready = writers_.front(); writers_.pop_front(); if (ready != \u0026amp;w) { ready-\u0026gt;status = status; ready-\u0026gt;done = true; ready-\u0026gt;cv.Signal(); } if (ready == last_writer) break; } //通知writers_队列中的第一个元素，发送signal信号  if (!writers_.empty()) { writers_.front()-\u0026gt;cv.Signal(); } return status; }   具体的实现流程如下图所示：\n  实例化一个 Writer 对象，并将其插入所示的 writers_ 队列中。\n  通过 Writer 中的条件变量 cv 调用 wait 方法将该线程挂起，等待其他线程发送 signal 信号，并且等待队列前面的 Writer 操作全部执行完毕：\n 如果线程收到了 signal 信号：则解除阻塞。 如果线程没有收到了 signal 信号：说明队列前面仍有其他的 Writer 操作，那么该线程会再次调用 wait 方法实现阻塞，从而保证了 Writer 操作按照队列生成次序执行。    当轮到本线程操作时，首先通过 MakeRoomForWrite 函数进行内存空间分配。\n  当获取到需要的内存后，根据一系列的批量操作，对 Log 文件以及 MemTable 分别进行更新。\n  依据批量操作的数目更新 SequenceNumber。\n  通过 Writer 中的条件变量 cv 发送 signal 信号，以通知处于等待状态的其他线程开始执行。\n  ","date":"2022-05-23T23:48:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-dbimpl%E6%A8%A1%E5%9D%97/","title":"LevelDB DBImpl模块"},{"content":"Compaction模块 LevelDB 中的 Level 代表层级，有 0～6 共 7 个层级，每个层级都由一定数量的 SSTable 文件组成。其中，高层级文件是由低层级的一个文件与高层级中与该文件有键重叠的所有文件使用归并排序算法生成，该过程称为Compaction。\nLevelDB 通过 Compaction 将冷数据逐层下移，并且在 Compaction 过程中重复写入的键只会保留一个最终值，已经删除的键不再写入，因此可以减少磁盘空间占用。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  // https://github.com/google/leveldb/blob/master/db/version_set.h  class Compaction { public: ~Compaction(); int level() const { return level_; } VersionEdit* edit() { return \u0026amp;edit_; } int num_input_files(int which) const { return inputs_[which].size(); } FileMetaData* input(int which, int i) const { return inputs_[which][i]; } uint64_t MaxOutputFileSize() const { return max_output_file_size_; } bool IsTrivialMove() const; void AddInputDeletions(VersionEdit* edit); bool IsBaseLevelForKey(const Slice\u0026amp; user_key); bool ShouldStopBefore(const Slice\u0026amp; internal_key); void ReleaseInputs(); private: friend class Version; friend class VersionSet; Compaction(const Options* options, int level); int level_; uint64_t max_output_file_size_; Version* input_version_; VersionEdit edit_; std::vector\u0026lt;FileMetaData*\u0026gt; inputs_[2]; // The two sets of inputs  std::vector\u0026lt;FileMetaData*\u0026gt; grandparents_; size_t grandparent_index_; // Index in grandparent_starts_  bool seen_key_; // Some output key has been seen  int64_t overlapped_bytes_; // Bytes of overlap between current output  // and grandparent files  size_t level_ptrs_[config::kNumLevels]; };   在 LevelDB 中，Compaction 共有两种，分别叫 Minor Compaction 和 Major Compaction。\n  Minor Compaction：将 Immtable dump 到 SStable 。\n  Major Compaction：Level 之间的 SSTable Compaction。\n  这两类compaction负责在不同的场景下进行不同的数据整理。\nMinor Compaction 定义 Minor Compaction 非常简单，其本质就是将一个内存数据库（Memtable）中的所有数据持久化到一个磁盘文件中（SSTable）。整体流程如下图：\n触发时机 在 LSM 树的实现中，会先将数据写入 MemTable，当 MemTable 大小超过 options_.write_buffer_size （默认4M）时，需要将其作为 SSTable 写入磁盘，此时就会采取 Minor Compaction。\n核心要点  每次 Minor Compaction 结束后，都会生成一个新的 SSTable 文件，也意味着 Leveldb 的版本状态发生了变化，会进行一个版本的更替。 Minor Compaction 是一个时效性要求非常高的过程，要求其在尽可能短的时间内完成，否则就会堵塞正常的写入操作，因此 Minor Compaction 的优先级高于 Major Compaction。当进行 Minor Compaction 的时候有 Major Compaction正在进行，则会首先暂停 Major Compaction。  Major Compaction 定义 Major Compaction 是将不同层级的 SSTable 文件进行合并。\n如下图，可以看出其比 Minor Compaction 复杂的多。\n触发时机 那么什么时候，会触发 LevelDB 进行 Major Compaction 呢？总的来说为以下三个条件：\n 当 0 层文件数超过预定的上限（默认为 4 个）。 当 Level i层文件的总大小超过 10 ^ i MB。 当某个文件无效读取的次数过多。  这也就引出了 Size Compaction 与 Seek Compaction 两种判断策略。LevelDB先按 Size Compaction 判断是否需要进行 Compaction ，如果 Size Compaction 不满足则通过 Seek Compaction 继续判断，如果仍不满足，则表明暂时不需要进行 Compaction。\nSize Compaction size_compaction 通过判断 Level 0 中的文件个数（Level 0 会被频繁访问）或者 Level 1 ～ Level 5 的文件总大小来计算得出需要进行 Compaction 的 Level。\n该赋值逻辑位于 VersionSet 中的 Finalize，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // https://github.com/google/leveldb/blob/master/db/version_set.cc  void VersionSet::Finalize(Version* v) { int best_level = -1; double best_score = -1; for (int level = 0; level \u0026lt; config::kNumLevels - 1; level++) { double score; //对于Level 0来说，其分数为文件个数 / 4  if (level == 0) { score = v-\u0026gt;files_[level].size() / static_cast\u0026lt;double\u0026gt;(config::kL0_CompactionTrigger); } else { //对于Level 1~5的分数由该层所有文件总大小除以每层允许的最大大小决定  const uint64_t level_bytes = TotalFileSize(v-\u0026gt;files_[level]); score = static_cast\u0026lt;double\u0026gt;(level_bytes) / MaxBytesForLevel(options_, level); } //选取最高分的一个level  if (score \u0026gt; best_score) { best_level = level; best_score = score; } } //更新level与score  v-\u0026gt;compaction_level_ = best_level; v-\u0026gt;compaction_score_ = best_score; }   compaction_score_ 的赋值逻辑如下：\n Level 0：将当前 Level 0 包含的文件个数除以 4 并赋值给 compaction_score_ ，如果 Level 0 的文件个数大于等于 4，则此时 compaction_score_ 会大于等于 1。 Level 1～Level 5：通过该层文件的总大小除以该层文件允许的最大大小并赋值给 compaction_score_。  每次当版本中的 compaction_score_ 大于等于 1 时，则需要进行一次 Compaction 操作。\nSeek Compaction Seek compaction 主要通过记录某个 SSTable 的 Seek 次数，当其无效读取次数到达阈值（allowed_seeks）之后，将会记录下它的 level，参与下一次压缩。\n阈值 allowed_seeks 是每个文件允许的最大无效读取次数，该值的计算代码在 VersionSet::Build 中的 Apply，代码如下：\n1 2  f-\u0026gt;allowed_seeks = static_cast\u0026lt;int\u0026gt;((f-\u0026gt;file_size / 16384U)); if (f-\u0026gt;allowed_seeks \u0026lt; 100) f-\u0026gt;allowed_seeks = 100;   allowed_seeks 计算逻辑为文件大小除以 16384 后取值。但如果计算得到的值小于 100，则将其设置为 100。\n 为什么是除以16384呢？LevelDB作者给出了这样的解释：\n 硬盘中的一次查找操作耗费10ms。 硬盘读取速度为100MB/s，因此读取或者写入1MB数据需要10ms。 执行Compaction操作时，1MB的数据需要25MB数据的I/O，因为从当前层级读取1MB后，相应地需要从下一个层级读取10MB～12MB（因为每一层的最大大小为前一层的10倍，并且考虑到边界重叠的情况，因此执行Compaction操作时需要读取下一层的10MB～12MB数据），然后执行归并排序后写入的10MB～12MB的数据到下一个层级，因此读取加写入最大需要25MB数据的I/O。 因此25次查找（约耗费250ms）约略等于一次执行Compaction操作时处理1MB数据的时间（1MB的当前层读取加10MB～12MB的下一层读取，再加10MB～12MB的下一层写入，约为25MB的数据读取和写入总量，因此也是消耗250ms）。那么一次查找的数据量约略等于处理Compaction操作时的40KB数据（1MB除以25）。进一步保守处理，取16384（16K）这个值，即当查找次数超出allowed_seeks时，执行一次Compaction操作是一个更加合理的选择。   file_to_compact_level_ 的赋值逻辑位于 Version 的 UpdateStats，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // https://github.com/google/leveldb/blob/master/db/version_set.cc  bool Version::UpdateStats(const GetStats\u0026amp; stats) { //此时的f为无效查找的文件  FileMetaData* f = stats.seek_file; if (f != nullptr) { //当前查找无效，allowed_seeks-1  f-\u0026gt;allowed_seeks--; //如果此时allowed_seeks小于等于0，则说明此时到达阈值，则将当前层级记录下来，待进行Compaction  if (f-\u0026gt;allowed_seeks \u0026lt;= 0 \u0026amp;\u0026amp; file_to_compact_ == nullptr) { file_to_compact_ = f; file_to_compact_level_ = stats.seek_file_level; return true; } } return false; }   假设进行无效查找的文件为f（FileMetaData结构），先将 f 的 allowed_seeks 次数减 1，此时判断如果allowed_seeks 变量已经小于等于 0 且 Version 中的 file_to_compact_ 成员变量为空，则将 f 赋值给file_to_compact_ ，并且将 f 所属层级赋值给 file_to_compact_level_ 变量。\nManual Compaction Manual Compaction 是指人工触发的 Compaction，由外部接口调用产生。\n实际其内部触发调用的接口是 DBImpl 中的 CompactRange，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  void DBImpl::CompactRange(const Slice* begin, const Slice* end) { int max_level_with_files = 1; { MutexLock l(\u0026amp;mutex_); Version* base = versions_-\u0026gt;current(); for (int level = 1; level \u0026lt; config::kNumLevels; level++) { if (base-\u0026gt;OverlapInLevel(level, begin, end)) { max_level_with_files = level; } } } //略过所有没有重叠的文件  TEST_CompactMemTable(); //一层层压缩存在重叠的文件  for (int level = 0; level \u0026lt; max_level_with_files; level++) { TEST_CompactRange(level, begin, end); } }   在 Manual Compaction 中会指定的 begin 和 end，它将会一个层层的分次的 Compact 所有 Level 中与 begin 和 end 有重叠（overlap）的 SSTable 文件。\n文件选取 每次进行 Compaction 时，首先决定在哪个层级进行该次操作，假设为Level n，接着选取 Level n 层参与的文件，然后选取 Level n+1 层需要参与的文件，最后对选中的文件使用归并排序生成一个新文件。\nPickCompaction 决定层级 n 以及选取 Level n 层参与文件的方法为 VersionSet 中的 PickCompaction。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Compaction* VersionSet::PickCompaction() { Compaction* c; int level; //根据current_判断是size_compaction还是seek_compaction  const bool size_compaction = (current_-\u0026gt;compaction_score_ \u0026gt;= 1); const bool seek_compaction = (current_-\u0026gt;file_to_compact_ != nullptr); /* 如果根据size_compaction触发，则根据每个层级的compact_pointer_选取本次Compaction的 level n层文件（记录每个层级下一次开始进行Compaction操作时需要从哪个键开始。） */ if (size_compaction) { //获取当前层级  level = current_-\u0026gt;compaction_level_; assert(level \u0026gt;= 0); assert(level + 1 \u0026lt; config::kNumLevels); c = new Compaction(options_, level); //选出第一个在compact_pointer_之后的文件  for (size_t i = 0; i \u0026lt; current_-\u0026gt;files_[level].size(); i++) { FileMetaData* f = current_-\u0026gt;files_[level][i]; if (compact_pointer_[level].empty() || icmp_.Compare(f-\u0026gt;largest.Encode(), compact_pointer_[level]) \u0026gt; 0) { c-\u0026gt;inputs_[0].push_back(f); break; } } //如果通过compact_pointer_没有选取到文件（Compaction已遍历本层），则选取本层第一个文件  if (c-\u0026gt;inputs_[0].empty()) { // Wrap-around to the beginning of the key space  c-\u0026gt;inputs_[0].push_back(current_-\u0026gt;files_[level][0]); } } else if (seek_compaction) { //如果根据seek_compaction触发，则直接将无效查找次数超限的文件选取为本次的Level n层文件  level = current_-\u0026gt;file_to_compact_level_; c = new Compaction(options_, level); c-\u0026gt;inputs_[0].push_back(current_-\u0026gt;file_to_compact_); } else { return nullptr; } c-\u0026gt;input_version_ = current_; c-\u0026gt;input_version_-\u0026gt;Ref(); //对level 0特殊处理  if (level == 0) { InternalKey smallest, largest; //取出Level 0中参与本次Compaction操作的文件的最小键和最大键  GetRange(c-\u0026gt;inputs_[0], \u0026amp;smallest, \u0026amp;largest); //根据最小键和最大键对比Level 0中的所有文件，如果存在文件与[Lkey,Hkey]有重叠，则扩大最小键和最大键范围，并继续查找。  current_-\u0026gt;GetOverlappingInputs(0, \u0026amp;smallest, \u0026amp;largest, \u0026amp;c-\u0026gt;inputs_[0]); assert(!c-\u0026gt;inputs_[0].empty()); } //调用SetupOtherInputs选取level n+1层需要参与的文件  SetupOtherInputs(c); return c; }   执行逻辑如下：\n 根据 current_ 判断是 size_compaction 还是 seek_compaction：  根据 size_compaction 触发：则根据每个层级的 compact_pointer_ 选取本次 Compaction 的 level n 层文件（记录每个层级下一次开始进行 Compaction 时需要从哪个键开始）。 根据 seek_compaction 触发：则直接将无效查找次数超限的文件选取为本次的 Level n 层文件。   对 level 0 特殊处理：  取出 Level 0 中参与本次 Compaction 的文件的最小键和最大键，假设其范围为 [Lkey,Hkey]。 根据最小键和最大键对比 Level 0 中的所有文件，如果存在文件与 [Lkey,Hkey] 有重叠，则扩大最小键和最大键范围，并继续查找。   调用 SetupOtherInputs 选取 level n+1 层需要参与的文件。   为何Level 0中需要扩展有键重叠的文件呢？\n举例说明，假设Level 0有4个文件：f1、f2、f3、f4，每个文件的键范围分别为[c,e]，[a,f]，[a,b]，[i,z]。\n通过第一步选取的inputs_ [0]文件是f1，f1中的键范围和f2有重叠，则扩大最小键和最大键范围到[a,f]，此时发现f3的键范围也和f2有重叠，因此最终inputs_[0]中的文件包括f1、f2、f3三个文件。\n假设有这样一种情况，我们首先写了d这个键，在f2中的序列号为10，然后删除了d，删除操作在f1中的序列号为100，假设Compaction操作时只是选取了f1，则下次查找d这个键时先从Level 0选取，会读取到f2中序列号为10的值（实际上该键已经删除），此时会出现错误。\n SetupOtherInputs PickCompaction 会选定进行 Compaction 操作的层级 n 以及 Level n 层的参与文件，之后会调用SetupOtherInputs 进行 Level n+1 层文件的选取，SetupOtherInputs 的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/db/version_set.cc  void VersionSet::SetupOtherInputs(Compaction* c) { const int level = c-\u0026gt;level(); InternalKey smallest, largest; AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level], \u0026amp;c-\u0026gt;inputs_[0]); //获取input_[0]所有文件的最大键和最小键  GetRange(c-\u0026gt;inputs_[0], \u0026amp;smallest, \u0026amp;largest); //根据input_[0]中的最大键/最小键查找level n+1层的文件，并分别赋值到input_[1]中  current_-\u0026gt;GetOverlappingInputs(level + 1, \u0026amp;smallest, \u0026amp;largest, \u0026amp;c-\u0026gt;inputs_[1]); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level + 1], \u0026amp;c-\u0026gt;inputs_[1]); InternalKey all_start, all_limit; //继续获取input_[0]和[1]的所有文件的最大键和最小键  GetRange2(c-\u0026gt;inputs_[0], c-\u0026gt;inputs_[1], \u0026amp;all_start, \u0026amp;all_limit); //在不扩大level n+1层的前提下，尝试扩大level n层的文件，并且扩大后的文件总大小不超过50M  if (!c-\u0026gt;inputs_[1].empty()) { std::vector\u0026lt;FileMetaData*\u0026gt; expanded0; current_-\u0026gt;GetOverlappingInputs(level, \u0026amp;all_start, \u0026amp;all_limit, \u0026amp;expanded0); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level], \u0026amp;expanded0); const int64_t inputs0_size = TotalFileSize(c-\u0026gt;inputs_[0]); const int64_t inputs1_size = TotalFileSize(c-\u0026gt;inputs_[1]); const int64_t expanded0_size = TotalFileSize(expanded0); if (expanded0.size() \u0026gt; c-\u0026gt;inputs_[0].size() \u0026amp;\u0026amp; inputs1_size + expanded0_size \u0026lt; ExpandedCompactionByteSizeLimit(options_)) { InternalKey new_start, new_limit; GetRange(expanded0, \u0026amp;new_start, \u0026amp;new_limit); std::vector\u0026lt;FileMetaData*\u0026gt; expanded1; current_-\u0026gt;GetOverlappingInputs(level + 1, \u0026amp;new_start, \u0026amp;new_limit, \u0026amp;expanded1); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level + 1], \u0026amp;expanded1); if (expanded1.size() == c-\u0026gt;inputs_[1].size()) { Log(options_-\u0026gt;info_log, \u0026#34;Expanding@%d %d+%d (%ld+%ld bytes) to %d+%d (%ld+%ld bytes)\\n\u0026#34;, level, int(c-\u0026gt;inputs_[0].size()), int(c-\u0026gt;inputs_[1].size()), long(inputs0_size), long(inputs1_size), int(expanded0.size()), int(expanded1.size()), long(expanded0_size), long(inputs1_size)); smallest = new_start; largest = new_limit; c-\u0026gt;inputs_[0] = expanded0; c-\u0026gt;inputs_[1] = expanded1; GetRange2(c-\u0026gt;inputs_[0], c-\u0026gt;inputs_[1], \u0026amp;all_start, \u0026amp;all_limit); } } } if (level + 2 \u0026lt; config::kNumLevels) { current_-\u0026gt;GetOverlappingInputs(level + 2, \u0026amp;all_start, \u0026amp;all_limit, \u0026amp;c-\u0026gt;grandparents_); } //将本次Compaction的最大键保存到compact_pointer_中，下次Compaction时根据该值选取level n层文件  compact_pointer_[level] = largest.Encode().ToString(); c-\u0026gt;edit_.SetCompactPointer(level, largest); }   当 Level n 层和 Level n+1 层的文件都已经选定，LevelDB 的实现中有一个优化点，即判断是否可以在不扩大 Level n+1 层文件个数的情况下，将 Level n 层的文件个数扩大，优化逻辑如下：\n1. inputs_ [1] 选取完毕之后，首先计算 inputs_ [0] 和 inputs_ [1] 所有文件的最大/最小键范围，然后通过该范围重新去 Level n 层计算 inputs_ [0]，此时有可能选取到新的文件进入 inputs_ [0]。\r2. 通过新的 inputs_ [0] 的键范围重新选取 inputs_ [1] 中的文件，如果 inputs_ [1] 中的文件个数不变并且扩大范围后所有文件的总大小不超过50MB，则使用新的 inputs_ [0] 进行本次 Compaction ，否则继续使用原来的inputs_ [0]。50 MB 的限制是防止执行一次 Compaction 导致大量的 I/O 操作，从而影响系统性能。\r3. 如果扩大 Level n 层的文件个数之后导致 Level n+1 层的文件个数也进行了扩大，则不能进行此次优化。因为Level 1到 Level 6 的所有文件键范围不能有重叠，如果继续执行该优化，会导致 Compaction 之后 Level n+1 层的文件有键重叠的情况产生。\r 整体流程 其代码实现在 DBImpl 中的 DoCompactionWork，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::DoCompactionWork(CompactionState* compact) { //...  //计算最小序列号，如果Compaction中存在重复写入或者删除的键，则根据序列号判断是否需要删除  //如果没有快照，则将当前版本的last_sequence赋值为最小的序列号  if (snapshots_.empty()) { compact-\u0026gt;smallest_snapshot = versions_-\u0026gt;LastSequence(); } else { //如果有则根据最老的快照获取序列号  compact-\u0026gt;smallest_snapshot = snapshots_.oldest()-\u0026gt;sequence_number(); } //获取一个归并排序迭代器，每次选取最小的键写入文件  Iterator* input = versions_-\u0026gt;MakeInputIterator(compact-\u0026gt;compaction); //...  input-\u0026gt;SeekToFirst(); //...  //遍历迭代器  while (input-\u0026gt;Valid() \u0026amp;\u0026amp; !shutting_down_.load(std::memory_order_acquire)) { //...  //用于标记一个文件是否需要删除  bool drop = false; //...  //如果需要写入新文件，则写入到SSTable中  if (!drop) { if (compact-\u0026gt;builder == nullptr) { status = OpenCompactionOutputFile(compact); if (!status.ok()) { break; } } if (compact-\u0026gt;builder-\u0026gt;NumEntries() == 0) { compact-\u0026gt;current_output()-\u0026gt;smallest.DecodeFrom(key); } compact-\u0026gt;current_output()-\u0026gt;largest.DecodeFrom(key); compact-\u0026gt;builder-\u0026gt;Add(key, input-\u0026gt;value()); if (compact-\u0026gt;builder-\u0026gt;FileSize() \u0026gt;= compact-\u0026gt;compaction-\u0026gt;MaxOutputFileSize()) { status = FinishCompactionOutputFile(compact, input); if (!status.ok()) { break; } } } //继续查找归并排序中下一个最小的键  input-\u0026gt;Next(); } //...  //生成一个SSTable文件并刷新到磁盘  if (status.ok() \u0026amp;\u0026amp; compact-\u0026gt;builder != nullptr) { status = FinishCompactionOutputFile(compact, input); } //...  //调用VersionSet中的LogAndApply生成新的版本  if (status.ok()) { status = InstallCompactionResults(compact); } //... }   DoCompactionWork 的执行步骤如下：\n 计算一个本次 Compaction 的最小序列号值，如果有快照，则取最老的快照的序列号，如果没有快照，则选取当前版本 current_ 的序列号。因为快照存在时需要有一个一致性的读取视图，因此如果一个键的序列号比该值大，则该键不能够删除。 生成一个归并排序的迭代器，该迭代器会遍历 inputs_ [0] 和 inputs_ [1] 中的所有文件，每次选取一个最小的键写入新生成的文件。 选取键之后，判断该键是否可以删除，两种情况下可以删除一个键：  第一种情况为重复写入一个键，因为新键的序列号更大，因此之前被覆盖的键可以删除（当然被删除键的序列号需要小于第一步中计算得到的最小序列号）。 第二种情况为删除了一个键（实际上也是写入该键，不过被标记为删除操作），并且更高层级没有该键，则该键可以彻底删除（前提也是该键的序列号需要小于第 1 步中计算得到的最小序列号），即不需要写入新生成的 SSTable。   如果该键不需要删除，则将其写入新生成的 SSTable，并且当一个 SSTable 大小大于 2 MB 时，将该文件刷新到磁盘并且重新打开一个新的 SSTable。 执行 VersionSet 的 LogAndApply，生成一个新的版本并挂载到 VersionSet 中，并且将新版本赋值为当前版本。  垃圾回收 随着 Compaction 操作的进行，会有新文件生成，生成新文件之后可以进行旧文件清理。每次当一个 MemTable生成 SSTable 并刷新到磁盘之后，该 MemTable 对应的日志也可以进行删除。\nLevelDB 中负责清理文件的是 RemoveObsoleteFiles，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  void DBImpl::RemoveObsoleteFiles() { mutex_.AssertHeld(); if (!bg_error_.ok()) { return; } //将所有正在Compaction和versionset中的版本文件放入live集合  std::set\u0026lt;uint64_t\u0026gt; live = pending_outputs_; versions_-\u0026gt;AddLiveFiles(\u0026amp;live); //将所有数据目录下的文件放入filenames数组中  std::vector\u0026lt;std::string\u0026gt; filenames; env_-\u0026gt;GetChildren(dbname_, \u0026amp;filenames); // Ignoring errors on purpose  uint64_t number; FileType type; std::vector\u0026lt;std::string\u0026gt; files_to_delete; //保存可以删除的文件  //遍历所有文件，判断是否可以删除  for (std::string\u0026amp; filename : filenames) { //解析出每个文件的序列号和类型  if (ParseFileName(filename, \u0026amp;number, \u0026amp;type)) { //keep标记文件是否需要保留  bool keep = true; switch (type) { //删除序列号小于versionset的log_number，且不等于prev_log_number的日志  case kLogFile: keep = ((number \u0026gt;= versions_-\u0026gt;LogNumber()) || (number == versions_-\u0026gt;PrevLogNumber())); break; //删除版本较低的Manifest  case kDescriptorFile: keep = (number \u0026gt;= versions_-\u0026gt;ManifestFileNumber()); break; //删除没有参与Compaction的且不在versionset中的sstable  case kTableFile: keep = (live.find(number) != live.end()); break; //临时文件  case kTempFile: keep = (live.find(number) != live.end()); break; case kCurrentFile: case kDBLockFile: case kInfoLogFile: keep = true; break; } //将keep为false的放入files_to_delete，后续删除。  if (!keep) { files_to_delete.push_back(std::move(filename)); //如果是sstable文件则从缓存中删除  if (type == kTableFile) { table_cache_-\u0026gt;Evict(number); } Log(options_.info_log, \u0026#34;Delete type=%d #%lld\\n\u0026#34;, static_cast\u0026lt;int\u0026gt;(type), static_cast\u0026lt;unsigned long long\u0026gt;(number)); } } } //删除所有files_to_delete中的文件  mutex_.Unlock(); for (const std::string\u0026amp; filename : files_to_delete) { env_-\u0026gt;RemoveFile(dbname_ + \u0026#34;/\u0026#34; + filename); } mutex_.Lock(); }   执行逻辑如下：\n 将正在进行 Compaction 操作的 SSTable 文件和 VersionSet 的所有版本中的 SSTable 文件放入 live 集合中。 通过 filename 找到对应的文件，解析出文件序列号和文件类型。 遍历所有文件，判断是否需要删除，如果需要删除则将 keep 标记为 false，并放入 files_to_delete 中。 遍历 files_to_delete 数组，调用 RemoveObsoleteFiles 删除文件。  ","date":"2022-05-23T23:47:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-compaction%E6%A8%A1%E5%9D%97/","title":"LevelDB Compaction模块"},{"content":"版本管理  为什么 LevelDB 需要版本的概念呢？\n 针对共享的资源，有三种方式：\n 悲观锁：这是最简单的处理方式。加锁保护，读写互斥。效率低。 乐观锁：它假设多用户并发的事物在处理时不会彼此互相影响，各事务能够在不产生锁的的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。 果其他事务有更新的话，正在提交的事务会进行回滚；这样做不会有锁竞争更不会产生死锁， 但如果数据竞争的概率较高，效率也会受影响 。 MVCC：MVCC 是一个数据库常用的概念。Multi Version Concurrency Control 多版本并发控制。每一个执行操作的用户，看到的都是数据库特定时刻的的快照 （Snapshot）， Writer 的任何未完成的修改都不会被其他的用户所看到；当对数据进行更新的时候并是不直接覆盖，而是先进行标记，然后在其他地方添加新的数据（这些变更存储在 VersionEdit），从而形成一个新版本，此时再来读取的 Reader 看到的就是最新的版本了。所以这种处理策略是维护了多个版本的数据的，但只有一个是最新的（VersionSet 中维护着全局最新的 Seqnum）。  LevelDB 通过 Version 以及 VersionSet 来管理元信息，用 Manifest 来保存元信息。\nManifest Manifest 文件专用于记录版本信息。LevelDB 采用了增量式的存储方式，记录每一个版本相较于一个版本的变化情况。\n 变化情况大致包括：\n（1）新增了哪些 SSTable 文件；\n（2）删除了哪些 SSTable 文件（由于Compaction导致）；\n（3）最新的 Journal 日志文件标号等；\n 一个 Manifest 文件中，包含了多条 Session Record。其中第一条 Session Record 记载了当时 LevelDB 的全量版本信息，其余若干条 Session Record 仅记录每次更迭的变化情况。具体结构如下图：\nLevelDB 启动时会先到数据目录寻找一个名为 CURRENT 的文件，该文件中会保存 Manifest 的文件名称，通过读取 Manifest 记录的 Session Record，从初始状态开始不断地应用这些版本改动，即可使得系统的版本信息恢复到最近一次使用的状态。\nVersion Version 表示当前的一个版本，该结构中会保存每个层级拥有的文件信息以及指向前一个和后一个版本的指针等。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85  class Version { public: struct GetStats { FileMetaData* seek_file; int seek_file_level; }; void AddIterators(const ReadOptions\u0026amp;, std::vector\u0026lt;Iterator*\u0026gt;* iters); Status Get(const ReadOptions\u0026amp;, const LookupKey\u0026amp; key, std::string* val, GetStats* stats); bool UpdateStats(const GetStats\u0026amp; stats); bool RecordReadSample(Slice key); void Ref(); void Unref(); void GetOverlappingInputs( int level, const InternalKey* begin, // nullptr means before all keys  const InternalKey* end, // nullptr means after all keys  std::vector\u0026lt;FileMetaData*\u0026gt;* inputs); bool OverlapInLevel(int level, const Slice* smallest_user_key, const Slice* largest_user_key); int PickLevelForMemTableOutput(const Slice\u0026amp; smallest_user_key, const Slice\u0026amp; largest_user_key); int NumFiles(int level) const { return files_[level].size(); } std::string DebugString() const; private: friend class Compaction; friend class VersionSet; class LevelFileNumIterator; explicit Version(VersionSet* vset) : vset_(vset), next_(this), prev_(this), refs_(0), file_to_compact_(nullptr), file_to_compact_level_(-1), compaction_score_(-1), compaction_level_(-1) {} Version(const Version\u0026amp;) = delete; Version\u0026amp; operator=(const Version\u0026amp;) = delete; ~Version(); Iterator* NewConcatenatingIterator(const ReadOptions\u0026amp;, int level) const; void ForEachOverlapping(Slice user_key, Slice internal_key, void* arg, bool (*func)(void*, int, FileMetaData*)); VersionSet* vset_; // VersionSet to which this Version belongs  Version* next_; // Next version in linked list  Version* prev_; // Previous version in linked list  int refs_; // Number of live refs to this version  std::vector\u0026lt;FileMetaData*\u0026gt; files_[config::kNumLevels]; FileMetaData* file_to_compact_; int file_to_compact_level_; double compaction_score_; int compaction_level_; }; struct FileMetaData { FileMetaData() : refs(0), allowed_seeks(1 \u0026lt;\u0026lt; 30), file_size(0) {} int refs;\t// 引用计数  int allowed_seeks; // 用于seek compaction  uint64_t number;\t// 唯一标识一个sstable  uint64_t file_size; // 文件大小  InternalKey smallest; // 最小key  InternalKey largest; // 最大key };    成员变量：  GetStats：键查找时用来保存中间状态的一个结构。 vset_：该版本属于的版本集合。 next_：指向后一个版本的指针。 prev_：指向前一个版本的指针。 refs_：该版本的引用计数。 files_：每个层级所包含的 SSTable 文件，每一个文件以一个 FileMetaData 结构表示。 file_to_compact_：下次需要进行 Compaction 操作的文件。 file_to_compact_level_：下次需要进行 Compaction 操作的文件所属的层级。 compaction_score_：如果 compaction_score_ 大于 1，说明需要进行一次 Compaction 操作。 compaction_level_：表明需要进行 Compaction 操作的层级。    VersionEdit **VersionEdit 是一个版本的中间状态，会保存一次 Compaction 操作后增加的删除文件信息以及其他一些元数据。**当数据库正常/非正常关闭，重新打开时，只需要按顺序把 Manifest 文件中的 VersionEdit 执行一遍，就可以把数据恢复到宕机前的最新版本。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  // https://github.com/google/leveldb/blob/master/db/version_edit.h  class VersionEdit { public: VersionEdit() { Clear(); } ~VersionEdit() = default; void Clear(); void SetComparatorName(const Slice\u0026amp; name) { has_comparator_ = true; comparator_ = name.ToString(); } void SetLogNumber(uint64_t num) { has_log_number_ = true; log_number_ = num; } void SetPrevLogNumber(uint64_t num) { has_prev_log_number_ = true; prev_log_number_ = num; } void SetNextFile(uint64_t num) { has_next_file_number_ = true; next_file_number_ = num; } void SetLastSequence(SequenceNumber seq) { has_last_sequence_ = true; last_sequence_ = seq; } void SetCompactPointer(int level, const InternalKey\u0026amp; key) { compact_pointers_.push_back(std::make_pair(level, key)); } void AddFile(int level, uint64_t file, uint64_t file_size, const InternalKey\u0026amp; smallest, const InternalKey\u0026amp; largest) { FileMetaData f; f.number = file; f.file_size = file_size; f.smallest = smallest; f.largest = largest; new_files_.push_back(std::make_pair(level, f)); } void RemoveFile(int level, uint64_t file) { deleted_files_.insert(std::make_pair(level, file)); } void EncodeTo(std::string* dst) const; Status DecodeFrom(const Slice\u0026amp; src); std::string DebugString() const; private: friend class VersionSet; typedef std::set\u0026lt;std::pair\u0026lt;int, uint64_t\u0026gt;\u0026gt; DeletedFileSet; std::string comparator_; uint64_t log_number_;\t//已经弃用  uint64_t prev_log_number_; uint64_t next_file_number_; SequenceNumber last_sequence_; bool has_comparator_; bool has_log_number_; bool has_prev_log_number_; bool has_next_file_number_; bool has_last_sequence_; std::vector\u0026lt;std::pair\u0026lt;int, InternalKey\u0026gt;\u0026gt; compact_pointers_; DeletedFileSet deleted_files_; std::vector\u0026lt;std::pair\u0026lt;int, FileMetaData\u0026gt;\u0026gt; new_files_; };    成员变量：  comparator_：比较器名称。 log_number_：日志文件序号。 **next_file_number_：**下一个文件序列号。 last_sequence_：下一个写入序列号。 has_xxxxx_：has_comparator_ ，has_log_number_ ，has_prev_log_number_ ，has_last_sequence_ ，has_next_file_number_，布尔型变量，表明相应的成员变量是否已经设置。 compact_pointers_：该变量用来指示 LevelDB 中每个层级下一次进行 Compaction 操作时需要从哪个键开始。 **deleted_files_：**记录每个层级执行 Compaction 操作之后删除掉的文件。 new_files_：记录每个层级执行 Compaction 操作之后新增的文件。新增文件记录为一个个FileMetaData 结构体。    EncodeTo EncodeTo 会将 VersionEdit 各个成员变量的信息编码为一个字符串，编码时会先给每个成员变量定义一个 Tag，Tag的枚举值如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  enum Tag { kComparator = 1,\t//比较器  kLogNumber = 2,\t//日志文件序列号  kNextFileNumber = 3,\t//下一个文件序列号  kLastSequence = 4,\t//下一个写入序列号  kCompactPointer = 5,\t//CompactPointer类型  kDeletedFile = 6,\t//删除的文件  kNewFile = 7,\t//增加的文件  // 8 曾经用于大Value的引用，现以弃用  kPrevLogNumber = 9\t//前一个日志文件序列号 };   接下来看看 EncodeTo 的实现逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  void VersionEdit::EncodeTo(std::string* dst) const { //如果为true，则先将kComparator Tag编码，然后将比较器名称编码写入。  if (has_comparator_) { PutVarint32(dst, kComparator); PutLengthPrefixedSlice(dst, comparator_); } //与上面类似，就不再重复  if (has_log_number_) { PutVarint32(dst, kLogNumber); PutVarint64(dst, log_number_); } if (has_prev_log_number_) { PutVarint32(dst, kPrevLogNumber); PutVarint64(dst, prev_log_number_); } if (has_next_file_number_) { PutVarint32(dst, kNextFileNumber); PutVarint64(dst, next_file_number_); } if (has_last_sequence_) { PutVarint32(dst, kLastSequence); PutVarint64(dst, last_sequence_); } //依次将compact_pointers_中的level和Key编码  for (size_t i = 0; i \u0026lt; compact_pointers_.size(); i++) { PutVarint32(dst, kCompactPointer); PutVarint32(dst, compact_pointers_[i].first); // level  PutLengthPrefixedSlice(dst, compact_pointers_[i].second.Encode()); } //依次将deleted_file_中的level和file number编码  for (const auto\u0026amp; deleted_file_kvp : deleted_files_) { PutVarint32(dst, kDeletedFile); PutVarint32(dst, deleted_file_kvp.first); // level  PutVarint64(dst, deleted_file_kvp.second); // file number  } //依次将new_files_中的level和FileMetaData进行编码  for (size_t i = 0; i \u0026lt; new_files_.size(); i++) { const FileMetaData\u0026amp; f = new_files_[i].second; PutVarint32(dst, kNewFile); PutVarint32(dst, new_files_[i].first); // level  //编码FileMetaData  PutVarint64(dst, f.number); PutVarint64(dst, f.file_size); PutLengthPrefixedSlice(dst, f.smallest.Encode()); PutLengthPrefixedSlice(dst, f.largest.Encode()); } }   EncodeTo 函数会依次以 Tag 开头，将比较器名称、日志序列号、上一个日志序列号、下一个文件序列号、最后一个序列号、CompactPointers、每个层级删除的文件以及增加的文件信息保存到一个字符串中。\nDecodeFrom 解码逻辑与上面的类似，根据不同的 tag 解析对应的成员变量，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  Status VersionEdit::DecodeFrom(const Slice\u0026amp; src) { Clear(); Slice input = src; const char* msg = nullptr; uint32_t tag; // Temporary storage for parsing  int level; uint64_t number; FileMetaData f; Slice str; InternalKey key; while (msg == nullptr \u0026amp;\u0026amp; GetVarint32(\u0026amp;input, \u0026amp;tag)) { //根据不同的tag执行对应的解码逻辑  switch (tag) { case kComparator: if (GetLengthPrefixedSlice(\u0026amp;input, \u0026amp;str)) { comparator_ = str.ToString(); has_comparator_ = true; } else { msg = \u0026#34;comparator name\u0026#34;; } break; case kLogNumber: if (GetVarint64(\u0026amp;input, \u0026amp;log_number_)) { has_log_number_ = true; } else { msg = \u0026#34;log number\u0026#34;; } break; case kPrevLogNumber: if (GetVarint64(\u0026amp;input, \u0026amp;prev_log_number_)) { has_prev_log_number_ = true; } else { msg = \u0026#34;previous log number\u0026#34;; } break; case kNextFileNumber: if (GetVarint64(\u0026amp;input, \u0026amp;next_file_number_)) { has_next_file_number_ = true; } else { msg = \u0026#34;next file number\u0026#34;; } break; case kLastSequence: if (GetVarint64(\u0026amp;input, \u0026amp;last_sequence_)) { has_last_sequence_ = true; } else { msg = \u0026#34;last sequence number\u0026#34;; } break; case kCompactPointer: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;key)) { compact_pointers_.push_back(std::make_pair(level, key)); } else { msg = \u0026#34;compaction pointer\u0026#34;; } break; case kDeletedFile: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;number)) { deleted_files_.insert(std::make_pair(level, number)); } else { msg = \u0026#34;deleted file\u0026#34;; } break; case kNewFile: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;f.number) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;f.file_size) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;f.smallest) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;f.largest)) { new_files_.push_back(std::make_pair(level, f)); } else { msg = \u0026#34;new-file entry\u0026#34;; } break; default: msg = \u0026#34;unknown tag\u0026#34;; break; } } if (msg == nullptr \u0026amp;\u0026amp; !input.empty()) { msg = \u0026#34;invalid tag\u0026#34;; } Status result; if (msg != nullptr) { result = Status::Corruption(\u0026#34;VersionEdit\u0026#34;, msg); } return result; }   VersionSet LevelDB 为了支持 MVCC，引入了 Version 和 VersionEdit 的概念，那么如何来有效的管理这些 Version 呢？于是引出了 VersionSet，VersionSet 是一个双向链表，而且整个 DB 只会有一个 VersionSet。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107  class VersionSet { public: VersionSet(const std::string\u0026amp; dbname, const Options* options, TableCache* table_cache, const InternalKeyComparator*); VersionSet(const VersionSet\u0026amp;) = delete; VersionSet\u0026amp; operator=(const VersionSet\u0026amp;) = delete; ~VersionSet(); Status LogAndApply(VersionEdit* edit, port::Mutex* mu) EXCLUSIVE_LOCKS_REQUIRED(mu); Status Recover(bool* save_manifest); Version* current() const { return current_; } uint64_t ManifestFileNumber() const { return manifest_file_number_; } uint64_t NewFileNumber() { return next_file_number_++; } void ReuseFileNumber(uint64_t file_number) { if (next_file_number_ == file_number + 1) { next_file_number_ = file_number; } } int NumLevelFiles(int level) const; int64_t NumLevelBytes(int level) const; uint64_t LastSequence() const { return last_sequence_; } void SetLastSequence(uint64_t s) { assert(s \u0026gt;= last_sequence_); last_sequence_ = s; } void MarkFileNumberUsed(uint64_t number); uint64_t LogNumber() const { return log_number_; } uint64_t PrevLogNumber() const { return prev_log_number_; } Compaction* PickCompaction(); Compaction* CompactRange(int level, const InternalKey* begin, const InternalKey* end); int64_t MaxNextLevelOverlappingBytes(); Iterator* MakeInputIterator(Compaction* c); bool NeedsCompaction() const { Version* v = current_; return (v-\u0026gt;compaction_score_ \u0026gt;= 1) || (v-\u0026gt;file_to_compact_ != nullptr); } void AddLiveFiles(std::set\u0026lt;uint64_t\u0026gt;* live); uint64_t ApproximateOffsetOf(Version* v, const InternalKey\u0026amp; key); struct LevelSummaryStorage { char buffer[100]; }; const char* LevelSummary(LevelSummaryStorage* scratch) const; private: class Builder; friend class Compaction; friend class Version; bool ReuseManifest(const std::string\u0026amp; dscname, const std::string\u0026amp; dscbase); void Finalize(Version* v); void GetRange(const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs, InternalKey* smallest, InternalKey* largest); void GetRange2(const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs1, const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs2, InternalKey* smallest, InternalKey* largest); void SetupOtherInputs(Compaction* c); Status WriteSnapshot(log::Writer* log); void AppendVersion(Version* v); Env* const env_; const std::string dbname_; const Options* const options_; TableCache* const table_cache_; const InternalKeyComparator icmp_; uint64_t next_file_number_; uint64_t manifest_file_number_; uint64_t last_sequence_; uint64_t log_number_; uint64_t prev_log_number_; // 0 or backing store for memtable being compacted  WritableFile* descriptor_file_; log::Writer* descriptor_log_; Version dummy_versions_; // Head of circular doubly-linked list of versions.  Version* current_; // == dummy_versions_.prev_  std::string compact_pointer_[config::kNumLevels]; };    关键成员变量：  **next_file_number_**下一个文件序列号。 manifest_file_number_：Manifest 文件的文件序列号。 last_sequence_：当前最大的写入序列号。 log_number_：Log文件的文件序列号。 current_：当前的最新版本。 compact_pointer_：记录每个层级下一次开始进行 Compaction 操作时需要从哪个键开始。    下面就介绍一下其中最核心的 LogAndApply 和 Recover 两个函数。\nLogAndApply 当每次进行完 Compaction 操作后，需要调用并执行 VersionSet 中的 LogAndApply 写入版本变化后并生成一个新的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) { //根据版本变化进行处理  if (edit-\u0026gt;has_log_number_) { assert(edit-\u0026gt;log_number_ \u0026gt;= log_number_); assert(edit-\u0026gt;log_number_ \u0026lt; next_file_number_); } else { edit-\u0026gt;SetLogNumber(log_number_); } if (!edit-\u0026gt;has_prev_log_number_) { edit-\u0026gt;SetPrevLogNumber(prev_log_number_); } edit-\u0026gt;SetNextFile(next_file_number_); edit-\u0026gt;SetLastSequence(last_sequence_); //生成新的版本，其为上一个Version+VersionEdit  Version* v = new Version(this); { Builder builder(this, current_); builder.Apply(edit); builder.SaveTo(v); } //计算这个新version的compact score和compact level，算出最应该被Compact的level  Finalize(v); std::string new_manifest_file; Status s; if (descriptor_log_ == nullptr) { //这里没有必要进行unlock操作，因为只有在第一次调用，也就是打开数据库的时候才会走到这个路径里面来  assert(descriptor_file_ == nullptr); new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_); edit-\u0026gt;SetNextFile(next_file_number_); s = env_-\u0026gt;NewWritableFile(new_manifest_file, \u0026amp;descriptor_file_); //此时重新开启一个新的MANIFEST文件，并将当前状态作为base状态写入快照  if (s.ok()) { descriptor_log_ = new log::Writer(descriptor_file_); //写入当前快照  s = WriteSnapshot(descriptor_log_); } } //在MANIFEST写入磁盘时解锁（此时没必要加锁，由后台线程完成）  { mu-\u0026gt;Unlock(); 后将其写入磁盘 if (s.ok()) { std::string record; edit-\u0026gt;EncodeTo(\u0026amp;record); //将版本变化写入MANIFEST  s = descriptor_log_-\u0026gt;AddRecord(record); if (s.ok()) { //将MANIFEST写入磁盘文件  s = descriptor_file_-\u0026gt;Sync(); } if (!s.ok()) { Log(options_-\u0026gt;info_log, \u0026#34;MANIFEST write: %s\\n\u0026#34;, s.ToString().c_str()); } } //当产生新的Manifest时更新current  if (s.ok() \u0026amp;\u0026amp; !new_manifest_file.empty()) { s = SetCurrentFile(env_, dbname_, manifest_file_number_); } mu-\u0026gt;Lock(); } // 将新生成的版本挂在到VersionSet，并且将当前版本（current_）设置为新生成的版本。  if (s.ok()) { AppendVersion(v); log_number_ = edit-\u0026gt;log_number_; prev_log_number_ = edit-\u0026gt;prev_log_number_; } else { delete v; if (!new_manifest_file.empty()) { delete descriptor_log_; delete descriptor_file_; descriptor_log_ = nullptr; descriptor_file_ = nullptr; env_-\u0026gt;RemoveFile(new_manifest_file); } } return s; }   执行逻辑如下:\n 将当前的版本根据版本变化（VersionEdit）进行处理，然后生成一个新的版本。 如果是第一次调用，则创建一个新的 Manifest 文件，并将当前状态作为 base 写入。 调用 Finalize 算出下一次需要 Compaction 的 level。 将版本变化写入 Manifest，把 Manifest 写入磁盘。 将新生成的版本挂载到 VersionSet 中，并且将 current_ 设置为新生成的版本。  Recover Recover 会根据 Manifest 文件中记录的每次版本变化（调用 VersionEdit 的 DecodeFrom 方法）逐次回放生成一个最新的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Status VersionSet::Recover(bool* save_manifest) { struct LogReporter : public log::Reader::Reporter { Status* status; void Corruption(size_t bytes, const Status\u0026amp; s) override { if (this-\u0026gt;status-\u0026gt;ok()) *this-\u0026gt;status = s; } }; //读取CURRENT文件，找到Manifest文件  std::string current; Status s = ReadFileToString(env_, CurrentFileName(dbname_), \u0026amp;current); if (!s.ok()) { return s; } if (current.empty() || current[current.size() - 1] != \u0026#39;\\n\u0026#39;) { return Status::Corruption(\u0026#34;CURRENT file does not end with newline\u0026#34;); } current.resize(current.size() - 1); std::string dscname = dbname_ + \u0026#34;/\u0026#34; + current; SequentialFile* file; s = env_-\u0026gt;NewSequentialFile(dscname, \u0026amp;file); if (!s.ok()) { if (s.IsNotFound()) { return Status::Corruption(\u0026#34;CURRENT points to a non-existent file\u0026#34;, s.ToString()); } return s; } bool have_log_number = false; bool have_prev_log_number = false; bool have_next_file = false; bool have_last_sequence = false; uint64_t next_file = 0; uint64_t last_sequence = 0; uint64_t log_number = 0; uint64_t prev_log_number = 0; Builder builder(this, current_); int read_records = 0; { LogReporter reporter; reporter.status = \u0026amp;s; log::Reader reader(file, \u0026amp;reporter, true /*checksum*/, 0 /*initial_offset*/); Slice record; std::string scratch; //读取versionedit并获取变更  while (reader.ReadRecord(\u0026amp;record, \u0026amp;scratch) \u0026amp;\u0026amp; s.ok()) { ++read_records; VersionEdit edit; s = edit.DecodeFrom(record); if (s.ok()) { if (edit.has_comparator_ \u0026amp;\u0026amp; edit.comparator_ != icmp_.user_comparator()-\u0026gt;Name()) { s = Status::InvalidArgument( edit.comparator_ + \u0026#34; does not match existing comparator \u0026#34;, icmp_.user_comparator()-\u0026gt;Name()); } } if (s.ok()) { builder.Apply(\u0026amp;edit); } if (edit.has_log_number_) { log_number = edit.log_number_; have_log_number = true; } if (edit.has_prev_log_number_) { prev_log_number = edit.prev_log_number_; have_prev_log_number = true; } if (edit.has_next_file_number_) { next_file = edit.next_file_number_; have_next_file = true; } if (edit.has_last_sequence_) { last_sequence = edit.last_sequence_; have_last_sequence = true; } } } delete file; file = nullptr; if (s.ok()) { if (!have_next_file) { s = Status::Corruption(\u0026#34;no meta-nextfile entry in descriptor\u0026#34;); } else if (!have_log_number) { s = Status::Corruption(\u0026#34;no meta-lognumber entry in descriptor\u0026#34;); } else if (!have_last_sequence) { s = Status::Corruption(\u0026#34;no last-sequence-number entry in descriptor\u0026#34;); } if (!have_prev_log_number) { prev_log_number = 0; } MarkFileNumberUsed(prev_log_number); MarkFileNumberUsed(log_number); } if (s.ok()) { //生成最新版本  Version* v = new Version(this); builder.SaveTo(v); Finalize(v); //加入versionset并设置current指针  AppendVersion(v); manifest_file_number_ = next_file; next_file_number_ = next_file + 1; last_sequence_ = last_sequence; log_number_ = log_number; prev_log_number_ = prev_log_number; //判断是否能够复用已有MANIFEST文件  if (ReuseManifest(dscname, current)) { } else { *save_manifest = true; } } else { std::string error = s.ToString(); Log(options_-\u0026gt;info_log, \u0026#34;Error recovering version set with %d records: %s\u0026#34;, read_records, error.c_str()); } return s; }   执行流程如下：\n 通过 current_ 获取到 Manifest，读取 Manifest 获取 base 状态。 读取 Manifest 文件中的 VersionEdit，并执行变更。 生成最终的版本，并将其加入 VersionSet，更新 current_。 判断是否能够复用已有 MANIFEST 文件。  ","date":"2022-05-23T23:43:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/","title":"LevelDB 版本管理"},{"content":"WAL日志模块 当向 LevelDB 写入数据时，只需要将数据写入内存中的 MemTable，而由于内存是易失性存储，因此 LevelDB 需要一个额外的持久化文件：预写日志（Write-Ahead Log，WAL），又称重做日志。这是一个追加修改、顺序写入磁盘的文件。当宕机或者程序崩溃时 WAL 能够保证写入成功的数据不会丢失。将 MemTable 成功写入 SSTable 后，相应的预写日志就可以删除了。\n结构 Log文件以块为基本单位，一条记录可能全部写到一个块上，也可能跨几个块。记录的格式如下图所示：\n首先我们来看看 Log 中的数据格式，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // https://github.com/google/leveldb/blob/master/db/log_format.h  namespace log { enum RecordType { // Zero is reserved for preallocated files  kZeroType = 0, kFullType = 1, // For fragments  kFirstType = 2, kMiddleType = 3, kLastType = 4 }; static const int kMaxRecordType = kLastType; static const int kBlockSize = 32768; // Header is checksum (4 bytes), length (2 bytes), type (1 byte).  static const int kHeaderSize = 4 + 2 + 1; } // namespace log   结合上面的代码和图片，我们可以看到每一个块大小为 32768 字节，并且每一个块由头部和正文组成。头部由 4 字节校验，2 字节的长度与 1 字节的类型构成，即每一个块的开始 7 字节属于头部。头部中的类型字段有如下 4 种：\n kZeroType：为预分配的文件保留。 kFullType：表示一条记录完整地写到了一个块上。 kFirstType：表示该条记录的第一部分。 kMiddleType：表示该条记录的中间部分。 kLastType：表示该条记录的最后一部分。  通过记录结构可以推测出 Log 文件的读取流程，即首先根据头部的长度字段确定需要读取多少字节，然后根据头部类型字段确定该条记录是否已经完整读取，如果没有完整读取，继续按该流程进行，直到读取到记录的最后一部分，其头部类型为 kLastType。\n读写流程 写入 Log 的读取主要由 Writer 中的 AddRecord 实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  // https://github.com/google/leveldb/blob/master/db/log_writer.h  class Writer { public: explicit Writer(WritableFile* dest); Writer(WritableFile* dest, uint64_t dest_length); Writer(const Writer\u0026amp;) = delete; Writer\u0026amp; operator=(const Writer\u0026amp;) = delete; ~Writer(); Status AddRecord(const Slice\u0026amp; slice); private: Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length); WritableFile* dest_; int block_offset_; // Current offset in block  uint32_t type_crc_[kMaxRecordType + 1]; }; // https://github.com/google/leveldb/blob/master/db/log_writer.cc  Status Writer::AddRecord(const Slice\u0026amp; slice) { const char* ptr = slice.data(); size_t left = slice.size(); Status s; //begin表明本条记录是第一次写入，即当前块中第一条记录  bool begin = true; do { //当前块剩余空间，用于判断头部能否完整写入  const int leftover = kBlockSize - block_offset_; assert(leftover \u0026gt;= 0); if (leftover \u0026lt; kHeaderSize) { //如果块剩余空间小于七个字节且不等于0，说明当前无法完整写入数据，此时填充\\x00，从下一个块写入  if (leftover \u0026gt; 0) { static_assert(kHeaderSize == 7, \u0026#34;\u0026#34;); dest_-\u0026gt;Append(Slice(\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34;, leftover)); } //此时块正好写满，将block_offset_置为0，表明开始写入新的块  block_offset_ = 0; } assert(kBlockSize - block_offset_ - kHeaderSize \u0026gt;= 0); //计算块剩余空间  const size_t avail = kBlockSize - block_offset_ - kHeaderSize; //计算当前块能够写入的数据大小（块剩余空间和记录剩余内容中最小的）  const size_t fragment_length = (left \u0026lt; avail) ? left : avail; RecordType type; //end表明该记录是否已经完整写入，即最后一条记录  const bool end = (left == fragment_length); //根据begin与end来确定记录类型  if (begin \u0026amp;\u0026amp; end) { //记录为第一条且同时又是最后一条，说明当前是完整的记录，状态为kFullType  type = kFullType; } else if (begin) { //记录为第一条，状态为kFirstType  type = kFirstType; } else if (end) { //记录为最后一条，标记状态为kLastType  type = kLastType; } else { //记录不为第一条，也并非最后一条，则说明是中间状态，标记为kMiddleType  type = kMiddleType; } //将数据按照格式写入，并刷新到磁盘文件中  s = EmitPhysicalRecord(type, ptr, fragment_length); ptr += fragment_length; left -= fragment_length; begin = false; } while (s.ok() \u0026amp;\u0026amp; left \u0026gt; 0); //循环至数据完全写入或者写入失败时才停止  return s; }   写入流程如下：\n 判断头部能否完整写入，如果不能则将剩余空间用 \\x00 填充，接着从新的块开始写入。 根据 begin 和 end 判断记录类型。 将数据按照格式写入，并刷新到磁盘文件中。 循环至数据完全写入或者写入失败后停止，将结果返回。  读取 Log 的读取主要由 Reader 中的 ReadRecord 实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148  // https://github.com/google/leveldb/blob/master/db/log_reader.h  class Reader { public: // Interface for reporting errors.  class Reporter { public: virtual ~Reporter(); virtual void Corruption(size_t bytes, const Status\u0026amp; status) = 0; }; Reader(SequentialFile* file, Reporter* reporter, bool checksum, uint64_t initial_offset); Reader(const Reader\u0026amp;) = delete; Reader\u0026amp; operator=(const Reader\u0026amp;) = delete; ~Reader(); bool ReadRecord(Slice* record, std::string* scratch); uint64_t LastRecordOffset(); private: enum { kEof = kMaxRecordType + 1, kBadRecord = kMaxRecordType + 2 }; }; // https://github.com/google/leveldb/blob/master/db/log_reader.cc  bool Reader::ReadRecord(Slice* record, std::string* scratch) { if (last_record_offset_ \u0026lt; initial_offset_) { if (!SkipToInitialBlock()) { return false; } } scratch-\u0026gt;clear(); record-\u0026gt;clear(); bool in_fragmented_record = false; uint64_t prospective_record_offset = 0; Slice fragment; while (true) { //ReadPhysicalRecord读取log文件并将记录保存到fragment，同时返回记录的类型  const unsigned int record_type = ReadPhysicalRecord(\u0026amp;fragment); uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size(); if (resyncing_) { if (record_type == kMiddleType) { continue; } else if (record_type == kLastType) { resyncing_ = false; continue; } else { resyncing_ = false; } } //根据记录的类型来判断是否需要将当前记录附加到scratch后并继续读取  switch (record_type) { //类型为kFullType则说明当前是完整的记录，直接赋值给record后返回  case kFullType: if (in_fragmented_record) { if (!scratch-\u0026gt;empty()) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;partial record without end(1)\u0026#34;); } } prospective_record_offset = physical_record_offset; scratch-\u0026gt;clear(); *record = fragment; last_record_offset_ = prospective_record_offset; return true; //类型为kFirstType则说明当前是第一部分，先将记录复制到scratch后继续读取  case kFirstType: if (in_fragmented_record) { if (!scratch-\u0026gt;empty()) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;partial record without end(2)\u0026#34;); } } prospective_record_offset = physical_record_offset; scratch-\u0026gt;assign(fragment.data(), fragment.size()); in_fragmented_record = true; break; //类型为kMiddleType则说明当前是中间部分，先将记录追加到scratch后继续读取  case kMiddleType: //初始读取到的类型为kMiddleType或者kLastType，则需要忽略并且继续偏移  if (!in_fragmented_record) { ReportCorruption(fragment.size(), \u0026#34;missing start of fragmented record(1)\u0026#34;); } else { scratch-\u0026gt;append(fragment.data(), fragment.size()); } break; //类型为kLastType则说明当前为最后，继续追加到scratch，并将scratch赋值给record并返回  case kLastType: if (!in_fragmented_record) { ReportCorruption(fragment.size(), \u0026#34;missing start of fragmented record(2)\u0026#34;); } else { scratch-\u0026gt;append(fragment.data(), fragment.size()); *record = Slice(*scratch); last_record_offset_ = prospective_record_offset; return true; } break; //如果状态为kEof、kBadRecord时说明日志损坏，此时清空scratch并返回false  case kEof: if (in_fragmented_record) { scratch-\u0026gt;clear(); } return false; case kBadRecord: if (in_fragmented_record) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;error in middle of record\u0026#34;); in_fragmented_record = false; scratch-\u0026gt;clear(); } break; //未定义的类型，输出日志，剩余同上处理  default: { char buf[40]; std::snprintf(buf, sizeof(buf), \u0026#34;unknown record type %u\u0026#34;, record_type); ReportCorruption( (fragment.size() + (in_fragmented_record ? scratch-\u0026gt;size() : 0)), buf); in_fragmented_record = false; scratch-\u0026gt;clear(); break; } } } return false; }   执行流程如下：\n ReadRecord 读取一条记录到 fragment 变量中，并且返回该条记录的类型。 根据记录的类型来判断是否需要将当前记录附加到 scratch 后并继续读取：  kFullType：当前是完整的记录，直接赋值给 record 后返回。 kFirstType：当前是第一部分，先将记录覆盖到 scratch 后继续读取。 kMiddleType：当前是中间部分，先将记录追加到 scratch 后继续读取。 kLastType：当前为最后部分，继续追加到 scratch，并将完整的 scratch 赋值给 record 后返回。 其它/异常：清空 scratch 并返回 false，如果是未定义类型需要输出日志。    这里还有一个需要注意的细节，由于读取 Log 文件时可以从指定偏移量开始，所以如果初始读取到的类型为 kMiddleType 或者 kLastType，则需要忽略并且继续偏移，直到碰见第一个 kFirstType。\n崩溃恢复 当打开一个 LevelDB 的数据文件时，需先检验是否进行崩溃恢复，如果需要，则会从 Log 文件生成一个MemTable，其实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::RecoverLogFile(uint64_t log_number, bool last_log, bool* save_manifest, VersionEdit* edit, SequenceNumber* max_sequence) { struct LogReporter : public log::Reader::Reporter { Env* env; Logger* info_log; const char* fname; Status* status; // null if options_.paranoid_checks==false  void Corruption(size_t bytes, const Status\u0026amp; s) override { Log(info_log, \u0026#34;%s%s: dropping %d bytes; %s\u0026#34;, (this-\u0026gt;status == nullptr ? \u0026#34;(ignoring error) \u0026#34; : \u0026#34;\u0026#34;), fname, static_cast\u0026lt;int\u0026gt;(bytes), s.ToString().c_str()); if (this-\u0026gt;status != nullptr \u0026amp;\u0026amp; this-\u0026gt;status-\u0026gt;ok()) *this-\u0026gt;status = s; } }; mutex_.AssertHeld(); //打开log文件  std::string fname = LogFileName(dbname_, log_number); SequentialFile* file; Status status = env_-\u0026gt;NewSequentialFile(fname, \u0026amp;file); if (!status.ok()) { MaybeIgnoreError(\u0026amp;status); return status; } //创建log reader.  LogReporter reporter; reporter.env = env_; reporter.info_log = options_.info_log; reporter.fname = fname.c_str(); reporter.status = (options_.paranoid_checks ? \u0026amp;status : nullptr); log::Reader reader(file, \u0026amp;reporter, true /*checksum*/, 0 /*initial_offset*/); Log(options_.info_log, \u0026#34;Recovering log #%llu\u0026#34;, (unsigned long long)log_number); //读取所有的records并写入一个memtable  std::string scratch; Slice record; WriteBatch batch; int compactions = 0; MemTable* mem = nullptr; //循环读取日志文件  while (reader.ReadRecord(\u0026amp;record, \u0026amp;scratch) \u0026amp;\u0026amp; status.ok()) { if (record.size() \u0026lt; 12) { reporter.Corruption(record.size(), Status::Corruption(\u0026#34;log record too small\u0026#34;)); continue; } WriteBatchInternal::SetContents(\u0026amp;batch, record); if (mem == nullptr) { mem = new MemTable(internal_comparator_); mem-\u0026gt;Ref(); } //将records写入memtable  status = WriteBatchInternal::InsertInto(\u0026amp;batch, mem); MaybeIgnoreError(\u0026amp;status); if (!status.ok()) { break; } const SequenceNumber last_seq = WriteBatchInternal::Sequence(\u0026amp;batch) + WriteBatchInternal::Count(\u0026amp;batch) - 1; if (last_seq \u0026gt; *max_sequence) { *max_sequence = last_seq; } //如果memtable大于阈值，则将其转换成sstable(默认4MB)\t if (mem-\u0026gt;ApproximateMemoryUsage() \u0026gt; options_.write_buffer_size) { compactions++; *save_manifest = true; status = WriteLevel0Table(mem, edit, nullptr); mem-\u0026gt;Unref(); mem = nullptr; if (!status.ok()) { break; } } } delete file; //判断是否应该继续重复使用最后一个日志文件  if (status.ok() \u0026amp;\u0026amp; options_.reuse_logs \u0026amp;\u0026amp; last_log \u0026amp;\u0026amp; compactions == 0) { assert(logfile_ == nullptr); assert(log_ == nullptr); assert(mem_ == nullptr); uint64_t lfile_size; if (env_-\u0026gt;GetFileSize(fname, \u0026amp;lfile_size).ok() \u0026amp;\u0026amp; env_-\u0026gt;NewAppendableFile(fname, \u0026amp;logfile_).ok()) { Log(options_.info_log, \u0026#34;Reusing old log %s \\n\u0026#34;, fname.c_str()); log_ = new log::Writer(logfile_, lfile_size); logfile_number_ = log_number; if (mem != nullptr) { mem_ = mem; mem = nullptr; } else { mem_ = new MemTable(internal_comparator_); mem_-\u0026gt;Ref(); } } } if (mem != nullptr) { if (status.ok()) { *save_manifest = true; status = WriteLevel0Table(mem, edit, nullptr); } mem-\u0026gt;Unref(); } return status; }   具体的逻辑如下：\n 打开 log，创建 log reader 开始读取数据。 循环读取日志文件，并将其写入 MemTable 中。 如果 MemTable 过大，则将其转换为 SSTable。  ","date":"2022-05-23T23:42:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-wal%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97/","title":"LevelDB WAL日志模块"},{"content":"SSTable模块 SSTable（Sorted Strings Table，有序字符串表），在各种存储引擎中得到了广泛的使用，包括 LevelDB、HBase、Cassandra 等。SSTable 会根据 Key 进行排序后保存一系列的 K-V 对，这种方式不仅方便进行范围查找，而且便于对 K-V 对进行更加有效的压缩。\nSSTable Format SSTable 文件由一个个块组成，块中可以保存数据、数据索引、元数据或者元数据索引。整体的文件格式如下图：\n如上图，SSTable 文件整体分为 4 个部分：\n Data Block（数据区域）：保存具体的键-值对数据。 Meta Block（元数据区域）：保存元数据，例如布隆过滤器。 Index Block（索引区域）：分为数据索引和元数据索引。  数据索引：数据索引块中的键为前一个数据块的最后一个键（即一个数据块中最大的键，因为键是有序排列保存的）与后一个数据块的第一个键（即一个数据块中的最小键）的最短分隔符。 元数据索引：元数据索引块可指示如何查找该布隆过滤器的数据。   File Footer（尾部）：总大小为48个字节。  BlockHandle BlockHandle在SSTable中是经常使用的一个结构，其定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  // https://github.com/google/leveldb/blob/master/table/format.h  class BlockHandle { public: // Maximum encoding length of a BlockHandle  enum { kMaxEncodedLength = 10 + 10 }; BlockHandle(); // The offset of the block in the file.  uint64_t offset() const { return offset_; } void set_offset(uint64_t offset) { offset_ = offset; } // The size of the stored block  uint64_t size() const { return size_; } void set_size(uint64_t size) { size_ = size; } void EncodeTo(std::string* dst) const; Status DecodeFrom(Slice* input); private: uint64_t offset_; uint64_t size_; };   BlockHandler 本质就是封装了 offset 和 size，用于定位某些区域。\nBlock Format SSTable 中一个块默认大小为 4 KB，由 4 部分组成：\n 键-值对数据：即我们保存到 LevelDB 中的多组键-值对。 重启点数据：最后 4 字节为重启点的个数，前边部分为多个重启点，每个重启点实际保存的是偏移量。并且每个重启点固定占据 4 字节的空间。 压缩类型：在 LevelDB 的 SSTable 中有两种压缩类型：  kNoCompression：没有压缩。 kSnappyCompression：Snappy压缩。   校验数据：4 字节的 CRC 校验字段。  Block读写流程 生成Block 块生成主要在 BlockBuilder 中实现，下面先看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // https://github.com/google/leveldb/blob/master/table/block_builder.h  class BlockBuilder { public: explicit BlockBuilder(const Options* options); BlockBuilder(const BlockBuilder\u0026amp;) = delete; BlockBuilder\u0026amp; operator=(const BlockBuilder\u0026amp;) = delete; void Reset(); void Add(const Slice\u0026amp; key, const Slice\u0026amp; value); Slice Finish(); size_t CurrentSizeEstimate() const; bool empty() const { return buffer_.empty(); } private: const Options* options_; std::string buffer_; // Destination buffer  std::vector\u0026lt;uint32_t\u0026gt; restarts_; // Restart points  int counter_; // Number of entries emitted since restart  bool finished_; // Has Finish() been called?  std::string last_key_; };    成员变量  options_：在 BlockBuilder 类构造函数中传入，表示一些配置选项。 buffer_：块的内容，所有的键-值对都保存到 buffer_ 中。 restarts_：每次开启新的重启点后，会将当前 buffer_ 的数据长度保存到 restarts_ 中，当前 buffer_ 中的数据长度即为每个重启点的偏移量。 counter_：开启新的重启点之后加入的键-值对数量，默认保存 16 个键-值对，之后会开启一个新的重启点。 finished_：指明是否已经调用了 Finish 方法，BlockBuilder 中的 Add 方法会将数据保存到各个成员变量中，而 Finish 方法会依据成员变量的值生成一个块。 last_key_：上一个保存的键，当加入新键时，用来计算和上一个键的共同前缀部分。    介绍完了结构，下面来看具体的生成方法。当需要保存一个键-值对时，需要调用 BlockBuilder 类中的 Add 方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  // https://github.com/google/leveldb/blob/master/table/block_builder.cc  void BlockBuilder::Add(const Slice\u0026amp; key, const Slice\u0026amp; value) { //保存上一个加入的key  Slice last_key_piece(last_key_); assert(!finished_); assert(counter_ \u0026lt;= options_-\u0026gt;block_restart_interval); assert(buffer_.empty() // No values yet?  || options_-\u0026gt;comparator-\u0026gt;Compare(key, last_key_piece) \u0026gt; 0); size_t shared = 0; //判断counter_是否大于block_restart_interval  if (counter_ \u0026lt; options_-\u0026gt;block_restart_interval) { const size_t min_length = std::min(last_key_piece.size(), key.size()); //计算相同前缀的长度  while ((shared \u0026lt; min_length) \u0026amp;\u0026amp; (last_key_piece[shared] == key[shared])) { shared++; } } else { //如果键-值对数量超过block_restart_interval，则开启新的重启点，清空计数器  restarts_.push_back(buffer_.size()); counter_ = 0; } const size_t non_shared = key.size() - shared; //将共同前缀长度、非共享部分长度、值长度追加到buffer_中  PutVarint32(\u0026amp;buffer_, shared); PutVarint32(\u0026amp;buffer_, non_shared); PutVarint32(\u0026amp;buffer_, value.size()); //将key的非共享数据追加到buffer_中_  buffer_.append(key.data() + shared, non_shared); //将Value数据追加到buffer_中  buffer_.append(value.data(), value.size()); //更新状态  last_key_.resize(shared); last_key_.append(key.data() + shared, non_shared); assert(Slice(last_key_) == key); counter_++; }   执行流程如下：\n1. 判断 counter_ 是否大于 block_restart_interval，如果大于，则开启新的重启点，清空计数器并保存 buffer_ 中数据长度的值（该值即每个重启点的偏移量）压到 restarts_ 数组中。\r2. 如果 counter_ 未超出配置的每个重启点可以保存的键-值对数值，则计算当前键和上一次保存键的共同前缀，然后将键-值对按格式保存到 buffer_ 中。\r3. 更新状态，将 last_key_ 置为当前保存的 key，并且将 counter_ 加 1。\r 从上面的代码中可以看出，Add 中将所有的键-值对按格式保存到成员变量 buffer_ 中。实际生成 Block 的其实是 Finish 。代码如下：\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/table/block_builder.cc  Slice BlockBuilder::Finish() { //将重启点偏移量写入buffer_中  for (size_t i = 0; i \u0026lt; restarts_.size(); i++) { PutFixed32(\u0026amp;buffer_, restarts_[i]); } PutFixed32(\u0026amp;buffer_, restarts_.size()); finished_ = true; return Slice(buffer_); }   Finish  首先将所有重启点偏移量的值依次以 4 字节大小追加到 buffer_ 字符串，最后将重启点个数继续以 4 字节大小追加到 buffer_ 后部，此时返回的结果就是一个完整的 Block。\n读取Block 读取 Block 由 Block 类实现，其主要依靠 NewIterator 生成一个 Block 迭代器，再借助迭代器的 Seek 来查找对应的 Key。首先来看看 Block 的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // https://github.com/google/leveldb/blob/master/table/block.h  class Block { public: // Initialize the block with the specified contents.  explicit Block(const BlockContents\u0026amp; contents); Block(const Block\u0026amp;) = delete; Block\u0026amp; operator=(const Block\u0026amp;) = delete; ~Block(); size_t size() const { return size_; } Iterator* NewIterator(const Comparator* comparator); private: class Iter; uint32_t NumRestarts() const; const char* data_; size_t size_; uint32_t restart_offset_; // Offset in data_ of restart array  bool owned_; // Block owns data_[] };   读取一个块通过在 NewIterator 中生成一个迭代器来实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/table/block.cc  Iterator* Block::NewIterator(const Comparator* comparator) { if (size_ \u0026lt; sizeof(uint32_t)) { return NewErrorIterator(Status::Corruption(\u0026#34;bad block contents\u0026#34;)); } const uint32_t num_restarts = NumRestarts(); if (num_restarts == 0) { return NewEmptyIterator(); } else { return new Iter(comparator, data_, restart_offset_, num_restarts); } }   这里的逻辑比较简单，就不多作介绍了。\n核心的查找逻辑主要是迭代器中的 Seek 下面直接看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/table/block.cc  void Seek(const Slice\u0026amp; target) override { uint32_t left = 0; uint32_t right = num_restarts_ - 1; int current_key_compare = 0; if (Valid()) { current_key_compare = Compare(key_, target); if (current_key_compare \u0026lt; 0) { left = restart_index_; } else if (current_key_compare \u0026gt; 0) { right = restart_index_; } else { return; } } //通过重启点进行二分查找  while (left \u0026lt; right) { uint32_t mid = (left + right + 1) / 2; uint32_t region_offset = GetRestartPoint(mid); uint32_t shared, non_shared, value_length; const char* key_ptr = DecodeEntry(data_ + region_offset, data_ + restarts_, \u0026amp;shared, \u0026amp;non_shared, \u0026amp;value_length); if (key_ptr == nullptr || (shared != 0)) { CorruptionError(); return; } Slice mid_key(key_ptr, non_shared); //如果key小于target，则将left置为mid  if (Compare(mid_key, target) \u0026lt; 0) { left = mid; //如果key大于等于target，则将right置为mid-1  } else { right = mid - 1; } } assert(current_key_compare == 0 || Valid()); //在块中线性查找，依次遍历每一个K-V对，将key与target对比，直到找到第一个大于等于target的后返  bool skip_seek = left == restart_index_ \u0026amp;\u0026amp; current_key_compare \u0026lt; 0; if (!skip_seek) { SeekToRestartPoint(left); } while (true) { if (!ParseNextKey()) { return; } if (Compare(key_, target) \u0026gt;= 0) { return; } } }   查找逻辑如下：\n1. 对重启点数组进行二分查找，找到可能包含数据的重启点。\r2. 在块中线性查找，依次遍历每一个 K-V 对，将 key 与 target 对比。\r3. 找到第一个 key 大于等于 target 的后将该 K-V 对存储后返回。\r SSTable读写 生成SSTable SSTable 的生成主要在 TableBuilder 中实现，下面先看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  // https://github.com/google/leveldb/blob/master/include/leveldb/table_builder.h  class LEVELDB_EXPORT TableBuilder { public: TableBuilder(const Options\u0026amp; options, WritableFile* file); TableBuilder(const TableBuilder\u0026amp;) = delete; TableBuilder\u0026amp; operator=(const TableBuilder\u0026amp;) = delete; ~TableBuilder(); Status ChangeOptions(const Options\u0026amp; options); void Add(const Slice\u0026amp; key, const Slice\u0026amp; value); void Flush(); Status status() const; Status Finish(); void Abandon(); uint64_t NumEntries() const; uint64_t FileSize() const; private: bool ok() const { return status().ok(); } void WriteBlock(BlockBuilder* block, BlockHandle* handle); void WriteRawBlock(const Slice\u0026amp; data, CompressionType, BlockHandle* handle); struct Rep; Rep* rep_; };   在介绍该 TableBuilder 的核心逻辑之前，首先我们要看看里面的一个结构体 Rep。其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  struct TableBuilder::Rep { Rep(const Options\u0026amp; opt, WritableFile* f) : options(opt), index_block_options(opt), file(f), offset(0), data_block(\u0026amp;options), index_block(\u0026amp;index_block_options), num_entries(0), closed(false), filter_block(opt.filter_policy == nullptr ? nullptr : new FilterBlockBuilder(opt.filter_policy)), pending_index_entry(false) { index_block_options.block_restart_interval = 1; } Options options; Options index_block_options; WritableFile* file; uint64_t offset; Status status; BlockBuilder data_block; BlockBuilder index_block; std::string last_key; int64_t num_entries; bool closed; // Either Finish() or Abandon() has been called.  FilterBlockBuilder* filter_block; bool pending_index_entry; BlockHandle pending_handle; // Handle to add to index block  std::string compressed_output; };   我们需要注意的关键变量如下：\n file：SSTable 生成的文件。 data_block：用于生成 SSTable 的数据区域。 index_block：用于生成 SSTable 的索引区域。 pending_index_entry：决定是否需要写数据索引。 **pending_handle：**写数据索引的方法。SSTable中每次完整写入一个块后需要生成该块的索引，索引中的键是当前块最大键与即将插入的键的最短分隔符，例如一个块中最大键为 abceg，即将插入的键为 abcqddh，则二者之间的最小分隔符为 abcf。  了解完结构后，接着就看看生成 SSTable 的核心函数 Add 和 Finish。\n首先来看 Add，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  void TableBuilder::Add(const Slice\u0026amp; key, const Slice\u0026amp; value) { Rep* r = rep_; assert(!r-\u0026gt;closed); if (!ok()) return; if (r-\u0026gt;num_entries \u0026gt; 0) { assert(r-\u0026gt;options.comparator-\u0026gt;Compare(key, Slice(r-\u0026gt;last_key)) \u0026gt; 0); } //判断是否需要写入数据索引块中  if (r-\u0026gt;pending_index_entry) { assert(r-\u0026gt;data_block.empty()); //找到最短分隔符，即大于等于上一个块最大的键，小于下一个块最小的键  r-\u0026gt;options.comparator-\u0026gt;FindShortestSeparator(\u0026amp;r-\u0026gt;last_key, key); std::string handle_encoding; r-\u0026gt;pending_handle.EncodeTo(\u0026amp;handle_encoding); //在数据索引块中写入key和BlockHandle  r-\u0026gt;index_block.Add(r-\u0026gt;last_key, Slice(handle_encoding)); r-\u0026gt;pending_index_entry = false; } //写入元数据块中  if (r-\u0026gt;filter_block != nullptr) { r-\u0026gt;filter_block-\u0026gt;AddKey(key); } //修改last_key为当前要插入的key  r-\u0026gt;last_key.assign(key.data(), key.size()); r-\u0026gt;num_entries++; //写入数据块中  r-\u0026gt;data_block.Add(key, value); //判断数据块大小是否大于配置的块大小，如果大于则调用Flush写入SSTable文件并刷新到硬盘  const size_t estimated_block_size = r-\u0026gt;data_block.CurrentSizeEstimate(); if (estimated_block_size \u0026gt;= r-\u0026gt;options.block_size) { Flush(); } }   Add 主要就是调用生成数据块与数据索引块的方法 BlockBuilder::Add 以及生成元数据块的方法 FilterBlockBuilder::Add 依次将键值对加入数据索引块、元数据块以及数据块。\n可以看到，最后会判断数据块大小是否大于配置的块大小，如果大于则调用 Flush 写入 SSTable 文件并刷新到硬盘中。我们接着来看看 Flush 的执行逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  void TableBuilder::Flush() { Rep* r = rep_; assert(!r-\u0026gt;closed); if (!ok()) return; if (r-\u0026gt;data_block.empty()) return; assert(!r-\u0026gt;pending_index_entry); //写入数据块  WriteBlock(\u0026amp;r-\u0026gt;data_block, \u0026amp;r-\u0026gt;pending_handle); if (ok()) { //将pending_index_entry修改为true，表明下一次将写入数据索引块。  r-\u0026gt;pending_index_entry = true; //将文件刷新到磁盘\t r-\u0026gt;status = r-\u0026gt;file-\u0026gt;Flush(); } if (r-\u0026gt;filter_block != nullptr) { r-\u0026gt;filter_block-\u0026gt;StartBlock(r-\u0026gt;offset); } }   执行逻辑如下：\n1. 将数据写入数据块中。\r2. 将 pending_index_entry 修改为 true，表明下一次调用 `Add` 时将写入数据索引块。\r3. 将文件刷新到磁盘中。\r 介绍完了 Add，下面来看看 Finish。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  Status TableBuilder::Finish() { Rep* r = rep_; //写入SSTable文件并刷新到硬盘中  Flush(); assert(!r-\u0026gt;closed); r-\u0026gt;closed = true; BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle; //写入元数据块  if (ok() \u0026amp;\u0026amp; r-\u0026gt;filter_block != nullptr) { WriteRawBlock(r-\u0026gt;filter_block-\u0026gt;Finish(), kNoCompression, \u0026amp;filter_block_handle); } //写入元数据块索引  if (ok()) { BlockBuilder meta_index_block(\u0026amp;r-\u0026gt;options); if (r-\u0026gt;filter_block != nullptr) { std::string key = \u0026#34;filter.\u0026#34;; key.append(r-\u0026gt;options.filter_policy-\u0026gt;Name()); std::string handle_encoding; filter_block_handle.EncodeTo(\u0026amp;handle_encoding); meta_index_block.Add(key, handle_encoding); } WriteBlock(\u0026amp;meta_index_block, \u0026amp;metaindex_block_handle); } //写入数据块索引  if (ok()) { if (r-\u0026gt;pending_index_entry) { r-\u0026gt;options.comparator-\u0026gt;FindShortSuccessor(\u0026amp;r-\u0026gt;last_key); std::string handle_encoding; r-\u0026gt;pending_handle.EncodeTo(\u0026amp;handle_encoding); r-\u0026gt;index_block.Add(r-\u0026gt;last_key, Slice(handle_encoding)); r-\u0026gt;pending_index_entry = false; } WriteBlock(\u0026amp;r-\u0026gt;index_block, \u0026amp;index_block_handle); } //写入尾部  if (ok()) { Footer footer; footer.set_metaindex_handle(metaindex_block_handle); footer.set_index_handle(index_block_handle); std::string footer_encoding; footer.EncodeTo(\u0026amp;footer_encoding); r-\u0026gt;status = r-\u0026gt;file-\u0026gt;Append(footer_encoding); if (r-\u0026gt;status.ok()) { r-\u0026gt;offset += footer_encoding.size(); } } return r-\u0026gt;status; }   Finish 会按照我们一开始给出的 SSTable 的格式，将数据分别写入数据块、数据块索引、元数据块、元数据块索引、尾部。最后生成 SSTable。\n读取SSTable 介绍完了写入，我们再来看看读取。SSTable 读取的逻辑与 Block 类似，都是借助于迭代器实现的。主要实现代码在 Table 类中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // https://github.com/google/leveldb/blob/master/include/leveldb/table.h  class LEVELDB_EXPORT Table { public: static Status Open(const Options\u0026amp; options, RandomAccessFile* file, uint64_t file_size, Table** table); Table(const Table\u0026amp;) = delete; Table\u0026amp; operator=(const Table\u0026amp;) = delete; ~Table(); Iterator* NewIterator(const ReadOptions\u0026amp;) const; uint64_t ApproximateOffsetOf(const Slice\u0026amp; key) const; private: friend class TableCache; struct Rep; static Iterator* BlockReader(void*, const ReadOptions\u0026amp;, const Slice\u0026amp;); explicit Table(Rep* rep) : rep_(rep) {} Status InternalGet(const ReadOptions\u0026amp;, const Slice\u0026amp; key, void* arg, void (*handle_result)(void* arg, const Slice\u0026amp; k, const Slice\u0026amp; v)); void ReadMeta(const Footer\u0026amp; footer); void ReadFilter(const Slice\u0026amp; filter_handle_value); Rep* const rep_; };   这里我们主要关注生成迭代器的 NewIterator 和 第二层生成迭代器的 BlockReader。\n首先看 NewIterator 的代码：\n1 2 3 4 5 6 7  // https://github.com/google/leveldb/blob/master/table/table.cc  Iterator* Table::NewIterator(const ReadOptions\u0026amp; options) const { return NewTwoLevelIterator( rep_-\u0026gt;index_block-\u0026gt;NewIterator(rep_-\u0026gt;options.comparator), \u0026amp;Table::BlockReader, const_cast\u0026lt;Table*\u0026gt;(this), options); }   这里返回了一个双层迭代器 NewTwoLevelIterator。第一层为数据索引块的迭代器，即 rep_-\u0026gt;index_block-\u0026gt;NewIterator。通过第一层的数据索引块迭代器查找一个键应该属于的块，然后通过第二层迭代器去读取这个块并查找该键。\n接着看生成第二层块迭代器的 BlockReader：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  // https://github.com/google/leveldb/blob/master/table/table.cc  Iterator* Table::BlockReader(void* arg, const ReadOptions\u0026amp; options, const Slice\u0026amp; index_value) { Table* table = reinterpret_cast\u0026lt;Table*\u0026gt;(arg); Cache* block_cache = table-\u0026gt;rep_-\u0026gt;options.block_cache; Block* block = nullptr; Cache::Handle* cache_handle = nullptr; BlockHandle handle; Slice input = index_value; Status s = handle.DecodeFrom(\u0026amp;input); if (s.ok()) { //contents保存一个块的内容  BlockContents contents; if (block_cache != nullptr) { char cache_key_buffer[16]; EncodeFixed64(cache_key_buffer, table-\u0026gt;rep_-\u0026gt;cache_id); EncodeFixed64(cache_key_buffer + 8, handle.offset()); Slice key(cache_key_buffer, sizeof(cache_key_buffer)); cache_handle = block_cache-\u0026gt;Lookup(key); if (cache_handle != nullptr) { block = reinterpret_cast\u0026lt;Block*\u0026gt;(block_cache-\u0026gt;Value(cache_handle)); } else { //通过BlockHandle存储的偏移量和大小读取一个块的数据到contents  s = ReadBlock(table-\u0026gt;rep_-\u0026gt;file, options, handle, \u0026amp;contents); if (s.ok()) { //根据contents生成一个Block结构  block = new Block(contents); if (contents.cachable \u0026amp;\u0026amp; options.fill_cache) { cache_handle = block_cache-\u0026gt;Insert(key, block, block-\u0026gt;size(), \u0026amp;DeleteCachedBlock); } } } } else { s = ReadBlock(table-\u0026gt;rep_-\u0026gt;file, options, handle, \u0026amp;contents); if (s.ok()) { block = new Block(contents); } } } Iterator* iter; if (block != nullptr) { //生成块迭代器，通过该迭代器读取数据  iter = block-\u0026gt;NewIterator(table-\u0026gt;rep_-\u0026gt;options.comparator); if (cache_handle == nullptr) { iter-\u0026gt;RegisterCleanup(\u0026amp;DeleteBlock, block, nullptr); } else { iter-\u0026gt;RegisterCleanup(\u0026amp;ReleaseBlock, block_cache, cache_handle); } } else { iter = NewErrorIterator(s); } return iter; }   SSTable 的读取先通过第一层迭代器（即数据索引）获取到一个键需要查找的块位置，读取该块的内容并且构造第二层迭代器遍历该块，通过两层迭代器即可在 SSTable 中进行键的查找。\n布隆过滤器 如果在 LevelDB 中查找某个不存在的键，必须先检查内存表 MemTable，然后逐层查找，为了优化这种读取，LevelDB 中会使用布隆过滤器。当布隆过滤器判定键不存在时，可以直接返回，无须继续查找。\n这里就不过多介绍原理，如果想了解可以看看我的往期博客 海量数据处理（一） ：位图与布隆过滤器的概念以及实现\n实现 布隆过滤器继承了 LevelDB 中抽象出的过滤器纯虚类 FilterPolicy，其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/include/leveldb/filter_policy.h  class LEVELDB_EXPORT FilterPolicy { public: virtual ~FilterPolicy(); virtual const char* Name() const = 0; virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const = 0; virtual bool KeyMayMatch(const Slice\u0026amp; key, const Slice\u0026amp; filter) const = 0; };   FilterPolicy 定义了几个接口：\n Name：返回过滤器名称。 CreateFilter：将一个字符串加入过滤器中。 KeyMayMatch：通过过滤器内容判断一个元素是否存在。  接下来我们看看 BloomFilterPolicy 是如何实现这些接口的，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  // https://github.com/google/leveldb/blob/master/util/bloom.cc  class BloomFilterPolicy : public FilterPolicy { public: explicit BloomFilterPolicy(int bits_per_key) : bits_per_key_(bits_per_key) { k_ = static_cast\u0026lt;size_t\u0026gt;(bits_per_key * 0.69); // 0.69 =~ ln(2)  if (k_ \u0026lt; 1) k_ = 1; if (k_ \u0026gt; 30) k_ = 30; } const char* Name() const override { return \u0026#34;leveldb.BuiltinBloomFilter2\u0026#34;; } void CreateFilter(const Slice* keys, int n, std::string* dst) const override { size_t bits = n * bits_per_key_; if (bits \u0026lt; 64) bits = 64; size_t bytes = (bits + 7) / 8; bits = bytes * 8; const size_t init_size = dst-\u0026gt;size(); dst-\u0026gt;resize(init_size + bytes, 0); dst-\u0026gt;push_back(static_cast\u0026lt;char\u0026gt;(k_)); // Remember # of probes in filter  char* array = \u0026amp;(*dst)[init_size]; //依次处理每一个Key  for (int i = 0; i \u0026lt; n; i++) { uint32_t h = BloomHash(keys[i]); //通过位运算获取delta  const uint32_t delta = (h \u0026gt;\u0026gt; 17) | (h \u0026lt;\u0026lt; 15); // Rotate right 17 bits  //计算哈希值，对每一个key计算k次哈希值（这里采用对每次计算出的的哈希值增加delta来模拟）  for (size_t j = 0; j \u0026lt; k_; j++) { const uint32_t bitpos = h % bits; array[bitpos / 8] |= (1 \u0026lt;\u0026lt; (bitpos % 8)); h += delta; } } } bool KeyMayMatch(const Slice\u0026amp; key, const Slice\u0026amp; bloom_filter) const override { const size_t len = bloom_filter.size(); if (len \u0026lt; 2) return false; const char* array = bloom_filter.data(); const size_t bits = (len - 1) * 8; const size_t k = array[len - 1]; if (k \u0026gt; 30) { return true; } //计算出key的哈希  uint32_t h = BloomHash(key); const uint32_t delta = (h \u0026gt;\u0026gt; 17) | (h \u0026lt;\u0026lt; 15); // Rotate right 17 bits  //判断对应位置是否全为1，如果有任何一个为0说明数据不可能存在布隆过滤器中，返回false，否则true  for (size_t j = 0; j \u0026lt; k; j++) { const uint32_t bitpos = h % bits; if ((array[bitpos / 8] \u0026amp; (1 \u0026lt;\u0026lt; (bitpos % 8))) == 0) return false; h += delta; } return true; } private: size_t bits_per_key_; size_t k_; };   具体逻辑都标识在了注释中，就不多说了，这里提一下这里用到的一个哈希小技巧：\n 为了避免哈希冲突，大部分布隆过滤器中都会采用多种哈希算法来计算。LevelDB 为了简化规则，使用位运算 计算出 delta，对每轮计算出的哈希值累加上 delta，模拟多轮哈希计算。  应用 LevelDB 中具体使用布隆过滤器时又封装了两个类，分别为 FilterBlockBuilder 和 FilterBlockReader。\nFilterBlockBuilder 的主要功能是通过调用 BloomFilterPolicy 的 CreateFilter 方法生成布隆过滤器，并且将布隆过滤器的内容写入 SSTable 的元数据块。其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // https://github.com/google/leveldb/blob/master/table/filter_block.h  class FilterBlockBuilder { public: explicit FilterBlockBuilder(const FilterPolicy*); FilterBlockBuilder(const FilterBlockBuilder\u0026amp;) = delete; FilterBlockBuilder\u0026amp; operator=(const FilterBlockBuilder\u0026amp;) = delete; void StartBlock(uint64_t block_offset); void AddKey(const Slice\u0026amp; key); Slice Finish(); private: void GenerateFilter(); const FilterPolicy* policy_; std::string keys_; // Flattened key contents  std::vector\u0026lt;size_t\u0026gt; start_; // Starting index in keys_ of each key  std::string result_; // Filter data computed so far  std::vector\u0026lt;Slice\u0026gt; tmp_keys_; // policy_-\u0026gt;CreateFilter() argument  std::vector\u0026lt;uint32_t\u0026gt; filter_offsets_; };    成员变量  policy_：实现了 FilterPolicy 接口的类，在布隆过滤器中为 BloomFilterPolicy。 keys_：生成布隆过滤器的键。 start_：数组类型，保存 keys_ 参数中每一个键的开始索引。 result_：保存生成的布隆过滤器内容。 tmp_keys_：生成布隆过滤器时，会通过 keys_ 和 start_ 拆分出每一个键，将拆分出的每一个键保存到 tmp_keys_ 数组中。 filter_offsets_：过滤器偏移量，即每一个过滤器在元数据块中的偏移量。    写入的核心逻辑主要在 AddKey 和 Finish 中，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // https://github.com/google/leveldb/blob/master/table/filter_block.cc  void FilterBlockBuilder::AddKey(const Slice\u0026amp; key) { Slice k = key; //记录key的索引位置  start_.push_back(keys_.size()); //将key追加到该字符串中  keys_.append(k.data(), k.size()); } Slice FilterBlockBuilder::Finish() { //写入内容  if (!start_.empty()) { GenerateFilter(); } //写入偏移量  const uint32_t array_offset = result_.size(); for (size_t i = 0; i \u0026lt; filter_offsets_.size(); i++) { PutFixed32(\u0026amp;result_, filter_offsets_[i]); } //写入总大小  PutFixed32(\u0026amp;result_, array_offset); //写入基数  result_.push_back(kFilterBaseLg); // Save encoding parameter in result  return Slice(result_); }   首先调用 AddKey 记录 Key 的索引位置，并将 Key 追加到 Keys_中。接着，调用 Finish 生成一个元数据块，并分别写入布隆过滤器的内容、偏移量、内容总大小和基数。\nFilterBlockReader 用于查找一个元素是否在一个布隆过滤器中，其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/table/filter_block.h  class FilterBlockReader { public: // REQUIRES: \u0026#34;contents\u0026#34; and *policy must stay live while *this is live.  FilterBlockReader(const FilterPolicy* policy, const Slice\u0026amp; contents); bool KeyMayMatch(uint64_t block_offset, const Slice\u0026amp; key); private: const FilterPolicy* policy_; const char* data_; // Pointer to filter data (at block-start)  const char* offset_; // Pointer to beginning of offset array (at block-end)  size_t num_; // Number of entries in offset array  size_t base_lg_; // Encoding parameter (see kFilterBaseLg in .cc file) };    成员变量  policy_：实现了 FilterPolicy 接口的类，在布隆过滤器中为 BloomFilterPolicy。 data_：指向元数据块的开始位置。 offset_：指向元数据块中过滤器偏移量的开始位置。 num_：过滤器偏移量的个数。 base_lg_：过滤器基数。    FilterBlockReader 中的 KeyMayMatch 根据数据块偏移量找到对应的过滤器内容，然后调用 BloomFilterPolicy 中的 KeyMayMatch 方法判断一个元素是否在该过滤器之中。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/table/filter_block.cc  bool FilterBlockReader::KeyMayMatch(uint64_t block_offset, const Slice\u0026amp; key) { uint64_t index = block_offset \u0026gt;\u0026gt; base_lg_; if (index \u0026lt; num_) { uint32_t start = DecodeFixed32(offset_ + index * 4); uint32_t limit = DecodeFixed32(offset_ + index * 4 + 4); if (start \u0026lt;= limit \u0026amp;\u0026amp; limit \u0026lt;= static_cast\u0026lt;size_t\u0026gt;(offset_ - data_)) { Slice filter = Slice(data_ + start, limit - start); return policy_-\u0026gt;KeyMayMatch(key, filter); } else if (start == limit) { // Empty filters do not match any keys  return false; } }   LRU Cache 我们希望经常使用的 SSTable 内容尽量保存在内存中，但如果磁盘中的 SSTable 文件的总大小大于服务器内存大小，或者需要控制 LevelDB 的内存总占用量时，就需要使用 LRU（least recentlyused）Cache 来管理内存。LRU 是一种缓存置换策略，根据该策略不仅可以管理内存的占用量，还可以将热数据尽量保存到内存中，以加快读取速度。\n内存是有限并且昂贵的资源，因此 LevelDB 通过 LRU 策略管理读取到内存的数据。LRU 基于这样一种假设：如果一个资源最近没有或者很少被使用到，那么将来也会很少甚至不被使用。因此如果内存不足，需要淘汰数据时，可以根据 LRU 策略来执行。\n这里就不过多介绍原理，如果想了解可以看看我的往期博客 高级数据结构与算法 | LRU缓存机制（Least Recently Used）\n结构 为了方便拓展其他的缓存置换算法（目前仅实现了 LRU），LevelDB 抽象出了一个 Cache 纯虚类，结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  // https://github.com/google/leveldb/blob/master/include/leveldb/cache.h  class LEVELDB_EXPORT Cache { public: Cache() = default; Cache(const Cache\u0026amp;) = delete; Cache\u0026amp; operator=(const Cache\u0026amp;) = delete; virtual ~Cache(); struct Handle {}; virtual Handle* Insert(const Slice\u0026amp; key, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)) = 0; virtual Handle* Lookup(const Slice\u0026amp; key) = 0; virtual void Release(Handle* handle) = 0; virtual void* Value(Handle* handle) = 0; virtual void Erase(const Slice\u0026amp; key) = 0; virtual uint64_t NewId() = 0; virtual void Prune() {} virtual size_t TotalCharge() const = 0; private: void LRU_Remove(Handle* e); void LRU_Append(Handle* e); void Unref(Handle* e); struct Rep; Rep* rep_; };   LevelDB 中 LRU 由 LRUCache 实现，并且 LRU 节点由 LRUHandle 实现。首先我们来看看 LRUHandle：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/util/cache.cc  struct LRUHandle { void* value; void (*deleter)(const Slice\u0026amp;, void* value); LRUHandle* next_hash; LRUHandle* next;\tLRUHandle* prev; size_t charge; // TODO(opt): Only allow uint32_t?  size_t key_length; bool in_cache; // Whether entry is in the cache.  uint32_t refs; // References, including cache reference, if present.  uint32_t hash; // Hash of key(); used for fast sharding and comparisons  char key_data[1]; // Beginning of key  Slice key() const { assert(next != this); return Slice(key_data, key_length); } };    成员变量  value：具体的值，指针类型。 deleter：自定义回收节点的回调函数。 next_hash：用于 hashtable 冲突时，下一个节点。 next：代表 LRU 中双向链表中下一个节点。 prev：代表 LRU 中双向链表中上一个节点。 charge：记录当前 value 所占用的内存大小，用于后面超出容量后需要进行 lru。 key_length：数据 key 的长度。 in_cache：表示是否在缓存中。 refs：引用计数，因为当前节点可能会被多个组件使用，不能简单的删除。 hash：记录当前 key 的 hash 值。    这个节点设计的非常巧妙，既可以用来当做 LRU 节点，也可以用来当作 hashtable 的节点。\n接着我们来看看 LRUCache 的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  // https://github.com/google/leveldb/blob/master/util/cache.cc  class LRUCache { public: LRUCache(); ~LRUCache(); void SetCapacity(size_t capacity) { capacity_ = capacity; } Cache::Handle* Insert(const Slice\u0026amp; key, uint32_t hash, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)); Cache::Handle* Lookup(const Slice\u0026amp; key, uint32_t hash); void Release(Cache::Handle* handle); void Erase(const Slice\u0026amp; key, uint32_t hash); void Prune(); size_t TotalCharge() const { MutexLock l(\u0026amp;mutex_); return usage_; } private: void LRU_Remove(LRUHandle* e); void LRU_Append(LRUHandle* list, LRUHandle* e); void Ref(LRUHandle* e); void Unref(LRUHandle* e); bool FinishErase(LRUHandle* e) EXCLUSIVE_LOCKS_REQUIRED(mutex_); size_t capacity_; mutable port::Mutex mutex_; size_t usage_ GUARDED_BY(mutex_); LRUHandle lru_ GUARDED_BY(mutex_); LRUHandle in_use_ GUARDED_BY(mutex_); HandleTable table_ GUARDED_BY(mutex_); };   实现 LRU Cache 的实现有如下两处细节需要注意：\n 减小锁的粒度：LRUCache 的并发操作不安全，因此操作时需要加锁。为了减小锁的粒度，LevelDB 中通过哈希将键分为 16 个段，可以理解为有 16 个相同的 LRU Cache 结构，每次进行 Cache 操作时需要先去查找键属于的段。 缓存淘汰：每个LRU Cache结构中有两个成员变量：lru_ 和 in_use_ 。lru_ 双向链表中的节点是可以进行淘汰的，而 in_use_ 双向链表中的节点表示正在使用，因此不可以进行淘汰。  减小锁的粒度 LevelDB 为了减小锁的粒度，封装了一个 ShardedLRUCache，其中有一个大小为 16（默认值） 的 shard_ 成员，shard_ 成员的每个元素均为一个 LRUCache 结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  // ShardedLRUCache class ShardedLRUCache : public Cache { private: LRUCache shard_[kNumShards]; port::Mutex id_mutex_; uint64_t last_id_; static inline uint32_t HashSlice(const Slice\u0026amp; s) { return Hash(s.data(), s.size(), 0); } static uint32_t Shard(uint32_t hash) { return hash \u0026gt;\u0026gt; (32 - kNumShardBits); } public: explicit ShardedLRUCache(size_t capacity) : last_id_(0) { const size_t per_shard = (capacity + (kNumShards - 1)) / kNumShards; for (int s = 0; s \u0026lt; kNumShards; s++) { shard_[s].SetCapacity(per_shard); } } ~ShardedLRUCache() override {} Handle* Insert(const Slice\u0026amp; key, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)) override { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } Handle* Lookup(const Slice\u0026amp; key) override { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } void Release(Handle* handle) override { LRUHandle* h = reinterpret_cast\u0026lt;LRUHandle*\u0026gt;(handle); shard_[Shard(h-\u0026gt;hash)].Release(handle); } void Erase(const Slice\u0026amp; key) override { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } void* Value(Handle* handle) override { return reinterpret_cast\u0026lt;LRUHandle*\u0026gt;(handle)-\u0026gt;value; } uint64_t NewId() override { MutexLock l(\u0026amp;id_mutex_); return ++(last_id_); } void Prune() override { for (int s = 0; s \u0026lt; kNumShards; s++) { shard_[s].Prune(); } } size_t TotalCharge() const override { size_t total = 0; for (int s = 0; s \u0026lt; kNumShards; s++) { total += shard_[s].TotalCharge(); } return total; } };   为了能使哈希值刚好能够映射 shard_ 数组，其通过位操作再次对哈希值进行处理，如下代码：\n1 2 3  // https://github.com/google/leveldb/blob/master/util/cache.cc  static uint32_t Shard(uint32_t hash) { return hash \u0026gt;\u0026gt; (32 - kNumShardBits); }   其将哈希值右移 28 位，只取最高的 4 位，这样就能够保证处理过的哈希值刚好小于 16。\n缓存淘汰 每个LRU Cache结构中有两个成员变量：lru_ 和 in_use_ 。lru_ 双向链表中的节点是可以进行淘汰的，而 in_use_ 双向链表中的节点表示正在使用，因此不可以进行淘汰。如果内存超出限制需要淘汰一个节点时，LevelDB 会将 lru_ 链表中的节点逐个淘汰。\n那我们如何决定节点放置的规则呢？其采用如下规则：如果一个缓存节点的 in_cache 为 true，并且 refs 等于 1，则放置到 lru_ 中；如果 in_cache 为 true，并且 refs 大于等于 2，则放置到 in_use_ 中。\n 对于 in_cache，其会在以下情况变为 false：  删除该节点后。 调用 LRUCache 的析构函数时，会将所有节点的 in_cache 置为 false。 插入一个节点时，如果已经存在一个键值相同的节点，则旧节点的 in_cache 会置为 false。   对于 refs，其变化规则如下:  每次调用 Ref 函数，会将 refs 变量加 1，调用 Unref 函数，会将 refs 变量减 1。 插入一个节点时，该节点会放到 in_use_ 链表中，并且初始的引用计数为 2，不再使用该节点时将引用计数减 1，如果此时节点也不再被其他地方引用，那么引用计数为 1，将其放到 lru_ 链表中。 查找一个节点时，如果查找成功，则调用 Ref，将该节点的引用计数加 1，如果引用计数大于等于 2，会将节点放到 in_use_ 链表中，同理，不再使用该节点时将引用计数减 1，如果此时节点也不再被其他地方引用，那么引用计数为 1，将其放到 lru_ 链表中。    Ref 和 Unref 代码如下：\n1 2 3 4 5 6 7 8 9  // https://github.com/google/leveldb/blob/master/util/cache.cc  void LRUCache::Ref(LRUHandle* e) { if (e-\u0026gt;refs == 1 \u0026amp;\u0026amp; e-\u0026gt;in_cache) { // If on lru_ list, move to in_use_ list.  LRU_Remove(e); LRU_Append(\u0026amp;in_use_, e); } e-\u0026gt;refs++; }   如果缓存节点在 lru_ 链表中（refs 为 1，in_cache 为 true），则首先从 lru_ 链表中删除该节点，然后将节点放到 in_use_ 链表中。最后将节点的 refs 变量加 1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/util/cache.cc  void LRUCache::Unref(LRUHandle* e) { assert(e-\u0026gt;refs \u0026gt; 0); e-\u0026gt;refs--; if (e-\u0026gt;refs == 0) { // Deallocate.  assert(!e-\u0026gt;in_cache); (*e-\u0026gt;deleter)(e-\u0026gt;key(), e-\u0026gt;value); free(e); } else if (e-\u0026gt;in_cache \u0026amp;\u0026amp; e-\u0026gt;refs == 1) { // No longer in use; move to lru_ list.  LRU_Remove(e); LRU_Append(\u0026amp;lru_, e); } }   Unref 首先将节点的 refs 变量减 1，然后判断如果 refs 已经等于 0，则删除并释放该节点，否则，如果 refs 变量等于 1 并且 in_cache 为 true，则将该节点从 in_use_ 链表中删除并且移动到 lru_ 链表中。如果内存超出限制需要淘汰一个节点时，LevelDB 会将 lru_ 链表中的节点逐个淘汰。\n应用 LevelDB 中 Cache 缓存的主要是 SSTable，即缓存节点的键为 8 字节的文件序号，值为一个包含了 SSTable 实例的结构。其主要逻辑由 TableCache 实现，首先我们先来看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  // https://github.com/google/leveldb/blob/master/db/table_cache.h  class TableCache { public: TableCache(const std::string\u0026amp; dbname, const Options\u0026amp; options, int entries); ~TableCache(); Iterator* NewIterator(const ReadOptions\u0026amp; options, uint64_t file_number, uint64_t file_size, Table** tableptr = nullptr); Status Get(const ReadOptions\u0026amp; options, uint64_t file_number, uint64_t file_size, const Slice\u0026amp; k, void* arg, void (*handle_result)(void*, const Slice\u0026amp;, const Slice\u0026amp;)); void Evict(uint64_t file_number); private: Status FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle**); Env* const env_; const std::string dbname_; const Options\u0026amp; options_; Cache* cache_; };     成员变量\n env_：用于读取 SSTable 文件。 dbname_：SSTable 名字。 options_：Cache 参数配置。 cache_：缓存基类句柄。    这里的核心逻辑 FindTable 方法会使用文件序号（file_number）作为键，并且在 cache_ 中查找是否存在该键，如果不存在，则需要打开一个 SSTable，经过处理之后作为值插入 cache_ 中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  Status TableCache::FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle** handle) { Status s; char buf[sizeof(file_number)]; EncodeFixed64(buf, file_number); Slice key(buf, sizeof(buf)); //在缓存中查找key是否存在  *handle = cache_-\u0026gt;Lookup(key); //如果不存在，则打开一个SSTable文件  if (*handle == nullptr) { //生成fname和RandomAccessFile实例  std::string fname = TableFileName(dbname_, file_number); RandomAccessFile* file = nullptr; Table* table = nullptr; s = env_-\u0026gt;NewRandomAccessFile(fname, \u0026amp;file); if (!s.ok()) { std::string old_fname = SSTTableFileName(dbname_, file_number); if (env_-\u0026gt;NewRandomAccessFile(old_fname, \u0026amp;file).ok()) { s = Status::OK(); } } //打开SSTable文件并且生成一个Table实例，并保存到table变量中。  if (s.ok()) { s = Table::Open(options_, file, file_size, \u0026amp;table); } if (!s.ok()) { assert(table == nullptr); delete file; } else { TableAndFile* tf = new TableAndFile; tf-\u0026gt;file = file; tf-\u0026gt;table = table; //以文件序号为key，TableAndFile为Value，插入缓存中。  *handle = cache_-\u0026gt;Insert(key, tf, 1, \u0026amp;DeleteEntry); } } return s; }   SSTable 在 Cache 中缓存时的键为文件序列号，值为一个 TableAndFile 实例，该实例中包括两个成员变量，分别为 file 和 table，查找时通过保存在 table 变量中的 Table 实例迭代器进行查找。\n","date":"2022-05-23T23:41:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-sstable%E6%A8%A1%E5%9D%97/","title":"LevelDB SSTable模块"},{"content":"MemTable模块 MemTable 在 LevelDB 中，MemTable 是底层数据结构 SkipList 的封装。\n结构 首先我们来看看它的结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  // https://github.com/google/leveldb/blob/master/db/memtable.h  class MemTable { public: explicit MemTable(const InternalKeyComparator\u0026amp; comparator); MemTable(const MemTable\u0026amp;) = delete; MemTable\u0026amp; operator=(const MemTable\u0026amp;) = delete; void Ref() { ++refs_; } void Unref() { --refs_; assert(refs_ \u0026gt;= 0); if (refs_ \u0026lt;= 0) { delete this; } } size_t ApproximateMemoryUsage(); Iterator* NewIterator(); void Add(SequenceNumber seq, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value); bool Get(const LookupKey\u0026amp; key, std::string* value, Status* s); private: friend class MemTableIterator; friend class MemTableBackwardIterator; struct KeyComparator { const InternalKeyComparator comparator; explicit KeyComparator(const InternalKeyComparator\u0026amp; c) : comparator(c) {} int operator()(const char* a, const char* b) const; }; typedef SkipList\u0026lt;const char*, KeyComparator\u0026gt; Table; ~MemTable(); // Private since only Unref() should be used to delete it  KeyComparator comparator_; int refs_; Arena arena_; Table table_; };   其组成如下：\n 成员变量  comparator_：比较器，用于决定 key 的顺序。 refs__：引用计数器，当计数为 0 时释放资源。 arena_：内存池，负责管理内存。 table_：底层存储的 SkipList。   成员函数  Ref ：引用计数增加。 Unref：引用计数减少。 ApproximateMemoryUsage：统计内存使用量。 NewIterator：返回首部迭代器。 Add：插入数据。 Get：查找数据。    接着我们来看看最为核心的插入与查找。\n插入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // https://github.com/google/leveldb/blob/master/db/memtable.cc  void MemTable::Add(SequenceNumber s, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value) { //计算需要的内存大小  size_t key_size = key.size(); size_t val_size = value.size(); size_t internal_key_size = key_size + 8; const size_t encoded_len = VarintLength(internal_key_size) + internal_key_size + VarintLength(val_size) + val_size; //分配内存，并按照 len key sequencelValueType value_len value顺序将数据写入缓冲区  char* buf = arena_.Allocate(encoded_len); char* p = EncodeVarint32(buf, internal_key_size); std::memcpy(p, key.data(), key_size); p += key_size; EncodeFixed64(p, (s \u0026lt;\u0026lt; 8) | type); p += 8; p = EncodeVarint32(p, val_size); std::memcpy(p, value.data(), val_size); assert(p + val_size == buf + encoded_len); //调用底层SkipList的Insert将数据插入  table_.Insert(buf) }   插入的逻辑主要分为三步：\n 计算需要的内存大小。 分配内存，并按照 len key sequencelValueType value_len value顺序将数据写入缓冲区。 调用底层SkipList的Insert将数据插入。  查找 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  // https://github.com/google/leveldb/blob/master/db/memtable.cc  bool MemTable::Get(const LookupKey\u0026amp; key, std::string* value, Status* s) { Slice memkey = key.memtable_key(); //获取迭代器  Table::Iterator iter(\u0026amp;table_);\t//通过迭代器的Seek查找数据  iter.Seek(memkey.data()); if (iter.Valid()) { // entry format is:  // klength varint32  // userkey char[klength]  // tag uint64  // vlength varint32  // value char[vlength]  const char* entry = iter.key(); uint32_t key_length; const char* key_ptr = GetVarint32Ptr(entry, entry + 5, \u0026amp;key_length); //判断查找是否成功  if (comparator_.comparator.user_comparator()-\u0026gt;Compare( Slice(key_ptr, key_length - 8), key.user_key()) == 0) { const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8); //判断查找到的类型  switch (static_cast\u0026lt;ValueType\u0026gt;(tag \u0026amp; 0xff)) { //如果数据存在，则将数据保存后返回true  case kTypeValue: { Slice v = GetLengthPrefixedSlice(key_ptr + key_length); value-\u0026gt;assign(v.data(), v.size()); return true; } //如果数据删除，则将状态标记为未找到并返回true  case kTypeDeletion: *s = Status::NotFound(Slice()); return true; } } } //没有找到则返回false  return false; }   查找流程如下：\n 获取迭代器，通过 Seek 查找数据。 判断查找结果：  查找成功，数据存在，保存数据后返回 true。 查找成功，数据已删除（曾经存在），标记状态为 NotFound 后返回 true。 查找失败，数据不存在，返回 false。    了解完了 MemTable，接下来再看看它底层使用的 SkipList 是如何实现的。\nSkipList SkipList 是一个多层有序链表结构，通过在每个节点中保存多个指向其他节点的指针，将有序链表平均的复杂度O（N） 降低到 O（logN）。SkipList 因具有实现简单、性能优良等特点得到了广泛应用，例如 Redis 中的 ZSet，以及 LevelDB 的 MemTable。\n具体信息可以看看我之前写的博客 高级数据结构与算法 | 跳跃表（Skip List）。这里就不多作介绍，直接看代码。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; class SkipList { private: struct Node; public: explicit SkipList(Comparator cmp, Arena* arena); SkipList(const SkipList\u0026amp;) = delete; SkipList\u0026amp; operator=(const SkipList\u0026amp;) = delete; void Insert(const Key\u0026amp; key); bool Contains(const Key\u0026amp; key) const; class Iterator { public: explicit Iterator(const SkipList* list); bool Valid() const; const Key\u0026amp; key() const; void Next(); void Prev(); void Seek(const Key\u0026amp; target); void SeekToFirst(); void SeekToLast(); private: const SkipList* list_; Node* node_; }; private: enum { kMaxHeight = 12 }; inline int GetMaxHeight() const { return max_height_.load(std::memory_order_relaxed); } Node* NewNode(const Key\u0026amp; key, int height); int RandomHeight(); bool Equal(const Key\u0026amp; a, const Key\u0026amp; b) const { return (compare_(a, b) == 0); } bool KeyIsAfterNode(const Key\u0026amp; key, Node* n) const; Node* FindGreaterOrEqual(const Key\u0026amp; key, Node** prev) const; Node* FindLessThan(const Key\u0026amp; key) const; Node* FindLast() const; Comparator const compare_; Arena* const arena_; Node* const head_; std::atomic\u0026lt;int\u0026gt; max_height_; Random rnd_; };   这里也是只介绍几个重要的函数——晋升、插入、查找。\n晋升 在 LevelDB 中，每次插入节点的层高由 RandomHeight 决定，比起 Redis 的 1/2 来说，LevelDB 中的晋升逻辑更加复杂，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; int SkipList\u0026lt;Key, Comparator\u0026gt;::RandomHeight() { static const unsigned int kBranching = 4; int height = 1; while (height \u0026lt; kMaxHeight \u0026amp;\u0026amp; ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u0026gt; 0); assert(height \u0026lt;= kMaxHeight); return height; }   上述代码中 rnd_.Next() 的作用是生成一个随机数，将该随机数对 4 取余，如果余数等于 0 并且层高小于规定的最大层高 12，则将层高加 1。因为对 4 取余数结果只有 0、1、2、3 这 4 种可能，因此可以推导得出每个节点层高为 1 的概率是 3/4，层高为 2 的概率是 1/4。依此类推，层高为 3 的概率是 3/16（ 1/4 × 3/4 ），层高为4的概率是3/64（ 1/4 × 1/4 × 3/4 ），即层级越高，概率越小。\n查找 SkipList 的查找主要是借助迭代器的 Seek 来实现的，而在 Seek 中又调用了 FindGreaterOrEqual，下面看看它们的实现逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; inline void SkipList\u0026lt;Key, Comparator\u0026gt;::Iterator::Seek(const Key\u0026amp; target) { node_ = list_-\u0026gt;FindGreaterOrEqual(target, nullptr); } SkipList\u0026lt;Key, Comparator\u0026gt;::FindGreaterOrEqual(const Key\u0026amp; key, Node** prev) const { //从顶层开始查询  Node* x = head_; int level = GetMaxHeight() - 1; while (true) { Node* next = x-\u0026gt;Next(level); //如果当前节点的值小于要查询的值，则在该层继续查找  if (KeyIsAfterNode(key, next)) { x = next; } else { //如果大于等于，则说明不可能在该层，前往下一层查找。  if (prev != nullptr) prev[level] = x; //如果查询到底就直接返回，此时有两种情况1.查询成功，返回底层结果 2.查询失败，返回对应最底层位置  if (level == 0) { return next; } else { // Switch to next list  level--; } } } }   查找逻辑与常规 SkipList 实现一样：\n 从顶层开始查询。 对比当前阶段的值是否小于查询的值：  小于：沿着当前层继续查找。 大于等于：前往下一层查找。   判断当前层数是否到底，没到就继续往下走。 到底了返回数据，如果返回的数据与 key 相同则说明查询成功，否则失败。  插入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; void SkipList\u0026lt;Key, Comparator\u0026gt;::Insert(const Key\u0026amp; key) { //记录每一层级查找到的位置  Node* prev[kMaxHeight]; //查找适合插入的位置  Node* x = FindGreaterOrEqual(key, prev); assert(x == nullptr || !Equal(key, x-\u0026gt;key)); //获取本次插入的最高层数  int height = RandomHeight(); //如果本次插入的最高层数大于目前最高层数，则将多出的几层指向新插入节点  if (height \u0026gt; GetMaxHeight()) { for (int i = GetMaxHeight(); i \u0026lt; height; i++) { prev[i] = head_; } max_height_.store(height, std::memory_order_relaxed); } x = NewNode(key, height); //将需要更新的节点依次更新  for (int i = 0; i \u0026lt; height; i++) { x-\u0026gt;NoBarrier_SetNext(i, prev[i]-\u0026gt;NoBarrier_Next(i)); prev[i]-\u0026gt;SetNext(i, x); } }   具体逻辑如下：\n 首先用一个数组存储每一层所查找到的位置。 使用 FindGreaterOrEqual 获取适合插入的位置。 通过 RandomHeight 获取本次插入的最高层数：  如果本次插入的最高层数大于目前最高层数，则将多出的几层指向新插入节点。 如果小于等于，则无需更新。   遍历 prev，将需要更新的节点依次更新。  MemTable写入SSTable 在 LSM 树的实现中，会先将数据写入 MemTable，当 MemTable 大于配置的阈值时，将其作为 SSTable 写入磁盘。\n这里我们只看核心逻辑，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { //...  //生成一个MemTable迭代器  Iterator* iter = mem-\u0026gt;NewIterator(); Log(options_.info_log, \u0026#34;Level-0 table #%llu: started\u0026#34;, (unsigned long long)meta.number); Status s; { mutex_.Unlock(); //调用BuildTable，将MemTable迭代器作为参数传入，生成一个SSTable  s = BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026amp;meta); mutex_.Lock(); } //...  return s; }   在这里首先会生成一个 MemTable 迭代器，调用 BuildTable ，将 MemTable 迭代器作为参数传入，生成一个 SSTable。\n我们接着来看 BuildTable 的逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  // https://github.com/google/leveldb/blob/master/db/builder.cc  Status BuildTable(const std::string\u0026amp; dbname, Env* env, const Options\u0026amp; options, TableCache* table_cache, Iterator* iter, FileMetaData* meta) { Status s; meta-\u0026gt;file_size = 0; //将迭代器移动到首部  iter-\u0026gt;SeekToFirst(); //生成SSTable文件名  std::string fname = TableFileName(dbname, meta-\u0026gt;number); if (iter-\u0026gt;Valid()) { WritableFile* file; s = env-\u0026gt;NewWritableFile(fname, \u0026amp;file); if (!s.ok()) { return s; } //创建一个TableBuilder  TableBuilder* builder = new TableBuilder(options, file); meta-\u0026gt;smallest.DecodeFrom(iter-\u0026gt;key()); Slice key; //遍历迭代器，将MemTable中的每一对K-V写入TableBuilder中  for (; iter-\u0026gt;Valid(); iter-\u0026gt;Next()) { key = iter-\u0026gt;key(); builder-\u0026gt;Add(key, iter-\u0026gt;value()); } if (!key.empty()) { meta-\u0026gt;largest.DecodeFrom(key); } //调用TableBuilder的Finish函数生成SSTable文件  s = builder-\u0026gt;Finish(); if (s.ok()) { meta-\u0026gt;file_size = builder-\u0026gt;FileSize(); assert(meta-\u0026gt;file_size \u0026gt; 0); } delete builder; //调用Sync将文件刷新到磁盘中\t if (s.ok()) { s = file-\u0026gt;Sync(); } //关闭文件  if (s.ok()) { s = file-\u0026gt;Close(); } //...  return s; }   核心逻辑如下：\n 将 MemTable 迭代器移动到首部。 生成 SSTable 文件名。 创建一个 TableBuilder。 遍历迭代器，将 MemTable 中的每一对 K-V 写入 TableBuilder 中。 调用 TableBuilder 的 Finish 函数生成 SSTable 文件。 调用 Sync 将文件刷新到磁盘中。 关闭文件。  ","date":"2022-05-23T23:40:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-memtable%E6%A8%A1%E5%9D%97/","title":"LevelDB MemTable模块"},{"content":"公共基础类 内存管理 Arena 内存频繁创建/释放的地方就会有内存池的出现，LevelDB 也不例外。在 Memtable 组件中，会有大量内存创建（数据持续 put）和释放（dump 到磁盘后内存结束），于是 LevelDB 通过 Arena 来管理内存。\n 它有什么好处呢？\n  提升性能：内存申请本身就需要占用一定的资源，消耗空间与时间。而 Arena 内存池的基本思路就是预先申请一大块内存，然后多次分配给不同的对象，从而减少 malloc 或 new 的调用次数。 提高内存利用率：频繁进行内存的申请与释放易造成内存碎片。即内存余量虽然够用，但由于缺乏足够大的连续空闲空间，从而造成申请一段较大的内存不成功的情况。而 Arena 具有整块内存的控制权，用户可以任意操作这段内存，从而避免内存碎片的产生。  结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // https://github.com/google/leveldb/blob/master/util/arena.h  class Arena { public: Arena(); Arena(const Arena\u0026amp;) = delete; Arena\u0026amp; operator=(const Arena\u0026amp;) = delete; ~Arena(); char* Allocate(size_t bytes); malloc. char* AllocateAligned(size_t bytes); size_t MemoryUsage() const { return memory_usage_.load(std::memory_order_relaxed); } private: char* AllocateFallback(size_t bytes); char* AllocateNewBlock(size_t block_bytes); char* alloc_ptr_; size_t alloc_bytes_remaining_; std::vector\u0026lt;char*\u0026gt; blocks_; std::atomic\u0026lt;size_t\u0026gt; memory_usage_; };   其组成如下：\n 成员变量 alloc_ptr_ ：当前已使用内存的指针  blocks_ ：实际分配的内存池_ alloc_bytes_remaining_ ：剩余内存字节数 memory_usage_ ：记录内存的使用情况 kBlockSize ：一个块大小（默认4k）   成员函数  AllocateFallback ：按需分配内存，可能会有内存浪费。 AllocateAligned ：分配偶数大小的内存，主要是 skiplist 节点时，目的是加快访问。 MemoryUsage：统计内存使用量。    内存分配 Arena 中与内存分配有关的两个接口函数：Allocate 与 AllocateAligned。\n首先我们来看看 Allocate 与它依赖的两个函数 AllocateFallback 与 AllocateNewBlock。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12  // https://github.com/google/leveldb/blob/master/util/arena.h  inline char* Arena::Allocate(size_t bytes) { assert(bytes \u0026gt; 0); if (bytes \u0026lt;= alloc_bytes_remaining_) { char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } return AllocateFallback(bytes); }   首先，对于 Allocate 来说，它存在两种情况\n 需要分配的字节数小于等于 alloc_bytes_remaining_：Allocate 直接返回 alloc_ptr_ 指向的地址空间，然后对 alloc_ptr_ 与 alloc_bytes_remaining_ 进行更新。 需要分配的字节数大于 alloc_bytes_remaining_：调用 AllocateFallback 方法进行扩容。  AllocateFallback 用于申请一个新 Block 内存空间，然后分配需要的内存并返回。因此当前 Block 剩余空闲内存就不可避免地浪费了。接着看看 AllocateFallback 的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateFallback(size_t bytes) { if (bytes \u0026gt; kBlockSize / 4) { char* result = AllocateNewBlock(bytes); return result; } alloc_ptr_ = AllocateNewBlock(kBlockSize); alloc_bytes_remaining_ = kBlockSize; char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; }   AllocateFallback 的使用也包括两种情况：\n 需要分配的空间大于 kBlockSize 的 1/4（即1024字节）：直接申请需要分配空间大小的 Block，从而避免剩余内存空间的浪费。 需要分配的空间小于等于 kBlockSize 的 1/4 ：申请一个大小为 kBlockSize 的新 Block 空间，然后在新的Block上分配需要的内存并返回其首地址。  对于 Block 的分配，这里又调用了 AllocateNewBlock，其逻辑如下：\n1 2 3 4 5 6 7 8 9  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); memory_usage_.fetch_add(block_bytes + sizeof(char*), std::memory_order_relaxed); return result; }   这里的逻辑非常简单，其通过 new，申请了一段大小为 block_bytes 的内存空间，并将这块空间的地址存储到 blocks_ 中，之后更新当前可用的总空间大小后将空间首地址返回。\n讲解完了 Allocate ，我们接着再来看看 AllocateAligned。虽然它也用于内存分配，但不同点在于它考虑了内存分配时的内存对齐问题（进行内存分配所返回的起始地址应为 b / 8 的倍数，在这里 b 代表操作系统平台的位数）。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateAligned(size_t bytes) { const int align = (sizeof(void*) \u0026gt; 8) ? sizeof(void*) : 8; static_assert((align \u0026amp; (align - 1)) == 0, \u0026#34;Pointer size should be a power of 2\u0026#34;); size_t current_mod = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(alloc_ptr_) \u0026amp; (align - 1); size_t slop = (current_mod == 0 ? 0 : align - current_mod); size_t needed = bytes + slop; char* result; if (needed \u0026lt;= alloc_bytes_remaining_) { result = alloc_ptr_ + slop; alloc_ptr_ += needed; alloc_bytes_remaining_ -= needed; } else { // AllocateFallback always returned aligned memory  result = AllocateFallback(bytes); } assert((reinterpret_cast\u0026lt;uintptr_t\u0026gt;(result) \u0026amp; (align - 1)) == 0); return result; }   由于主流的服务器平台采用 64 位的操作系统，64位 操作系统的指针同样为 64 位（即 8 个字节），因此这里的对齐就需要使分配的内存起始地址必然为 8 的倍数。要满足这一条件，采用的主要办法就是判断当前空闲内存的起始地址是否为 8 的倍数：如果是，则直接返回；如果不是，则对 8 求模，然后向后寻找最近的 8 的倍数的内存地址并返回。\n由于计算机进行乘除或求模运算的速度远慢于位操作运算，因此这里巧妙的用了位运算来进行优化。\n 用位运算判断某个数值是否为2的正整数幂：   static_assert((align \u0026amp; (align - 1)) == 0, \u0026quot;Pointer size should be a power of 2\u0026quot;);\n用位运算进行求模：  size_t current_mod = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(alloc_ptr_) \u0026amp; (align - 1);\n位运算的细节这里就i不提了，大家可以自行百度了解。\n内存使用率统计 memory_usage_ 用于存储当前 Arena 所申请的总共的内存空间大小，为了保证线程安全，其为 atomic\u0026lt;size_t\u0026gt; 变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/util/arena.h  size_t MemoryUsage() const { return memory_usage_.load(std::memory_order_relaxed); } // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); memory_usage_.fetch_add(block_bytes + sizeof(char*), std::memory_order_relaxed); return result; }   在 AllocateNewBock 申请内存的过程中，其会通过以下的公式来更新当前总大小\nmemory_usage_ = memory_usage_ + block_bytes + sizeof(char*) \n 为什么这里还需要加上一个sizeof（char*）呢？\n 因为在申请完 Block 后，Block 的首地址需要存储在一个 vector\u0026lt;char*\u0026gt; 的动态数组 blocks_ 中，因而需要额外占用一个指针的空间。\nTCMalloc LevelDB 中针对一些需要调用 new 或 malloc 方法进行堆内存操作的情况（即非内存池的内存分配），其使用 TCMalloc 进行优化。\nTCMalloc（Thread-Caching Malloc）是 google-perftool 中一个管理堆内存的内存分配器工具，可以降低内存频繁分配与释放所造成的性能损失，并有效控制内存碎片。默认 C/C++ 在编译器中主要采用 glibc 的内存分配器 ptmalloc2。同样的 malloc 操作，TCMalloc 比 ptmalloc2 更具性能优势。\nTCMalloc 的详细介绍可以参见http://goog-perftools.sourceforge.net/doc/tcmalloc.html。\nEnv家族 Env 是一个抽象接口类，用纯虚函数的形式定义了一些与平台操作的相关接口，如文件系统、多线程、时间操作等。接口定义如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT Env { public: Env(); Env(const Env\u0026amp;) = delete; Env\u0026amp; operator=(const Env\u0026amp;) = delete; virtual ~Env(); static Env* Default(); virtual Status NewSequentialFile(const std::string\u0026amp; fname, SequentialFile** result) = 0; virtual Status NewRandomAccessFile(const std::string\u0026amp; fname, RandomAccessFile** result) = 0; virtual Status NewWritableFile(const std::string\u0026amp; fname, WritableFile** result) = 0; virtual Status NewAppendableFile(const std::string\u0026amp; fname, WritableFile** result); virtual bool FileExists(const std::string\u0026amp; fname) = 0; virtual Status GetChildren(const std::string\u0026amp; dir, std::vector\u0026lt;std::string\u0026gt;* result) = 0; virtual Status RemoveFile(const std::string\u0026amp; fname); virtual Status DeleteFile(const std::string\u0026amp; fname); virtual Status CreateDir(const std::string\u0026amp; dirname) = 0; virtual Status RemoveDir(const std::string\u0026amp; dirname); virtual Status DeleteDir(const std::string\u0026amp; dirname); virtual Status GetFileSize(const std::string\u0026amp; fname, uint64_t* file_size) = 0; virtual Status RenameFile(const std::string\u0026amp; src, const std::string\u0026amp; target) = 0; virtual Status LockFile(const std::string\u0026amp; fname, FileLock** lock) = 0; virtual Status UnlockFile(FileLock* lock) = 0; virtual void Schedule(void (*function)(void* arg), void* arg) = 0; virtual void StartThread(void (*function)(void* arg), void* arg) = 0; virtual Status GetTestDirectory(std::string* path) = 0; virtual Status NewLogger(const std::string\u0026amp; fname, Logger** result) = 0; virtual uint64_t NowMicros() = 0; virtual void SleepForMicroseconds(int micros) = 0; };   Env 作为抽象类，有 3 个派生子类：PosixEnv、EnvWrapper 与 InMemoryEnv。\n这里就不过多的介绍它们的实现原理，只是大概的描述一下概念，方便理解。\nPosixEnv PosixEnv 是 LevelDB 中默认的 Env 实例对象。从字面意思上看，PosixEnv 就是针对 POSIX 平台的 Env 接口实现。\nEnvWrapper EnvWrapper 也是 Env 的一个派生类，与 PosixEnv 不同的是，EnvWrapper 中并没有定义众多纯虚函数接口的具体实现，而是定义了一个私有成员变量 Env* target_，并在构造函数中通过传递预定义的 Env 实例对象，从而实现对 target_ 的初始化操作。基于 EnvWrapper 的派生类，易于实现用户在某一个 Env 派生类的基础上改写其中一部分接口的需求。\nInMemoryEnv InMemoryEnv 就是 EnvWrapper 的一个子类，主要对 Env 中有关文件的接口进行了重写。InMemoryEnv 主要是将所有的操作都置于内存中，从而提升文件I/O的读取速度。\n文件操作 在 LevelDB 中，主要有三种文件 I/O 操作：\n SequentialFile：顺序读，如日志文件的读取、Manifest文件的读取。 WritableFile：顺序写，用于日志文件、SSTable文件、Manifest文件的写入。 RandomAccessFile：随机读，如SSTable文件的读取。  SequentialFile SequentialFile 定义了文件顺序读抽象接口，其接口定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT SequentialFile { public: SequentialFile() = default; SequentialFile(const SequentialFile\u0026amp;) = delete; SequentialFile\u0026amp; operator=(const SequentialFile\u0026amp;) = delete; virtual ~SequentialFile(); virtual Status Read(size_t n, Slice* result, char* scratch) = 0; virtual Status Skip(uint64_t n) = 0; };   其主要有两个接口方法，即 Read 与 Skip：\n Read：用于从文件当前位置顺序读取指定的字节数。 Skip：用于从当前位置，顺序向后忽略指定的字节数。   无论是Read方法还是Skip方法，对于多线程环境而言均不是线程安全的访问方法，需要开发者在调用过程中采用外部手段进行线程同步操作。\n PosixSequentialFile，是在符合POSIX标准的文件系统上对顺序读的实现。这里就不过多介绍，感兴趣的可以自己去了解。\nWritableFile WritableFile定义了文件顺序写抽象接口，其定义下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT WritableFile { public: WritableFile() = default; WritableFile(const WritableFile\u0026amp;) = delete; WritableFile\u0026amp; operator=(const WritableFile\u0026amp;) = delete; virtual ~WritableFile(); virtual Status Append(const Slice\u0026amp; data) = 0; virtual Status Close() = 0; virtual Status Flush() = 0; virtual Status Sync() = 0; };   WritableFile 主要有4个纯虚函数接口：Append、Close、Flush 与 Sync：\n Append：用于以追加的方式对文件顺序写入。 Close：用于关闭文件。 Flush：用于将 Append 操作写入到缓冲区的数据强制刷新到内核缓冲区。 Sync：用于将内存缓冲区的数据强制保存到磁盘。  PosixWritableFile 是对符合 POSIX 标准平台的 WritableFile 的派生实现。\nRandomAccessFile 随机读就是指可以定位到文件任意某个位置进行读取。RandomAccessFile 是文件随机读的抽象接口，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT RandomAccessFile { public: RandomAccessFile() = default; RandomAccessFile(const RandomAccessFile\u0026amp;) = delete; RandomAccessFile\u0026amp; operator=(const RandomAccessFile\u0026amp;) = delete; virtual ~RandomAccessFile(); virtual Status Read(uint64_t offset, size_t n, Slice* result, char* scratch) const = 0; };   与顺序读接口相比，随机读没有 Skip 操作，只有一个 Read 方法：\n Read：可从文件指定的任意位置读取一段指定长度数据。  在 LevelDB 中，RandomAccessFile 有两个派生类：PosixRandomAccessFile 与 PosixMmapReadableFile。这两个派生类是两种对随机文件操作的实现形式：一种是基于 pread() 方法的随机访问；另一种是基于 mmap() 方法的随机访问。\n数值编码 LevelDB 是一个嵌入式的存储库，其存储的内容可以是字符，也可以是数值。LevelDB 为了减少数值型内容对内存空间的占用，分别针对不同的需求定义了两种编码方式：一种是定长编码，另一种是变长编码。\n字节序 在讲编码之前，先聊聊字节序。字节序是处理器架构的特性，比如一个16 位的整数，他是由 2 个字节组成。内存中存储 2 个字节有两种方法：\n 将低序字节存储在起始地址，称为小端。 将高序字节存储在起始地址，称为大端。  LevelDB 为了便于操作，编码的数据统一采用小端模式，并存放到对应的字符串中，即数据低位存在内存低地址，数据高位存在内存高地址。\n具体的关于大小端的信息可以参考往期博客 大端小端存储解析以及判断方法。\n定长编码 定长的数值编码比较简单，主要是将原有的 uint64 或 uint32 的数据直接存储在对应的 8 字节或 4 字节中。而在直接存储的过程中，会基于大小端的不同，采取不同的执行逻辑（新版代码简化逻辑，统一按照大端逻辑处理）：\n 大端：在复制过程中调换字节顺序，以小端模式进行编码保存。 小端：直接利用 memcpy 进行内存间的复制。  具体实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  // https://github.com/google/leveldb/blob/master/util/coding.h  //32位数据定长编码 void EncodeFixed32(char* buf, uint32_t value) { if (port::kLittleEndian) { memcpy(buf, \u0026amp;value, sizeof(value)); } else { buf[0] = value \u0026amp; 0xff; buf[1] = (value \u0026gt;\u0026gt; 8) \u0026amp; 0xff; buf[2] = (value \u0026gt;\u0026gt; 16) \u0026amp; 0xff; buf[3] = (value \u0026gt;\u0026gt; 24) \u0026amp; 0xff; } } //64位数据定长编码 void EncodeFixed64(char* buf, uint64_t value) { if (port::kLittleEndian) { memcpy(buf, \u0026amp;value, sizeof(value)); } else { buf[0] = value \u0026amp; 0xff; buf[1] = (value \u0026gt;\u0026gt; 8) \u0026amp; 0xff; buf[2] = (value \u0026gt;\u0026gt; 16) \u0026amp; 0xff; buf[3] = (value \u0026gt;\u0026gt; 24) \u0026amp; 0xff; buf[4] = (value \u0026gt;\u0026gt; 32) \u0026amp; 0xff; buf[5] = (value \u0026gt;\u0026gt; 40) \u0026amp; 0xff; buf[6] = (value \u0026gt;\u0026gt; 48) \u0026amp; 0xff; buf[7] = (value \u0026gt;\u0026gt; 56) \u0026amp; 0xff; } } //32位数据定长解码 inline uint32_t DecodeFixed32(const char* ptr) { if (port::kLittleEndian) { // Load the raw bytes  uint32_t result; memcpy(\u0026amp;result, ptr, sizeof(result)); // gcc optimizes this to a plain load  return result; } else { return ((static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[0]))) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[1])) \u0026lt;\u0026lt; 8) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[2])) \u0026lt;\u0026lt; 16) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[3])) \u0026lt;\u0026lt; 24)); } } //64位数据定长解码 inline uint64_t DecodeFixed64(const char* ptr) { if (port::kLittleEndian) { // Load the raw bytes  uint64_t result; memcpy(\u0026amp;result, ptr, sizeof(result)); // gcc optimizes this to a plain load  return result; } else { uint64_t lo = DecodeFixed32(ptr); uint64_t hi = DecodeFixed32(ptr + 4); return (hi \u0026lt;\u0026lt; 32) | lo; } }   变长编码 对于 4 字节或 8 字节表示的无符号整型数据而言，数值较小的整数的高位空间基本为 0，如 uint32 类型的数据 128，高位的 3 个字节都是 0。为了更好的利用这些高位空间，如果能基于某种机制，将为 0 的高位数据忽略，有效地保留其有效位，从而减少所需字节数、节约存储空间。于是 LevelDB 就采用了 Google 的另一个开源项目 Protobuf 中提出的 varint 变长编码。\n**varint 将实际的一个字节分成了两个部分，最高位定义为 MSB（mostsignificant bit），后续低 7 位表示实际数据。**MSB 是一个标志位，用于表示某一数值的字节是否还有后续的字节，如果为 1 表示该数值后续还有字节，如果为 0 表示该数值所编码的字节至此完毕。每一个字节中的第 1 到第 7 位表示的是实际的数据，由于有 7 位，则只能表示大小为 0～127 的数值。\n下图给出了 127、300、2^28 - 1 的编码结果\n具体实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  // https://github.com/google/leveldb/blob/master/util/coding.cc  //32位数据变长编码 char* EncodeVarint32(char* dst, uint32_t v) { // Operate on characters as unsigneds  uint8_t* ptr = reinterpret_cast\u0026lt;uint8_t*\u0026gt;(dst); static const int B = 128; if (v \u0026lt; (1 \u0026lt;\u0026lt; 7)) { *(ptr++) = v; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 14)) { *(ptr++) = v | B; *(ptr++) = v \u0026gt;\u0026gt; 7; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 21)) { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = v \u0026gt;\u0026gt; 14; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 28)) { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = (v \u0026gt;\u0026gt; 14) | B; *(ptr++) = v \u0026gt;\u0026gt; 21; } else { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = (v \u0026gt;\u0026gt; 14) | B; *(ptr++) = (v \u0026gt;\u0026gt; 21) | B; *(ptr++) = v \u0026gt;\u0026gt; 28; } return reinterpret_cast\u0026lt;char*\u0026gt;(ptr); } //32位数据变长解码 const char* GetVarint32PtrFallback(const char* p, const char* limit, uint32_t* value) { uint32_t result = 0; for (uint32_t shift = 0; shift \u0026lt;= 28 \u0026amp;\u0026amp; p \u0026lt; limit; shift += 7) { uint32_t byte = *(reinterpret_cast\u0026lt;const uint8_t*\u0026gt;(p)); p++; if (byte \u0026amp; 128) { // More bytes are present  result |= ((byte \u0026amp; 127) \u0026lt;\u0026lt; shift); } else { result |= (byte \u0026lt;\u0026lt; shift); *value = result; return reinterpret_cast\u0026lt;const char*\u0026gt;(p); } } return nullptr; }   ","date":"2022-05-23T23:29:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%85%AC%E5%85%B1%E5%9F%BA%E7%A1%80%E7%B1%BB/","title":"LevelDB 公共基础类"},{"content":"LevelDB 架构 源码结构 LevelDB 的源码托管在 GitHub 上（https://github.com/google/leveldb），其中与程序实现源码相关的主要有以下几项：\n db：包含数据库的一些基本接口操作与内部实现。 table：为排序的字符串表 SSTable（Sorted String Table）的主体实现。 helpers：定义了 LevelDB 底层数据部分完全运行于内存环境的实现方法，主要用于相关的测试或某些全内存的运行场景。 util：包含一些通用的基础类函数，如内存管理、布隆过滤器、编码、CRC 等相关函数。 include：包含 LevelDB 库函数、可供外部访问的接口、基本数据结构等。 port：定义了一个通用的底层文件，以及多个进程操作接口，还有基于操作系统移植性实现的各平台的具体接口。  具体的环境搭建流程在前面的实战章节就讲过，这里就不过多介绍了。\n整体架构 LevelDB 总体模块架构主要包括接口 API（DB API 与 POSIX API）、Utility 公用基础类、LSM 树（Log、MemTable、SSTable）3个部分，如下图：\n 接口 API：接口 API 主要包括客户端调用的 DB API 以及针对操作系统底层的统一接口 POSIX API：  DB API：主要用于封装一些供客户端应用进行调用的接口，即头文件中的相关 API 函数接口，客户端应用可以通过这些接口实现数据引擎的各种操作。 POSIX API：实现了对操作系统底层相关操作的接口封装，主要用于保证 LevelDB 的可移植性，从而实现 LevelDB 在各 POSIX 操作系统的跨平台特性   Utility 公用基础类：主要用于实现主体功能所依赖的各种对象功能，例如内存管理 Arena、布隆过滤器、缓存、CRC 校验、哈希表、测试框架等。 LSM 树：LSM 树是 LevelDB 最重要的组件，也是实现其他功能的核心。一般而言，在常规的物理硬盘存储介质上，顺序写比随机写速度要快，而 LSM 树正是充分利用了这一物理特性，从而实现对频繁、大量数据写操作的支持。  LSM 架构如下：\n MemTable：内存数据结构，具体实现是 SkipList。接受用户的读写请求，新的数据修改会首先在这里写入。 Immutable MemTable：当 MemTable 的大小达到设定的阈值时，会变成 Immutable MemTable，只接受读操作，不再接受写操作，后续由后台线程 Flush 到磁盘上。 SSTable：Sorted String Table Files，磁盘数据存储文件。分为 Level0 到 LevelN 多层，每一层包含多个 SST 文件，文件内数据有序。Level0 直接由 Immutable Memtable Flush 得到，其它每一层的数据由上一层进行 Compaction 得到。 Manifest ：Manifest 文件中记录 SST 文件在不同 Level 的分布，单个 SST 文件的最大、最小 key，以及其他一些 LevelDB 需要的元信息。由于 LevelDB 支持 snapshot，需要维护多版本，因此可能同时存在多个 Manifest 文件。 Current ：由于 Manifest 文件可能存在多个，Current 记录的是当前的 Manifest 文件名。 Log ：用于防止 MemTable 丢数据的日志文件。  基本组件 Slice Slice 是 LevelDB 中的一种基本的、以字节为基础的数据存储单元，既可以存储 key，也可以存储 data。Slice 对数据字节的大小没有限制，其内部采用一个const char* 的常量指针存储数据，具有两个接口 data() 和size()，分别返回其表示的数据及数据的长度。\n此外，为了方便使用，其内部封装了 compare 函数，以及重载了用于比较的运算符 ==、!=，同时其可以与标准库中的 string 相互转换。\n 为什么 C++ 中已经有 string 了，还需要实现一个 Slice 呢？\n  string 默认语意为拷贝，传参和返回时会损失性能（在可预期的条件下，指针传递即可）。 Slice 不以 \u0026rsquo; \\0\u0026rsquo; 作为字符的终止符，可以存储值为 \u0026lsquo;\\0\u0026rsquo; 的数据。 string 不支持 remove_prefix 和 starts_with 等前缀操作函数，使用不方便。  具体实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  // https://github.com/google/leveldb/blob/master/include/leveldb/slice.h  class LEVELDB_EXPORT Slice { public: Slice() : data_(\u0026#34;\u0026#34;), size_(0) {} Slice(const char* d, size_t n) : data_(d), size_(n) {} Slice(const std::string\u0026amp; s) : data_(s.data()), size_(s.size()) {} Slice(const char* s) : data_(s), size_(strlen(s)) {} Slice(const Slice\u0026amp;) = default; Slice\u0026amp; operator=(const Slice\u0026amp;) = default; const char* data() const { return data_; } size_t size() const { return size_; } bool empty() const { return size_ == 0; } char operator[](size_t n) const { assert(n \u0026lt; size()); return data_[n]; } void clear() { data_ = \u0026#34;\u0026#34;; size_ = 0; } void remove_prefix(size_t n) { assert(n \u0026lt;= size()); data_ += n; size_ -= n; } std::string ToString() const { return std::string(data_, size_); } int compare(const Slice\u0026amp; b) const; bool starts_with(const Slice\u0026amp; x) const { return ((size_ \u0026gt;= x.size_) \u0026amp;\u0026amp; (memcmp(data_, x.data_, x.size_) == 0)); } private: const char* data_; size_t size_; }; inline bool operator==(const Slice\u0026amp; x, const Slice\u0026amp; y) { return ((x.size() == y.size()) \u0026amp;\u0026amp; (memcmp(x.data(), y.data(), x.size()) == 0)); } inline bool operator!=(const Slice\u0026amp; x, const Slice\u0026amp; y) { return !(x == y); } inline int Slice::compare(const Slice\u0026amp; b) const { const size_t min_len = (size_ \u0026lt; b.size_) ? size_ : b.size_; int r = memcmp(data_, b.data_, min_len); if (r == 0) { if (size_ \u0026lt; b.size_) r = -1; else if (size_ \u0026gt; b.size_) r = +1; } return r; }   Status 在 LevelDB 中，为了便于抛出异常，定义了一个 Status 类。Status 主要用于记录 LevelDB 中的状态信息，保存错误码和对应的字符串错误信息。（写死的，不支持拓展）\n对于错误代码，LevelDB 中定义了 6 种状态码在一个枚举类型 Code 中，如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/include/leveldb/status.h  class LEVELDB_EXPORT Status { private: enum Code { kOk = 0, kNotFound = 1, kCorruption = 2, kNotSupported = 3, kInvalidArgument = 4, kIOError = 5 }; }   其中 KoK 代表正常；kNotFound 代表未找到 ；kCorruption 代表数据异常崩溃；kNotSupported 代表不支持；kInvalidArgument 代表参数非法；kIOError 代表 I/O 错误；\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/include/leveldb/status.h  class LEVELDB_EXPORT Status { private: // OK status has a null state_. Otherwise, state_ is a new[] array  // of the following form:  // state_[0..3] == length of message  // state_[4] == code  // state_[5..] == message  const char* state_; }   在 Status 类中，所有状态信息，包括状态码与具体描述，都保存在一个私有的成员变量 const char*state_ 中。其具体表示方法如下。\n 当状态为 OK 时，state_ 为 NULL，说明当前操作一切正常。 否则，state_ 为一个 char 数组，即 state[0...3] 为 msg 的长度，state[4] 为状态码 code，state[5...] 为具体的 msg。  如下图：具体的处理逻辑如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // https://github.com/google/leveldb/blob/master/util/status.cc Status::Status(Code code, const Slice\u0026amp; msg, const Slice\u0026amp; msg2) { assert(code != kOk); const uint32_t len1 = static_cast\u0026lt;uint32_t\u0026gt;(msg.size()); const uint32_t len2 = static_cast\u0026lt;uint32_t\u0026gt;(msg2.size()); const uint32_t size = len1 + (len2 ? (2 + len2) : 0); char* result = new char[size + 5]; std::memcpy(result, \u0026amp;size, sizeof(size)); result[4] = static_cast\u0026lt;char\u0026gt;(code); std::memcpy(result + 5, msg.data(), len1); if (len2) { result[5 + len1] = \u0026#39;:\u0026#39;; result[6 + len1] = \u0026#39; \u0026#39;; std::memcpy(result + 7 + len1, msg2.data(), len2); } state_ = result; }   Comparator LevelDB 是按 key 排序后进行存储，因无论是插入还是删除，都必然少不了对 key 的比较。于是乎 LevelDB 抽象出了一个纯虚类 Comparator。\n具体定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/include/leveldb/comparator.h  class LEVELDB_EXPORT Comparator { public: virtual ~Comparator(); virtual int Compare(const Slice\u0026amp; a, const Slice\u0026amp; b) const = 0; virtual const char* Name() const = 0; virtual void FindShortestSeparator(std::string* start, const Slice\u0026amp; limit) const = 0; virtual void FindShortSuccessor(std::string* key) const = 0; };   在 LevelDB 中，有两个实现 Comparator 的类：一个是 BytewiseComparatorImpl，另一个是InternalKeyComparator。\n BytewiseComparatorImpl：默认比较器，主要采用字典序对两个字符串进行比较。 InternalKeyComparator：内部调用的也是 BytewiseComparatorImpl。  Iterate LevelDB 具有迭代器遍历功能，设计人员只需要调用相应的接口，就可以实现对容器数据类型的遍历访问。\n迭代器定义了一个纯虚类的接口，LevelDB 中的任何集合类型对象的迭代器均基于这一纯虚类进行实现。具体代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  // https://github.com/google/leveldb/blob/master/include/leveldb/iterator.h  class LEVELDB_EXPORT Iterator { public: Iterator(); Iterator(const Iterator\u0026amp;) = delete; Iterator\u0026amp; operator=(const Iterator\u0026amp;) = delete; virtual ~Iterator(); virtual bool Valid() const = 0; virtual void SeekToFirst() = 0; virtual void SeekToLast() = 0; virtual void Seek(const Slice\u0026amp; target) = 0; virtual void Next() = 0; virtual void Prev() = 0; virtual Slice key() const = 0; virtual Slice value() const = 0; virtual Status status() const = 0; using CleanupFunction = void (*)(void* arg1, void* arg2); void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2); private: struct CleanupNode { bool IsEmpty() const { return function == nullptr; } void Run() { assert(function != nullptr); (*function)(arg1, arg2); } CleanupFunction function; void* arg1; void* arg2; CleanupNode* next; }; CleanupNode cleanup_head_; };   从上面的接口可以看出，LevelDB 实现的是双向迭代器，并且支持 Seek 定位功能。同时还嵌套了一个子结构 CleanupNode 用于释放链表，其中的 CleanupFunction 即用户自定义的清除函数。当用户需要释放资源时，就会通过遍历 CleanupNode 中的 next 链表，并对每一个节点调用一次 CleanupFunction 将资源释放。\nOption 头文件 options.h 中定义了一系列与数据库操作相关的选项参数类型，例如与数据库操作相关的 Options，与读操作相关的 ReadOptions，与写操作相关的 WriteOptions。这几个类型均为结构体，在进行数据库的初始化、数据库的读写等操作时，这些参数直接决定了数据库相关的性能指标。\n Options（DB 参数）  Comparator：被用来表中 key 比较，默认是字典序。 create_if_missing：打开数据库，如果数据库不存在，是否创建新的。默认为 false。 error_if_exists：打开数据库，如果数据库存在，是否抛出错误。默认为 false。 paranoid_checks：默认为 false。如果为 true，则实现将对其正在处理的数据进行积极检查，如果检测到任何错误，则会提前停止。 这可能会产生不可预见的后果：例如，一个数据库条目的损坏可能导致大量条目变得不可读或整个数据库变得无法打开。 env：环境变量，封装了平台相关接口。 info_log：db 日志句柄。 write_buffer_size：memtable 的大小(默认4mb)  值大有利于性能提升 但是内存可能会存在两份，太大需要注意oom 过大刷盘之后，不利于数据恢复   max_open_files：允许打开的最大文件数。 block_cache：block 的缓存。 block_size：每个 block 的数据包大小(未压缩)，默认是4k。 block_restart_interval：block 中记录完整 key 的间隔。 max_file_size：生成新文件的阈值(对于性能较好的文件系统可以调大该阈值，但会增加数据恢复的时间)，默认 2k compression：数据压缩类型，默认是 kSnappyCompression，压缩速度快  kSnappyCompression 在 Intel(R) Core(TM)2 2.4GHz 上的典型速度：  ~200-500MB/s 压缩 ~400-800MB/s 解压     reuse_logs：是否复用之前的 MANIFES 和 log files。 filter_policy：block 块中的过滤策略，支持布隆过滤器。   ReadOptions（读操作参数）  verify_checksums：是否对从磁盘读取的数据进行校验。 fill_cache：读取到 block 数据，是否加入到 cache 中。 snapshot：记录的是当前的快照。   WriteOptions（写操作参数）  sync：是否同步刷盘，也就是调用完 write 之后是否需要显式 fsync。    ","date":"2022-05-23T23:22:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E6%9E%B6%E6%9E%84/","title":"LevelDB 架构"},{"content":"基本操作 环境搭建 1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 下载源码 git clone https://github.com.cnpmjs.org/google/leveldb.git # 下载依赖第三方库（benchmark、googletest） git submodule update --init # 执行编译 cd leveldb/ mkdir -p build \u0026amp;\u0026amp; cd build cmake -DCMAKE_BUILD_TYPE=Debug .. \u0026amp;\u0026amp; cmake --build . # 头文件加入系统目录 cp -r ./include/leveldb /usr/include/ cp build/libleveldb.a /usr/local/lib/   实战使用 创建、关闭数据库 创建数据库或打开数据库，均通过 Open 函数实现。Open 为一个静态成员函数，其函数声明如下所示：\n1 2  static Status Open(const Options\u0026amp; options, const std::string\u0026amp; name, DB** dbptr);    const Options \u0026amp; options：用于指定数据库创建或打开后的基本行为。 const std::string \u0026amp; name：用于指定数据库的名称。 DB** dbptr：定义了一个DB类型的指针的指针，该指针作为Open函数操作后传给调用者使用的DB类型的实际指针。 Status：当操作成功时，函数返回 status.ok() 的值为 True，*dbptr 分配了不为 NULL 的实际指针地址；若其中的操作存在错误，则 status.ok() 的值为 False，并且对应的 *dbptr 为 NULL。  关闭数据库非常简单，只需要使用 delete 释放 db 即可。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026lt;string\u0026gt;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //关闭数据库  delete db; return 0; }   数据读、写、删除 数据库中的读、写与删除操作分别用 Get 、 Put、Delete 这3个接口函数实现。接口定义如下：\n1 2 3 4 5  Status Get(const ReadOptions\u0026amp; options, const Slice\u0026amp; key, std::string* value); Status Put(const WriteOptions\u0026amp;, const Slice\u0026amp; key, const Slice\u0026amp; value); Status Delete(const WriteOptions\u0026amp;, const Slice\u0026amp; key);    ReadOptions\u0026amp; options：代表实际读操作传入的行为参数。 WriteOptions\u0026amp; options：代表实际写操作传入的行为参数。  这里需要注意的是 Delete 并不会直接删除数据，而是在对应位置插入一个 key 的删除标志，然后在后续的Compaction 过程中才最终去除这条 key-value 记录。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //写入数据  leveldb::Slice key(\u0026#34;hello\u0026#34;); string value(\u0026#34;world\u0026#34;); status = db-\u0026gt;Put(leveldb::WriteOptions(), key, value); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; \u0026#34; 写入成功。\u0026#34; \u0026lt;\u0026lt; endl; } //查找数据  status = db-\u0026gt;Get(leveldb::ReadOptions(), key, \u0026amp;value); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; \u0026#34; 查找成功。\u0026#34; \u0026lt;\u0026lt; endl; } //删除数据  status = db-\u0026gt;Delete(leveldb::WriteOptions(), key); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; 删除成功。\u0026#34; \u0026lt;\u0026lt; endl; } //关闭数据库  delete db; return 0; }   批量处理 针对大量的操作，LevelDB 不具有传统数据库所具备的事务操作机制，然而它提供了一种批量操作的方法。这种批量操作方法主要具有两个作用：一是提供了一种原子性的批量操作方法；二是提高了整体的数据操作速度。\nLevelDB 针对批量操作定义了 WriteBatch 的类型。WriteBatch 有 3 个非常重要的接口：数据写（Put）、数据删 除（Delete）以及清空批量写入缓存（Clear），具体定义如下所示：\n1 2 3 4 5 6 7 8 9 10  class LEVELDB_EXPORT WriteBatch { public:\t//...  void Put(const Slice\u0026amp; key, const Slice\u0026amp; value); void Delete(const Slice\u0026amp; key); void Clear(); //... };   当我们想将 WriteBatch 中的数据写入 DB 时，只需要调用 Write 接口，其主要用于处理之前保存在 WriteBatch 对象中的所有批量操作，其详细接口定义如下所示：\n1  Status Write(const WriteOptions\u0026amp; options, WriteBatch* updates);    注意：一旦我们写入完成后，就会调用 updates 中的 Clear 来清空之前保存的批量操作。  代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026#34;leveldb/write_batch.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //批量写入  leveldb::Slice key; string value; leveldb::WriteBatch batch; for(int i = 0; i \u0026lt; 10; i++) { value = (\u0026#39;0\u0026#39; + i); key = \u0026#34;k\u0026#34; + value; batch.Put(key, value); } status = db-\u0026gt;Write(leveldb::WriteOptions(), \u0026amp;batch); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;批量写入成功\u0026#34; \u0026lt;\u0026lt; endl; } //批量删除  for(int i = 0; i \u0026lt; 10; i++) { key = \u0026#34;k\u0026#34; + (\u0026#39;0\u0026#39; + i); batch.Delete(key); } status = db-\u0026gt;Write(leveldb::WriteOptions(), \u0026amp;batch); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;批量删除成功\u0026#34; \u0026lt;\u0026lt; endl; } //关闭数据库  delete db; return 0; }   迭代器遍历 针对 DB 中所有的数据记录，LevelDB 不仅支持前向的遍历，也支持反向的遍历。在 DB 对象类型中，通过调用NewIterator 创建一个新的迭代器对象，该接口具体定义如下：\n1  Iterator* NewIterator(const ReadOptions\u0026amp;);    ReadOptions\u0026amp; option：用于指定在遍历访问过程中的相关设置。  这里有一个需要注意的地方，这里返回的迭代器不能直接使用，而是需要先使用对应的 Seek 操作偏移到指定位置后才能进行对应的迭代操作。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026#34;leveldb/iterator.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } leveldb::Iterator* it = db-\u0026gt;NewIterator(leveldb::ReadOptions()); //正向遍历  cout \u0026lt;\u0026lt; \u0026#34;开始正向遍历:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;SeekToFirst(); it-\u0026gt;Valid(); it-\u0026gt;Next()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } //逆向遍历  cout \u0026lt;\u0026lt; \u0026#34;开始逆向遍历:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;SeekToLast(); it-\u0026gt;Valid(); it-\u0026gt;Prev()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } //范围查询  cout \u0026lt;\u0026lt; \u0026#34;开始范围查询:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;Seek(\u0026#34;k4\u0026#34;); it-\u0026gt;Valid() \u0026amp;\u0026amp; it-\u0026gt;key().ToString() \u0026lt; \u0026#34;k8\u0026#34;; it-\u0026gt;Next()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } delete it; //关闭数据库  delete db; return 0; }   常用优化方案 压缩 当每一个块写入存储设备中时，可以选择是否对块进行压缩后再存储，以及 Options 参数的 compression 成员变量决定是否开启压缩。默认情况下，压缩是开启的，且压缩的速度很快，基本对整体的性能没有太大影响\n用户在调用时，可以用 kNoCompression 或 kSnappyCompression 对 compression 参数进行设定，从而确定块在实际存储过程中是否进行压缩。\n1 2 3  leveldb::Options op; op.compression = leveldb::kNoCompression;\t//不启用压缩 op.compression = leveldb::kSnappyCompression;\t//Snappy压缩   缓存 Cache的作用是充分利用内存空间，减少磁盘的 I/O 操作，从而提升整体运行性能。LevelDB 默认的 Cache 采用的是 LRU 算法，即近期最少使用的数据优先从 Cache 中淘汰，而经常使用的数据驻留在内存，从而实现对需要频繁读取的数据的快速访问。\nLevelDB 中定义了一个全局函数 NewLRUCache 用于创建一个 LRUCache。\n1 2 3  leveldb::Options op; op.block_cache = leveldb::NewLRUCache(10 * 1024 * 1024); //参数主要用于指定LevelDB的块的Cache空间，如果为NULL则默认为8MB   过滤器 由于 LevelDB 中所有的数据均保存在磁盘中，因而一次 Get 的调用，有可能导致多次的磁盘 I/O 操作。为了尽可能减少读过程时磁盘 I/O 的操作次数，LevelDB 采用了 FilterPolicy 机制。LevelDB 中 Options 对象类型的filter_policy 参数，主要用于确定运行过程中 Get 所遵循的 FilterPolicy 机制。\n用户可以通过调用 NewBloomFilterPolicy 接口函数以创建布隆过滤器，并将其赋值给对应的 filter_policy 参数。\n1 2  leveldb::Options op; op.filter_policy = leveldb::NewBloomFilterPolicy(10);   命名 LevelDB 中磁盘数据读取与缓存均以块为单位，并且实际存储中所有的数据记录均以 key 进行顺序存储。根据排序结果，相邻的 key 所对应的数据记录一般均会存储在同一个块中。正是由于这一特性，用户针对自身的应用场景需要充分考虑如何优化 key 的命名设计，从而最大限度地提升整体的读取性能。\n为了提升性能，命名规则是：**针对需要经常同时访问的数据，其 key 在命名时，可以通过将这些 key 设置相同的前缀保证这些数据的 key 是相邻近的，从而使这些数据可存储在同一个块内。**基于此，那些不常用的数据记录自然会放置在其他块内。\n","date":"2022-05-23T23:12:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/","title":"LevelDB 基础操作"},{"content":"LevelDB 基本概念 LevelDB 是一个由 Google 公司所研发的 K-V 存储嵌入式数据库管理系统编程库，以开源的 BSD 许可证发布。其作为 LSM Tree 的经典实现，具有很高的随机写，顺序读/写性能，但是随机读的性能很一般，也就是说，LevelDB很适合应用在查询较少，而写很多的场景。\n为什么需求K-V数据库？ K-V 数据库主要用于存取、管理关联性的数组。关联性数组又称映射、字典，是一种抽象的数据结构，其数据对象为一个个的 key-value 数据对，且在整个数据库中每个 key 均是唯一的。\n随着近年来互联网的兴起，云计算、大数据应用越来越广泛，对于数据库来说也出现了一些需要面对的新情况：\n 数据量呈指数级增长，存储也开始实现分布式。 查询响应时间要求越来越快，需在1秒内完成查询操作。 应用一般需要 7 × 24 小时连续运行，因此对稳定性要求越来越高，通常要求数据库支持热备份，并实现故障下快速无缝切换。 在某些应用中，写数据比读数据更加频繁，对数据写的速度要求也越来越高。 在实际应用中，并不是所有环境下的数据都是完整的结构化数据，非结构化数据普遍存在，因此如何实现对灵活多变的非结构化数据的支持是需要考虑的一个问题。  正是在上述情况的催生下，2010年开始兴起了一场 NoSQL 运动，而 K-V 数据库作为 NoSQL 中一种重要的数据库也日益繁荣，因此催生出了许多成功的商业化产品，并得到了广泛应用。\nBigTable与LevelDB 早在 2004 年，Google 开始研发一种结构化的分布式存储系统，它被设计用来处理海量数据，通常是分布在数千台普通服务器上的 PB 级的数据——这一系统就是风靡全球的 Bigtable。\n2006 年，Google 发表了一篇论文——《Bigtable: A Distributed Storage System for StructuredData》。这篇论文公布了 Bigtable 的具体实现方法，揭开了 Bigtable 的技术面纱。Bigtable 虽然也有行、列、表的概念，但不同于传统的关系数据库，从本质上讲，它是一个稀疏的、分布式的、持久化的、多维的排序键-值映射。\n虽然 Google 公布了 Bigtable 的实现论文，但 Bigtable 依赖于 Google 其他项目所开发的未开源的库，Google 一直没有将 Bigtable 的代码开源。然而这一切在 2011 年迎来了转机。Sanjay Ghemawat 和 Jeff Dean 这两位来自 Google 的重量级工程师，为了能将 Bigtable 的实现原理与技术细节分享给大众开发者，于2011年基于 Bigtable 的基本原理，采用 C++ 开发了一个高性能的 K-V 数据库——LevelDB。由于没有历史的产品包袱，LevelDB 结构简单，不依赖于任何第三方库，具有很好的独立性，虽然其有针对性地对 Bigtable 进行了一定程度的简化，然而Bigtable的主要技术思想与数据结构均在 LevelDB 予以体现了。因此 LevelDB 可看作 Bigtable 的简化版或单机版。\n特点 优点： key 与 value 采用字符串形式，且长度没有限制。 数据能持久化存储，同时也能将数据缓存到内存，实现快速读取。 基于 key 按序存放数据，并且 key 的排序比较函数可以根据用户需求进行定制。 支持简易的操作接口 API，如 Put、Get、Delete，并支持批量写入。 可以针对数据创建数据内存快照。 支持前向、后向的迭代器。 采用 Google 的 Snappy 压缩算法对数据进行压缩，以减少存储空间。 基本不依赖其他第三方模块，可非常容易地移植到 Windows、Linux、UNIX、Android、iOS。  缺点： 不是传统的关系数据库，不支持SQL查询与索引。 只支持单进程，不支持多进程。 不支持多种数据类型。 不支持 C/S 的访问模式。用户在应用时，需要自己进行网络服务的封装。  应用场景 LevelDB主要应用于查少写多的场景，如：\n  常见的 Web 场景，可以存储用户的个人信息资料、对文章或博客的评论、邮件等。\n  具体到电子商务领域，可以存储购物车中的商品、产品类别、产品评论。\n  存储整个网页，其将网页的 URL 作为 key，网页的内容作为 value。\n  构建更为复杂的存储系统，如日志系统、缓存、消息队列等。\n  ……\n  RocksDB RocksDB 是基于 LevelDB 开发的，并保留、继承了 LevelDB 原有的基本功能，也是一个嵌入式的 K-V 数据存储库。RocksDB 设计之初，正值 SSD 硬盘兴起。然而在当时，无论是传统的关系数据库如 MySQL，还是分布式数据库如 HDFS、HBase，均没有充分发挥 SSD 硬盘的数据读写性能。因而 Facebook 当时的目标就是开发一款针对 SSD 硬盘的数据存储产品，从而有了后面的 RocksDB。RocksDB 采用嵌入式的库模式，充分发挥了 SSD 的性能。\n 为什么要基于LevelDB实现RocksDB？\n一般而言，数据库产品有两种访问模式可供选择。一种是直接访问本地挂载的硬盘，即嵌入式的库模式；另一种是客户端通过网络访问数据服务器，并获取数据。假设 SSD 硬盘的读写约 100 μs，机械硬盘的读写约 10 ms，两台 PC 间的网络传输延迟为 50 μs。可以分析得知，如果在机械硬盘时代，采用 C/S 的数据服务模式，客户端进行一次数据查询约为 10.05 ms，可见网络延迟对于数据查询速度的影响微乎其微；而在 SSD 硬盘时代，客户端进行一次数据查询约为 150 μs，但与直接访问 SSD 硬盘相比，整体速度慢了 50%，因而直接影响了整体性能。正是在这样的背景下，Facebook的工程师们选择了 LevelDB 来实现 RocksDB 的原型。\n RocksDB 使用了一个插件式的架构，这使得它能够通过简单的扩展适用于各种不同的场景。插件式架构主要体现在以下几个方面：\n 不同的压缩模块插件：例如 Snappy、Zlib、BZip 等（LevelDB 只使用 Snappy）。 Compaction 过滤器：一个应用能够定义自己的 Compaction 过滤器，从而可以在 Compaction 操作时处理键。例如，可以定义一个过滤器处理键过期，从而使 RocksDB 有了类似过期时间的概念。 MemTable 插件：LevelDB 中的 MemTable 是一个 SkipList，适用于写入和范围扫描，但并不是所有场景都同时需要良好的写入和范围扫描功能，此时用 SkipList 就有点大材小用了。因此 RocksDB 的 MemTable 定义为一个插件式结构，可以是 SkipList，也可以是一个数组，还可以是一个前缀哈希表（以字符串前缀进行哈希，哈希之后寻找桶，桶内的数据可以是一个 B 树）。因为数组是无序的，所以大批量写入比使用 SkipList 具有更好的性能，但不适用于范围查找，并且当内存中的数组需要生成为 SSTable 时，需要进行再排序后写入Level 0。前缀哈希表适用于读取、写入和在同一前缀下的范围查找。因此可以根据场景使用不同的MemTable。 SSTable 插件：SSTable 也可以自定义格式，使之更适用于 SSD 的读取和写入。除了插件式架构，RocksDB 还进行了一些写入以及 Compaction 操作方面的优化，主要有以下几个方面：  线程池：可以定义一个线程池进行 Level 0～Level 5 的 Compaction 操作，另一个线程池进行将MemTable 生成为 SSTable 的操作。如果 Level 0～Level 5 的 Compaction 操作没有重叠的文件，可以并行操作，以加快 Compaction 操作的执行。 多个 Immutable MemTable：当 MemTable 写满之后，会将其赋值给一个 ImmutableMemTable，然后由后台线程生成一个 SSTable。但如果此时有大量的写入，MemTable 会迅速再次写满，此时如果Immutable MemTable 还未执行完 Compaction 操作就会阻塞写入。因此 RocksDB 使用一个队列将Immutable MemTable 放入，依次由后台线程处理，实现同时存在多个 ImmutableMemTable。以此优化写入，避免写放大，当使用慢速存储时也能够加大写吞吐量。    ","date":"2022-05-23T23:11:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"LevelDB 基本概念"},{"content":"分布式原理 分布式存储 路由 当索引一个文档的时候，Elasticsearch会通过哈希来决定将文档存储到哪一个主分片中，路由计算公式如下：\n1 2 3 4  shard = hash(routing) % number_of_primary_shards //routing：默认为文档id,也可以自定义。 //number_of_primary_shards：主分片的数量     查询时指定routing：可以直接根据routing信息定位到某个分片查询，不需要查询所有的分配，经\n过协调节点排序。\n  查询时不指定routing：因为不知道要查询的数据具体在哪个分片上，所以整个过程分为 2 个步骤\n 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上。 聚合：协调节点搜集到每个分片上查询结果，在将查询的结果进行排序，之后给用户返回结果。    从上面的这个公式我们也可以看到一个问题，路由的逻辑与当前主分片的数量强关联，也就是说如果分片数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。这也就是为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量。\n 分片数量固定是否意味着会使索引难以进行扩容？\n 答案是否定的，Elasticsearch还提供了其他的一些方案来让我们轻松的实现扩容，如：\n 分片预分配：一个分片存在于单个节点，但一个节点可以持有多个分片。因此我们可以根据未来的数据的扩张状况来预先分配一定数量的分片到各个节点中。（注意⚠️：预先分配过多的分片会导致性能的下降以及影响搜索结果的相关度） 新建索引：分片数不够时，可以考虑新建索引，搜索1个有着50个分片的索引与搜索50个每个都有1个分片的索引完全等价。  更多关于水平拓展的内容可以参考官方文档扩容设计。\n新增、索引和删除文档 我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。 在下面的例子中，将所有的请求发送到 Node 1 ，我们将其称为协调节点(coordinating node) 。\n 当发送请求的时候， 为了扩展负载，更好的做法是轮询集群中所有的节点。\n 新建、索引和删除请求都是写操作， 必须在主分片上面完成之后才能被复制到相关的副本分片。\n流程如下：\n 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。  取回文档 由于取回文档为读操作，我们可以从主分片或者从其它任意副本分片检索文档。\n流程如下：\n 客户端向 Node 1 发送获取请求。 节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。 Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。  在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。\n并发控制 在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失：\n 悲观并发控制：这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。 乐观并发控制：Elasticsearch中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。 然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。 例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。  Elasticsearch是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许会乱序。所以Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。\n在Elasticsearch中，其通过版本号机制来实现乐观并发控制。即每一个文档中都会有一个_version版本号字段，当文档被修改时版本号递增。 Elasticsearch使用_version来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。\n我们可以利用_version号来确保应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的 version 号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  // 例如我们想更新文档的内容，并指定版本号为1 PUT /website/blog/1?version=1 { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Starting to get the hang of this...\u0026#34; } // 当文档的版本号为1时，次请求成功，同时响应体告诉我们版本号递增到2 { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2 \u0026#34;created\u0026#34;: false } // 此时我们再次尝试更新文档的内容，仍然指定版本号为1，由于版本号不符合，此时返回409 Conflict HTTP 响应码 { \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[blog][1]: version conflict, current [2], provided [1]\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;shard\u0026#34;: \u0026#34;3\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[blog][1]: version conflict, current [2], provided [1]\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;shard\u0026#34;: \u0026#34;3\u0026#34; }, \u0026#34;status\u0026#34;: 409 }   分布式搜索 搜索需要一种更加复杂的执行模型，因为我们不知道查询会命中哪些文档，这些文档有可能在集群的任何分片上。 一个搜索请求必须询问我们关注的索引的所有分片的某个副本来确定它们是否含有任何匹配的文档。\n但是找到所有的匹配文档仅仅完成事情的一半。 在 search 接口返回一个 page 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为query then fetch（查询后取回）。\n查询阶段 在查询阶段时， 查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的优先队列。\n查询阶段包含以下三个步骤\n 客户端发送一个 search 请求到 Node 3 ，此时Node 3成为协调节点，由它来负责本次的查询。 Node 3 将查询请求广播到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的ID和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。至此查询过程结束。   一个索引可以由一个或几个主分片组成， 所以一个针对单个索引的搜索请求需要能够把来自多个分片的结果组合起来。 针对 multiple 或者 all 索引的搜索工作方式也是完全一致的——仅仅是包含了更多的分片而已。\n 取回阶段 在查询阶段中，我们标识了哪些文档满足搜索请求，而接下来我们就需要取回这些文档。\n取回阶段由以下步骤构成\n 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。例如，如果我们的查询指定了 { \u0026quot;from\u0026quot;: 90, \u0026quot;size\u0026quot;: 10 } ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。 每个分片加载并丰富文档（如_source字段和高亮参数），接着返回文档给协调节点。 协调节点等待所有文档被取回，将结果返回给客户端。  集群内部原理 集群与节点 一个运行中的Elasticsearch实例称为一个节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n由于Elasticsearch采用了主从模式，所以当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 因为主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。\n作为用户，我们可以将请求发送到集群中的任何节点（这个处理请求的节点也叫做协调节点）。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。\n分片 在分布式系统中，单机无法存储规模巨大的数据，要依靠大规模集群处理和存储这些数据，一般通过增加机器数量来提高系统水平扩展能力。因此，需要将数据分成若干小块分配到各个机器上。然后通过某种路由策略找到某个数据块所在的位置。\n分片（shard）是底层的基本读写单元，分片的目的是分割巨大索引，让读写可以并行操作，由多台机器共同完成。读写请求最终落到某个分片上，分片可以独立执行读写工作。Elasticsearch利用分片将数据分发到集群内各处。分片是数据的容器，文档保存在分片内，不会跨分片存储。分片又被分配到集群内的各个节点里。当集群规模扩大或缩小时，Elasticsearch会自动在各节点中迁移分片，使数据仍然均匀分布在集群里。\n为了应对并发更新问题，Elasticsearch将分片分为两部分，即主分片（primary shard）和副本分片（replica shard）。主数据作为权威数据，写过程中先写主分片，成功后再写副分片，恢复阶段以主分片为准。\n一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。\n 那索引与分片之间又有什么关系呢？\n 一个Elasticsearch索引包含了很多个分片，每个分片又是一个Lucene的索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。Lucene索引又由很多分段组成，每个分段都是一个倒排索引。Elasticsearch每次refresh都会生成一个新的分段，其中包含若干文档的数据。在每个分段内部，文档的不同字段被单独建立索引。每个字段的值由若干词（Term）组成，Term是原文本内容经过分词器处理和语言处理后的最终结果。\n选举 在主节点选举算法的选择上，基本原则是不重复造轮子。最好实现一个众所周知的算法，这样的好处是其中的优点和缺陷是已知的。Elasticsearch的选举算法的选择上主要考虑下面两种。\n Bully算法：Leader选举的基本算法之一。它假定所有节点都有一个唯一的ID，使用该ID对节点进行排序。任何时候的当前Leader都是参与集群的最高ID节点。该算法的优点是易于实现。但是，当拥有最大ID的节点处于不稳定状态的场景下会有问题。例如，Master负载过重而假死，集群拥有第二大ID的节点被选为新主，这时原来的Master恢复，再次被选为新主，然后又假死…… Paxos算法：Paxos非常强大，尤其在什么时机，以及如何进行选举方面的灵活性比简单的Bully算法有很大的优势，因为在现实生活中，存在比网络连接异常更多的故障模式。但Paxos实现起来非常复杂。  Elasticsearch的选主算法是基于Bully算法的改进，主要思路是对节点ID排序，取ID值最大的节点作为Master，每个节点都运行这个流程。同时，为了解决Bully算法的缺陷，其通过推迟选举，直到当前的Master失效来解决上述问题，只要当前主节点不挂掉，就不重新选主。但是容易产生脑裂（双主），为此，再通过法定得票人数过半解决脑裂问题。\nElasticsearch对Bully附加的三个约定条件\n 参选人数需要过半。当达到多数时就选出临时主节点，为什么是临时的？每个节点运行排序取最大值的算法，结果不一定相同。举个例子，集群有5台主机，节点ID分别是1、2、3、4、5。当产生网络分区或节点启动速度差异较大时，节点1看到的节点列表是1、2、3、4，选出4；节点2看到的节点列表是2、3、4、5，选出5。结果就不一致了，由此产生下面的第二条限制。 得票数需要过半。某节点被选为主节点，必须判断加入它的节点数达到半数以上，才确认Master身份（推迟选举）。 当探测到节点离开事件时，必须判断当前节点数是否过半。如果达不到半数以上，则放弃Master身份，重新加入集群。如果不这么做，则设想以下情况：假设5台机器组成的集群产生网络分区，2台一组，3台一组，产生分区前，Master位于2台中的一个，此时3台一组的节点会重新并成功选取Master，产生双主，俗称脑裂。（节点失效检测）  流程如下图\n节点失效检测会监控节点是否离线，然后处理其中的异常。失效检测是选主流程之后不可或缺的步骤，不执行失效检测可能会产生脑裂（双主或多主）。在此我们需要启动两种失效探测器：\n 在Master节点，启动NodesFaultDetection，简称NodesFD。定期探测加入集群的节点是否活跃。 非Master节点启动MasterFaultDetection，简称MasterFD。定期探测Master节点是否活跃。  分片内部原理 索引不变性 早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。\n倒排索引被写入磁盘后是不可改变的，索引的不变性具有以下好处：\n 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。 一旦索引被读入内核的文件系统缓存，便会留在哪里。由于其不变性，只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。 缓存(像过滤器缓存)在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。  当然，一个不变的索引也有不好的地方。最大的缺点就是它是不可变的，我们无法对其进行修改。如果我们需要让一个新的文档可被搜索，就需要重建整个索引。这不仅对一个索引所能包含的数据量造成了巨大的限制，而且对索引可被更新的频率同样造成了影响。\n动态更新索引  那么我们如何能在保留不变性的前提下实现倒排索引的动态更新呢？\n 答案就是使用更多的索引，即新增内容并写到一个新的倒排索引中，查询时，每个倒排索引都被轮流查询，查询完再对结果进行合并。\nElasticsearch基于Lucene引入了按段写入的概念——每次内存缓冲的数据被写入文件时，会产生一个新的Lucene段，每个段都是一个倒排索引。同时，在提交点中描述了当前Lucene索引都含有哪些分段。\n按段写入的流程如下：\n 新文档被收集到内存的索引中缓存 当缓存堆积到一定规模时，就会进行提交  一个新的段（倒排索引）被写入磁盘。 一个新的提交点被写入磁盘。 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。   新的段被开启，让它包含的文档可见以被搜索 内存缓存被清空，等待接收新的文档  当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。\n 那插入和更新又如何实现呢？\n 段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。\n  当一个文档被删除时，它实际上只是在 .del 文件中被标记删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。\n  当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。\n  近实时搜索 Elasticsearch和磁盘之间是文件系统缓存，在执行写操作时，为了降低从索引到可被搜索的延迟，一般新段会被先写入到文件系统缓存，再将这些数据写入硬盘（磁盘I/O是性能瓶颈）。\n在写操作中，一般会先在内存中缓冲一段数据，再将这些数据写入硬盘，每次写入硬盘的这批数据称为一个分段。如同任何写操作一样，通过操作系统的write接口写到磁盘的数据会先到达系统缓存（内存）。write函数返回成功时，数据未必被刷到磁盘。通过手工调用flush，或者操作系统通过一定策略将文件系统缓存刷到磁盘。\n这种策略大幅提升了写入效率。从write函数返回成功开始，无论数据有没有被刷到磁盘，只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\nLucene允许新段被写入和打开——使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\nElasticsearch中将写入和打开一个新段的过程叫做refresh(刷新) 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说Elasticsearch是近实时搜索——文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n事务日志 由于系统先缓冲一段数据才写，且新段不会立即刷入磁盘，这两个过程中如果出现某些意外情况（如主机断电），则会存在丢失数据的风险。\n为了解决这个问题，Elasticsearch增加了一个translog（事务日志），在每一次对Elasticsearch进行操作时均进行了日志记录，当Elasticsearch启动的时候，重放translog中所有在最后一次提交后发生的变更操作。\n其执行流程如下：\n 一个文档被索引之后，就会被添加到内存缓冲区，并且追加到了translog  新的文档被添加到内存缓冲区并且被追加到了事务日志，如下图    分片会每秒自动执行一次刷新，这些内存缓冲区的文档被写入新的段中并打开以便搜索，同时清空内存缓冲区。  刷新完成后, 缓存被清空但是事务日志不会，同时新段写入文件系统缓冲区    这个进程继续工作，更多的文档被添加到内存缓冲区和追加到translog  事务日志不断积累文档    当translog足够大时，就会执行全量提交，对文件系统缓存执行flush，将其内容全部写入硬盘中，并清空事务日志。  在刷新（flush）之后，段被全量提交，并且事务日志被清空     除此之外，translog还有下面这些功能\n translog提供所有还没有被刷到磁盘的操作的一个持久化纪录。当Elasticsearch启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放translog中所有在最后一次提交后发生的变更操作。 translog也被用来提供实时CRUD 。当你试着通过ID查询、更新、删除一个文档，在从相应的段中检索之前， 首先检查translog任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。  段合并 由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段，所以段越多，搜索也就越慢。\nElasticsearch通过在后台进行段合并来解决这个问题，其会选择大小相似的分段进行合并。在合并过程中，标记为删除（更新）的数据不会写入新分段，当合并过程结束，旧的分段数据被删除，标记删除的数据才从磁盘删除。\n流程如下图\n合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。\n整体写入流程如下图\n ","date":"2022-05-23T22:20:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86/","title":"ElasticSearch 分布式原理"},{"content":"索引原理 倒排索引 例如，假设我们有两个文档，每个文档的 content 域包含如下内容：\n The quick brown fox jumped over the lazy dog Quick brown foxes leap over lazy dogs in summer  为了创建倒排索引，首先我们需要借助分词器，将每个文档的 content 域拆分成单独的词（我们称它为 词条 或 tokens、term ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  Term Doc_1 Doc_2 ------------------------- Quick | | X The | X | brown | X | X dog | X | dogs | | X fox | X | foxes | | X in | | X jumped | X | lazy | X | X leap | | X over | X | X quick | X | summer | | X the | X | ------------------------   如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档：\n1 2 3 4 5 6  Term Doc_1 Doc_2 ------------------------- brown | X | X quick | X | ------------------------ Total | 2 | 1   这里我们匹配到了两个文档（为了节省空间，这里返回的只是文档ID，最后再通过文档id去查询到具体文档。）。对于搜索引擎来说，用户总希望能够先看到相关度更高的结果，因此实际使用时我们通过一些算法来进行权重计算，将查询的结果按照权重降序返回。\nTerm dictionary与Term index Elasticsearch为了能够快速的在倒排索引中找到某个term，他会按照字典序对所有的term进行排序，再通过二分查找来找到term，这就是Term Dictionary，但即使有了Term Dictionary，O(logN)的磁盘读写仍然是影响性能的一大问题。\nB-Tree通过减少磁盘寻道次数来提高查询性能，Elasticsearch也是采用同样的思路，直接通过内存查找term，不读磁盘，但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index，从下图可以看出，Term Index其实就是一个Trie树（前缀树）\n这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。\n 为什么Elasticsearch/Lucene检索比mysql快呢？\n Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。检索一个term需要若干次的random access的磁盘操作。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。\n列式存储——Doc Values Doc values的存在是因为倒排索引只对某些操作是高效的。 倒排索引的优势在于查找包含某个项的文档，而对于从另外一个方向的相反操作并不高效，即：确定哪些项是否存在单个文档里，聚合需要这种次级的访问模式。\n以排序来举例——虽然倒排索引的检索性能非常快，但是在字段值排序时却不是理想的结构。\n 在搜索的时候，我们能通过搜索关键词快速得到结果集。 当排序的时候，我们需要倒排索引里面某个字段值的集合。换句话说，我们需要转置倒排索引。  转置 结构经常被称作 列式存储 。它将所有单字段的值存储在单数据列中，这使得对其进行操作是十分高效的，例如排序、聚合等操作。\n在Elasticsearch中，Doc Values就是一种列式存储结构，在索引时与倒排索引同时生成。也就是说Doc Values和倒排索引一样，基于 Segement生成并且是不可变的。同时Doc Values和倒排索引一样序列化到磁盘。\nDoc Values常被应用到以下场景：\n 对一个字段进行排序 对一个字段进行聚合 某些过滤，比如地理位置过滤 某些与字段相关的脚本计算  下面举一个例子，来讲讲它是如何运作的\n 假设存在以下倒排索引\n1 2 3 4 5 6  Term Doc_1 Doc_2 Doc_3 ------------------------------------ brown | X | X | dog | X | | X dogs | | X | X ------------------------------------   那么其生成的DocValues如下（实际存储时不会存储doc_id，值所在的顺位即为doc_id）\n1 2 3 4 5 6 7 8 9  Doc_id Values ------------------ Doc_1 | brown | Doc_1 | dog | Doc_2 | brown | Doc_2 | dogs | Doc_3 | dog | Doc_3 | dogs | ------------------   假设我们需要计算出brown出现的次数\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  GET /my_index/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;:{ \u0026#34;body\u0026#34;:\u0026#34;brown\u0026#34; } }, \u0026#34;aggs\u0026#34;:{ \u0026#34;popular_terms\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;body\u0026#34; } } }, \u0026#34;size\u0026#34; : 0 }   下面来分析上述请求在ES中是如何来进行查询的：\n 定位数据范围。通过倒排索引，来找到所有包含brown的doc_id。 进行聚合计算。借助doc_id在doc_values中定位到为brown的字段，此时进行聚合累加得到计算结果。browm的count=2。   但是doc_values仍存在一些问题，其不支持analyzed类型的字段，因为这些字段在进行文本分析时可能会被分词处理，从而导致doc_values将其存储为多行记录。但是在我们实际使用时，为什么仍然能对analyzed的字段进行聚合操作呢？这时就需要介绍一下Fielddata\n Fielddata doc values不生成分析的字符串，那为什么这些字段仍然可以使用聚合呢？是因为使用了fielddata的数据结构。与doc values不同，fielddata构建和管理100%在内存中，常驻于JVM内存堆。\n 从历史上看，fielddata 是所有字段的默认设置。但是Elasticsearch已迁移到doc values以减少 OOM 的几率。分析的字符串是仍然使用fielddata的最后一块阵地。 最终目标是建立一个序列化的数据结构类似于doc values ，可以处理高维度的分析字符串，逐步淘汰 fielddata。\n 它的一些特性如下\n 延迟加载。如果你从来没有聚合一个分析字符串，就不会加载fielddata到内存中，其是在查询时候构建的。 基于字段加载。 只有很活跃地使用字段才会增加fielddata的负担。 会加载索引中（针对该特定字段的） 所有的文档，而不管查询是否命中。逻辑是这样：如果查询会访问文档 X、Y 和 Z，那很有可能会在下一个查询中访问其他文档。 如果空间不足，使用最久未使用（LRU）算法移除fielddata。  因此，在聚合字符串字段之前，请评估情况：\n 这是一个not_analyzed字段吗？如果是，可以通过doc values节省内存 。 否则，这是一个analyzed字段，它将使用fielddata并加载到内存中。这个字段因为N-grams有一个非常大的基数？如果是，这对于内存来说极度不友好。  索引压缩 FOR编码 在Elasticsearch中，为了能够更方便的计算交集和并集，它要求倒排索引是有序的，而这个特点同时也带来了一个额外的好处，我们可以使用增量编码来压缩倒排索引，也就是FOR（Frame of Reference）编码\n 增量编码压缩，将大数变小数，按字节存储\n FOR编码分为三个步骤\n 增量编码 增量分区 位压缩  如下图所示，如果我们的倒排索引中存储的文档id为[73, 300, 302, 332, 343, 372]，那么经过增量编码后的结果则为[73, 227, 2, 30, 11, 29]。这种压缩的好处在哪里呢？我们通过增量将原本的大数变成了小数，使得所有的增量都在0～255之间，因此每一个值就只需要使用一个字节就可以存储，而不会使用int或者bigint，大大的节约了空间。\n接着，第二步我们将这些增量分到不同的区块中（Lucene底层用了256个区块，下面为了方便展示之用了两个）。\n第三步，我们计算出每组数据中最大的那个数所占用的bit位数，例如下图中区块1最大的为227，所以只占用8个bit位，所以三个数总共占用3 * 8bits即3字节。而区块2最大为29，只占用5个bit位，因此这三个数总共占用3 * 5bits即差不多2字节。通过这种方法，将原本6个整数从24字节压缩到了5字节，效果十分出色。\nRoaring Bitmaps FOR编码对于倒排索引来说效果很好，但对于需要存储在内存中的过滤器缓存等不太合适，两者之间有很多不同之处：\n 由于我们仅仅缓存那些经常使用的过滤器，因此它的压缩率并不需要像倒排索引那么高（倒排索引需要对每个词都进行编码）。 缓存过滤器的目的就是为了加速处理效率，因此它必须要比重新执行过滤器要快，因此使用一个好的数据结构和算法非常重要。 缓存的过滤器存储在内存之中，而倒排索引通常存储在磁盘中。  基于以上的不同，对于缓存来说FOR编码并不适用，因此我们还需要考虑其他的一些选择。\n 整数数组：数组可能是我们马上能想到的最简单的实现方式，我们将文档id存储在数组中，这样就使得我们的迭代变得非常简单，但是这种方法的内存利用率又十分低下，因为每个文档都需要4个字节。 Bitmaps：在数据分布密集的下，位图是一个很好的选择。它本质上就是一个数组，其中每一个文档id占据一个位，用0和1来标记文档是否存在。这种方法大大节约了内存，将一个文档从4字节降低到了一个位，但是一旦数据分布稀疏，此时的位图性能将大打折扣，因为无论数据量多少，位图的大小都是由数据的上下区间来决定的。 Roaring Bitmaps：Roaring Bitmaps即是对位图的一种优化，它会根据16位最高位将倒排索引划分为多块，如第一个块将对0到65535之间的值进行编码，第二个块将在65536和131071之间进行编码。在每一个块中，我们再对低16位进行编码，如果它的值小于4096在使用数组，否则就使用位图。由于我们编码的时候只会对低16位进行编码，因此在这里数组每个元素只需要2个字节   为什么要使用4096作为数组和位图选取的阈值呢？\n 下面是官方给出的数据报告，在一个块中只有文档数量超过4096，位图的内存优势才会凸显出来\n这就是Roaring Bitmaps高效率的原因，它基于两种完全不同的方案来进行编码，并根据内存效率来动态决定使用哪一种方案。\n官方也给出了几种方案的性能测试\n从上述对比可以看出，没有一种方法是完美的，但是以下两种方法的巨大劣势使得它们不会被选择\n 数组：性能很好，但是内存占用巨大。 Bitmaps：数据稀疏分布的时候内存和性能都会大打折扣。  因此在综合考量下，Elasticsearch还是选择使用Roaring Bitmaps，并且在很多大家了解的开源大数据框架中，也都使用了这一结构，如Hive、Spark、Kylin、Druid等。\n联合索引 如果多个字段索引的联合查询，倒排索引如何满足快速查询的要求呢？\n 跳表：同时遍历多个字段的倒排索引，互相skip。 位图：对多个过滤器分别求出位图，对这几个位图做AND操作。  Elasticsearch支持以上两种的联合索引方式，如果查询的过滤器缓存到了内存中（以位图的形式），那么合并就是两个位图的AND。如果查询的过滤器没有缓存，那么就用跳表的方式去遍历两个硬盘中的倒排索引。\n假设有下面三个倒排索引需要联合索引：\n 如果使用跳表，则对最短的倒排索引中的每一个id，逐个在另外两个倒排索引中查看是否存在，来判断是否存在交集。 如果使用位图，则直接将几个位图按位与运算，最终得到的结果就是最后的交集。  ","date":"2022-05-23T22:18:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/","title":"ElasticSearch 索引原理"},{"content":"权重计算原理 在Elasticsearch中，每个文档都有相关性评分，用一个正浮点数字段 _score 来表示 。 _score 的评分越高，相关性越高。为了保证搜索到的结果相关度更高，在默认情况下返回结果会按照相关度降序排序。\nElasticsearch使用布尔模型查找匹配文档，并用一个名为实用评分函数的公式来计算相关度。这个公式借鉴了词频/逆向文档频率和向量空间模型，同时也加入了一些现代的新特性，如协调因子，字段长度归一化，以及词或查询语句权重提升。\n布尔模型 布尔模型（Boolean Model） 适用于在查询中使用 AND 、 OR 和 NOT （与、或、非）这样的条件来查找匹配的文档，如以下查询：\n1  fullANDtextANDsearchAND(elasticsearchORlucene)  会将所有包括词 full 、 text 和 search ，以及 elasticsearch 或 lucene 的文档作为结果集。\n这个过程简单且快速，它将所有可能不匹配的文档排除在外。\n词频/逆向文档频率（TF/IDF） Elasticsearch的相似度算法被定义为检索词频率/反向文档频率（TF/IDF），主要依赖以下内容\n  检索词频率\n  检索词在该字段出现的频率。出现频率越高，相关性也越高。5次提到同一词的字段比只提到1次的更相关。\n  词频的计算方式如下：\n1  tf(t in d) = √frequency   词 t 在文档 d 的词频（ tf ）是该词在文档中出现次数的平方根。\n    逆向文档频率\n  每个检索词在索引中出现的频率。频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。\n  逆向文档频率的计算公式如下：\n1  idf(t) = 1 + log ( numDocs / (docFreq + 1))   词 t 的逆向文档频率（ idf ）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。\n    字段长度归一值\n  字段的长度。长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。\n  字段长度的归一值公式如下：\n1  norm(d) = 1 / √numTerms   字段长度归一值（ norm ）是字段中词数平方根的倒数。\n    以上三个因素是在索引时计算并存储的。最后将它们结合在一起计算单个词在特定文档中的权重 。\n向量空间模型 当然，查询通常不止一个词，所以需要一种合并多词权重的方式——向量空间模型（vector space model）。\n向量空间模型（vector space model）提供一种比较多词查询的方式，单个评分代表文档与查询的匹配程度，为了做到这点，这个模型将文档和查询都以 向量（vectors） 的形式表示，而向量实际上就是包含多个数的一维数组，例如：\n1  [1,2,5,22,3,8]   在向量空间模型里，向量空间模型里的每个数字都代表一个词的权重 ，与词频/逆向文档频率计算方式类似，下面举一个例子。\n设想如果查询 “happy hippopotamus” ，常见词 happy 的权重较低，不常见词 hippopotamus 权重较高，假设 happy 的权重是 2 ， hippopotamus 的权重是 5 ，可以将这个二维向量—— [2,5] ——在坐标系下作条直线，线的起点是 (0,0) 终点是 (2,5) ，如下图\n现在，设想我们有三个文档：\n I am happy in summer 。 After Christmas I’m a hippopotamus 。 The happy hippopotamus helped Harry 。  三篇文档的命中词如下\n 文档 1： (happy,____________) —— [2,0] 文档 2： ( ___ ,hippopotamus) —— [0,5] 文档 3： (happy,hippopotamus) —— [2,5]  可以为每个文档都创建包括每个查询词—— happy 和 hippopotamus ——权重的向量，然后将这些向量置入同一个坐标系中，如下图\n向量之间是可以比较的，只要测量查询向量和文档向量之间的角度就可以得到每个文档的相关度，文档 1 与查询之间的角度最大，所以相关度低；文档 2 与查询间的角度较小，所以更相关；文档 3 与查询的角度正好吻合，完全匹配。\n 在实际中，只有二维向量（两个词的查询）可以在平面上表示，幸运的是， 线性代数为我们提供了计算两个多维向量间角度工具，这意味着可以使用如上同样的方式来解释多个词的查询。\n ","date":"2022-05-23T22:17:18+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E6%9D%83%E9%87%8D%E8%AE%A1%E7%AE%97/","title":"ElasticSearch 权重计算"},{"content":"基本操作 环境搭建 搭建Elasticsearch环境 下载docker镜像\n1  docker pull elasticsearch:7.4.2   映射配置文件\n1 2 3 4 5 6 7 8 9  # 配置映射文件夹 mkdir -p /mydata/elasticsearch/config mkdir -p /mydata/elasticsearch/data # 设置文件夹权限任何用户可读可写 chmod 777 /mydata/elasticsearch -R # 配置 http.host echo \u0026#34;http.host: 0.0.0.0\u0026#34; \u0026gt;\u0026gt; /mydata/elasticsearch/config/elasticsearch.yml   启动容器\n1 2 3 4 5 6 7  docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\ -e \u0026#34;discovery.type\u0026#34;=\u0026#34;single-node\u0026#34; \\\t # 设置为单节点 -e ES_JAVA_OPTS=\u0026#34;-Xms64m -Xmx128m\u0026#34; \\ # 设置启动时ES的初始内存以及最大内存 -v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\ -v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\ -v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\ -d elasticsearch:7.4.2   访问ES服务，http://82.157.127.173:9200/\n得到相应体如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  { \u0026#34;name\u0026#34; : \u0026#34;de85ed684243\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;UeIP1PrXT2OFd7FlEEl3hQ\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.4.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;2f90bbf7b93631e52bafb59b3b049cb44ec25e96\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2019-10-28T20:40:44.881551Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.2.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; }   可以通过/_cat来获取节点信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # 访问http://82.157.127.173:9200/_cat # 属性列表 /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates   搭建Kibana环境 下载docker镜像\n1  docker pull kibana:7.4.2   启动容器\n1  docker run --name kibana -e ELASTICSEARCH_HOSTS=http://192.168.0.128:9200 -p 5601:5601 -d kibana:7.4.2   访问Kibana服务，http://192.168.0.128:5601/\nRESTful  一种软件架构风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。\n ES的基本REST命令：\n   Method Url 描述     PUT localhost:9200/索引名称/类型名称/文档id 创建文档（指定文档id）   GET localhost:9200/索引名称/类型名称/文档id 通过文档id查询文档   POST localhost:9200/索引名称/类型名称 创建文档（随机文档id）   POST localhost:9200/索引名称/类型名称/文档id/_update 修改文档   POST localhost:9200/索引名称/类型名称/_search 查询所有数据   DELETE localhost:9200/索引名称/类型名称/文档id 删除文档    CRUD 创建索引 在创建索引时，我们可以声明字段与数据类型的映射\n请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13  PUT /test0 { \u0026#34;mappings\u0026#34;:{ \u0026#34;properties\u0026#34;:{ \u0026#34;name\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; }, \u0026#34;author\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; } } } }   响应:\n1 2 3 4 5  { \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;index\u0026#34;: \u0026#34;test0\u0026#34; }   即使如果我们没有配置类型，ES也会根据字段的内容来自行推导。\n 注意⚠️：由于索引具有不变性，我们只能进行追加而不能更改已经存在的映射字段，必须创建新的索引后进行数据迁移。\n 1 2 3 4 5 6 7 8 9  POST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;test0\u0026#34; }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;test1\u0026#34; } }   插入文档 PUT和POST都可以插入文档：\n POST：如果不指定 id，自动生成 id。如果指定 id，则修改这条记录，并新增版本号。 PUT：必须指定 id，如果没有这条记录，则新增，如果有，则更新。   示例：在 test1 索引下的books类型中保存标识为 1 的文档。\n 请求：\n1 2 3 4 5  PUT /test1/books/1 { \u0026#34;name\u0026#34;:\u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34; : \u0026#34;Daniel Defoe\u0026#34; }   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test\u0026#34;,\t//索引  \u0026#34;_type\u0026#34;: \u0026#34;book\u0026#34;,\t//类型  \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;,\t//id  \u0026#34;_version\u0026#34;: 1,\t//版本号  \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;,\t//操作类型  \u0026#34;_shards\u0026#34;: {\t//分片  \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1,\t//并发控制字段，每次更新就会+1，用来做乐观锁  \u0026#34;_primary_term\u0026#34;: 1 //同上，主分片重新分配，如重启，就会变化 }   查询文档  示例：查询test1索引下的books类型中保存标识为 1 的文档的内容。\n 请求：\n1  GET /test1/books/1   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Daniel Defoe\u0026#34; } }   更新文档 使用POST命令，在ID后面加_update，并把需要修改的内容放入doc属性中\n 示例：更新test1 索引下的books类型中保存标识为 1 的文档的内容。\n 请求：\n1 2 3 4 5 6 7 8  POST /test1/books/1/_update { \u0026#34;doc\u0026#34; : { \u0026#34;name\u0026#34;:\u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34; : \u0026#34;Daniel Defoe\u0026#34;, \u0026#34;country\u0026#34; : \u0026#34;England\u0026#34; } }   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1, \u0026#34;_primary_term\u0026#34;: 1 }   删除文档和索引 删除使用DELETE命令\n 示例：删除文档/test1/books/1\n 请求：\n1  DELETE /test1/books/1   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 3, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 2, \u0026#34;_primary_term\u0026#34;: 1 }    示例：删除索引/test1\n 1  DELETE /test1   响应:\n1 2 3  { \u0026#34;acknowledged\u0026#34;: true }   Search 为了方便测试，可以从官网导入测试数据https://download.elastic.co/demos/kibana/gettingstarted/accounts.zip\n1 2 3 4 5 6  POST /test_data/account/_bulk {\u0026#34;index\u0026#34;:{\u0026#34;_id\u0026#34;:\u0026#34;1\u0026#34;}} {\u0026#34;account_number\u0026#34;:1,\u0026#34;balance\u0026#34;:39225,\u0026#34;firstname\u0026#34;:\u0026#34;Amber\u0026#34;,\u0026#34;lastname\u0026#34;:\u0026#34;Duke\u0026#34;,\u0026#34;age\u0026#34;:32,\u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;880 Holmes Lane\u0026#34;,\u0026#34;employer\u0026#34;:\u0026#34;Pyrami\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;amberduke@pyrami.com\u0026#34;,\u0026#34;city\u0026#34;:\u0026#34;Brogan\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;IL\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_id\u0026#34;:\u0026#34;6\u0026#34;}} {\u0026#34;account_number\u0026#34;:6,\u0026#34;balance\u0026#34;:5686,\u0026#34;firstname\u0026#34;:\u0026#34;Hattie\u0026#34;,\u0026#34;lastname\u0026#34;:\u0026#34;Bond\u0026#34;,\u0026#34;age\u0026#34;:36,\u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;671 Bristol Street\u0026#34;,\u0026#34;employer\u0026#34;:\u0026#34;Netagy\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;hattiebond@netagy.com\u0026#34;,\u0026#34;city\u0026#34;:\u0026#34;Dante\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;TN\u0026#34;} ....................   查询方式 ES支持两种查询方式，一种是直接在URL后加上参数，另一种是在URL后加上JSON格式的请求体。\n 示例：查找到收入最高的十条记录\n URL + 参数 常用的参数如下\n q：用于指定搜索的关键词。 from \u0026amp; size：类似于SQL中的offset和limit。 sort：对结果排序，默认为降序。 _source：指定想要返回的属性。  1  GET /test_data/_search?q=*\u0026amp;sort=balance:desc\u0026amp;from=0\u0026amp;size=10   URL + QueryDSL 1 2 3 4 5 6 7 8 9 10 11  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34; : {} }, \u0026#34;sort\u0026#34; : [{ \u0026#34;balance\u0026#34; : \u0026#34;desc\u0026#34; }], \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 10 }   虽然URL+参数的写法非常简洁，但是随着逻辑的复杂化，其可读性也越来越差，所以通常都会使用URL + QueryDSL的格式。\nmatch 匹配 match 匹配查询 无论你在任何字段上进行的是全文搜索还是精确查询，match 查询是你可用的标准查询。\n对于not_analyzed的字段，match能做到精确查询，而对于analyzed的字段，match能做到匹配查询（全文搜索）。\n 示例：查找所有年龄为25岁的记录（精确查询）\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34; : { \u0026#34;age\u0026#34;: 25 } } }    示例：查询所有地址与976 Lawrence Street相关的记录（全文搜索）\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34; : { \u0026#34;address\u0026#34;: \u0026#34;976 Lawrence Street\u0026#34; } } }   match_all 全部匹配 match_all 用于查询所有文档。在没有指定查询方式时，它是默认。\n 示例：查询年龄最小的十条记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34; : {} }, \u0026#34;sort\u0026#34; : [{ \u0026#34;age\u0026#34; : \u0026#34;asc\u0026#34; }], \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 10 }   match_phase 短语匹配 match_phase用于进行短语的匹配，它查询时并不是像term一样不进行分词直接查询，而是借助分析器返回的查询词的相对顺序以及偏移量来做判断——满足所有查询词且顺序完全相同的记录才会被匹配上。\n 示例：地址包含502 Baycliff Terrace的记录\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_phase\u0026#34; : { \u0026#34;address\u0026#34;: \u0026#34;502 Baycliff Terrace\u0026#34; } } }   multi_match 多字段匹配 multi_match 可以在多个字段上执行相同的 match 查询。\n 示例：查找city或address字段中包含Dixie或Street的记录。\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;Dixie Street\u0026#34;, \u0026#34;fields\u0026#34;:[ \u0026#34;city\u0026#34;, \u0026#34;address\u0026#34; ] } } }   term 精确查询 term即直接在倒排索引中查询，也就是精确查找，不进行分词器分析，文档中必须包含整个搜索的词汇。\nterm和match的区别:\n match是经过分析处理的，查询词先被文本分析器处理后再进行查询。所以根据不同的文本分析器，分析出  的结果也会不同。\n term是不经过分析处理的，直接去倒排索引查找精确的值。   由于text字段会被文本分析器处理，所以通常全文检索字段用match，其他非text字段（not_analyzed）匹配用term。\n 1 2 3 4 5 6 7 8 9 10  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;Street\u0026#34; } } } // 虽然文档中存在”702 Quentin Street“，但是由于文本分析器默认会转为小写，无法搜到任何数据   布尔查询（复合查询） 借助布尔查询可以实现如SQL中（and、or、!=）等逻辑条件的判断，并且可以合并任何其他查询语句，包括复合语句。复合语句之间可以相互嵌套，可以表达复杂的逻辑。\n  must（and）：文档必须匹配这些条件才能被包含进来。（影响相关性得分）\n  must_not（not）：文档必须不匹配这些条件才能被包含进来。（不影响相关性得分）\n  should（or）：如果满足这些语句中的任意语句，将增加得分 。（用于修正相关性得分）\n   示例：查找年龄不等于18的地址包含Street的男性，且优先展示居住在30岁以上的的记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;must\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;address\u0026#34;:\u0026#34;Street\u0026#34; } }, { \u0026#34;match\u0026#34;:{ \u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34; } } ], \u0026#34;must_not\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;age\u0026#34;:\u0026#34;18\u0026#34; } } ], \u0026#34;should\u0026#34;:[ { \u0026#34;range\u0026#34;:{ \u0026#34;age\u0026#34;:{ \u0026#34;gt\u0026#34;:30 } } } ] } } }   Filter 过滤器 Filter通常搭配布尔查询一起使用，用于过滤出所有满足Filter的记录，不影响相关性得分。\n 示例：查找年龄在30～60之间的记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;filter\u0026#34;:[ { \u0026#34;range\u0026#34;:{ \u0026#34;age\u0026#34;:{ \u0026#34;gte\u0026#34;:30, \u0026#34;lte\u0026#34;:60 } } } ] } } }   Aggregations 聚合 要掌握聚合，你只需要明白两个主要的概念：\n 桶（Buckets）：满足特定条件的文档的集合 指标（Metrics）：对桶内的文档进行统计计算  翻译成SQL的形式来理解的话：\n1 2 3 4 5  SELECTCOUNT(1),MAX(balance)FROMtableGROUPBYgender;  桶在概念上类似于 SQL 的分组（GROUP BY，如上面的GROUP BY gender），而指标则类似于 COUNT() 、 SUM() 、 MAX() 等统计方法，如MAX(balance)。\n聚合的语法如下：\n1 2 3 4 5 6 7 8 9 10  \u0026#34;aggregations\u0026#34; : { \u0026#34;\u0026lt;聚合名称 1\u0026gt;\u0026#34; : { \u0026#34;\u0026lt;聚合类型\u0026gt;\u0026#34; : { \u0026lt;聚合体内容\u0026gt; } [,\u0026#34;元数据\u0026#34; : { [\u0026lt;meta_data_body\u0026gt;] }]? [,\u0026#34;aggregations\u0026#34; : { [\u0026lt;sub_aggregation\u0026gt;]+ }]? } [\u0026#34;聚合名称 2\u0026gt;\u0026#34; : { ... }]* }    示例：按照性别进行分组，计算平均年龄和最高收入\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_all\u0026#34;: {} }, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_gender\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;gender\u0026#34; }, \u0026#34;aggs\u0026#34;:{ \u0026#34;avg_age\u0026#34;:{ \u0026#34;avg\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34; } }, \u0026#34;max_balance\u0026#34;:{ \u0026#34;max\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;balance\u0026#34; } } } } }, \u0026#34;size\u0026#34;:0 }   ","date":"2022-05-23T22:17:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"ElasticSearch 基本操作"},{"content":"ElasticSearch 基本概念 Elasticsearch是一个分布式的免费开源搜索和分析引擎，适用于包括文本、数字、地理空间、结构化和非结构化数据等在内的所有类型的数据。\nElasticsearch在Apache Lucene的基础上开发而成。然而，Elasticsearch不仅仅是Lucene，并且也不仅仅只是一个全文搜索引擎。 它可以被下面这样准确的形容：\n 一个分布式的实时文档存储，每个字段都可以被索引与搜索 一个分布式实时分析搜索引擎 能胜任上百个服务节点的扩展，并支持PB级别的结构化或者非结构化数据  Lucene Lucene是一套用于全文索引和搜索的开源程式库，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜索。但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎（英文与德文两种西方语言）。\nLucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。\n在Java开发环境里Lucene是一个成熟的免费开源工具。就其本身而言，Lucene是当前以及最近几年最受欢迎的免费Java信息检索程序库。人们经常提到信息检索程序库，虽然与搜索引擎有关，但不应该将信息检索程序库与搜索引擎相混淆。\nELK Elastic Stack是一套适用于数据采集、扩充、存储、分析和可视化的免费开源工具。人们通常将Elastic Stack称为ELK Stack（代指 Elasticsearch、Logstash 和 Kibana）。\n Logstash是什么？\n Logstash 是 Elastic Stack 的核心产品之一，可用来对数据进行聚合和处理，并将数据发送到 Elasticsearch。Logstash 是一个开源的服务器端数据处理管道，允许您在将数据索引到 Elasticsearch 之前同时从多个来源采集数据，并对数据进行充实和转换。\n Kibana是什么？\n Kibana 是一款适用于Elasticsearch的数据可视化和管理工具，可以提供实时的直方图、线形图、饼状图和地图。Kibana同时还包括诸如 Canvas和Elastic Maps等高级应用程序\n Canvas允许用户基于自身数据创建定制的动态信息图表， Elastic Maps用来对地理空间数据进行可视化。  Elasticsearch的特点   Elasticsearch 很快。 由于Elasticsearch是在Lucene基础上构建而成的，所以在全文本搜索方面表现十分出色。Elasticsearch同时还是一个近实时的搜索平台，这意味着从文档索引操作到文档变为可搜索状态之间的延时很短，一般只有一秒。因此，Elasticsearch 非常适用于对时间有严苛要求的用例，例如安全分析和基础设施监测。\n  Elasticsearch 具有分布式的本质特征。 Elasticsearch中存储的文档分布在不同的容器中，这些容器称为分片，可以进行复制以提供数据冗余副本，以防发生硬件故障。Elasticsearch的分布式特性使得它可以扩展至数百台（甚至数千台）服务器，并处理 PB 量级的数据。\n  Elasticsearch 包含一系列广泛的功能。 除了速度、可扩展性和弹性等优势以外，Elasticsearch 还有大量强大的内置功能（例如数据汇总和索引生命周期管理），可以方便用户更加高效地存储和搜索数据。\n  Elastic Stack 简化了数据采集、可视化和报告过程。 通过与 Beats 和 Logstash 进行集成，用户能够在向 Elasticsearch 中索引数据之前轻松地处理数据。同时，Kibana 不仅可针对 Elasticsearch 数据提供实时可视化，同时还提供 UI 以便用户快速访问应用程序性能监测 (APM)、日志和基础设施指标等数据。\n  应用场景 Elasticsearch在速度和可扩展性方面都表现出色，而且还能够索引多种类型的内容，这意味着其可用于多种用例：\n 应用程序搜索 网站搜索 企业搜索 日志处理和分析 基础设施指标和容器监测 应用程序性能监测 地理空间数据分析和可视化 安全分析 业务分析  架构设计  Gateway：Elasticsearch用来存储索引的文件系统，支持多种类型。ElasticSearch默认先把索引存储在内存中，然后当内存满的时候，再持久化到Gateway里。当ES集群关闭或重启的时候，它就会从Gateway里去读取索引数据。比如LocalFileSystem和HDFS、AS3等。 DistributedLucene Directory：Lucene里的一些列索引文件组成的目录。它负责管理这些索引文件。包括数据的读取、写入，以及索引的添加和合并等。 Mapping：映射解析模块。 Search Moudle：搜索模块。 Index Moudle：索引模块。 Disvcovery：节点发现模块。不同机器上的节点要组成集群需要进行消息通信，集群内部需要选举master节点，这些工作都是由Discovery模块完成。支持多种发现机制，如 Zen 、EC2、gce、Azure。 Scripting：Scripting用来支持在查询语句中插入javascript、python等脚本语言。 3rd plugins：第三方插件。 Transport：传输模块，支持多种传输协议，如 Thrift、memecached、http，默认使用http。 JMX：JMX是java的管理框架，用来管理ES应用。 Java(Netty)：java的通信框架。 RESTful Style API：提供给用户的接口，通过RESTful方式来实现API编程。  基本概念 Elasticsearch是一个文档型数据库，为了方便理解，下面给出其与传统的关系型数据库的对比\n   Relational DB Elasticsearch     数据库 Database 索引 Index   表 Table 类型 Type   行 Rows 文档 Document   列 columns 字段 fields   表结构 schema 映射 mapping    文档 Elasticsearch是面向文档的，索引和搜索数据的最小单位是文档，并且使用JSON来作为文档的序列化格式。每个文档可以由一个或多个字段组成，类似于关系型数据库中的一行记录，在Elasticsearch中，文档有几个重要属性\n 自我包含：一篇文档同时包含字段和对应的值。 层次型：文档中还可以包含新的文档，一个字段的取值也可以包含其他字段和取值。 结构灵活：文档不依赖预先定义的模式。在关系型数据库中，要提前定义字段才能使用，在Elasticsearch中，对于字段是非常灵活的，有时候，我们可以忽略该字段，或者动态的添加一个新的字段。  尽管我们可以随意的新增或者忽略某个字段，但是，每个字段的类型非常重要，比如一个年龄字段类型，可以是字 串也可以是整形。因为Elasticsearch会保存字段和类型之间的映射及其他的设置。这种映射具体到每个映射的每种类型，这也是为什么在Elasticsearch中，类型有时候也称为映射类型。\n类型 类型是文档的逻辑容器，类似于表格是行的容器。在不同的类型中，最好放入不同结构的文档。（有点类似于关系型数据库中的表的概念）。\n每个类型中对于字段的定义称为映射。比如name字段映射为string类型。 我们说文档是无模式的，它们不需要拥有映射中所定义的所有字段，例如我们可能会新增一个映射中不存在的字段，那么Elasticsearch是怎么做的呢?\nElasticsearch会自动的将新字段加入映射，由于不确定这个字段是什么类型，Elasticsearch就会根据字段的值来推导它的类型，如果这个值是10，那么Elasticsearch会认为它是整形。 但是这种推导也可能会存在问题，如果这里的10实际上是字符串“10”，如果后续在索引字符串“hello world”，就会因为类型不一致而导致索引失败。 所以最安全的方式就是在索引数据之前，就定义好所需要的映射，这点跟关系型数据库殊途同归了，先定义好字段，然后再使用。\n映射类型只是将文档进行逻辑划分。从物理角度来看，同一索引的文档都是写入磁盘，而不考虑它们所属的映射类型。\n 注意⚠️：为什么ElasticSearch要在7.X版本去掉type?\n 在Elasticsearch设计初期，是直接查考了关系型数据库的设计模式，存在了类型（表）的概念。 但是，其搜索引擎是基于Lucene的，这种基因决定了类型是多余的。Lucene的全文检索功能之所以快，是因为倒序索引的存在。 而这种倒序索引的生成是基于索引的，而并非 类型。多个类型反而会减慢搜索的速度——两个不同type下的两个user_name，在ES同一个索引下其实被认为是同一个filed，你必须在两个不同的type中定义相同的filed映射。否则，不同type中的相同字段名称就会在处理中出现冲突的情况，导致Lucene处理效率下降。\n索引 在Elasticsearch中，索引有两个含义：\n  名词\n 索引是映射类型的容器，Elasticsearch中的索引是一个非常大的文档集合，非常类似关系型数据库中库的概念。索引存储了所有映射类型的字段。    动词\n 索引一个文档就是存储一个文档到一个索引 （名词）中以便被检索和查询。这非常类似于SQL中的INSERT关键词，不同的是文档已存在时，新文档会替换旧文档（UPSERT）。    在Elasticsearch中，索引使用了一种称为倒排索引（即通过查询词索引到文档）的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。\n","date":"2022-05-23T22:16:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ElasticSearch 基本概念"},{"content":"集群 集群角色 通常在分布式系统中，构成一个集群的每一台机器都有自己的角色，最常见的集群模式就是Master/Slave模式（主从模式），在这种模式中，通常Maste服务器作为主服务器提供写服务，其他的Slave服务器从服务器通过异步复制的方式获取Master服务器最新的数据提供读服务。\n但是ZooKeeper并没有沿用传统的主从模式，而是引入了Leader、Follower和Observer三种角色，如下图所示：\nZooKeeper集群中的所有机器通过一个Leader 选举过程来选定一台称为Leader的机器，Leader既可以为客户端提供写服务又能提供读服务。除了Leader外，Follower和Observer都只能提供读服务。Follower和Observer唯一的区别在于Observer机器不参与Leader的选举过程，也不参与写操作的“过半写成功”策略，因此Observer机器可以在不影响写性能的情况下提升集群的读性能。\n Leader：为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。 Follower：为客户端提供读服务，如果是写服务则转发给Leader。在选举过程中参与投票。 Observer：与Follower工作原理基本一致，唯一的区别是Observer不参与任何形式的投票。Observer主要用来在不影响写性能的情况下提升集群的读性能，是ZooKeeper3.3中新增的角色。  ZooKeeper用以下四种状态来表示各个节点\n LOOKING：竞选状态。 LEADING：Leader状态。 FOLLOWING：Follower状态。 OBSERVING：Observer状态。  选举 Leader选举是ZooKeeper中最重要的技术之一，也是保证分布式数据一致性的关键所在。\n当ZooKeeper集群中的一台服务器出现以下两种情况之一时，就会开始进入Leader选举\n 服务器初始化启动 服务器运行期间无法和Leader保持连接  在介绍选举之前，首先介绍三个重要参数\n 服务器ID（myid）：编号越大在选举算法中权重越大。 事务ID（zxid）：值越大说明数据越新，权重越大。 逻辑时钟（epoch-logicalclock）：同一轮投票过程中的逻辑时钟值是相同的，每投完一次值会增加。  服务器启动时期的选举 ZooKeeper集群初始化时，当满足以下两个条件时，就会开始进入Leader选举流程：\n 集群规模至少为2台机器 集群内的机器能够互相通信  如上图，选举流程如下：\n 每个服务器会发出一个投票：由于是初始化状态，每一个节点都会将自己作为Leader来进行投票，每次投票的内容包含：推举服务器的myid（在集群中的机器序号）和ZXID（事务ID），以(myid, ZXID)的形式表示。初始化阶段每个节点都会将票投给自己，然后将这个投票发给集群总其他所有机器。 接受来自各个服务器的投票：每个服务器都会接收来自其他服务器的投票，在接收投票后首先会判断该投票的有效性，如是否为本轮投票（逻辑时钟）、是否来自LOOING状态的服务器。 处理投票：在接收到来自其他服务器的投票后，针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下  优先检查ZXID，ZXID较大的优先作为Leader。 如果Leader相同则比较myid，myid较大的作为Leader。    统计投票：每次投票后，服务器都会统计所有投票，判断是否已经有过半的机器接收到相同的投票信息。当票数达到半数以上时，此时就认为已经选出了Leader。 改变服务器状态：一旦确认了Leader，则每个服务器开始改变自己的状态。如果是Follower则更改成FOLLOWING；如果是Observer则更改成OBSERVING；如果是Leader则更改成LEADING；  服务器运行时期的选举 在ZooKeeper集群正常运行中，一旦选出一个Leader，那么所有服务器的集群角色一般不会再发生变化——也就是说Leader服务器将一直作为集群的Leader，即使集群中有非Leader机器挂了或者是有机器加入集群也不会影响Leader。但是一旦Leader所在的机器挂掉，那么此时整个集群将暂时无法对外服务，马上进入新一轮的Leader选举。\n服务器运行期间的Leader选举和启动时期的Leader选举基本是一致的，只增加了一个变更状态的步骤，流程如下：\n 变更状态：当Leader挂了之后，剩下的所有非Observer服务器都会将自己的服务器状态变更为LOOKING，然后开始进入Leader选举流程。 每个服务器会发出一个投票 接受来自各个服务器的投票 处理投票 统计投票 改变服务器状态  集群的机器数量  为什么ZooKeeper提倡集群的机器数量最好要做奇数台呢？\n ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n/2。先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。\n综上，何必增加那一个不必要的 ZooKeeper 呢？\n","date":"2022-05-23T21:13:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E9%9B%86%E7%BE%A4/","title":"ZooKeeper 集群"},{"content":"系统模型 数据模型 ZooKeeper数据存储的结构与标准的Unix文件系统非常类似，但是并没有引入传统文件系统中目录和文件等概念，而是使用了其特有的数据节点的概念，我们称之为ZNode。Znode是zookeeper中的最小数据单元，每个Znode上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。\n如上下图，Znode的节点路径标识方式和Unix文件系统路径非常相似，都是由一系列使用斜杠（/）进行分割的路径表示，最上层的根结点以/代表，并且每个Znode都有一个唯一的路径标识。\n ⚠️注意：ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的上限是每个结点的数据大小最大是 1M。\n 节点特性 节点类型 在ZooKeeper中，Znode节点类型分为持久节点（PERSISTENT）、临时节点（EPHEMERAL）、**顺序节点（SEQUENTIAL）**三大类，通过组合使用，可以生成以下四种组合型节点类型：\n 持久节点：该数据节点被创建后，就会一直存在于ZooKeeper的服务器上，直到有删除操作来主动清除这个节点。 持久顺序节点：基本特性与持久节点一致，额外的特性是顺序性，即一个父节点可以为其子节点维护一个创建的先后顺序 ，这个顺序体现在节点名称上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数，上限是整型最大值。 临时节点：临时节点的生命周期与客户端的会话绑定在一起，如果客户端会话失效，则这个节点就会自动被清理掉。同时，临时节点不能创建子节点，它只能作为叶子节点使用。 临时顺序节点：基本特性与临时节点一致，额外的特性是顺序性。  节点状态 每个数据节点除了存储数据内容之外，还存储了数据节点本身的一些状态信息（如子节点数量、事务id、版本信息等），这些状态信息由Stat这个类来维护。\n czxid：Created ZXID，该数据节点被创建时的事务ID。 mzxid：Modified ZXID，节点最后一次被更新时的事务ID。 ctime：Created Time，该节点被创建的时间。 mtime： Modified Time，该节点最后一次被修改的时间。 version：节点的版本号。 cversion：子节点的版本号。 aversion：节点的 ACL 版本号。 ephemeralOwner：创建该节点的会话的sessionID ，如果该节点为持久节点，该值为0。 dataLength：节点数据内容的长度。 numChildre：该节点的子节点个数，如果为临时节点为0。 pzxid：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的列表 ，不是内容。  版本 ZooKeeper中为节点引入了版本的概念，每个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化。\n dataVersion：当前Znode节点数据内容版本号 cversion：当前Znode节点的子节点版本号。 aclVersion：当前Znode的ACL变更版本号。  为了解决那些数据更新竞争激烈的场景，ZooKeeper借助版本号机制来实现乐观并发控制。\nWatcher Watcher（事件监听器），是ZooKeeper中的一个很重要的特性。ZooKeeper允许客户端在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。\n从上图中我们可以看到，ZooKeeper的Watcher机制主要包括客户端线程、客户端WatchManager、ZooKeeper服务器三个部分。工作流程如下：\n 客户端向ZooKeeper服务器注册Watcher对象 客户端将Watcher对象存储在客户端的WatchManager中。 当ZooKeeper服务器触发Watcher事件后，会向客户端发送通知。 客户端从WatchManager中取出对应的Watcher对象来执行回调逻辑。  ACL ZooKeeper提供了一套完善的ACL（Access Control Lists）权限控制机制来保障数据的安全，类似于Unix/Linux中的UGO权限控制机制。\nACL机制由以下三部分组成，我们通常使用scheme:id:permission来标识一个有效的ACL信息。\n 权限模式（Scheme） 授权对象（ID） 权限（Permission）  权限模式：Scheme 权限模式是用来确定权限验证过程中使用的校验策略。在ZooKeeper中最常用的就是以下四种权限模式\n IP：IP模式通过IP地址粒度来进行权限控制。 Digest：Digest是最常用的权限控制模式，其以类似于username:password形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。当我们配置了权限标识后，为了保证安全，ZooKeeper会分别使用SHA-1算法和BASE64编码进行处理，将其混淆为无法辨识的字符串。 World：World是最开放的权限控制模式，是一种特殊的Digest模式。在该模式下数据节点的访问权限对所有用户开放，即所有用户都可以在不进行任何数据校验的情况下操作ZooKeeper上的数据。权限标识为world:anyone Super：Super模式即超级用户模式，是一种特殊的Digest模式。在该模式下超级用户可以对任意ZooKeeper上的数据节点进行任何操作。  权限对象：ID 授权对象指的是权限赋予的用户或一个指定实体，例如IP地址或是机器等。在不同的权限关系下，授权对象是不同的，对应关系如下图：\n IP：通常是一个IP地址或者一个IP网段，如192.168.0.110或192.168.0.1/24 Digest：Digest是最常用的权限控制模式，其以类似于username:password形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。当我们配置了权限标识后，为了保证安全，ZooKeeper会分别使用SHA-1算法和BASE64编码进行处理，将其混淆为无法辨识的字符串。 World：只有一个ID：anyone Super：与Digest模式一致。  权限：Permission 权限就是指那些通过权限检查后可以被允许执行的操作。在ZooKeeper中，所有对数据的操作权限分为以下五大类：\n CREATE：数据节点的创建权限，允许授权对象在该数据节点下创建子节点。 DELETE：数据节点的删除权限，允许授权对象删除该数据节点的子节点。 READ：数据节点的读取权限，允许授权对象访问该数据节点并读取其数据内容或子节点列表等。 WRITE：数据节点的更新权限，允许授权对象对该数据节点进行更新操作。 ADMIN：数据节点的管理权限，允许授权对象在该数据节点进行ACL相关的设置操作。  Session Session（会话） 可以看作是ZooKeeper服务器与客户端的之间的一个TCP长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watcher事件通知。\nSession有一个属性叫做：sessionTimeout ，sessionTimeout 代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。\n另外在每次客户端向服务端发起会话创建请求时，服务端都会为其分配一个SessionID，SessionID用来唯一标识一个会话，因此ZooKeeper必须保证SessionID的全局唯一性。\n","date":"2022-05-23T21:11:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E7%B3%BB%E7%BB%9F%E6%A8%A1%E5%9E%8B/","title":"ZooKeeper 系统模型"},{"content":"ZooKeeper 基本概念 ZooKeeper是一个开源的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。\n 原语： 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。\n 特点 ZooKeeper可以保证如下分布式一致性特性：\n 顺序一致性：从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一视图：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性：一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 实时性：ZooKeeper保证在一定时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。  设计目标 ZooKeeper致力于实现一个高性能、高可用，具有严格顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务。高性能使得ZooKeeper能够应用于那些对系统吞吐有明确要求的大型分布式系统，高可用使得分布式的单点问题得到了很好的解决，而严格的顺序访问控制使得客户端能够基于ZooKeeper实现一些复杂的同步原语。\n针对以上需求，ZooKeeper的四个设计目标如下：\n  简单的数据模型：ZooKeeper使用一个共享的、树形结构的命名空间来协调分布式程序。其数据模型类似一个文件系统，不过与传统的文件系统不同，ZooKeeper将全量数据存储在内存中，以此来实现提高服务器吞吐、减少延迟的目的。\n  可以构建集群：一个ZooKeeper集群通常由一组机器组成，组成ZooKeeper集群的每台机器都会在内存中维护当前服务器状态，并且每台机器之间都互相保持通信。只要集群中存在半数以上的机器能够正常工作，整个集群就可以正常对外提供服务。\n  顺序访问：对于来自客户端的每个更新请求，ZooKeeper都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序，可以根据这个特性来实现更高层次的同步原语。\n  高性能：由于ZooKeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。\n  应用场景 ZooKeeper是一个典型的分布式数据一致性的解决方案，通常用于以下这些场景：\n 数据发布/订阅 负载均衡 命名服务 分布式协调/通知 集群管理 Master选举 分布式锁 分布式队列  ","date":"2022-05-23T21:10:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ZooKeeper 基本概念"},{"content":"分布式文件系统 基本概念 分布式文件系统（Distributed File System）是网络文件系统的延伸，其关键点在于存储端可以灵活地横向扩展。也就是可以通过增加设备（主要是服务器）数量的方法来扩充存储系统的容量和性能。同时，分布式文件系统还要对客户端提供统一的视图。也就是说，虽然分布式文件系统服务由多个节点构成，但客户端并不感知。在客户端来看就好像只有一个节点提供服务，而且是一个统一的分布式文件系统。\n分布式文件系统 VS 网络文件系统 从本质上来说，分布式文件系统其实也是网络文件系统的一种，其与网络文件系统的差异在于服务端包含多个节点，也就是服务端是可以横向扩展的，其可以通过增加节点的方式增加文件系统的容量，提升性能。\n由于其数据被存储在多个节点上，因此还有其他特点：\n 支持按照既定策略在多个节点上放置数据。 可以保证在出现硬件故障时，仍然可以访问数据。 可以保证在出现硬件故障时，不丢失数据。 可以在硬件故障恢复时，保证数据的同步。 可以保证多个节点访问的数据一致性。  横向拓展结构 对于存储集群端主要有两种类型的架构模式：一种是以有中心控制节点的分布式架构，另一种是对等的分布式架构，也就是没有中心控制节点的架构。\n中心架构 中心架构是指在存储集群中有一个或多个中心节点，中心节点维护整个文件系统的元数据，为客户端提供统一的命名空间。 在实际生产环境中，中心节点通常是多于一个的，其主要目的是保证系统的可用性和可靠性。\n在中心架构中，集群节点的角色分为两种：\n 控制节点：这种类型的节点会存储文件系统的元数据信息，并对请求进行协调与处理，根据元数据将请求转发只对应的节点上。 数据节点：这种类型的节点用于存储文件系统的用户数据。  架构示意图如下：\n当客户端需要对一个文件进行读/写时，首先会访问控制节点，控制节点通过对一些元数据进行处理（鉴权、文件锁、位置计算等），并将文件所在的数据节点的位置响应给客户端。此时客户端再与数据节点交互，完成数据的访问。\n对等架构 对等架构是没有中心节点的架构，集群中并没有一个特定的节点负责文件系统元数据的管理。在集群中所有节点既是元数据节点，也是数据节点。 在实际实现中，其实并不进行角色的划分，只是作为一个普通的存储节点。\n由于在对等架构中没有中心节点，因此主要需要解决两个问题：\n 客户端需要一种位置计算算法来计算数据应该存储的位置。 需要将元数据存储在各个存储节点，在某些情况下需要客户端来汇总。  关键原理 分布式文件系统本身也是文件系统，因此它与本地文件系统和网络文件系统等具备一些公共技术。除此之外，鉴于其分布式的特点，还涉及一些分布式的技术。\n数据布局 分布式文件系统的数据布局与本地文件系统不同，其关注的不是数据在磁盘的布局，而是数据在存储集群各个节点的放置问题。\n在分布式文件系统中，数据布局解决的主要问题是性能和负载均衡的问题。其解决方案就是通过多个节点来均摊客户端的负载，也就是实现存储集群的横向扩展。因此数据布局的核心，就是要保证数据量均衡与负载均衡。\n基于动态监测的数据布局 基于动态监测的数据布局是指通过监测存储集群各个节点的负载、存储容量和网络带宽等系统信息来决定新数据放置的位置。 另外，集群节点之间还要有一些心跳信息，这样当有数据节点故障的情况下，控制节点可以及时发现，保证在决策时剔除。\n由于需要汇总各个节点的信息进行决策，因此基于动态监测的数据布局通常需要一个中心节点。中心节点负责汇总各种信息并进行决策，并且会记录数据的位置信息等元数据信息。当客户端需要写入数据时，客户端首先与控制节点交互；控制节点根据汇总的信息计算出新数据的位置，然后反馈给客户端；客户端根据位置信息，直接与对应的数据节点交互。\n基于计算位置的数据布局 基于计算位置的数据布局是一种固定的数据分配方式。在该架构中通过一个算法来计算文件或数据存储的具体位置。 当客户端要访问某个文件时，请求在客户端或经过的某个代理节点计算出数据的具体位置，然后将请求路由到该节点进行处理。\n当客户端访问集群数据时，首先计算出数据的位置（根据请求的特征来计算数据具体应该放到哪个节点，例如一致性哈希算法），然后与该节点交互。\n数据可靠性 分布式数据的可靠性是指在出现组件故障的情况下依然能够能提供正常服务的能力。\n复制（Replication） 复制技术是通过将数据复制到多个节点的方式来实现系统的高可靠。 由于同一份数据会被复制到多个节点，这样同一个数据就存在多个副本，因此也称为多副本技术，这样当出现节点故障时就不会影响数据的完整性和可访问性。\n复制技术有两种不同的模式：\n  主从节点复制：即在副本节点中有一个节点是主节点，所有的数据请求先经过主节点。对于一个写数据请求，客户端将请求发送到主节点，主节点将数据复制到从节点，再给客户端应答。\n  无主节点复制：即在集群端并没有一个主节点，副本逻辑在客户端或代理层完成。 当客户端发送一个写数据请求时，客户端会根据策略自行（或者通过代理层）找到副本服务器，并将多个副本发送到副本服务器上。\n  纠删码（Erasure Code） 副本技术的本质就是冗余存储，因此需要消耗很多额外的存储空间。以 3 个副本为例，需要额外消耗 2 倍的存储空间来保证数据的可靠性。虽然副本技术在性能和可靠性方面优势明显，但成本明显比较高。为了降低存储的成本，很多公司采用纠删码技术来保证数据的可靠性。\n纠删码是一种通过校验数据来保证数据可靠性的技术，也就是该技术通过保存额外的一个或多个校验块来提供数据冗余。与副本技术不同，这种数据冗余技术不能通过简单复制来恢复数据，而是经过计算来得到丢失的数据。\n纠删码的基本原理是采用矩阵运算，将 n 个数据转换为 n+m 个数据进行存储。其基本流程如下：\n 校验数据生成：找到一个生成矩阵，通过该矩阵与原始数据的运算可以得到最终要存储的校验数据。 数据恢复：由于生成矩阵是可逆的，因此可利用生成矩阵和剩余可用数据来计算出原始数据。  数据一致性 在分布式文件系统中，由于同一个数据块被放置在不同的节点上，我们无法保证多个节点的数据时时刻刻是相同的，因此会出现一致性的问题。这里的一致性包括两个方面：一个方面是各个节点数据的一致性问题；另一个方面是从客户端访问角度一致性的问题。\n通常来说，我们是无法保证各个节点上数据是完全一致的（故障、宕机、延迟、网络分区等原因），只能保证客户端访问的一致性。为了保证客户端访问数据的一致性，通常需要对存储系统进行特殊的设计，从而在系统层面保证数据的一致性。通常提供的一致性保证有如下两种：\n 强一致性：当数据的写入操作反馈给客户端后，任何对该数据的读操作都会读到刚刚写入的数据。 最终一致性：在执行一个写入操作后，如果没有新的写入操作的情况下， 该写入的数据会最终同步到所有副本节点上，但中间会有时间窗口。  故障与容错 在分布式文件系统中必须要解决设备故障的问题。这是因为在大规模分布式文件系统中设备的总量达到数万个甚至数十万个，设备发生故障就会成为常态。\n设备的故障分为两种类型：\n 临时故障：指短时间可以恢复的故障，如服务器重启、网线松动或交换机掉电等。 永久故障：指设备下线，且永远不会恢复，如硬盘损坏等。  为了应对系统随时出现的故障，分布式文件系统在设计时必须要考虑容错处理。容错主要包括以下几方面内容：\n 故障预测：在故障发生前，预知设备故障，然后有计划地将该设备下线，避免突然下线导致的性能等问题。 故障检测：在故障发生时，及时发现故障原因，方便进行问题的修复。如检测磁盘、通信链路或服务的故障等。 故障恢复：在故障发生后，快速进行响应，保证系统仍然能够对外提供无损的服务。如通过部件冗余、主备链路等。当系统发生故障时，可以通过切换链路，或者通过冗余节点来提供服务。  ","date":"2022-05-23T18:55:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"分布式文件系统"},{"content":"网络文件系统 基本概念 网络文件系统（Network File System）是基于 TCP/IP 协议（整个协议可能会跨层）的文件系统，它可以将远端服务器文件系统的目录挂载到本地文件系统的目录上，允许用户或者应用程序像访问本地文件系统的目录结构一样，访问远端服务器文件系统的目录结构，而无需理会远端服务器文件系统和本地文件系统的具体类型，非常方便地实现了目录和文件在不同机器上进行共享。\n网络文件系统通常分为客户端和服务端，其中客户端类似本地文件系统，差异是在读/写数据时不是访问磁盘等设备，而是通过网络将请求传输到服务端。而服务端则是对数据进行管理的系统，将数据存储到磁盘等存储介质上。\n网络文件系统 VS 本地文件系统 网络文件系统与本地文件系统主要存在以下差异：\n 数据的访问过程。 本地文件系统的数据是持久化存储到磁盘上的，而网络文件系统则需要将数据传输到服务端进行持久化处理。 是否需要格式化。 本地文件系统需要进行格式化处理才可以使用。而网络文件系统则不需要客户端进行格式化操作，通常只需要挂载到客户端就可以直接使用。当然在服务端通常是要做一些配置工作的，包括格式化操作。  网络文件系统最主要的特性是实现了数据的共享。 基于数据共享的特性，使得网络文件系统有很多优势，如增大存储空间的利用效率（降低成本）、方便组织之间共享数据和易于实现系统的高可用等。\n关键原理 由于网络文件系统基于网络实现，其分为本地的客户端与远端的服务端。因此其除了之前提到的本地文件系统相关的技术，还引入文件系统协议、RPC，网络文件锁等机制。\n文件系统协议 网络文件系统本质上是一个基于 C/S（客户端/服务端）架构的应用，其大部分功能是通过客户端与服务端交互来实现的。因此，对于网络文件系统来说，其核心之一是客户端与服务端的交互语言——文件系统协议。\n网络文件系统的协议的定义类似函数调用，包含 ID（可以理解为函数名称），参数和返回值。这里以 NFS v3 协议举例：\n从上图可以看出，协议语义与文件系统操作的语义基本上一一对应，因此客户端对网络文件系统的访问都可以通过协议传输到服务端进行相应的处理。\n远程过程调用（RPC） 由于在客户端与服务端都要实现对协议数据的封装和解析，因此实现起来比较复杂。为了降低复杂性，通常会在文件系统业务层与 TCP/IP 层之间实现一层交互层，这就是 RPC 协议。\nRPC（Remote Procedure Call，远程过程调用） 是 TCP/IP 模型中应用层的网络协议（OSI 模型中会话层的协议）。RPC 协议通过一种类似函数调用的方式实现了客户端对服务端功能的访问，简化了客户端访问服务端功能的复杂度。\nRPC 协议通常架构如下：\n 客户端：文件系统。 服务端：文件系统服务。 存根：定义的函数集。以网络文件系统为例，函数集包括创建文件、删除文件、写数据和读数据等。函数集通常需要分别在客户端和服务端定义一套接口。 RPC 运行时库：实现了 RPC 协议的公共功能，如请求的封装与解析、消息收发和网络层面的错误处理等。  在客户端调用 RPC 函数时，会调用 RPC 库的接口将该函数调用转化为一个网络消息转发到服务端，而服务端的 RPC 库则对网络数据包进行反向解析，调用服务端注册的函数集（存根）中的函数实现功能，最后将执行的结果反馈给客户端。\n对于客户端的应用，这个函数调用与本地函数调用并没有明显的差异。\n网络文件锁 本地文件系统可以在文件系统内实现文件锁。但由于网络文件系统会有多个不同的客户端文件系统访问同一个服务端的文件系统，文件锁是无法在客户端的文件系统中实现的，只能在服务端实现。这样就需要一个协议将客户端的加锁、解锁等请求传输到服务端，并且在服务端维护文件锁的状态。\n在 NFS 协议族中，其通过 NLM（Network Lock Manager） 协议实现了一个网络文件锁服务。由于网络文件锁需要考虑到网络分区、丢包、服务端/客户端宕机等多方面因素，实现要比本地文件锁服务复杂得多，但其核心原理还是与本地文件锁一样：维护一个数据结构，负责记录每个文件的加锁情况。当客户端传来新的加锁请求时查找该结构，判断是否存在锁冲突，来考虑是允许加锁还是报错返回。\n","date":"2022-05-23T18:54:13+08:00","permalink":"https://blog.orekilee.top/p/%E7%BD%91%E7%BB%9C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"网络文件系统"},{"content":"本地文件系统 关键技术 虚拟文件系统 虚拟文件系统（Virtual File System，VFS） 是具体文件系统（如 Ext2、Ext4 和 XFS 等）与应用程序之间的一个接口层，它对 Linux 的每个文件系统的所有细节进行抽象，使得不同的文件系统在 Linux 核心以及系统中运行的其他进程看来都是相同的。以此达到使操作系统能够适配多种文件系统的目的。\nVFS 提供了一个文件系统框架，本地文件系统可以基于 VFS 实现，其主要做了如下几方面的工作：\n 作为抽象层为应用层提供了统一的接口（ read、write 和 chmod 等）。 实现了一些公共的功能，如 Inode 缓存（Inode Cache）和页缓存（Page Cache）等。 规范了具体文件系统应该实现的接口。  基于上述设定，其他具体的文件系统只需要按照 VFS 的约定实现相应的接口及内部逻辑，并注册在系统中，就可以完成文件系统的功能了。当用户调用操作系统提供的文件系统 API 时，会通过软中断的方式调用内核 VFS 实现的函数。\n磁盘空间布局 文件系统的核心功能是实现对磁盘空间的管理。对磁盘空间的管理是指要知道哪些空间被使用了，哪些空间没有被使用。这样，在用户层需要使用磁盘空间时，文件系统就可以从未使用的区域分配磁盘空间。\n为了对磁盘空间进行管理，文件系统往往将磁盘空间通常被划分为元数据区与数据区两个区域。数据区就是存储数据的地方，用户在文件中的数据都会存储在该区域；而元数据区则是对数据区进行管理的地方。\n基于固定功能区的磁盘空间布局 基于固定功能区的磁盘空间布局是指将磁盘的空间按照功能划分为不同的子空间，每种子空间有具体的功能。 以 Linux 中的 Ext 文件系统为例，其空间被划分为数据区和元数据区，而元数据区又被划分为数据块位图、inode 位图和 inode 表等区域。如下图：\n基于功能分区的磁盘空间布局空间职能清晰，便于手动进行丢失数据的恢复。但是由于元数据功能区大小固定，因此容易出现资源不足的情况==（位图长度有限）==。比如，在海量小文件的应用场景下，有可能会出现磁盘剩余空间充足，但 inode 不够用的情况\n基于非固定功能区的磁盘空间布局 在磁盘空间管理中有一种非固定功能区的磁盘空间管理方法。这种方法也分为元数据和数据，但是元数据和数据的区域并非固定的，而是随着文件系统对资源的需求而动态分配的。\n例如比较经典的实现 XFS，其它不是通过固定的位图区域来管理磁盘空间的，而是通过 B+ 树管理磁盘空间。这样做就可以动态的去分配元数据空间，而不是像位图一样具有限制。但是这也带来了新的问题，就是 B+ 树无法根据 inode 的偏移量来确定编号，因此 XFS 又引入复杂的编号机制来解决这个问题。\n基于数据追加的磁盘空间布局 前文介绍的磁盘空间布局方式对于数据的变化都是原地修改的，也就是对于已经分配的逻辑块，当对应的文件数据改动时都是在该逻辑块进行修改的。在文件随机 I/O 比较多的情况下，不太适合使用 SSD 设备，这主要由 SSD 设备的修改和擦写特性所决定。\n有一种基于数据追加的磁盘空间布局方式，也被称为基于日志（Log structured）的磁盘空间布局方式。这种磁盘空间布局方式对数据的变更并非在原地修改，而是以追加写的方式写到后面的剩余空间。这样，所有的随机写都转化为顺序写，非常适合用于 SSD 设备。\n文件数据管理 对于文件系统来说，无论文件是什么格式，存储的是什么内容，它都不关心。文件就是一个线性空间，类似一个大数组。而且文件的空间被文件系统划分为与文件系统块一样大小的若干个逻辑块。文件系统要做的事情就是将文件的逻辑块与磁盘的物理块建立关系。这样当应用访问文件的数据时，文件系统可以找到具体的位置，进行相应的操作。\n基于连续区域的文件数据管理 基于连续区域的文件数据管理方式是一次性为文件分配其所需要的空间，且空间在磁盘上是连续的。 由于文件数据在磁盘上是连续存储的，因此只要知道文件的起始位置所对应的磁盘位置和文件的长度就可以知道文件数据在磁盘上是如何存储的。如下图：\n这种文件数据管理方式的最大缺点是不够灵活，特别是对文件进行追加写操作非常困难。如果该文件后面没有剩余磁盘空间，那么需要先将该文件移动到新的位置，然后才能追加写操作。如果整个磁盘的可用空间没有能够满足要求的空间，那么会导致写入失败。\n除了追加写操作不够灵活，该文件数据管理方式还有另一个缺点就是容易形成碎片空间。由于文件需要占用连续的空间，因此很多小的可用空间就可能无法被使用，从而降低磁盘空间利用率。\n基于链表的文件数据管理 基于链表的文件数据管理方式将磁盘空间划分为大小相等的逻辑块。 在目录项中包含文件名、数据的起始位置和终止位置。在每个数据块的后面用一个指针来指向下一个数据块。如下图：\n这种方式可以有效地解决连续区域的碎片问题，但是对文件的随机读/写却无能为力。这主要是因为在文件的元数据中没有足够的信息描述每块数据的位置。为了实现随机读写，还需要在实现时附加一些额外的机制。\n基于索引的文件数据管理 索引方式的数据管理是指通过索引项来实现对文件内数据的管理。 如下图所示，与文件名称对应的是索引块在磁盘的位置，索引块中存储的并非用户数据，而是索引列表。当读/写数据时，根据文件名可以找到索引块的位置，然后根据索引块中记录的索引项可以找到数据块的位置，并访问数据。\n常见的索引方式有以下两种：\n 基于间接块的文件数据管理：在索引方式中，最为直观、简单的就是对文件的每个逻辑块都有一个对应的索引项，并将索引项用一个数组进行管理。当想要访问文件某个位置的数据时，就可以根据该文件逻辑偏移计算出数组的索引值，然后根据数组的索引值找到索引项，进而找到磁盘上的数据。    基于 Extent 的文件数据管理：在 Extent 文件数据管理方式中，每一个索引项记录的值不是一个数据块的地址，而是数据块的起始地址和长度。\n  缓存 文件系统的缓存（Cache）的作用主要用来解决磁盘访问速度慢的问题。缓存技术是指在内存中存储文件系统的部分数据和元数据而提升文件系统性能的技术。由于内存的访问延时是机械硬盘访问延时的十万分之一，因此采用缓存技术可以大幅提升文件系统的性能。\n文件系统缓存的原理主要还是基于数据访问的时间局部性和空间局部性特性。时间局部性就是如果一块数据之前被访问过，那么最近很有可能会被再次访问。空间局部性则是指在访问某一个区域之后，通常会访问临近的区域。\n缓存置换算法 由于内存的容量要比磁盘的容量小得多，当用户持续写入数据时就会面临缓存不足的情况，此时就涉及如何将缓存数据刷写到磁盘，然后存储新数据的问题。而将缓存数据落盘，并置换新数据的这一过程被称为缓存置换。\n在文件系统中通常会采取以下这些缓存置换算法：\n LRU（Least Recently Used）：LRU 基于时间局部性原理。将最久没有使用的数据淘汰出缓存。因此，当空间满时，最久没有使用的数据即被淘汰。 LFU（Least Frequently Used）：LFU 基于缓存命中频次作为置换依据。如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，将命中频次最少的数据淘汰掉。  预读算法 预读算法是针对读数据的一种缓存算法。预读算法通过识别 I/O 模式方式来提前将数据从磁盘读到缓存中。这样，应用读取数据时就可以直接从缓存读取数据，从而极大地提高读数据的性能。\n通常有两种情况会触发预读操作：\n 当有多个地址连续地读请求时会触发预读操作。 应用访问到有预读标记的缓存时会触发预读操作。这里，预读标记的缓存是在预读操作完成时在缓存页做的标记，当应用读到有该标记的缓存时会触发下一次的预读，从而省略对 I/O 模式的识别。  快照与克隆 快照（Snapshot）和克隆（Clone）技术可以应用于整个文件系统或单个文件。快照技术可以实现文件的可读备份，而克隆技术则可以实现文件的可写备份。针对文件，用的更多的是克隆技术。\n快照 目前快照技术有两种实现方式：\n 写时拷贝（Copy-On-Write，COW）：刚开始创建快照时原始文件和快照文件指向相同的存储区域。当原始文件被修改时，就需要将该位置的原始数据拷贝到新的位置，并且更新快照文件中的地址信息。 写时重定向（Redirect-On-Write，ROW）：当原始文件写数据时并不在原始位置写入数据，而是分配一个新的位置写入。在读取时，创建快照前的数据从源卷读，创建快照后产生的数据，从快照卷读。  可以看出 COW 在写入时需要拷贝一份数据到快照卷的动作，而 ROW 是直接重定向到快照卷写入，所以 ROW 适合写密集型；相反，由于 ROW 不断进行指针重定向，读性能会有较大影响，而 COW 不会，所以 COW 适合读密集型。对文件系统而言，用得最多的是 ROW 方式。\n克隆 克隆技术的原理与快照技术的原理类似，其相同点在于其实现方式依然是 ROW 或 COW，而差异点则主要表现在两个方面：一个方面，克隆生成的克隆文件是可以写的；另一个方面，克隆的数据最终会与原始文件的数据完全隔离。\n日志 在文件系统中，一个简单的写操作在底层可能会分解为多个步骤的修改。如果不能保证操作的原子性，那么倘若出现宕机、断电的情况，此时就会导致数据的丢失、不一致，甚至是文件系统的不可用。为了解决这个问题，在文件系统中引入了日志机制。\n日志通常是一块独立的空间，其采用环形缓冲区的结构，当进行文件修改操作时，相关数据块会被打包成一组操作写入日志空间，再更新实际数据。这里的一组操作被称为一个事务。\n由于实际数据的更新在写入日志之后，如果在数据更新过程中出现了系统崩溃，那么通过读取日志来更新。这样就能保证数据是我们所期望的数据。还有一种异常场景是日志数据刷写过程中。由于此时日志完成标记还没有置位，而且实际数据还没有更新，那么只需要放弃该条日志即可。\n权限管理 在多用户操作系统中，由于多个用户的存在，就必须实现用户之间资源访问的隔离。这种管理用户可访问资源的特性就是权限管理。\nRWX 权限管理 Linux最常用的权限管理就是 RWX-UGO 权限管理。其中，RWX 是 Read、Write 和 eXecute 的缩写。而 UGO 则是 User、Group 和 Other 的缩写。通过该机制建立用户和组实现对文件的不同的访问权限。\n如下图所示：\n当我们对文件进行操作时，首先会调用 inode_permission() 函数来判断执行用户的访问权限，如果权限不足则阻止对应操作的执行。\nACL 权限管理 ACL（Access Control List，访问控制列表），是一个针对文件/目录的访问控制列表。它在 RWX-UGO 权限管理的基础上为文件系统提供一个额外的、更灵活的权限管理机制。ACL 允许给任何用户或用户组设置任何文件/目录的访问权限，这样就能形成网状的交叉访问关系。\n根据是否继承父目录的 ACL 属性，ACL 分为以下两类：\n access ACL：每一个对象（文件/目录）都可以关联一个 ACL 来控制其访问权限，这样的 ACL 被称为 access ACL。 default ACL：目录关联的一种 ACL。当目录具备该属性时，在该目录中创建的对象（文件或子目录）默认具有相同的 ACL。  ACL 在操作系统内部是通过文件的扩展属性实现的。 当用户为文件添加一个 ACL 规则时，其实就是为该文件添加一个扩展属性。如下图：\n需要注意的是，ACL 的数据与文件的普通扩展属性数据存储在相同的位置，只不过通过特殊的标记进行了区分，这样就可以屏蔽普通用户对 ACL 的访问。\nSELinux 权限管理 SELinux（Security-Enhanced Linux）是一个在内核中实现的强制存取控制（MAC）安全性机制。SELinux 与 RWX、ACL 最大的区别是基于访问者（应用程序）与资源的规则，而不是用户与资源的规则，因此其安全性更高。这里基于应用程序与资源的规则是指规定了哪些应用程序可以访问哪些资源，而与运行该应用程序的用户无关。\nSELinux 通过将权限从用户收缩到应用，即使应用被攻破，黑客也只能访问应用程序所限定的资源，而不会扩散到其他地方。\n其原理图如下：\n当访问者访问被访问者（资源）时，需要调用内核的接口，此时会经过 SELinux 内核的判断逻辑，该判断逻辑根据策略数据库的内容确定访问者是否有权访问被访问者，如果允许访问则放行，否则拒绝该请求并记录审计日志\n配额管理 在多用户环境中，不仅要防止用户对其他用户数据的非法访问，还要确保某些用户所使用的存储空间不能太多。在这种情况下，就需要一种配额 （Quota）管理技术。\n配额管理是一种对使用空间进行限制的技术，其主要包括针对用户（或组）的限制和针对目录的限制两种方式。针对用户（或组）的限制是指某一个用户（或组）对该文件系统的空间使用不能超过设置的上限，如果超过上限则无法写入新的数据。针对目录的限制是指该目录中的内容总量不能超过设置的上限。\n在配额管理中，通常涉及三个基本概念：\n 软上限（Soft Limit）：指数据总量可以超过该上限。如果超过该上限则会有一个告警信息。 硬上限（Hard Limit）：指数据总量不可以超过该上限，如果超过该上限则无法写入新的数据。 宽限期（Grace Period）：宽限期通常是针对软上限而言的。如果设置了该值（如 3 天），则在 3 天内允许数据量超过软上限，当超过3天后，无法写入新的数据。  其实现的原理非常简单，当有新的写入请求时，配额模块就会去分析该请求。如果需要计入配额管理中，则进行配额上限的检查，在小于上限的情况下会更新配额管理数据，否则将阻止新的数据写入，并发出告警信息。\n文件锁 文件锁的基本作用就是保证多个进程对某个文件并发访问时数据的一致性。如果没有文件锁，就有可能出现多个进程同时访问文件相同位置数据的问题，从而导致文件数据的不一致性。\n文件锁分为以下两种类型：\n 劝告锁（Advisory Lock）：劝告锁是一种建议性的锁，通过该锁告诉访问者现在该文件被其他进程访问，但不强制阻止访问。 强制锁（Mandatory Lock）：强制锁则是在有进程对文件锁定的情况下，其他进程是无法访问该文件的。  为了提高性能，减少等待锁时间，以上两种锁都引入了共享锁、排他锁机制。\n 共享锁（Shared Lock）：在任意时间内，一个文件的共享锁可以被多个进程拥有，共享锁不会导致系统进程的阻塞。 排他锁（Exclusive Lock）：在任意时间内，一个文件的排他锁只能被一个进程拥有。  当有新的加锁请求到来时，其会与已有的锁信息逐一比对，判断是否存在锁冲突。如果存在，则将该进程置为休眠状态（阻塞，如果为非阻塞则返回直接错误码）。如果不存在，则将锁信息记录下来后允许用户访问。\n","date":"2022-05-23T18:53:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"本地文件系统"},{"content":"文件系统 基础 什么是文件系统？  文件系统是一种存储和组织计算机数据的方法，它使得对其访问和查找变得容易，文件系统使用文件和树形目录的抽象逻辑概念代替了硬盘和光盘等物理设备使用数据块的概念，用户使用文件系统来保存数据不必关心数据实际保存在硬盘（或者光盘）的地址为多少的数据块上，只需要记住这个文件的所属目录和文件名。在写入新数据之前，用户不必关心硬盘上的那个块地址没有被使用，硬盘上的存储空间管理（分配和释放）功能由文件系统自动完成，用户只需要记住数据被写入到了哪个文件中。\n 上文是维基百科对于文件系统的描述，我们将其精炼一下，得到以下结论：\n 文件系统是一套实现了数据的存储、分级组织、访问和获取等操作的抽象数据类型（Abstract data type）。其可以构建在磁盘、内存、甚至是网络上。  用于解决什么问题？  我们为什么要使用文件系统呢？换句话说，文件系统它解决了什么问题呢？\n 文件系统解决的是对磁盘空间使用的问题。 如果我们不抽象出一个文件系统，而是每个应用程序都独立去访问磁盘空间，那会出现什么样的情况呢？\n 磁盘空间的访问会存在冲突。 如果没有一个第三方去统一管理磁盘空间，那么此时应用程序的数据访问并不会进行隔离，而是会互相影响，产生冲突。 磁盘空间的管理会非常复杂。 由于文件有不同格式、大小，存储的设备与位置也有所不同（本地磁盘、内存、甚至远端机器）。直接管理磁盘空间将会非常的复杂。  文件系统正是为了解决这两个问题，而在应用程序与磁盘空间之间抽象出的一层。文件系统对下实现了对磁盘空间的管理，对上为应用程序呈现层级数据组织形式和统一的访问接口。\n基于文件系统，应用程序只需要创建、删除或读取文件即可，他们并不需要关注磁盘空间的细节，所有磁盘空间管理相关的动作交由文件系统来处理。\n相关概念  目录：在文件系统中目录是一种容器，它可以容纳子目录和普通文件。但同时目录本身也是一种文件。只不过目录中存储的数据是特殊的数据，这些数据就是关于文件名称等元数据（管理数据的数据）的信息。 文件：在文件系统中，最基本的概念是文件，文件是存储数据的实体。从用户的角度来看，文件是文件系统中最小粒度的单元。为了便于用户对文件进行识别和管理，文件系统为每个文件都分配了一个名称，称为文件名。同时为了方便用户辨别文件的类型，每个文件都会有一个拓展名来标识其类型。（但在文件系统中都以字节流存储） 链接：Linux 中的链接分为软链接（Soft Link）和硬链接（Hard Link）两种。其中，软链接又被称为符号链接（Symbolic Link），它是文件的另外一种形态，其内容指向另外一个文件路径（相对路径或绝对路径）。硬链接则不同，它是一个已经存在文件的附加名称，也就是同一个文件的第 2 个或第 N 个名称。  基本原理  文件系统的主要核心就是对下实现了对磁盘空间的管理，对上为应用程序呈现层级数据组织形式和统一的访问接口。\n 在文件系统中，为了简化用户对数据的访问，向用户屏蔽数据的存储方式，其会对磁盘空间进行规划、组织和编号处理。\n以 Ext4 文件系统为例，它会将磁盘空间进行划分。 首先将磁盘空间划分为若干个子空间，这些子空间称为块组。然后将每个子空间划分为等份的逻辑块，逻辑块是最小的管理单元。\n为了管理这些逻辑块，需要一些区域来记录哪些逻辑块已经被使用了，哪些还没有被使用。记录这些数据的数据通常在磁盘的特殊区域，我们称这些数据为文件系统的元数据（Metadata）。通过元数据，文件系统实现了对磁盘空间的管理，最终为用户提供了简单易用的接口。\n当我们调用这些接口时，用户对文件的操作就转化为文件系统对磁盘空间的操作。 比如，当用户向某个文件写入数据时，文件系统会将该请求转换为对磁盘的操作，包括分配磁盘空间、写入数据等。而对文件的读操作则转换为定位到磁盘的某个位置、从磁盘读取数据等。\n分类 目前，常见的文件系统有几十种。虽然文件系统的具体实现形式纷繁复杂，具体特性也各不相同，但是有一定规律可循。下面将介绍一下常见的文件系统都有哪些。\n本地文件系统 本地文件系统是对磁盘空间进行管理的文件系统，也是最常见的文件系统形态。从呈现形态上来看，本地文件系统就是一个树形的目录结构。本地文件系统本质上就是实现对磁盘空间的管理，实现磁盘线性空间与目录层级结构的转换。\n从普通用户的角度来说，本地文件系统主要方便了对磁盘空间的使用，降低了使用难度，提高了利用效率。常见的本地文件系统有 ExtX、Btrfs、XFS 和 ZFS 等。\n伪文件系统 伪文件系统是 Linux 中的概念，它是对传统文件系统的延伸。伪文件系统存在于内存，不占用硬盘。它以文件系统的形态实现用户与内核数据交互的接口，其通过文件的形式向用户提供一些系统信息，用户通过读写这些文件就可以读取、修改系统的一些信息。 常见的伪文件系统有 proc、sysfs 和 configfs 等。\n例如我们在 Linux 中通常用来检测性能的一些工具（如 iostat、top 等），其本质上就是通过访问 /proc/cpuinfo、/proc/diskstats、/proc/meminfo 等文件来获取内核信息。\n网络文件系统 网络文件系统（Network File System）是基于 TCP/IP 协议（整个协议可能会跨层）的文件系统，它可以将远端服务器文件系统的目录挂载到本地文件系统的目录上，允许用户或者应用程序像访问本地文件系统的目录结构一样，访问远端服务器文件系统的目录结构，而无需理会远端服务器文件系统和本地文件系统的具体类型，非常方便地实现了目录和文件在不同机器上进行共享。\n网络文件系统通常分为客户端和服务端，其中客户端类似本地文件系统，而服务端则是对数据进行管理的系统。网络文件系统的使用与本地文件系统的使用没有任何差别，只需要执行 mount 命令挂载远端文件系统即可。常见的网络文件系统如 NFS、SMB 等。\n集群文件系统 集群文件系统（Clustered File System）是指运行在多台计算机之上，之间通过某种方式相互通信从而将集群内所有存储空间资源整合、虚拟化并对外提供文件访问服务的文件系统。\n其本质上还是一种本地文件系统，但与 NTFS、EXT 等本地文件系统的目的不同，它通常构建在基于网络的 区域存储网络（SAN） 设备上，且在多个节点中共享 SAN 磁盘。前者是为了扩展性，后者运行在单机环境，纯粹管理块和文件之间的映射以及文件属性。\n集群文件系统最大的特点是可以实现客户端节点对磁盘介质的共同访问，且视图具有一致性。其访问模式如下图：\n其最大的特点是多个节点可以同时为应用层提供文件系统服务，特别适合用于业务多活的场景，通过集群文件系统提供高可用集群机制，避免因为宕机造成服务失效。\n分布式文件系统 分布式文件系统（Distributed File System）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点（可简单的理解为一台计算机）相连；或是若干不同的逻辑磁盘分区或卷标组合在一起而形成的完整的有层次的文件系统。\n从本质上来说，分布式文件系统其实也是网络文件系统的一种，其与网络文件系统的差异在于服务端包含多个节点，也就是服务端是可以横向扩展的。\n","date":"2022-05-23T18:52:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-%E5%9F%BA%E7%A1%80/","title":"文件系统 基础"},{"content":"Git 分支 什么是分支？ Git 保存的不是文件的变化或者差异，而是一系列不同时刻的文件快照。 在进行提交操作时，Git 会保存一个提交对象，该提交对象会包含一个指向暂存内容快照的指针，以及作者的姓名和邮箱、提交时输入的信息以及指向它的父对象的指针。\n 首次提交产生的提交对象没有父对象，普通提交操作产生的提交对象有一个父对象，而由多个分支合并产生的提交对象有多个父对象，\n 假设现在有一个工作目录，里面包含了三个将要被暂存和提交的文件。暂存操作会为每一个文件计算校验和，然后会把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 对象来保存它们），最终将校验和加入到暂存区域等待提交。\n当进行提交操作时，Git 会先计算每一个子目录的校验和，然后将 Git 仓库中这些校验和保存为树对象。 随后 Git 便会创建一个提交对象，它不仅包含上面提到的那些信息，还包含指向这个树对象（项目根目录）的指针。如此一来，Git 就可以在需要的时候重现此次保存的快照。\n如下图，此时Git 仓库中有五个对象：三个 blob 对象（保存着文件快照）、一个树对象（记录着目录结构和 blob 对象索引）以及一个提交对象（包含着指向前述树对象的指针和所有提交信息）。\n做些修改后再次提交，那么这次产生的提交对象会包含一个指向上次提交对象（父对象）的指针。\nGit 的分支，其实本质上仅仅是指向提交对象的可变指针。 Git 的默认分支名字是 master。 在多次提交操作之后， master 分支会在每次的提交操作中自动向前移动，并指向最后一个提交对象。\n分支管理 查看分支 当我们使用 git branch 命令，并不加任何参数时，就可以得到当前所有分支的列表：\n1 2 3  $ git branch master * test   ps：* 代表当前所在的分支，即当前 HEAD 指针所指向的分支。\n我们可以在后面加上这两个参数，来获取到这个列表中已经合并或尚未合并到当前分支的分支。\n1 2  git branch --merged # 查看哪些分支已经合并到当前分支 git branch --no-merged # 查看所有包含未合并工作的分支   我们还可以通过 -v 选项，查看每一个分支的最后一次提交：\n1 2 3  $ git branch -v master f876558 [ahead 1, behind 1] test * test da8f5f4 commit   创建分支 当我们要创建一个分支时，可以使用以下命令：\n1  git branch [branch_name]   例如我们创建一个 testing 分支，这会在当前所在的提交对象上创建一个指针。\n 那么，Git 又是怎么知道当前在哪一个分支上呢？ 也很简单，它有一个名为 HEAD 的特殊指针。\n 此时指针关系如下图：\n想要新建一个分支并同时切换到那个分支上，你可以运行一个带有 -b 参数的 git checkout 命令：\n1 2 3 4  git checkout -b [branch_name] 等价于 git branch [branch_name] git checkout [branch_name]   切换分支 当我们要切换到一个已经存在的分支上时，可以使用下面这个命令将 HEAD 指向该分支：\n1  git checkout [branch_name]   例如我们切换到 testing 分支上，此时指向关系如下图：\n如果我们在这个新分支上做了修改，并将修改提交后，此时 testing 分支就会向前移动，如下图：\n此时由于两个分支的版本不同，当我们再次切换回 master 分支时，此时会执行两个操作：\n 使 HEAD 指向 master。 将工作目录恢复成 master 分支所指向的快照内容。  如果我们再次在 master 分支上进行修改后提交，此时两个分支就会完全分叉，走上不同的路线，如下图：\n此时我们就需要通过 merge 来对分支进行合并，使其重新回到统一的一个版本。\n合并分支 当我们在特性分支上完成开发后，此时就需要将特性分支合并入 master 分支。此时我们就需要先切换到主分支，在主分支上执行 git merge：\n1 2  git checkout master git merge [branch]   Git 会自行决定选取哪一个提交作为最优的共同祖先，并以此作为合并的基础。接着它会把两个分支的最新快照以及二者最近的共同祖先进行三方合并，合并的结果是生成一个新的快照（并提交）。\n但事实上，只有在这些分支没有任何冲突时才能这么顺利的进行合并，如果存在冲突，我们必须要将冲突的内容解决后，才能进行合并。\n合并冲突解决 如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法合并它们。此时 Git 会暂停下来，等待你去解决合并产生的冲突。\n1 2 3 4 5  $ git merge test Removing test.md Auto-merging README.md CONFLICT (content): Merge conflict in README.md Automatic merge failed; fix conflicts and then commit the result.   此时我们可以使用 git status 命令来查看那些因包含合并冲突而处于未合并状态的文件，并查看它的内容：\n1 2 3 4 5  \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD Hello world123123:` ======= HELLO WORLD !!! \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; test   此时 Git 用 ======= 分割了两个分支的冲突的内容，\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; 后面显示了 HEAD 所指向的版本，\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 后面展示了 test 指向的版本。此时我们就需要进行权衡，选择应该怎么样去解决冲突。\n当我们将 =======、\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;、\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 删除后，并对冲突内容进行解决。此时我们可以对每个文件使用 git add 命令来将其标记为冲突已解决。 一旦暂存这些原本有冲突的文件，Git 就会将它们标记为冲突已解决。此时我们再进行 git commit，就会自动完成 merge 操作。\n删除分支 当我们想要删除某个分支的时候，可以使用下面这个命令：\n1  git branch -d [branch name]   如果该分支还未被 merge，则此时会报错\n1 2 3  $ git branch -d testing error: The branch \u0026#39;testing\u0026#39; is not fully merged. If you are sure you want to delete it, run \u0026#39;git branch -D testing\u0026#39;.   如果仍然需要删除，则可以使用 -D 选项强制删除分支。\n分支开发的工作流程 长期分支 在项目的开发过程中，为了维护不同层次的稳定性，通常会拥有几个长期存在的分支。大多数人采用下面这种开发模式，只在 master 分支上保留完全稳定的代码，而在 develop、next 等分支上进行后续的开发，当这些分支通过测试，达到稳定状态时就可以将它们并入到 master 分支中。\n如上图，稳定分支的指针总是在提交历史中落后一大截，而前沿分支的指针往往比较靠前。\n特性分支 特性分支是一种短期分支，它被用来实现单一特性或其相关工作。例如我们需要快速实现一个新功能，或者修复一些遗留的问题，就可以在特性分支上进行开发，然后将其并入主分支后再将其删除。\n由于工作被分散到不同的流水线中，在不同的流水线中每个分支都仅与其目标特性相关，因此，在做代码审查之类的工作的时候就能更加容易地看出你做了哪些改动。 我们可以把做出的改动在特性分支中保留几分钟、几天甚至几个月，等它们成熟之后再合并，而不用在乎它们建立的顺序或工作进度。对于不想要的分支，也可以直接将其抛弃，并且不会造成任何影响。\n远程分支 远程引用是对远程仓库的引用（指针），包括分支、标签等等。 可以通过 git ls-remote (remote) 来显式地获得远程引用的完整列表，或者通过 git remote show (remote) 获得远程分支的更多信息。 然而，一个更常见的做法是利用远程跟踪分支。\n远程跟踪分支是远程分支状态的引用。 它们是你不能移动的本地引用，当你做任何网络通信操作时，它们会自动移动。 远程跟踪分支像是你上次连接到远程仓库时，那些分支所处状态的书签。它们以 (remote)/(branch) 形式命名。\n如上图，例如我们从 Git 服务器上 git clone 一个仓库时，会为自动将其该仓库命名为 origin，拉取它的所有数据，创建一个指向它的 master 分支的指针，并且在本地将其命名为 origin/master。 Git 也会给你一个与 origin 的 master 分支在指向同一个地方的本地 master 分支，这样我们就可以在本地进行开发工作。\n推送 当你想要公开分享一个分支时，需要将其推送到有写入权限的远程仓库上，此时可以使用以下命令：\n1 2 3  git push [remote] [branch] # 如果想要修改远程仓库的分支名，可以使用下面这个命令 git push [remote] [local_branch_name]:[remote_branch_name]   跟踪 从一个远程跟踪分支检出一个本地分支会自动创建一个跟踪分支。 跟踪分支是与远程分支有直接关系的本地分支。 如果在一个跟踪分支上输入 git pull，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。（例如当克隆一个仓库时，自动创建的跟踪 origin/master 的 master 分支）。\n如果我们想要设置一个新的跟踪分支，可以使用下面这个命令：\n1  git checkout -b [branch] [remote]/[branch]   如果我们想要让已有的本地分支跟踪一个远程分支，或者是修改已有分支的跟踪目标，就可以使用下面这个命令：\n1 2 3  git branch -u [remote]/[branch] 或者 git branch --set-upstream-to [remote]/[branch]   如果我们想查看设置的所有跟踪分支，可以使用下面这个命令：\n1  git branch -vv   这些数字的值来自于你从每个服务器上最后一次抓取的数据。 这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。\n1 2  git fetch --all git branch -vv   拉取 如果我们想要同步远程分支上的数据时，可以使用下面两种方式抓取本地没有的数据，并且更新本地数据库，移动 remote/branch 指针指向新的、更新后的位置。\n  git fetch ：该命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。 它只会获取数据然后让你自己使用 git merge 进行合并。\n1 2 3 4 5 6 7 8  # 下载远端分支到本地 git fetch [remote] [remote_branch]:[local_branch] # 比较差异 git diff local_branch # 合并分支 git merge local_branch # 旧删除分支 git branch -d local_branch     git pull：它其实相当于 git fetch + git merge，它会查找当前分支所跟踪的服务器与分支，从服务器上抓取数据然后尝试合并入那个远程分支。\n1  git pull [remote] [remote_branch]:[local_branch]     虽然 git pull 简化了流程，但由于 git pull 会对本地工作目录造成修改，所以通常都会使用 git fetch 来拉取分支。\n删除 当我们在某个分支上已经完成了开发，并将其合并到了远程仓库的 master 分支后，此时该分支就失去了作用，可以将其从服务器上删除。此时我们可以使用带有 --delete 选项的 git push 命令来删除一个远程分支：\n1  git push [remote] --delete [branch]   这个命令做的只是从服务器上移除这个指针。 Git 服务器通常会保留数据一段时间直到垃圾回收运行，所以如果不小心删除掉了，通常是很容易恢复的。\nRebase 在 Git 中整合不同分支的方法除了 merge 以外，还有 rebase。\n什么是 Rebase？ 例如下图这个场景，此时出现了两个不同的分支，并且各自又提交了更新，导致出现了分叉：\n如果我们要整合这两个分支，最简单的方法就是使用 merge，它会两个分支的最新快照（C3 和 C4）以及二者最近的共同祖先（C2）进行三方合并，合并的结果是生成一个新的快照（并提交）。\n但除此之外，还有另一种方法，即提取在 C4 中引入的补丁和修改，然后在 C3 的基础上应用一次。在 Git 中，这种操作就叫做 rebase（变基）。 你可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上。\n1 2 3  git rebase [topicbranch] # 此时basebranch为当前HEAD指向的分支 or git rebase [basebranch] [topicbranch]   rebase 的原理是首先找到这两个分支的最近共同祖先，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底，最后以此将之前另存为临时文件的修改依序应用。\n例如执行下面这个命令：\n1 2 3 4  git checkout experiment git rebase master 或者 git rebase master experiment # 等价于前两句，将master变基到experiment上，避免切换分支   此时就会将 C4 中的修改变基到 C3 上，如下图：\n此时 experiment 与 master 处于同一条之间中，我们回到 master 分支上执行一次快进合并，即可再次回到一条直线：\n1 2  git checkout master git merge experiment   此时就已经完成了分支的合并，不仅和 merge 的效果相同，而且还是得提交历史更加整洁，是一条没有分叉的直线。\n因此当我们需要确保在向远程分支推送时能保持提交历史的整洁时就可以使用 rebase，我们在自己的分支开发完毕后将代码 rebase 到 master 分支上，master 在进行快进合并即可整合分支。\n 除了直接 rebase 一个分支，rebase 还支持更细粒度的操作\n 1 2 3  git rebase --onto [branch] [from] [to] # from 待合并片段的起始commitId（不包含） # to 待合并片段的结束commitId（包含）   即取出 to 分支，找出处于 from 分支和 to 分支的共同祖先之后的修改，然后把它们在 base 分支上重放一遍。 以下图为例，执行 git rebase --onto master server client，此时会将 client 的 c8 和 c9 在 master 上重放：\nRebase 的风险 如果要使用 rebase，就必须记住这条原则：只对尚未推送或分享给别人的本地修改执行变基操作清理历史，从不对已推送至别处的提交执行变基操作。\n假设这样一个场景，我们与项目的其他成员在同一个远程仓库中进行协作开发，此时我们拉取下来项目，并在其基础上进行开发，如图：\n此时用户 A 也在该仓库进行开发，它首先提交了修改 c4 和 c5，并将它们 merge 得到 c6。此时我们使用 pull 将这些修改拉取到本地，此时提交历史如下图：\n此时用户 A 对 merge 不满意，他想通过 rebase 来使提交记录更加清晰，于是将 merge 回滚后改为使用 rebase，并使用 git push --force 强制覆盖了提交历史。此时提交记录如下：\n如果我们再次使用 pull 拉取数据，此时提交历史如下：\n此时就出现了意料之外的情况：\n 对我们而言，我们将相同的内容又合并了一次，生成了一个新的提交，并且用户 A 使用了 rebase，因此这两次提交的 log 中作者、日期、日志等信息是一模一样的。 对于用户 A 而言，如果我们此时将修改 push 到远程仓库中，又再次将用户 A 丢弃的 c4 和 c6 提交记录找回，而这又违背了用户 A 简化提交历史的初衷。   那在这种情况下，有什么方法能解决这个问题呢？\n 可以使用 git pull --rebase 命令而不是直接 git pull。 又或者可以自己手动完成这个过程，先 git fetch，再 git rebase teamone/master。但这种方法需要每一个人都执行该命令，因此我们通常把变基命令当作是在推送前清理提交使之整洁的工具，并且只在从未推送至共用仓库的提交上执行变基命令，以避免这种场景的出现。\n","date":"2022-05-23T18:39:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%88%86%E6%94%AF/","title":"Git 分支"},{"content":"Git 基础 Git 的安装与配置 这里以 CentOS 8 举例。\n首先使用 yum 安装 Git\n1  sudo yum install git   检查是否安装成功并查看版本号\n1  git --version   接着开始配置用户信息\n1 2 3 4 5  # 配置用户名 git config --global user.name \u0026#34;yourname\u0026#34; # 配置邮箱 git config --global user.email \u0026#34;yourname@email.com\u0026#34;   接着列出所有配置，查看是否配置成功\n1  git config --list   获取仓库 获取 Git 项目仓库的方法有两种：\n 在现有项目或目录下导入所有文件到 Git 中； 从一个服务器克隆一个现有的 Git 仓库。  初始化新仓库 如果打算使用当前已有的项目来初始化一个新仓库，你只需要进入该项目目录并输入：\n1  git init   该命令将创建一个名为 .git 的子目录，这个子目录含有你初始化的 Git 仓库中所有的必须文件。接下来使用 git add 来跟踪已有的文件，再使用 git commit 来进行提交，就可以完成当前仓库的构建。\n克隆已有仓库 如果想获得一份已经存在了的 Git 仓库的拷贝，可以使用下面这个命令：\n1  git clone [url] (可选，本地仓库名称)   其会在当前目录下创建一个与项目同名（默认）的目录，并在这个目录下初始化一个 .git 文件夹，从远程仓库拉取下所有数据放入 .git 文件夹，然后从中读取最新版本的文件的拷贝。此时所有的项目文件已经存放在本地仓库中，准备就绪等待后续的开发和使用。\n Git 克隆的是该 Git 仓库服务器上的几乎所有数据，而不是仅仅复制完成你的工作所需要文件。 当你执行 git clone 命令的时候，默认配置下远程 Git 仓库中的每一个文件的每一个版本都将被拉取下来。\n 记录更新至仓库 查看文件状态 当我们要查看文件的状态时，可以使用 git status 命令。例如：\n1 2 3 4 5 6 7 8 9  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) test2.cpp   我们可以看到，其说明了我们当前所处的分支，以及展示了当前未被跟踪的文件，以及暂存区中待提交的文件。\n我们还可以在后面加上 -s 或者 --short 参数来使输出更加简洁\n1 2 3 4  $ git status -s M README.md A test.cpp ?? test2.cpp   左边的状态码的含义如下；\n  新添加的未跟踪文件前面有 ?? 标记。\n  新添加到暂存区中的文件前面有 A 标记。\n  修改过的文件前面有 M 标记。出现在右边的 M 表示该文件被修改了但是还没放入暂存区，出现在靠左边的 M 表示该文件被修改了并放入了暂存区。\n  跟踪新文件  git add 是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。\n 我们可以使用命令 git add 来跟踪一个文件，例如：\n1  git add test.cpp    git add 命令使用文件或目录的路径作为参数；如果参数是目录的路径，该命令将递归地跟踪该目录下的所有文件。\n 此时用 git status 就可以看到文件已经被跟踪，并放入暂存区中：\n1 2 3 4 5  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp   暂存已修改文件 如果我们修改了一个已经被跟踪的文件，此时它的状态如下：\n1 2 3 4 5 6 7 8 9 10  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: README.md   此时文件内容发生了变化，但是还没有被放入暂存区中，此时就需要我们使用 git add 将其添加到暂存区\n1 2 3 4 5 6 7  $ git add README.md $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp   此时文件已被加入到暂存区中，在下次使用 git commit 提交时就会被添加到仓库。但是，如果我们此时修改一个暂存区中的文件，会怎么样呢？\n1 2 3 4 5 6 7 8 9 10 11  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: README.md   此时看起来像是文件同时出现在了暂存区和非暂存区，但事实是这样吗？答案是否定的，git 中有版本的概念，此时存在暂存区中的是我们上一次提交的那个版本，而此时在非暂存区中的即是最新的版本。\n此时，我们就需要再次使用 git add 来将最新的版本暂存起来\n1 2 3 4 5 6 7  $ git add README.md $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp   忽略文件 文件 .gitignore 的格式规范如下：\n 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。   glob 模式是指 shell 所使用的简化了的正则表达式。\n 星号（*）匹配零个或多个任意字符； [abc] 匹配任何一个列在方括号中的字符； 问号（?）只匹配一个任意字符； 如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（*) 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。   例如一个 C++ 的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # Prerequisites *.d # Compiled Object files *.slo *.lo *.o *.obj # Precompiled Headers *.gch *.pch # Compiled Dynamic libraries *.so *.dylib *.dll # Fortran module files *.mod *.smod # Compiled Static libraries *.lai *.la *.a *.lib # Executables *.exe *.out *.app    GitHub 有一个十分详细的针对数十种项目及语言的 .gitignore 文件列表：https://github.com/github/gitignore 。\n 差异比较 git diff 用于比较已写入暂存区和已经被修改但尚未写入暂存区文件的区别。\n例如此时我们修改了一个已写入暂存区的文件，此时结果如下：\n1 2 3 4 5 6 7 8  $ git diff diff --git a/README.md b/README.md index d0e7d60..a261b14 100644 --- a/README.md +++ b/README.md @@ -1 +1 @@ -HELLO WORLD +HELLO WORLD !!!   如果要查看已暂存的将要添加到下次提交里的内容，可以用下面这些命令：\n1 2 3  git diff --cached\t// 老版本 或者 git diff --staged // 新版本   请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。\n提交更新 如果暂存区中的内容已经准备就绪了，此时就可以使用 git commit 来提交更新，例如：\n1 2  $ git commit -m \u0026#34;commit\u0026#34; # -m 用于将提交信息与命令放在同一行   提交完成后，它会告诉你当前是在哪个分支提交的，本次提交的完整 SHA-1 校验和是什么，以及在本次提交中，有多少文件修订过，多少行添加和删改过。\n1 2 3  [test da8f5f4] commit 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 test.md   跳过使用暂存区域 如果觉得每次提交前都要将已跟踪的文件存入暂存区过于麻烦，可以为 git commit 加上一个 -a 参数，例如：\n1 2 3 4 5 6 7  $ git commit -a -m README.md [test fa38827] README.md 1 file changed, 1 insertion(+) $ git status On branch test nothing to commit, working tree clean   此时 Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。\n删除文件 如果我们想要从 Git 中删除一个文件，就必须要将其从已跟踪文件清单移除，然后 commit 提交。\n我们可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。\n1 2 3 4 5 6 7 8  $ git rm test.md rm \u0026#39;test.md\u0026#39; $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) deleted: test.md    如果我们直接删除文件，会怎么样呢？\n 1 2 3 4 5 6 7 8 9  $ rm test.md $ git status On branch test Changes not staged for commit: (use \u0026#34;git add/rm \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) deleted: test.md no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;)   此时这个操作会被加入未暂存清单中，我们仍然需要使用 git rm 来进行删除。\n 如果只是想从仓库中删除文件（或者暂存区），而不想在本地删除，那该怎么做呢？\n 此时可以使用 –cached 参数：\n1  git rm --cached [file]    如果在删除前，这个文件就已经在暂存区中，那该怎么办呢？\n 1 2 3 4 5  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: test.md   此时文件已经存储在暂存区了，我们尝试使用 git rm 删除它：\n1 2 3 4  $ git rm test.md error: the following file has changes staged in the index: test.md (use --cached to keep the file, or -f to force removal)   此时我们发现其无法直接被删除，原因是 Git 为了防止误删还没有添加到快照中的数据（这种数据无法被 Git 恢复），为其添加了安全策略，需要使用强制删除的选项 -f 才行。\n1  git rm -rf test.md   移动/重命名文件 当我们想要将一个文件移动到其他目录时，可以使用下面命令：\n1  git mv [源文件] [新路径]   同样，我们也可以使用 git mv 来完成文件的重命名，例如：\n1  git mv [原文件名] [新文件名]   那它是如何做到既能移动，又能够重命名呢？其实运行 git mv 就相当于运行了下面三条命令：\n1 2 3  mv [原文件名] [新文件名] git rm [原文件名] git add [新文件名]   此操作必须要在暂存区或者文件commit之后才能进行\n查看提交记录 当我们想要查看提交记录时，可以使用 git log 命令，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  $ git log commit af1248beeb9de2b58d339441c5fccc2cbd652374 (HEAD -\u0026gt; master, origin/master, origin/HEAD) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 11 14:43:39 2022 +0800 update commit 1801c1003b9250e7854eec43dd53c4974b5b04b2 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Sat Apr 9 18:13:42 2022 +0800 C++ 20 done commit 11ed22aab5099608427492711c560a2c967ba7d3 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Fri Apr 8 21:34:39 2022 +0800 Cpp 17 done 20 doing commit 9d519fa0ada5991fc8791012d98c736704ee3b68 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Thu Apr 7 21:49:42 2022 +0800   我们可以看到，其默认以提交时间降序排序，同时列出了每个提交的 SHA-1 校验和、作者和邮箱、提交时间以及提交说明。\n其常用选项如下：\n   选项 说明     -p 按补丁格式显示每个更新之间的差异。   \u0026ndash;stat 显示每次更新的文件修改统计信息。   \u0026ndash;shortstat 只显示 \u0026ndash;stat 中最后的行数修改添加移除统计。   \u0026ndash;name-only 仅在提交信息后显示已修改的文件清单。   \u0026ndash;name-status 显示新增、修改、删除的文件清单。   \u0026ndash;abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。   \u0026ndash;relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。   \u0026ndash;graph 显示 ASCII 图形表示的分支合并历史。   \u0026ndash;pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。    其常用的限制输出的参数如下：\n   选项 说明     -(n) 仅显示最近的 n 条提交   \u0026ndash;since, \u0026ndash;after 仅显示指定时间之后的提交。   \u0026ndash;until, \u0026ndash;before 仅显示指定时间之前的提交。   \u0026ndash;author 仅显示指定作者相关的提交。   \u0026ndash;committer 仅显示指定提交者相关的提交。   \u0026ndash;grep 仅显示含指定关键字的提交   -S 仅显示添加或移除了某个关键字的提交    撤销操作 补充文件/信息 如果我们已经 git commit 提交之后，才发现可能有几个文件忘记提交，又或者是提交的信息写错了。这时可以使用下列命令来尝试重新提交\n1  git commit --amend   这个命令会将暂存区中的文件提交。 如果自上次提交以来你还未做任何修改，那么快照会保持不变，而你所修改的只是提交信息。而如果进行了修改，则第二次提交将代替第一次提交的结果。\n撤销暂存的文件 如果我们不小心使用 git add 将不必要的文件加入暂存区，如何将这些文件撤销呢？这时候就要用到下述命令：\n1 2 3  git reset HEAD \u0026lt;file\u0026gt;...\t// 老版本 或者 git restore --staged \u0026lt;file\u0026gt;... // 新版本   在调用时加上 \u0026ndash;hard 选项会令 git reset 成为一个危险的命令，即可能导致工作目录中所有当前进度丢失。\n撤销对文件的修改 当我们对一个文件进行修改后，如果不满意刚才的修改，像将其回滚到上次提交的状态时，可以使用如下命令：\n1 2 3  git checkout -- \u0026lt;file\u0026gt;...\t// 老版本 或者 git restore \u0026lt;file\u0026gt;... // 新版本   git checkout -- [file] 是一个危险的命令。 你对那个文件做的任何修改都会消失， 除非确定不想要那个文件，否则不要使用这个命令。\n远程仓库 查看远程仓库 如果想查看你已经配置的远程仓库服务器，可以运行 git remote 命令。 它会列出你指定的每一个远程服务器的简写。也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。例如：\n1 2 3  $ git remote -v origin https://gitee.com/HONGYU-LEE/git-tes.git (fetch) origin https://gitee.com/HONGYU-LEE/git-tes.git (push)   如果还想要查看某一个远程仓库的更多信息，可以使用 git remote show [remote-name] 命令。例如：\n1 2 3 4 5 6 7 8 9 10 11  $ git remote show origin * remote origin Fetch URL: https://gitee.com/HONGYU-LEE/git-tes.git Push URL: https://gitee.com/HONGYU-LEE/git-tes.git HEAD branch: master Remote branch: master tracked Local branch configured for \u0026#39;git pull\u0026#39;: master merges with remote master Local ref configured for \u0026#39;git push\u0026#39;: master pushes to master (local out of date)   它同样会列出远程仓库的 URL 与跟踪分支的信息，并且告诉你正处于 master 分支，并且如果运行 git pull，就会抓取所有的远程引用，然后将远程 master 分支合并到本地 master 分支。 它也会列出拉取到的所有远程引用。\n添加远程仓库并拉取数据 可以通过以下命令来添加一个新的远程 Git 仓库：\n1  git remote add \u0026lt;shortname\u0026gt; \u0026lt;url\u0026gt;    如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写。\n 从远程仓库中拉取数据 使用下面这个命令，就可以从远端仓库中获得数据：\n1  git fetch [remote-name]   这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。\n虽然 git fetch 命令会将数据拉取到你的本地仓库，但是它并不会自动合并或修改你当前的工作，所以在准备好时必须手动将其合并入你的工作。\n如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。\n推送到远程仓库 如果我们想分享项目时，必须将其推送到上游。此时可以使用下面这个命令：\n1  git push [remote-name] [branch-name]   只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。\n远端仓库移除/重命名 如果想要修改一个远程仓库的简写名，可以使用以下命令：\n1  git remote rename [old_name] [new_name]   值得注意的是这同样也会修改你的远程分支名字。 例如那些过去引用 old_name/master 的现在会引用 new_name/master。\n如果因为一些原因想要移除一个远程仓库，可以使用以下命令：\n1  git remote rm [remote-name]   标签（Tag） 与其他版本控制系统一样，Git 可以给历史中的某一个提交打上标签，以示重要。 Git 使用两种主要类型的标签：轻量标签（lightweight）与附注标签（annotated）。\n查看标签 在 Git 中列出已有的标签是非常简单直观的， 只需要输入 git tag，就会以字典序排列出所有的标签：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  $ git tag 1.21 1.22 1.23 v1.10 v1.11 v1.12 v1.13 v1.14 v1.15 v1.16 v1.17 v1.18 v1.19 v1.20 v1.3 v1.4 v1.5 v1.6 v1.7 v1.8 v1.9   也可以使用特定的模式查找标签，例如想查找 v1.10 ~ v1.13 版本的标签，则可以使用正则表达式：\n1 2 3 4 5  $ git tag -l \u0026#39;v1.1[0-3]\u0026#39; v1.10 v1.11 v1.12 v1.13   附注标签 附注标签是存储在 Git 数据库中的一个完整对象。 它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。\n 通常建议创建附注标签，这样你可以拥有以上所有信息；但是如果你只是想用一个临时的标签，或者因为某些原因不想要保存那些信息，轻量标签也是可用的。\n 它的创建也非常简单，只需要加上参数 -a 即可：\n1  $ git tag -a v1.0 -m \u0026#39;version 1.0\u0026#39;   如果是想要对过去的提交打上标签，只需要在末尾指定已提交的校验和（部分校验和也可以，会自动识别）即可：\n1  $ git tag -a v0.8 fa38827eb9d27a2559e8178aa67189105d882fca   此时查看 git tag，发现已经给那个位置追加上了标签\n1 2 3 4  $ git tag v0.8 v1.0 v1.0-test   通过使用 git show 命令可以看到标签信息与对应的提交信息：\n1 2 3 4 5 6 7 8 9 10  $ git show commit da8f5f463cad799940543c7304df4d436a9efb2d (HEAD -\u0026gt; test, tag: v1.0) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 18 17:35:08 2022 +0800 commit diff --git a/test.md b/test.md deleted file mode 100644 index e69de29..0000000   输出显示了打标签者的信息、打标签的日期时间、附注信息，然后显示具体的提交信息。\n轻量标签 轻量标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。 本质上是将提交校验和存储到一个文件中（没有保存任何其他信息）。\n创建一个轻量标签时只需要使用 git tag，而不需要加别的参数，例如：\n1 2 3 4 5 6 7 8 9 10 11  $ git tag v1.0-test $ git show commit da8f5f463cad799940543c7304df4d436a9efb2d (HEAD -\u0026gt; test, tag: v1.0-test, tag: v1.0) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 18 17:35:08 2022 +0800 commit diff --git a/test.md b/test.md deleted file mode 100644 index e69de29..0000000   此时不会看到额外的标签信息， 命令只会显示出提交信息。\n发布标签 默认情况下，git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。\n1  git push origin [tagname]   如果想要一次性推送很多标签，也可以使用带有 --tags 选项的 git push 命令。 这将会把所有不在远程仓库服务器上的标签全部传送到那里。\n1  git push origin --tags   Tag 与 Commit Git 的标签虽然是版本库的快照，但其实它就是指向某个 commit 的指针，但是它与 commit 又有些不同（分支可以移动，标签不能移动）。\n那么为什么有了 commit 后，还要引入 tag 呢？\n \u0026ldquo;请把上周一的那个版本打包发布，commit号是6a5819e…\u0026rdquo;\n\u0026ldquo;一串乱七八糟的数字不好找！\u0026rdquo;\n如果换一个办法：\n\u0026ldquo;请把上周一的那个版本打包发布，版本号是v1.2\u0026rdquo;\n\u0026ldquo;好的，按照tag v1.2查找commit就行！\u0026rdquo;\n所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。\n ","date":"2022-05-23T18:38:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"Git 基本操作"},{"content":"Git 介绍 Git 是什么？ Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。也是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。\n与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。\n版本控制 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。\n本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。 这么做唯一的好处就是简单，但是特别容易犯错。有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。\n为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。\n这时人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 为了解决这个问题，集中化的版本控制系统（Centralized Version Control Systems，CVCS）应运而生。\n集中化版本控制系统 这类系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 多年以来，这已成为版本控制系统的标准做法。\n但此时人们发现，集中化的版本控制系统存在单点问题，如果中央服务器宕机，则在宕机的这段时间所有人都无法提交更新，也就无法协同工作。而如果数据库发生损坏，而有没有及时备份时，所有的数据都将会丢失，只剩下人们在各自机器上保留的单独快照。\n为了解决这个问题，分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。\n分布式版本控制系统 在这类系统中，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。\n更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。 你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。\nGit 的特性   直接记录快照，而非差异比较：Git 把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。\n  近乎所有操作都是本地执行：在 Git 中的绝大多数操作都只需要访问本地文件和资源，一般不需要来自网络上其它计算机的信息。\n  完整性保证：Git 中所有数据在存储前都计算校验和（SHA-1），然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。\n  一般只添加数据：你执行的 Git 操作，几乎只往 Git 数据库中增加数据。 很难让 Git 执行任何不可逆操作，或者让它以任何方式清除数据。\n  Git 状态与执行流程 文件状态 Git 中文件有四种状态：\n 未跟踪（Untracked）：标识未被纳入版本控制的文件，它们既不存在于上次快照的记录中，也没有放入暂存区。除此状态之外的所有状态都是已跟踪。 未修改（Unmodified）：表示数据已经安全的提交，并且从上次提交到现在都没有被修改过。 已修改（Modified）：表示自上次提交后修改了文件，但还没保存到数据库中。 已暂存（Staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。  状态转移图如下：\n工作区域 Git 中存在三个工作区域：Git 仓库、工作目录以及暂存区域。\n  Repository（Git 仓库）：是 Git 用来保存项目的元数据和对象数据库的地方。 这是 Git 中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。\n  Working Directory（工作目录）：是对项目的某个版本独立提取出来的内容。 这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。\n  Staging Area（暂存区）：是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。 有时候也被称作`‘索引’\u0026rsquo;，不过一般说法还是叫暂存区域。\n  工作流程 对应上面提到的状态，我们来根据一个 Git 文件的工作流程，来分析一个 Git 文件的状态变化以及所处区域：\n 当我们在工作目录下创建一个新文件，此时文件的状态是未跟踪。 我们使用 git add 将文件加入跟踪列表，此时文件的状态变为已暂存，同时被放入暂存区中。 此时我们使用 git commit 将文件进行提交，此时文件被保存到 Git 仓库中，状态变化为未修改。 此时我们再次修改文件，文件状态变为已修改，同时处于未暂存区（工作目录）。  Git VS SVN 我们常用 SVN 与其进行对比，那么它们有什么区别呢？\n Git 是分布式的，SVN 不是：这是 Git 和其它非分布式的版本控制系统，例如 SVN，CVS 等，最核心的区别。 Git 把内容按元数据方式存储，而 SVN 是按文件：所有的资源控制系统都是把文件的元信息隐藏在一个类似 .svn、.cvs 等的文件夹里。 Git 分支和 SVN 的分支不同：分支在 SVN 中一点都不特别，其实它就是版本库中的另外一个目录。 Git 没有一个全局的版本号，而 SVN 有：目前为止这是跟 SVN 相比 Git 缺少的最大的一个特征。 Git 的内容完整性要优于 SVN：Git 的内容存储使用的是 SHA-1 哈希算法。这能确保代码内容的完整性，确保在遇到磁盘故障和网络问题时降低对版本库的破坏。  ","date":"2022-05-23T18:34:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%9F%BA%E7%A1%80/","title":"Git 基础"},{"content":"Distributed 引擎 Distributed 表引擎是分布式表的代名词，它自身不存储任何数据，而是作为数据分片的透明代理，能够自动路由数据至集群中的各个节点，所以 Distributed 表引擎需要和其他数据表引擎一起协同工作。\nClickHouse 并不像其他分布式系统那样，拥有高度自动化的分片功能。ClickHouse 提供了**本地表（Local Table）与分布式表（Distributed Table）**的概念\n 本地表：通常以 _local 为后缀进行命名。本地表是承接数据的载体，可以使用非 Distributed 的任意表引擎，一张本地表对应了一个数据分片。 分布式表：通常以 _all 为后缀进行命名。分布式表只能使用 Distributed 表引擎，它与本地表形成一对多的映射关系，日后将通过分布式表代理操作多张本地表。  分布式写入流程 在向集群内的分片写入数据时，通常有两种思路\n  借助外部计算系统，事先将数据均匀分片，再借由计算系统直接将数据写入 ClickHouse 集群的各个本地表。\n  通过 Distributed 表引擎代理写入分片数据。\n  第一种方案通常拥有更好的写入性能，因为分片数据是被并行点对点写入的。但是这种方案的实现主要依赖于外部系统，而不在于 ClickHouse 自身，所以这里主要会介绍第二种思路。为了便于理解整个过程，这里会将分片写入、副本复制拆分成两个部分进行讲解。\n数据写入分片   在第一个分片节点写入本地分片数据：首先在 CH5 节点，对分布式表 test_shard_2_all 执行 INSERT，尝试写入 10、30、200 和 55 四行数据。执行之后分布式表主要会做两件事情：\n  根据分片规则划分数据\n  将属于当前分片的数据直接写入本地表 test_shard_2_local。\n    第一个分片建立远端连接，准备发送远端分片数据：将归至远端分片的数据以分区为单位，分别写入 /test_shard_2_all存储目录下的临时 bin 文件，接着，会尝试与远端分片节点建立连接。\n  第一个分片向远端分片发送数据：此时，会有另一组监听任务负责监听 /test_shard_2_all 目录下的文件变化，这些任务负责将目录数据发送至远端分片，其中，每份目录将会由独立的线程负责发送，数据在传输之前会被压缩。\n  第二个分片接收数据并写入本地：CH6 分片节点确认建立与 CH5 的连接，在接收到来自 CH5 发送的数据后，将它们写入本地表。\n  由第一个分片确认完成写入：最后，还是由 CH5 分片确认所有的数据发送完毕。\n  可以看到，在整个过程中，Distributed 表负责所有分片的写入工作。本着谁执行谁负责的原则，在这个示例中，由 CH5 节点的分布式表负责切分数据，并向所有其他分片节点发送数据。\n在由 Distributed 表负责向远端分片发送数据时，有异步写和同步写两种模式：如果是异步写，则在 Distributed 表写完本地分片之后，INSERT 查询就会返回成功写入的信息；如果是同步写，则在执行 INSERT 查询之后，会等待所有分片完成写入。\n副本复制数据 如果在集群的配置中包含了副本，那么除了刚才的分片写入流程之外，还会触发副本数据的复制流程。数据在多个副本之间，有两种复制实现方式：\n  Distributed 表引擎：副本数据的写入流程与分片逻辑相同，所以 Distributed 会同时负责分片和副本的数据写入工作。但在这种实现方案下，它很有可能会成为写入的单点瓶颈，所以就有了接下来将要说明的第二种方案。\n  ReplicatedMergeTree 表引擎：如果使用 ReplicatedMergeTree 作为本地表的引擎，则在该分片内，多个副本之间的数据复制会交由 ReplicatedMergeTree 自己处理，不再由 Distributed 负责，从而为其减负。\n  分布式查询流程 与数据写入有所不同，在面向集群查询数据的时候，只能通过 Distributed 表引擎实现。当 Distributed 表接收到SELECT查询的时候，它会依次查询每个分片的数据，再合并汇总返回，流程如下：\n多副本的路由规则 在查询数据的时候，如果集群中的某一个分片有多个副本，此时 Distributed 引擎就会通过负载均衡算法从众多的副本中选取一个，负载均衡算法有以下四种。\n在 ClickHouse 的服务节点中，拥有一个全局计数器errors_count，当服务发生任何异常时，该计数累积加1。\n random（默认）：random 算法会选择errors_count 错误数量最少的副本，如果多个副本的errors_count计数相同，则在它们之中随机选择一个。 nearest_hostname：nearest_hostname 可以看作 random 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个 replica 的errors_count计数相同，则选择集群配置中 host 名称与当前 host 最相似的一个。而相似的规则是以当前 host 名称为基准按字节逐位比较，找出不同字节数最少的一个。 in_order：in_order 同样可以看作 random 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个副本的errors_count计数相同，则按照集群配置中 replica 的定义顺序逐个选择。 first_or_random：first_or_random 可以看作 in_order 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个副本的errors_count计数相同，它首先会选择集群配置中第一个定义的副本，如果该副本不可用，则进一步随机选择一个其他的副本。  多分片查询的流程 分布式查询与分布式写入类似，同样本着谁执行谁负责的原则，它会由接收SELECT查询的 Distributed 表，并负责串联起整个过程。首先它会将针对分布式表的 SQL 语句，按照分片数量将查询拆分成若干个针对本地表的子查询，然后向各个分片发起查询，最后再汇总各个分片的返回结果。\n1 2 3 4 5  --查询分布式表 SELECT*FROMdistributor_table--转换为查询本地表，并将该命令推送到各个分片节点上执行 SELECT*FROMlocal_table  如下图\n 查询各个分片数据：One 和 Remote 步骤是并行执行的，它们分别负责了本地和远端分片的查询动作。 合并返回结果：多个分片数据均查询返回后，在执行节点将所有数据union合并  使用 Global 优化分布式子查询 如果现在有一项查询需求，例如要求找到同时拥有两个仓库的用户，应该如何实现？对于这类交集查询的需求，可以使用IN子查询，此时你会面临两难的选择：IN查询的子句应该使用本地表还是分布式表？（使用JOIN面临的情形与IN类似）。\n使用本地表的问题（可能查询不到结果） 如果在IN查询中使用本地表时，如下列语句\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidIN(SELECTidFROMlocal_tableWHERErepo=200)  在实际执行时，分布式表在接收到查询后会将上述 SQL 替换成本地表的形式，再发送到每个分片进行执行，此时，每个分片上实际执行的是以下语句\n1  SELECTuniq(id)FROMlocal_tableWHERErepo=100ANDidIN(SELECTidFROMlocal_tableWHERErepo=200)  那么此时查询的最终结果就有可能是错误的，因为在单个分片上只保存了部分的数据，这就导致该 SQL 语句可能没有匹配到任何数据，如下图\n使用分布式表的问题（查询请求被放大 N^2 倍，N 为节点数量） 如果在 IN 查询中使用本地表时，如下列语句\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidIN(SELECTidFROMdistributed_tableWHERErepo=200)  对于此次查询，每个分片节点不仅需要查询本地表，还需要再次向其他的分片节点再次发起远端查询，如下图\n因此可以得出结论，在 IN 查询子句使用分布式表的时候，虽然查询的结果得到了保证，但是查询请求会被放大 N 的平方倍，其中 N 等于集群内分片节点的数量，假如集群内有 10 个分片节点，则在一次查询的过程中，会最终导致 100 次的查询请求，这显然是不可接受的。\n使用 GLOBAL 优化查询 为了解决查询放大的问题，我们可以使用 GLOBAL IN 或 GLOBAL JOIN 进行优化，下面就简单介绍一下 GLOBAL 的执行流程\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidGLOBALIN(SELECTidFROMdistributed_tableWHERErepo=200)  如上图，主要有以下五个步骤\n 将IN子句单独提出，发起了一次分布式查询。 将分布式表转 local 本地表后，分别在本地和远端分片执行查询。 将IN子句查询的结果进行汇总，并放入一张临时的内存表进行保存。 将内存表发送到远端分片节点。 将分布式表转为本地表后，开始执行完整的 SQL 语句，IN 子句直接使用临时内存表的数据。  在使用 GLOBAL 修饰符之后，ClickHouse 使用内存表临时保存了 IN 子句查询到的数据，并将其发送到远端分片节点，以此到达了数据共享的目的，从而避免了查询放大的问题。由于数据会在网络间分发，所以需要特别注意临时表的大小，IN 或者 JOIN 子句返回的数据不宜过大。如果表内存在重复数据，也可以事先在子句 SQL 中增加 DISTINCT 以实现去重。\n","date":"2022-05-23T18:27:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86distributed%E5%BC%95%E6%93%8E/","title":"ClickHouse 分布式原理：Distributed引擎"},{"content":"ReplicatedMergeTree引擎 ReplicatedMergeTree是MergeTree的派生引擎，它在MergeTree的基础上加入了分布式协同的能力，只有使用了ReplicatedMergeTree复制表系列引擎，才能应用副本的能力。或者用一种更为直接的方式理解，即使用ReplicatedMergeTree的数据表就是副本。\n在MergeTree中，一个数据分区由开始创建到全部完成，会历经两类存储区域。\n 内存：数据首先会被写入内存缓冲区。 本地磁盘：数据接着会被写入tmp临时目录分区，待全部完成后再将临时目录重命名为正式分区。  ReplicatedMergeTree在上述基础之上增加了ZooKeeper的部分，它会进一步在ZooKeeper内创建一系列的监听节点，并以此实现多个实例之间的通信。在整个通信过程中，ZooKeeper并不会涉及表数据的传输。\n特点 作为数据副本的主要实现载体，ReplicatedMergeTree在设计上有一些显著特点。\n  依赖ZooKeeper：在执行INSERT和ALTER查询的时候，ReplicatedMergeTree需要借助ZooKeeper的分布式协同能力，以实现多个副本之间的同步。但是在查询副本的时候，并不需要使用ZooKeeper。\n  表级别的副本：副本是在表级别定义的，所以每张表的副本配置都可以按照它的实际需求进行个性化定义，包括副本的数量，以及副本在集群内的分布位置等。\n  多主架构（Multi Master）：可以在任意一个副本上执行INSERT和ALTER查询，它们的效果是相同的。这些操作会借助ZooKeeper的协同能力被分发至每个副本以本地形式执行。\n  Block数据块：在执行INSERT命令写入数据时，会依据max_insert_block_size的大小（默认1048576行）将数据切分成若干个Block数据块。所以Block数据块是数据写入的基本单元，并且具有写入的原子性和唯一性。\n  原子性：在数据写入时，一个Block块内的数据要么全部写入成功，要么全部失败。\n  唯一性：在写入一个Block数据块的时候，会按照当前Block数据块的数据顺序、数据行和数据大小等指标，计算Hash信息摘要并记录在案。在此之后，如果某个待写入的Block数据块与先前已被写入的Block数据块拥有相同的Hash摘要（Block数据块内数据顺序、数据大小和数据行均相同），则该Block数据块会被忽略。\n  数据结构 ZooKeeper内的节点结构 ReplicatedMergeTree需要依靠ZooKeeper的事件监听机制以实现各个副本之间的协同。所以，在每张ReplicatedMergeTree表的创建过程中，它会以zk_path为根路径，在Zoo-Keeper中为这张表创建一组监听节点。按照作用的不同，监听节点可以大致分成如下几类：\n 元数据  /metadata：保存元数据信息，包括主键、分区键、采样表达式等。 /columns：保存列字段信息，包括列名称和数据类型。 /replicas：保存副本名称，对应设置参数中的replica_name。   判断标识  /leader_election：用于主副本的选举工作，主副本会主导MERGE和MUTATION操作（ALTER DELETE和ALTER UPDATE）。这些任务在主副本完成之后再借助ZooKeeper将消息事件分发至其他副本。 /blocks：记录Block数据块的Hash信息摘要，以及对应的partition_id。通过Hash摘要能够判断Block数据块是否重复；通过partition_id，则能够找到需要同步的数据分区。 /block_numbers：按照分区的写入顺序，以相同的顺序记录partition_id。各个副本在本地进行MERGE时，都会依照相同的block_numbers顺序进行。 /quorum：记录quorum的数量，当至少有quorum数量的副本写入成功后，整个写操作才算成功。quorum的数量由insert_quorum参数控制，默认值为0。   操作日志  /log：常规操作日志节点（INSERT、MERGE和DROP PARTITION），它是整个工作机制中最为重要的一环，保存了副本需要执行的任务指令。log使用了ZooKeeper的持久顺序型节点，每条指令的名称以log-为前缀递增，例如log-0000000000、log-0000000001等。每一个副本实例都会监听/log节点，当有新的指令加入时，它们会把指令加入副本各自的任务队列，并执行任务。 /mutations：MUTATION操作日志节点，作用与log日志类似，当执行ALERT DELETE和ALERTUPDATE查询时，操作指令会被添加到这个节点。mutations同样使用了ZooKeeper的持久顺序型节点，但是它的命名没有前缀，每条指令直接以递增数字的形式保存，例如0000000000、0000000001等。 /replicas/{replica_name}/*：每个副本各自的节点下的一组监听节点，用于指导副本在本地执行具体的任务指令，其中较为重要的节点有如下几个：  /queue：任务队列节点，用于执行具体的操作任务。当副本从/log或/mutations节点监听到操作指令时，会将执行任务添加至该节点下，并基于队列执行。 /log_pointer：log日志指针节点，记录了最后一次执行的log日志下标信息。 /mutation_pointer：mutations日志指针节点，记录了最后一次执行的mutations日志名称。      Entry日志对象的数据结构 ReplicatedMergeTree在ZooKeeper中有两组非常重要的父节点，那就是/log和/mutations。它们的作用犹如一座通信塔，是分发操作指令的信息通道，而发送指令的方式，则是为这些父节点添加子节点。所有的副本实例，都会监听父节点的变化，当有子节点被添加时，它们能实时感知。这些被添加的子节点在ClickHouse中被统一抽象为Entry对象，而具体实现则由LogEntry和MutationEntry对象承载，分别对应/log和/mutations节点\n LogEntry  source replica：发送这条Log指令的副本来源，对应replica_name。 type：操作指令类型，主要有get、merge和mutate三种，分别对应从远程副本下载分区、合并分区和MUTATION操作。 block_id：当前分区的BlockID，对应/blocks路径下子节点的名称。 partition_name：当前分区目录的名称。   MutationEntry  source replica：发送这条MUTATION指令的副本来源，对应replica_name。 commands：操作指令，主要有ALTER DELETE和ALTER UPDATE。 mutation_id：MUTATION操作的版本号。 partition_id：当前分区目录的ID。    副本协同的核心流程 副本协同的核心流程主要有INSERT、MERGE、MUTATION和ALTER四种，分别对应了数据写入、分区合并、数据修改和元数据修改。INSERT和ALTER是分布式执行的，借助ZooKeeper的事件通知机制，多个副本之间会自动进行有效协同，但是它们不会使用ZooKeeper存储任何分区数据。而其他操作并不支持分布式执行，包括SELECT、CREATE、DROP、RENAME和ATTACH。\n在下列例子中，使用ReplicatedMergeTree实现一张拥有1分片、1副本的数据表来分别执行INSERT、MERGE、MUTATION和ALTER操作，演示执行流程。\nINSERT 当需要在ReplicatedMergeTree中执行INSERT查询以写入数据时，即会进入INSERT核心流程，它的核心流程如下图所示\n 向副本A写入数据 由副本A推送Log日志 各个副本拉取Log日志 各个副本向远端副本发起下载请求  选择一个远端的其他副本作为数据的下载来源。远端副本的选择算法大致是这样的：  从/replicas节点拿到所有的副本节点。 遍历这些副本，选取其中一个。选取的副本需要拥有最大的log_pointer下标，并且/queue子节点数量最少。log_pointer下标最大，意味着该副本执行的日志最多，数据应该更加完整；而/queue最小，则意味着该副本目前的任务执行负担较小。     远端副本响应其它副本的数据下载 各个副本下载数据并完成本地写入  在INSERT的写入过程中，ZooKeeper不会进行任何实质性的数据传输。本着谁执行谁负责的原则，在这个案例中由CH5首先在本地写入了分区数据。之后，也由这个副本负责发送Log日志，通知其他副本下载数据。如果设置了insert_quorum并且insert_quorum\u0026gt;=2，则还会由该副本监控完成写入的副本数量。其他副本在接收到Log日志之后，会选择一个最合适的远端副本，点对点地下载分区数据。\nMERGE 当ReplicatedMergeTree触发分区合并动作时，即会进入这个部分的流程，它的核心流程如下图所示\n无论MERGE操作从哪个副本发起，其合并计划都会交由主副本来制定。\n 创建远程连接，尝试与主副本通信 主副本接收通信 由主副本制定MERGE计划并推送Log日志 各个副本分别拉取Log日志 各个副本分别在本地执行MERGE  可以看到，在MERGE的合并过程中，ZooKeeper也不会进行任何实质性的数据传输，所有的合并操作，最终都是由各个副本在本地完成的。而无论合并动作在哪个副本被触发，都会首先被转交至主副本，再由主副本负责合并计划的制定、消息日志的推送以及对日志接收情况的监控。\nMUTATION 当对ReplicatedMergeTree执行ALTER DELETE或者ALTER UPDATE操作的时候（ClickHouse把delete和update操作也加入到了alter table的范畴中，它并不支持裸的delete或者update操作），即会进入MUTATION部分的逻辑\n与MERGE类似，无论MUTATION操作从哪个副本发起，首先都会由主副本进行响应。\n 推送MUTATION日志 所有副本实例各自监听MUTATION日志 由主副本实例响应MUTATION日志并推送Log日志 各个副本实例分别拉取Log日志 各个副本实例分别在本地执行MUTATION  在MUTATION的整个执行过程中，ZooKeeper同样不会进行任何实质性的数据传输。所有的MUTATION操作，最终都是由各个副本在本地完成的。而MUTATION操作是经过/mutations节点实现分发的。CH6负责了消息的推送。但是无论MUTATION动作从哪个副本被触发，之后都会被转交至主副本，再由主副本负责推送Log日志，以通知各个副本执行最终的MUTATION逻辑。同时也由主副本对日志接收的情况实行监控。\nALTER 当对ReplicatedMergeTree执行ALTER操作进行元数据修改的时候，即会进入ALTER部分的逻辑，例如增加、删除表字段等，核心流程如下图\nALTER的流程与前几个相比简单很多，其执行过程中并不会涉及/log日志的分发，整个流程大致分成3个步骤\n 修改共享元数据 监听共享元数据变更并各自执行本地修改 确认所有副本完成修改  在ALTER整个的执行过程中，ZooKeeper不会进行任何实质性的数据传输。所有的ALTER操作，最终都是由各个副本在本地完成的。本着谁执行谁负责的原则，在这个案例中由CH6负责对共享元数据的修改以及对各个副本修改进度的监控。\n","date":"2022-05-23T18:19:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%89%AF%E6%9C%AC%E5%8D%8F%E5%90%8C%E5%8E%9F%E7%90%86replicatedmergetree%E5%BC%95%E6%93%8E/","title":"ClickHouse 副本协同原理：ReplicatedMergeTree引擎"},{"content":"MergeTree 引擎 存储结构  partition：分区目录，余下各类数据文件（primary.idx、[Column].mrk、[Column]. bin等）都是以分区目录的形式被组织存放的，属于相同分区的数据，最终会被合并到同一个分区目录，而不同分区的数据，永远不会被合并在一起。 checksums：校验文件，使用二进制格式存储。它保存了余下各类文件(primary. idx、count.txt等)的size大小及size的哈希值，用于快速校验文件的完整性和正确性。 columns.txt：列信息文件，使用明文格式存储，用于保存此数据分区下的列字段信息。 count.txt：计数文件，使用明文格式存储，用于记录当前数据分区目录下数据的总行数。 primary.idx：一级索引文件，使用二进制格式存储。用于存放稀疏索引，一张MergeTree表只能声明一次一级索引。借助稀疏索引，在数据查询的时能够排除主键条件范围之外的数据文件，从而有效减少数据扫描范围，加速查询速度。 [Column].bin：数据文件，使用压缩格式存储，用于存储某一列的数据。由于MergeTree采用列式存储，所以每一个列字段都拥有独立的.bin数据文件，并以列字段名称命名。 [Column].mrk：使用二进制格式存储。标记文件中保存了.bin文件中数据的偏移量信息。标记文件与稀疏索引对齐，又与.bin文件一一对应，所以MergeTree通过标记文件建立了primary.idx稀疏索引与.bin数据文件之间的映射关系。即首先通过稀疏索引（primary.idx）找到对应数据的偏移量信息（.mrk），再通过偏移量直接从.bin文件中读取数据。由于.mrk标记文件与.bin文件一一对应，所以MergeTree中的每个列字段都会拥有与其对应的.mrk标记文件 [Column].mrk2：如果使用了自适应大小的索引间隔，则标记文件会以．mrk2命名。它的工作原理和作用与．mrk标记文件相同。 partition.dat与minmax_[Column].idx：如果使用了分区键，例如PARTITION BY EventTime，则会额外生成partition.dat与minmax索引文件，它们均使用二进制格式存储。partition.dat用于保存当前分区下分区表达式最终生成的值；而minmax索引用于记录当前分区下分区字段对应原始数据的最小和最大值。 skp_idx_[Column].idx与skp_idx_[Column].mrk：如果在建表语句中声明了二级索引，则会额外生成相应的二级索引与标记文件，它们同样也使用二进制存储。二级索引在ClickHouse中又称跳数索引。  一级索引 稀疏索引 当我们定义主键之后，MergeTree会依据index_granularity间隔（默认8192行），为数据表生成一级索引并保存至primary.idx文件内，索引数据按照主键排序。相比使用主键定义，更为常见的简化形式是通过ORDER BY指代主键。在此种情形下，主键与ORDER BY定义相同，所以索引（primary.idx）和数据（.bin）会按照完全相同的规则排序。\n一级索引底层采用了稀疏索引来实现，从下图我们可以看出它和稠密索引的区别。\n对于稠密索引而言，每一行索引标记都会对应到具体的一行记录上。而在稀疏索引中，每一行索引标记对应的一大段数据，而不是具体的一行（他们之间的区别就有点类似mysql中innodb的聚集索引与非聚集索引）。\n稀疏索引的优势是显而易见的，它只需要使用少量的索引标记就能够记录大量数据的区间位置信息，并且数据量越大优势愈发明显。例如我们使用默认的索引粒度（8192）时，MergeTree只需要12208行索引标记就能为1亿行数据记录提供索引。由于稀疏索引占用空间小，所以primary.idx内的索引数据能够常驻内存，取用速度自然极快。\n索引粒度index_granularity 索引粒度就如同标尺一般，会丈量整个数据的长度，并依照刻度对数据进行标注，最终将数据标记成多个间隔的小段。数据以index_granularity的粒度(老版本默认8192，新版本实现了自适应粒度)被标记成多个小的区间，其中每个区间最多8192行数据，MergeTree使用MarkRange表示一个具体的区间，并通过start和end表示其具体的范围。\n如下图所示。\nindex_granularity的命名虽然取了索引二字，但它不单只作用于一级索引(.idx)，同时也会影响数据标记(.mrk)和数据文件(.bin)。因为仅有一级索引自身是无法完成查询工作的，它需要借助数据标记才能定位数据，所以一级索引和数据标记的间隔粒度相同(同为index_granularity行)，彼此对齐。而数据文件也会依照index_granularity的间隔粒度生成压缩数据块。\n索引的查询过程 索引查询其实就是两个数值区间的交集判断。其中，一个区间是由基于主键的查询条件转换而来的条件区间；而另一个区间是刚才所讲述的与MarkRange对应的数值区间。\n整个索引的查询过程可以分为三大步骤\n  生成查询条件区间： 将查询条件转换为条件区间。即便是单个值的查询条件，也会被转换成区间的形式。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  --举例-- WHEREID=\u0026#39;A000\u0026#39;=[\u0026#39;A000\u0026#39;,\u0026#39;A000\u0026#39;]WHEREID\u0026gt;\u0026#39;A000\u0026#39;=(\u0026#39;A000\u0026#39;,\u0026#39;+inf\u0026#39;)WHEREID\u0026lt;\u0026#39;A000\u0026#39;=(\u0026#39;-inf\u0026#39;,\u0026#39;A000\u0026#39;)WHEREIDLIKE\u0026#39;A000%\u0026#39;=[\u0026#39;A000\u0026#39;,\u0026#39;A001\u0026#39;)      递归交集判断： 以递归的形式，依次对MarkRange的数值区间与条件区间做交集判断。从最大的区间[A000 , +inf)开始。\n 如果不存在交集，则直接通过剪枝算法优化此整段MarkRange 如果存在交集，且MarkRange步长大于N，则将这个区间进一步拆分为N个子区间，并重复此规则，继续做递归交集判断（N由merge_tree_coarse_index_granularity指定，默认值为8） 如果存在交集，且MarkRange不可再分解，则记录MarkRange并返回    合并MarkRange区间： 将最终匹配的MarkRange聚在一起，合并它们的范围。\n  MergeTree通过递归的形式持续向下拆分区间，最终将MarkRange定位到最细的粒度，以帮助在后续读取数据的时候，能够最小化扫描数据的范围。\n联合主键 当我们以需要以多个字段为主键时，此时数据的查询和存储就涉及到另外一种规则。\n例如以 (CounterID, Date) 以主键，片段中数据首先按 CounterID 排序，具有相同 CounterID 的部分按 Date 排序。排序好的索引的图示会是下面这样：\n1 2 3 4 5 6  全部数据:[-------------------------------------------------------------------------] CounterID:[aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]Date:[1111111222222233331233211111222222333211111112122222223111112223311122333]标记:|||||||||||a,1a,2a,3b,3e,2e,3g,1h,2i,1i,3l,3标记号:012345678910  如果指定查询如下：\n CounterID in ('a', 'h')，服务器会读取标记号在 [0, 3) 和 [6, 8) 区间中的数据。 CounterID IN ('a', 'h') AND Date = 3，服务器会读取标记号在 [1, 3) 和 [7, 8) 区间中的数据。 Date = 3，服务器会读取标记号在 [1, 10] 区间中的数据。  上面例子可以看出使用索引通常会比全表描述要高效。\n  稀疏索引会引起额外的数据读取。当读取主键单个区间范围的数据时，每个数据块中最多会多读 index_granularity * 2 行额外的数据。\n  稀疏索引使得你可以处理极大量的行，因为大多数情况下，这些索引常驻与内存（RAM）中。\n  从上面可以看出，ClickHouse的联合主键在某种程度上与我们熟知的最左前缀规则有点类似，通常在以下几种场景下我们才会考虑使用联合索引\n 查询会使用 b 列作为条件 很长的数据范围（ index_granularity 的数倍）里 a 都是相同的值，并且这样的情况很普遍。换言之，就是加入另一列后，可以让你的查询略过很长的数据范围。 数据量大，需要改善数据压缩（以主键排序片段数据，数据的一致性越高，压缩越好）  长的主键会对插入性能和内存消耗有负面影响，但主键中额外的列并不影响 SELECT 查询的性能。\n二级索引 除了一级索引之外，MergeTree同样支持二级索引。二级索引又称跳数索引，由数据的聚合信息构建而成。根据索引类型的不同，其聚合信息的内容也不同。跳数索引的目的与一级索引一样，也是帮助查询时减少数据扫描的范围。\n==（二级索引目前还处于测试阶段，官方不建议大量使用）==\n跳数索引 目前，MergeTree共支持4种跳数索引，分别是minmax（最值）、set（集合行数）、ngrambf_v1（N-Gram布隆过滤器）和tokenbf_v1（Token布隆过滤器）。一张数据表支持同时声明多个跳数索引。\n  minmax（最值索引）：minmax索引记录了一段数据内的最小和最大极值，其索引的作用类似分区目录的minmax索引，能够快速跳过无用的数据区间。\n  1  示例：INDEX[index_name][column]TYPEminmaxGRANULARITY[GRANULARITYSIZE]      set（集合行数索引）：set索引直接记录了声明字段或表达式的不重复值，用于检测数据块是否满足WHERE条件。\n  1 2 3  示例：INDEX[index_name][column]TYPEset(max_rows)GRANULARITY[index_granularity]-- max_rows是一个阈值，表示在一个index_granularity内，索引最多记录的数据行数。(如果max_rows=0，则表示无限制)       ngrambf_v1（N-Gram布隆过滤器）：ngrambf_v1索引记录的是指定长度的数据短语的布隆表过滤器，只支持String和FixedString数据类型，同时只能够提升in、notIn、like、equals和notEquals查询的性能。\n  1 2 3 4 5 6 7 8  示例：INDEX[index_name][column]TYPEngrambf_v1(n,size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed)GRANULARITY[index_granularity]/* n:token长度，依据n的长度将数据切割为token短语。 size_of_bloom_filter_in_bytes：布隆过滤器的大小。 number_of_hash_functions：布隆过滤器中使用Hash函数的个数。 random_seed: Hash函数的随机种子。 */    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  布隆过滤器可能会包含不符合条件的匹配，所以ngrambf_v1,tokenbf_v1和bloom_filter索引不能用于负向的函数，例如：--可以用来优化的场景 sLIKE\u0026#39;%test%\u0026#39;NOTsNOTLIKE\u0026#39;%test%\u0026#39;s=1NOTs!=1startsWith(s,\u0026#39;test\u0026#39;)i--不能用来优化的场景 NOTsLIKE\u0026#39;%test%\u0026#39;sNOTLIKE\u0026#39;%test%\u0026#39;NOTs=1s!=1NOTstartsWith(s,\u0026#39;test\u0026#39;)      tokenbf_v1（Token布隆过滤器）：tokenbf_v1索引是ngrambf_v1的变种，同样也是一种布隆过滤器索引。tokenbf_v1除了短语token的处理方法外，其他与ngrambf_v1是完全一样的。tokenbf_v1会自动按照非字符的、数字的字符串分割token。\n  1  示例：INDEXdIDTYPEtokenbf_v1(size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed)      granularity 对于跳数索引而言，index_granularity定义了数据的粒度，而granularity定义了聚合信息汇总的粒度。换言之，granularity定义了一行跳数索引能够跳过多少个index_granularity区间的数据。\n作用规则如下：首先，按照index_granularity粒度间隔将数据划分成n段，总共有[0 , n-1]个区间（n = total_rows /index_granularity，向上取整）。接着，根据索引定义时声明的表达式，从0区间开始，依次按index_granularity粒度从数据中获取聚合信息，每次向前移动1步(n+1)，聚合信息逐步累加。最后，当移动granularity次区间时，则汇总并生成一行跳数索引数据。\n以minmax索引为例，假设index_granularity=8192且granularity=3，则数据会按照index_granularity划分为n等份，MergeTree从第0段分区开始，依次获取聚合信息。当获取到第3个分区时（granularity=3），则汇总并会生成第一行minmax索引（前3段minmax极值汇总后取值为[1 , 9]），如下图\n数据标记 如果把MergeTree比作一本书，primary.idx一级索引好比这本书的一级章节目录，.bin文件中的数据好比这本书中的文字，那么数据标记(.mrk)就好比书签一样，会为一级章节目录和具体的文字之间建立关联。\n对于数据标记而言，它记录了两点重要信息：\n 一级章节对应的页码信息。 一段文字在某一页中的起始位置信息。  这样一来，通过数据标记就能够很快地从一本书中立即翻到关注内容所在的那一页，并知道从第几行开始阅读。\n标记数据与一级索引数据不同，它并不能常驻内存，而是使用LRU（最近最少使用）缓存策略加快其取用速度。\n生成规则 从上图可以看出，数据标记和索引区间是对齐的，均按照index_granularity的粒度间隔。如此一来，只需简单通过索引区间的下标编号就可以直接找到对应的数据标记。\n为了能够与数据衔接，数据标记文件也与.bin文件一一对应。即每一个列字段[Column].bin文件都有一个与之对应的[Column].mrk数据标记文件，用于记录数据在.bin文件中的偏移量信息。同时，.mrk包含了.bin压缩和解压缩这两种不同状态的偏移量，如下图\n工作方式 MergeTree在读取数据时，必须通过标记数据的位置信息才能够找到所需要的数据。整个查找过程大致可以分为读取压缩数据块和读取数据两个步骤。\n对于下图来说，表的index_granularity粒度为8192，所以一个索引片段的数据大小恰好是8192B。按照压缩数据块的生成规则，如果单个批次数据小于64KB，则继续获取下一批数据，直至累积到size\u0026gt;=64KB时，生成下一个压缩数据块。因此在JavaEnable的标记文件中，每8行标记数据对应1个压缩数据块（1B * 8192 = 8192B, 64KB = 65536B, 65536 / 8192 =8）。\n从图能够看到，其左侧的标记数据中，8行数据的压缩文件偏移量都是相同的，因为这8行标记都指向了同一个压缩数据块。而在这8行的标记数据中，它们的解压缩数据块中的偏移量，则依次按照8192B（每行数据1B，每一个批次8192行数据）累加，当累加达到65536(64KB)时则置0。因为根据规则，此时会生成下一个压缩数据块。\n  读取压缩数据块： 在查询某一列数据时，MergeTree无须一次性加载整个.bin文件，而是可以根据需要，只加载特定的压缩数据块。而这项特性需要借助标记文件中所保存的压缩文件中的偏移量。\n  读取数据： 在读取解压后的数据时，MergeTree并不需要一次性扫描整段解压数据，它可以根据需要，以index_granularity的粒度加载特定的一小段。为了实现这项特性，需要借助标记文件中保存的解压数据块中的偏移量。\n  数据标记与压缩数据块的对应关系 由于压缩数据块的划分，与一个间隔index_granularity内的数据大小相关，每个压缩数据块的体积都被严格控制在64KB～1MB。而一个间隔index_granularity的数据，又只会产生一行数据标记。那么根据一个间隔内数据的实际字节大小，数据标记和压缩数据块之间会产生三种不同的对应关系。\n 一对一  一个数据标记对应一个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size大于等于64KB且小于等于1MB时，会出现这种对应关系。    一对多  一个数据标记对应多个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size直接大于1MB时，会出现这种对应关系。    多对一  多个数据标记对应一个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size小于64KB时，会出现这种对应关系。     工作流程 存储流程 数据的存储流程主要有以下几个步骤\n  首先生成分区目录，伴随着每一批数据的写入，都会生成一个新的分区目录。\n  在后续的某一时刻，属于相同分区的目录会依照规则合并到一起\n  接着，按照index_granularity索引粒度，会分别生成primary.idx一级索引（如果声明了二级索引，还会创建二级索引文件）、每一个列字段的．mrk数据标记和．bin压缩数据文件。\n  查询流程 数据查询的本质，可以看作一个不断减小数据范围的过程。在最理想的情况下，MergeTree首先可以依次借助分区索引、一级索引和二级索引，将数据扫描范围缩至最小。然后再借助数据标记，将需要解压与计算的数据范围缩至最小。\n如果一条查询语句没有指定任何WHERE条件，或是指定了WHERE条件，但条件没有匹配到任何索引（分区索引、一级索引和二级索引），那么MergeTree就不能预先减小数据范围。在后续进行数据查询时，它会扫描所有分区目录，以及目录内索引段的最大区间。虽然不能减少数据范围，但是MergeTree仍然能够借助数据标记，以多线程的形式同时读取多个压缩数据块，以提升性能。\n","date":"2022-05-23T18:18:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86mergetree%E5%BC%95%E6%93%8E/","title":"ClickHouse 数据存储原理：MergeTree引擎"},{"content":"ClickHouse 基本概念 ClickHouse 是一个用于联机分析（OLAP）的列式数据库管理系统（DBMS）。\nOLAP 什么是 OLAP? OLAP 名为联机分析，又可以称为多维分析，是由关系型数据库之父埃德加·科德（EdgarFrank Codd）于 1993 年提出的概念。顾名思义，它指的是通过多种不同的维度审视数据，进行深层次分析。\n维度可以看作观察数据的一种视角，例如人类能看到的世界是三维的，它包含长、宽、高三个维度。直接一点理解，维度就好比是一张数据表的字段，而多维分析则是基于这些字段进行聚合查询。\n如上图，多维分析包含以下几种操作：\n 下钻： 从高层次向低层次明细数据穿透，例如从省下钻到市。 上卷： 和下钻相反，从低层次向高层次汇聚，例如从市汇聚成省。 切片： 观察立方体的一层，将一个或多个维度设为单个固定值，然后观察剩余的维度，例如将商品维度固定为足球。 切块： 与切片类似，只是将单个固定值变成多个值。例如将商品维度固定成足球、篮球。 旋转： 旋转立方体的一面，如果要将数据映射到一张二维表，那么就要进行旋转，这就等同于行列置换。  OLAP 与 OLTP OLTP（on-line transaction processing）翻译为联机事务处理， OLAP（On-Line Analytical Processing）翻译为联机分析处理。\n  从字面上来看 OLTP 是做事务处理，OLAP 是做分析处理。\n  从对数据库操作来看，OLTP 主要是对数据的增删改，OLAP 是对数据的查询。\n  因为 OLTP 所产生的业务数据分散在不同的业务系统中，而 OLAP 往往需要将不同的业务数据集中到一起进行统一综合的分析，这时候就需要根据业务分析需求做对应的数据清洗后存储在数据仓库中，然后由数据仓库来统一提供 OLAP 分析。\n  OLTP 是数据库的应用，OLAP 是数据仓库的应用\n  下面用一张图来简要对比。\n列式存储 列式存储与行式存储 在传统的行式数据库系统中，处于同一行中的数据总是被物理的存储在一起，存储方式如下图：\n在列式数据库系统中，来自不同列的值被单独存储，来自同一列的数据被存储在一起，数据按如下的顺序存储：\n不同的数据存储方式适用不同的业务场景，而对于 OLAP 来说，列式存储是最适合的选择。\n列式存储与 OLAP 为什么列式数据库更适合于 OLAP 场景呢？下面这两张图就可以给你答案\n 行式数据库     列式数据库      下面分别从两个 I/O 和 CPU 两个角度来分析为什么他们有如此之大的差别\n I/O  针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。 由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了 I/O 的体积。 由于 I/O 的降低，这将帮助更多的数据被系统缓存,进一步降低了数据传输的成本。   CPU  由于执行一个查询需要处理大量的行，因此在整个向量上执行所有操作将比在每一行上执行所有操作更加高效。同时这将有助于实现一个几乎没有调用成本的查询引擎。如果你不这样做，使用任何一个机械硬盘，查询引擎都不可避免的停止 CPU 进行等待。所以，在数据按列存储并且按列执行是很有意义的。    列式存储与数据压缩 如果你想让查询变得更快，最简单且有效的方法是减少数据扫描范围和数据传输时的大小，而列式存储和数据压缩就可以帮助我们实现上述两点。\n列式存储和数据压缩通常是伴生的。数据按列存储。而具体到每个列字段，数据也是独立存储的，每个列字段都拥有一个与之对应的 .bin 数据文件，相同类型的数据放在同一个文件中，对压缩更加友好。数据默认使用 LZ4 算法压缩，在 Yandex.Metrica 的生产环境中，数据总体的压缩比可以达到 8:1（未压缩前 17PB，压缩后 2PB）。\n核心特点 完备的 DBMS 功能 ClickHouse 拥有完备的管理功能，所以它称得上是一个 DBMS（Database Management System，数据库管理系统），而不仅是一个数据库。作为一个 DBMS，它具备了一些基本功能，\n如下所示。\n DDL（数据定义语言）：可以动态地创建、修改或删除数据库、表和视图，而无须重启服务。 DML（数据操作语言）：可以动态查询、插入、修改或删除数据。 权限控制：可以按照用户粒度设置数据库或者表的操作权限，保障数据的安全性。 数据备份与恢复：提供了数据备份导出与导入恢复机制，满足生产环境的要求。 分布式管理：提供集群模式，能够自动管理多个数据库节点。  关系模型与 SQL 查询 相比 HBase 和 Redis 这类 NoSQL 数据库，ClickHouse 使用关系模型描述数据并提供了传统数据库的概念（数据库、表、视图和函数等）。与此同时，ClickHouse 完全使用 SQL 作为查询语言（支持 GROUP BY、ORDER BY、JOIN、IN 等大部分标准 SQL），这使得它平易近人，容易理解和学习。\n向量化表引擎 向量化执行，可以简单地看作从硬件的角度上消除程序中循环的优化。\n为了实现向量化执行，需要利用 CPU 的 SIMD 指令。SIMD 的全称是 Single Instruction MultipleData，即用单条指令操作多条数据。现代计算机系统概念中，它是通过数据并行以提高性能的一种实现方式，它的原理是在 CPU 寄存器层面实现数据的并行操作。例如有 8 个 32 位整形数据都需要进行移位运行，则由一条对 32 位整形数据进行移位的指令重复执行 8 次完成。SIMD 引入了一组大容量的寄存器，一个寄存器包含 8 * 32 位，可以将这 8 个数据按次序同时放到一个寄存器。同时，CPU 新增了处理这种 8 * 32 位寄存器的指令，可以在一个指令周期内完成 8 个数据的位移运算。（本质就是将每次处理的数据从一条变为一批）\n多样化的表引擎 与 MySQL 类似，ClickHouse 也将存储部分进行了抽象，把存储引擎作为一层独立的接口。ClickHouse 共拥有合并树、内存、文件、接口和其他 6 大类 20 多种表引擎。其中每一种表引擎都有着各自的特点，用户可以根据实际业务场景的要求，选择合适的表引擎使用。\n多主架构 ClickHouse 则采用 Multi-Master多主架构，集群中的每个节点角色对等，客户端访问任意一个节点都能得到相同的效果。这种多主的架构有许多优势，例如对等的角色使系统架构变得更加简单，不用再区分主控节点、数据节点和计算节点，集群中的所有节点功能相同。所以它天然规避了单点故障的问题，非常适合用于多数据中心、异地多活的场景。\n多线程与分布式 在各服务器之间，通过网络传输数据的成本是高昂的，所以相比移动数据，更为聪明的做法是预先将数据分布到各台服务器，将数据的计算查询直接下推到数据所在的服务器。ClickHouse 在数据存取方面，既支持分区（纵向扩展，利用多线程原理），也支持分片（横向扩展，利用分布式原理），可以说是将多线程和分布式的技术应用到了极致。\n分片与分布式查询 数据分片是将数据进行横向切分，这是一种在面对海量数据的场景下，解决存储和查询瓶颈的有效手段，是一种分治思想的体现。ClickHouse 支持分片，而分片则依赖集群。每个集群由 1 到多个分片组成，而每个分片则对应了 ClickHouse 的 1 个服务节点。分片的数量上限取决于节点数量（1 个分片只能对应 1 个服务节点）。\nClickHouse 并不像其他分布式系统那样，拥有高度自动化的分片功能。ClickHouse 提供了**本地表（Local Table）与分布式表（Distributed Table）**的概念。一张本地表等同于一份数据的分片，而分布式表本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。借助分布式表，能够代理访问多个数据分片，从而实现分布式查询。\n应用场景 擅长的场景  绝大多数是读请求 数据以相当大的批次（\u0026gt; 1000 行）更新，而不是单行更新;或者根本没有更新。 已添加到数据库的数据不能修改。 对于读取，从数据库中提取相当多的行，但只提取列的一小部分。 宽表，即每个表包含着大量的列。 查询相对较少（通常每台服务器每秒查询数百次或更少）。 对于简单查询，允许延迟大约 50 毫秒。 列中的数据相对较小：数字和短字符串（例如，每个 URL 60 个字节）。 处理单个查询时需要高吞吐量（每台服务器每秒可达数十亿行）。 事务不是必须的。 对数据一致性要求低。 每个查询有一个大表。除了他以外，其他的都很小。 查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的 RAM 中。  不擅长的场景  OLTP 事务性操作（不支持事务，不支持真正的更新/删除） 不擅长根据主键按行粒度进行查询（如 select * from table where user_id in (xxx, xxx, xxx, ...)） 不擅长存储和查询 blob 或者大量文本类数据（按列存储） 不擅长执行有大量 join 的查询（Distributed 引擎局限） 不支持高并发，官方建议 QPS \u0026lt;= 100  Clickhouse 为什么会这么快？ 首先亮出官方的测试报告：Clickhouse 性能对比报告\n所有用于对比的数据库都使用了相同配置的服务器，在单个节点的情况下，对一张拥有 133 个字段的数据表分别在 1000 万、1 亿和 10 亿这三种数据体量下执行基准测试，基准测试的范围涵盖 43 项 SQL 查询。\n市面上有很多与 Clickhouse 采用了同样技术(如列式存储、向量化引擎等)的数据库，但为什么 ClickHouse 的性能能够将其他数据库远远甩在身后呢？这主要依赖于下面几个方面\n 着眼硬件，先想后做  ClickHouse 会在内存中进行 GROUP BY，并且使用 HashTable 装载数据。 ClickHouse 非常在意 CPU L3 级别的缓存，因为一次 L3 的缓存失效会带来 70 ～ 100ns 的延迟。这意味着在单核CPU上，它会浪费 4000 万次/秒的运算；而在一个 32 线程的 CPU 上，则可能会浪费 5 亿次/秒的运算。   算法在前，抽象在后  对于常量，使用 Volnitsky 算法； 对于非常量，使用 CPU 的向量化执行 SIMD（用于文本转换、数据过滤、数据解压和 JSON 转换等），暴力优化； 正则匹配使用 re2 和 hyperscan 算法。性能是算法选择的首要考量指标。   勇于尝鲜，不行就换  除了字符串之外，其余的场景也与它类似，ClickHouse 会使用最合适、最快的算法。如果世面上出现了号称性能强大的新算法，ClickHouse 团队会立即将其纳入并进行验证。如果效果不错，就保留使用；如果性能不尽人意，就将其抛弃。   特定场景，特殊优化  针对同一个场景的不同状况，选择使用不同的实现方式，尽可能将性能最大化。 例如去重计数 uniqCombined 函数，会根据数据量的不同选择不同的算法：当数据量较小的时候，会选择 Array 保存；当数据量中等的时候，会选择 HashSet；而当数据量很大的时候，则使用 HyperLogLog 算法。 针对不同的场景，Clickhouse 提供了 MergeTree 引擎家族，如 MergeTree、ReplacingMergeTree、SummingMergeTree、AggregatingMergeTree、CollapsingMergeTree和VersionedCollapsingMergeTree 等。   持续测试，持续改进  由于 Yandex 的天然优势，ClickHouse 经常会使用真实的数据进行测试，这一点很好地保证了测试场景的真实性。 ClickHouse 差不多每个月都能发布一个版本，正因为拥有这样的发版频率，ClickHouse 才能够快速迭代、快速改进。    ClickHouse 的架构 目前 ClickHouse 公开的资料相对匮乏，比如在架构设计层面就很难找到完整的资料，甚至连一张整体的架构图都没有，根据官网提供的信息，我们能够得出一个大概的架构，如下图\n  Parser： Parser 分析器可以将一条 SQL 语句以递归下降的方法解析成 AST 语法树的形式。不同的 SQL 语句，会经由不同的 Parser 实现类解析。\n  Interpreter：Interpreter 解释器的作用就像 Service 服务层一样，起到串联整个查询过程的作用，它会根据解释器的类型，聚合它所需要的资源。首先它会解析AST对象；然后执行“业务逻辑”（例如分支判断、设置参数、调用接口等）；最终返回IBlock对象，以线程的形式建立起一个查询执行管道。\n  Tables：Tables由 IStorage 接口表示。该接口的不同实现对应不同的表引擎。比如 StorageMergeTree、StorageMemory 等。这些类的实例就是表。\n IStorage 接口定义了DDL（如 ALTER、RENAME、OPTIMIZE 和 DROP 等）、read 和 write 方法，它们分别负责数据的定义、查询与写入。在数据查询时，IStorage 负责根据 AST 查询语句的指示要求，返回指定列的原始数据。 后续对数据的进一步加工、计算和过滤，则会统一交由Interpreter解释器对象处理。对Table发起的一次操作通常都会经历这样的过程，接收AST查询语句，根据AST返回指定列的数据，之后再将数据交由Interpreter做进一步处理。    Block与Block Streams：ClickHouse 内部的数据操作是面向 Block 对象进行的，并且采用了流的形式。\n Block：虽然 Column 和 Filed 组成了数据的基本映射单元，但对应到实际操作，它们还缺少了一些必要的信息，比如数据的类型及列的名称。于是 ClickHouse 设计了 Block对象，Block 对象可以看作数据表的子集。Block 对象的本质是由数据对象、数据类型和列名称组成的三元组，即 Column、DataType 及列名称字符串。Column 提供了数据的读取能力，而DataType知道如何正反序列化，所以 Block 在这些对象的基础之上实现了进一步的抽象和封装，从而简化了整个使用的过程，仅通过Block对象就能完成一系列的数据操作。在具体的实现过程中，Block 并没有直接聚合Column和DataType对象，而是通过ColumnWithTypeAndName对象进行间接引用。 Block Streams：Block Streams 用于处理数据。我们可以使用 Block Streams 从某个地方读取数据，执行数据转换，或将数据写到某个地方。IBlockInputStream 具有 read 方法，其能够在数据可用时获取下一个块。IBlockOutputStream 具有 write 方法，其能够将块写到某处。    Functions：ClickHouse 主要提供两类函数——普通函数和聚合函数。\n Function：普通函数由IFunction接口定义，其是没有状态的，函数效果作用于每行数据之上。当然，在函数具体执行的过程中，并不会一行一行地运算，而是采用向量化的方式直接作用于一整列数据。 AggregateFunction：聚合函数由IAggregateFunction接口定义，相比无状态的普通函数，聚合函数是有状态的，并且聚合函数的状态支持序列化与反序列化，所以能够在分布式节点之间进行传输，以实现增量计算。    DataType：数据的序列化和反序列化工作由 DataType 负责。根据不同的数据类型，IDataType 接口会有不同的实现类。DataType 虽然会对数据进行正反序列化，但是它不会直接和内存或者磁盘做交互，而是转交给 Column 和 Filed 处理。\n  Column 与 Field：Column 和 Field 是 ClickHouse 数据最基础的映射单元。\n Column：内存中的一列数据由一个 Column 对象表示。Column 对象分为接口和实现两个部分，在IColumn接口对象中，定义了对数据进行各种关系运算的方法，例如插入数据的insertRangeFrom和insertFrom方法、用于分页的cut，以及用于过滤的filter方法等。而这些方法的具体实现对象则根据数据类型的不同，由相应的对象实现。 Field：在大多数场合，ClickHouse 都会以整列的方式操作数据，但凡事也有例外。如果需要操作单个具体的数值（也就是单列中的一行数据），则需要使用 Field 对象，Field 对象代表一个单值。与 Column 对象的泛化设计思路不同，Field 对象使用了聚合的设计模式。在 Field 对象内部聚合了 Null、UInt64、String 和 Array 等 13 种数据类型及相应的处理逻辑。    ","date":"2022-05-23T18:16:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ClickHouse 基本概念"}]