[{"content":"DBImpl模块 Open 数据库 Open 操作主要用于创建新的 LevelDB 数据库或打开一个已存在的数据库。Open 操作的主要函数共需传递 3 个参数：两个输入参数 options 与 dbname，一个输出参数 dbptr。\n首先我们来看看它的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DB::Open(const Options\u0026amp; options, const std::string\u0026amp; dbname, DB** dbptr) { *dbptr = nullptr; //初始化dbimpl  DBImpl* impl = new DBImpl(options, dbname); impl-\u0026gt;mutex_.Lock(); VersionEdit edit; //尝试恢复之前已经存在的数据库文件中的数据  bool save_manifest = false; Status s = impl-\u0026gt;Recover(\u0026amp;edit, \u0026amp;save_manifest); //判断Memtable是否为空  if (s.ok() \u0026amp;\u0026amp; impl-\u0026gt;mem_ == nullptr) { //创建新的Log和MemTable  uint64_t new_log_number = impl-\u0026gt;versions_-\u0026gt;NewFileNumber(); WritableFile* lfile; s = options.env-\u0026gt;NewWritableFile(LogFileName(dbname, new_log_number), \u0026amp;lfile); if (s.ok()) { edit.SetLogNumber(new_log_number); impl-\u0026gt;logfile_ = lfile; impl-\u0026gt;logfile_number_ = new_log_number; impl-\u0026gt;log_ = new log::Writer(lfile); impl-\u0026gt;mem_ = new MemTable(impl-\u0026gt;internal_comparator_); impl-\u0026gt;mem_-\u0026gt;Ref(); } } //判断是否需要保存Manifest文件  if (s.ok() \u0026amp;\u0026amp; save_manifest) { edit.SetPrevLogNumber(0); edit.SetLogNumber(impl-\u0026gt;logfile_number_); //生成新的版本  s = impl-\u0026gt;versions_-\u0026gt;LogAndApply(\u0026amp;edit, \u0026amp;impl-\u0026gt;mutex_); } if (s.ok()) { //请理无用的文件  impl-\u0026gt;RemoveObsoleteFiles(); //尝试进行Compaction  impl-\u0026gt;MaybeScheduleCompaction(); } impl-\u0026gt;mutex_.Unlock(); if (s.ok()) { assert(impl-\u0026gt;mem_ != nullptr); *dbptr = impl; } else { delete impl; } return s; }   具体的实现流程如下图所示：\n 初始化一个 DBImpl 的对象 impl，将相关的参数选项 options 与数据库名称 dbname 作为构造函数的参数。 调用 DBImpl 对象的 Recover 函数，尝试恢复之前存在的数据库文件数据。 进行 Recover 操作后，判断 impl 对象中的 MemTable 对象指针 mem_ 是否为空，如果为空，则进入第 4 步，不为空则进入第 5 步。 创建新的 Log 文件以及对应的 MemTable 对象。这一步主要分别实例化 log::Writer 和 MemTable 两个对象，并赋值给 impl 中对应的成员变量，后续通过 impl 中的成员变量操作 Log 文件和 MemTable。 判断是否需要保存 Manifest 相关信息，如果需要，则保存相关信息。 判断前面步骤是否都成功了，如果成功，则调用 DeleteObsoleteFiles 函数对一些过时文件进行删除，且调用 MaybeScheduleCompaction 函数尝试进行数据文件的 Compaction 操作。  Get Get 主要用于从 LevelDB 中获取对应的键-值对数据，它是单个数据读取的主要接口。Get 的主要参数为数据读参数选项 options、键 key，以及一个用于返回数据值的 string 类型指针 value。其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Get(const ReadOptions\u0026amp; options, const Slice\u0026amp; key, std::string* value) { Status s; MutexLock l(\u0026amp;mutex_); SequenceNumber snapshot; //获取序列号并赋值给snapshot  if (options.snapshot != nullptr) { snapshot = static_cast\u0026lt;const SnapshotImpl*\u0026gt;(options.snapshot)-\u0026gt;sequence_number(); } else { snapshot = versions_-\u0026gt;LastSequence(); } MemTable* mem = mem_; MemTable* imm = imm_; Version* current = versions_-\u0026gt;current(); mem-\u0026gt;Ref(); if (imm != nullptr) imm-\u0026gt;Ref(); current-\u0026gt;Ref(); bool have_stat_update = false; Version::GetStats stats; { mutex_.Unlock(); //首先查找memtable  LookupKey lkey(key, snapshot); if (mem-\u0026gt;Get(lkey, value, \u0026amp;s)) { //如果查找不到，接着查找immutable  } else if (imm != nullptr \u0026amp;\u0026amp; imm-\u0026gt;Get(lkey, value, \u0026amp;s)) { //如果还是没找到，则继续查找SSTable  } else { s = current-\u0026gt;Get(options, lkey, value, \u0026amp;stats); have_stat_update = true; } mutex_.Lock(); } if (have_stat_update \u0026amp;\u0026amp; current-\u0026gt;UpdateStats(stats)) { MaybeScheduleCompaction(); } mem-\u0026gt;Unref(); if (imm != nullptr) imm-\u0026gt;Unref(); current-\u0026gt;Unref(); return s; }   具体的实现流程如下图所示：\nGet 在查询读取数据时，依次从 MemTable、Immutable MemTable 以及当前保存的 SSTable 文件中进行查找。如果在 MemTabel 中找到，立即返回对应的数值，如果没有找到，再从 Immutable MemTable 中查找。而如果Immutable MemTable 中还是没有找到，则会从持久化的文件 SSTable 中查找，直到找出该键对应的数值为止。\n SequenceNumber 有什么用呢？\n其主要作用是对 DB 的整个存储空间进行时间刻度上的序列备份，即要从 DB 中获取某一个数据，不仅需要其对应的键 key，而且需要其对应的时间序列号。对数据库进行写操作会改变序列号，每进行一次写操作，则序列号加 1。\n Put、Delete、Write Put 主要有3个参数：写操作参数 opt、操作数据的 key 与操作数据新值 value。其代码如下：\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Put(const WriteOptions\u0026amp; o, const Slice\u0026amp; key, const Slice\u0026amp; val) { return DB::Put(o, key, val); } Status DB::Put(const WriteOptions\u0026amp; opt, const Slice\u0026amp; key, const Slice\u0026amp; value) { WriteBatch batch; batch.Put(key, value); return Write(opt, \u0026amp;batch); }   从上面的代码可以看出， Put 其实也是将单条数据的操作变更为一个批量操作，然后调用 Write 进行实现。\nDelete 不会直接删除数据，而是在对应位置插入一个 key 的删除标志，然后在后续的 Compaction 过程中才最终去除这条 key-value 记录。其代码如下:\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Delete(const WriteOptions\u0026amp; options, const Slice\u0026amp; key) { return DB::Delete(options, key); } Status DB::Delete(const WriteOptions\u0026amp; opt, const Slice\u0026amp; key) { WriteBatch batch; batch.Delete(key); return Write(opt, \u0026amp;batch); }   从上面的代码可以看出 Delete 的本质其实也是一个 Write 操作。\n在介绍 Write 之前，首先介绍其封装的消息结构 Writer 与任务队列 writes_。\nWriter 用于保存基本信息，如批量操作 batch、状态信息 status、是否同步 sync、是否完成 done 以及用于多线程操作的条件变量cv 。\n1 2 3 4 5 6 7 8 9 10 11 12  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  struct DBImpl::Writer { explicit Writer(port::Mutex* mu) : batch(nullptr), sync(false), done(false), cv(mu) {} Status status; //状态  WriteBatch* batch; //批量写入对象  bool sync;//表示是否已经同步了  bool done;//表示是否已经处理完成  port::CondVar cv;//这个是条件变量 };   接着看看任务队列 writers_，该队列对象中的元素节点为 Writer 对象指针。可见 writes_ 与写操作的缓存空间有关，批量操作请求均存储在这个队列中，按顺序执行，已完成的出队，而未执行的则在这个队列中处于等待状态。\n1 2  // https://weread.qq.com/web/reader/9f932e70727ac58e9f9d8cck636320102206364d3f0ffdc std::deque\u0026lt;Writer*\u0026gt; writers_ GUARDED_BY(mutex_);   Write 主要有两个参数：WriteOptions 对象与 WriteBatch 对象。WriteOptions 主要包含一些关于写操作的参数选项，而WriteBatch对象，相当于一个缓冲区，用于定义、保存一系列的批量操作。其代码实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::Write(const WriteOptions\u0026amp; options, WriteBatch* updates) { //实例化一个Writer对象b并插入writers_队列中等待执行  Writer w(\u0026amp;mutex_); w.batch = updates; w.sync = options.sync; w.done = false; MutexLock l(\u0026amp;mutex_); writers_.push_back(\u0026amp;w); while (!w.done \u0026amp;\u0026amp; \u0026amp;w != writers_.front()) { w.cv.Wait(); } if (w.done) { return w.status; } Status status = MakeRoomForWrite(updates == nullptr); uint64_t last_sequence = versions_-\u0026gt;LastSequence(); Writer* last_writer = \u0026amp;w; if (status.ok() \u0026amp;\u0026amp; updates != nullptr) { //合并写入操作  WriteBatch* write_batch = BuildBatchGroup(\u0026amp;last_writer); WriteBatchInternal::SetSequence(write_batch, last_sequence + 1); last_sequence += WriteBatchInternal::Count(write_batch); { mutex_.Unlock(); //将更新写入日志文件中，并且将日志文件写入磁盘中  status = log_-\u0026gt;AddRecord(WriteBatchInternal::Contents(write_batch)); bool sync_error = false; if (status.ok() \u0026amp;\u0026amp; options.sync) { status = logfile_-\u0026gt;Sync(); if (!status.ok()) { sync_error = true; } } //将更新写入Memtable中  if (status.ok()) { status = WriteBatchInternal::InsertInto(write_batch, mem_); } mutex_.Lock(); if (sync_error) { RecordBackgroundError(status); } } if (write_batch == tmp_batch_) tmp_batch_-\u0026gt;Clear(); versions_-\u0026gt;SetLastSequence(last_sequence); } //由于和并写入操作一次可能会处理多个writer_队列中的元素，因此将所有已经处理的元素状态进行变更，并且发送signal信号  while (true) { Writer* ready = writers_.front(); writers_.pop_front(); if (ready != \u0026amp;w) { ready-\u0026gt;status = status; ready-\u0026gt;done = true; ready-\u0026gt;cv.Signal(); } if (ready == last_writer) break; } //通知writers_队列中的第一个元素，发送signal信号  if (!writers_.empty()) { writers_.front()-\u0026gt;cv.Signal(); } return status; }   具体的实现流程如下图所示：\n  实例化一个 Writer 对象，并将其插入所示的 writers_ 队列中。\n  通过 Writer 中的条件变量 cv 调用 wait 方法将该线程挂起，等待其他线程发送 signal 信号，并且等待队列前面的 Writer 操作全部执行完毕：\n 如果线程收到了 signal 信号：则解除阻塞。 如果线程没有收到了 signal 信号：说明队列前面仍有其他的 Writer 操作，那么该线程会再次调用 wait 方法实现阻塞，从而保证了 Writer 操作按照队列生成次序执行。    当轮到本线程操作时，首先通过 MakeRoomForWrite 函数进行内存空间分配。\n  当获取到需要的内存后，根据一系列的批量操作，对 Log 文件以及 MemTable 分别进行更新。\n  依据批量操作的数目更新 SequenceNumber。\n  通过 Writer 中的条件变量 cv 发送 signal 信号，以通知处于等待状态的其他线程开始执行。\n  ","date":"2022-05-23T23:48:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-dbimpl%E6%A8%A1%E5%9D%97/","title":"LevelDB DBImpl模块"},{"content":"Compaction模块 LevelDB 中的 Level 代表层级，有 0～6 共 7 个层级，每个层级都由一定数量的 SSTable 文件组成。其中，高层级文件是由低层级的一个文件与高层级中与该文件有键重叠的所有文件使用归并排序算法生成，该过程称为Compaction。\nLevelDB 通过 Compaction 将冷数据逐层下移，并且在 Compaction 过程中重复写入的键只会保留一个最终值，已经删除的键不再写入，因此可以减少磁盘空间占用。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  // https://github.com/google/leveldb/blob/master/db/version_set.h  class Compaction { public: ~Compaction(); int level() const { return level_; } VersionEdit* edit() { return \u0026amp;edit_; } int num_input_files(int which) const { return inputs_[which].size(); } FileMetaData* input(int which, int i) const { return inputs_[which][i]; } uint64_t MaxOutputFileSize() const { return max_output_file_size_; } bool IsTrivialMove() const; void AddInputDeletions(VersionEdit* edit); bool IsBaseLevelForKey(const Slice\u0026amp; user_key); bool ShouldStopBefore(const Slice\u0026amp; internal_key); void ReleaseInputs(); private: friend class Version; friend class VersionSet; Compaction(const Options* options, int level); int level_; uint64_t max_output_file_size_; Version* input_version_; VersionEdit edit_; std::vector\u0026lt;FileMetaData*\u0026gt; inputs_[2]; // The two sets of inputs  std::vector\u0026lt;FileMetaData*\u0026gt; grandparents_; size_t grandparent_index_; // Index in grandparent_starts_  bool seen_key_; // Some output key has been seen  int64_t overlapped_bytes_; // Bytes of overlap between current output  // and grandparent files  size_t level_ptrs_[config::kNumLevels]; };   在 LevelDB 中，Compaction 共有两种，分别叫 Minor Compaction 和 Major Compaction。\n  Minor Compaction：将 Immtable dump 到 SStable 。\n  Major Compaction：Level 之间的 SSTable Compaction。\n  这两类compaction负责在不同的场景下进行不同的数据整理。\nMinor Compaction 定义 Minor Compaction 非常简单，其本质就是将一个内存数据库（Memtable）中的所有数据持久化到一个磁盘文件中（SSTable）。整体流程如下图：\n触发时机 在 LSM 树的实现中，会先将数据写入 MemTable，当 MemTable 大小超过 options_.write_buffer_size （默认4M）时，需要将其作为 SSTable 写入磁盘，此时就会采取 Minor Compaction。\n核心要点  每次 Minor Compaction 结束后，都会生成一个新的 SSTable 文件，也意味着 Leveldb 的版本状态发生了变化，会进行一个版本的更替。 Minor Compaction 是一个时效性要求非常高的过程，要求其在尽可能短的时间内完成，否则就会堵塞正常的写入操作，因此 Minor Compaction 的优先级高于 Major Compaction。当进行 Minor Compaction 的时候有 Major Compaction正在进行，则会首先暂停 Major Compaction。  Major Compaction 定义 Major Compaction 是将不同层级的 SSTable 文件进行合并。\n如下图，可以看出其比 Minor Compaction 复杂的多。\n触发时机 那么什么时候，会触发 LevelDB 进行 Major Compaction 呢？总的来说为以下三个条件：\n 当 0 层文件数超过预定的上限（默认为 4 个）。 当 Level i层文件的总大小超过 10 ^ i MB。 当某个文件无效读取的次数过多。  这也就引出了 Size Compaction 与 Seek Compaction 两种判断策略。LevelDB先按 Size Compaction 判断是否需要进行 Compaction ，如果 Size Compaction 不满足则通过 Seek Compaction 继续判断，如果仍不满足，则表明暂时不需要进行 Compaction。\nSize Compaction size_compaction 通过判断 Level 0 中的文件个数（Level 0 会被频繁访问）或者 Level 1 ～ Level 5 的文件总大小来计算得出需要进行 Compaction 的 Level。\n该赋值逻辑位于 VersionSet 中的 Finalize，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // https://github.com/google/leveldb/blob/master/db/version_set.cc  void VersionSet::Finalize(Version* v) { int best_level = -1; double best_score = -1; for (int level = 0; level \u0026lt; config::kNumLevels - 1; level++) { double score; //对于Level 0来说，其分数为文件个数 / 4  if (level == 0) { score = v-\u0026gt;files_[level].size() / static_cast\u0026lt;double\u0026gt;(config::kL0_CompactionTrigger); } else { //对于Level 1~5的分数由该层所有文件总大小除以每层允许的最大大小决定  const uint64_t level_bytes = TotalFileSize(v-\u0026gt;files_[level]); score = static_cast\u0026lt;double\u0026gt;(level_bytes) / MaxBytesForLevel(options_, level); } //选取最高分的一个level  if (score \u0026gt; best_score) { best_level = level; best_score = score; } } //更新level与score  v-\u0026gt;compaction_level_ = best_level; v-\u0026gt;compaction_score_ = best_score; }   compaction_score_ 的赋值逻辑如下：\n Level 0：将当前 Level 0 包含的文件个数除以 4 并赋值给 compaction_score_ ，如果 Level 0 的文件个数大于等于 4，则此时 compaction_score_ 会大于等于 1。 Level 1～Level 5：通过该层文件的总大小除以该层文件允许的最大大小并赋值给 compaction_score_。  每次当版本中的 compaction_score_ 大于等于 1 时，则需要进行一次 Compaction 操作。\nSeek Compaction Seek compaction 主要通过记录某个 SSTable 的 Seek 次数，当其无效读取次数到达阈值（allowed_seeks）之后，将会记录下它的 level，参与下一次压缩。\n阈值 allowed_seeks 是每个文件允许的最大无效读取次数，该值的计算代码在 VersionSet::Build 中的 Apply，代码如下：\n1 2  f-\u0026gt;allowed_seeks = static_cast\u0026lt;int\u0026gt;((f-\u0026gt;file_size / 16384U)); if (f-\u0026gt;allowed_seeks \u0026lt; 100) f-\u0026gt;allowed_seeks = 100;   allowed_seeks 计算逻辑为文件大小除以 16384 后取值。但如果计算得到的值小于 100，则将其设置为 100。\n 为什么是除以16384呢？LevelDB作者给出了这样的解释：\n 硬盘中的一次查找操作耗费10ms。 硬盘读取速度为100MB/s，因此读取或者写入1MB数据需要10ms。 执行Compaction操作时，1MB的数据需要25MB数据的I/O，因为从当前层级读取1MB后，相应地需要从下一个层级读取10MB～12MB（因为每一层的最大大小为前一层的10倍，并且考虑到边界重叠的情况，因此执行Compaction操作时需要读取下一层的10MB～12MB数据），然后执行归并排序后写入的10MB～12MB的数据到下一个层级，因此读取加写入最大需要25MB数据的I/O。 因此25次查找（约耗费250ms）约略等于一次执行Compaction操作时处理1MB数据的时间（1MB的当前层读取加10MB～12MB的下一层读取，再加10MB～12MB的下一层写入，约为25MB的数据读取和写入总量，因此也是消耗250ms）。那么一次查找的数据量约略等于处理Compaction操作时的40KB数据（1MB除以25）。进一步保守处理，取16384（16K）这个值，即当查找次数超出allowed_seeks时，执行一次Compaction操作是一个更加合理的选择。   file_to_compact_level_ 的赋值逻辑位于 Version 的 UpdateStats，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // https://github.com/google/leveldb/blob/master/db/version_set.cc  bool Version::UpdateStats(const GetStats\u0026amp; stats) { //此时的f为无效查找的文件  FileMetaData* f = stats.seek_file; if (f != nullptr) { //当前查找无效，allowed_seeks-1  f-\u0026gt;allowed_seeks--; //如果此时allowed_seeks小于等于0，则说明此时到达阈值，则将当前层级记录下来，待进行Compaction  if (f-\u0026gt;allowed_seeks \u0026lt;= 0 \u0026amp;\u0026amp; file_to_compact_ == nullptr) { file_to_compact_ = f; file_to_compact_level_ = stats.seek_file_level; return true; } } return false; }   假设进行无效查找的文件为f（FileMetaData结构），先将 f 的 allowed_seeks 次数减 1，此时判断如果allowed_seeks 变量已经小于等于 0 且 Version 中的 file_to_compact_ 成员变量为空，则将 f 赋值给file_to_compact_ ，并且将 f 所属层级赋值给 file_to_compact_level_ 变量。\nManual Compaction Manual Compaction 是指人工触发的 Compaction，由外部接口调用产生。\n实际其内部触发调用的接口是 DBImpl 中的 CompactRange，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  void DBImpl::CompactRange(const Slice* begin, const Slice* end) { int max_level_with_files = 1; { MutexLock l(\u0026amp;mutex_); Version* base = versions_-\u0026gt;current(); for (int level = 1; level \u0026lt; config::kNumLevels; level++) { if (base-\u0026gt;OverlapInLevel(level, begin, end)) { max_level_with_files = level; } } } //略过所有没有重叠的文件  TEST_CompactMemTable(); //一层层压缩存在重叠的文件  for (int level = 0; level \u0026lt; max_level_with_files; level++) { TEST_CompactRange(level, begin, end); } }   在 Manual Compaction 中会指定的 begin 和 end，它将会一个层层的分次的 Compact 所有 Level 中与 begin 和 end 有重叠（overlap）的 SSTable 文件。\n文件选取 每次进行 Compaction 时，首先决定在哪个层级进行该次操作，假设为Level n，接着选取 Level n 层参与的文件，然后选取 Level n+1 层需要参与的文件，最后对选中的文件使用归并排序生成一个新文件。\nPickCompaction 决定层级 n 以及选取 Level n 层参与文件的方法为 VersionSet 中的 PickCompaction。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Compaction* VersionSet::PickCompaction() { Compaction* c; int level; //根据current_判断是size_compaction还是seek_compaction  const bool size_compaction = (current_-\u0026gt;compaction_score_ \u0026gt;= 1); const bool seek_compaction = (current_-\u0026gt;file_to_compact_ != nullptr); /* 如果根据size_compaction触发，则根据每个层级的compact_pointer_选取本次Compaction的 level n层文件（记录每个层级下一次开始进行Compaction操作时需要从哪个键开始。） */ if (size_compaction) { //获取当前层级  level = current_-\u0026gt;compaction_level_; assert(level \u0026gt;= 0); assert(level + 1 \u0026lt; config::kNumLevels); c = new Compaction(options_, level); //选出第一个在compact_pointer_之后的文件  for (size_t i = 0; i \u0026lt; current_-\u0026gt;files_[level].size(); i++) { FileMetaData* f = current_-\u0026gt;files_[level][i]; if (compact_pointer_[level].empty() || icmp_.Compare(f-\u0026gt;largest.Encode(), compact_pointer_[level]) \u0026gt; 0) { c-\u0026gt;inputs_[0].push_back(f); break; } } //如果通过compact_pointer_没有选取到文件（Compaction已遍历本层），则选取本层第一个文件  if (c-\u0026gt;inputs_[0].empty()) { // Wrap-around to the beginning of the key space  c-\u0026gt;inputs_[0].push_back(current_-\u0026gt;files_[level][0]); } } else if (seek_compaction) { //如果根据seek_compaction触发，则直接将无效查找次数超限的文件选取为本次的Level n层文件  level = current_-\u0026gt;file_to_compact_level_; c = new Compaction(options_, level); c-\u0026gt;inputs_[0].push_back(current_-\u0026gt;file_to_compact_); } else { return nullptr; } c-\u0026gt;input_version_ = current_; c-\u0026gt;input_version_-\u0026gt;Ref(); //对level 0特殊处理  if (level == 0) { InternalKey smallest, largest; //取出Level 0中参与本次Compaction操作的文件的最小键和最大键  GetRange(c-\u0026gt;inputs_[0], \u0026amp;smallest, \u0026amp;largest); //根据最小键和最大键对比Level 0中的所有文件，如果存在文件与[Lkey,Hkey]有重叠，则扩大最小键和最大键范围，并继续查找。  current_-\u0026gt;GetOverlappingInputs(0, \u0026amp;smallest, \u0026amp;largest, \u0026amp;c-\u0026gt;inputs_[0]); assert(!c-\u0026gt;inputs_[0].empty()); } //调用SetupOtherInputs选取level n+1层需要参与的文件  SetupOtherInputs(c); return c; }   执行逻辑如下：\n 根据 current_ 判断是 size_compaction 还是 seek_compaction：  根据 size_compaction 触发：则根据每个层级的 compact_pointer_ 选取本次 Compaction 的 level n 层文件（记录每个层级下一次开始进行 Compaction 时需要从哪个键开始）。 根据 seek_compaction 触发：则直接将无效查找次数超限的文件选取为本次的 Level n 层文件。   对 level 0 特殊处理：  取出 Level 0 中参与本次 Compaction 的文件的最小键和最大键，假设其范围为 [Lkey,Hkey]。 根据最小键和最大键对比 Level 0 中的所有文件，如果存在文件与 [Lkey,Hkey] 有重叠，则扩大最小键和最大键范围，并继续查找。   调用 SetupOtherInputs 选取 level n+1 层需要参与的文件。   为何Level 0中需要扩展有键重叠的文件呢？\n举例说明，假设Level 0有4个文件：f1、f2、f3、f4，每个文件的键范围分别为[c,e]，[a,f]，[a,b]，[i,z]。\n通过第一步选取的inputs_ [0]文件是f1，f1中的键范围和f2有重叠，则扩大最小键和最大键范围到[a,f]，此时发现f3的键范围也和f2有重叠，因此最终inputs_[0]中的文件包括f1、f2、f3三个文件。\n假设有这样一种情况，我们首先写了d这个键，在f2中的序列号为10，然后删除了d，删除操作在f1中的序列号为100，假设Compaction操作时只是选取了f1，则下次查找d这个键时先从Level 0选取，会读取到f2中序列号为10的值（实际上该键已经删除），此时会出现错误。\n SetupOtherInputs PickCompaction 会选定进行 Compaction 操作的层级 n 以及 Level n 层的参与文件，之后会调用SetupOtherInputs 进行 Level n+1 层文件的选取，SetupOtherInputs 的代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/db/version_set.cc  void VersionSet::SetupOtherInputs(Compaction* c) { const int level = c-\u0026gt;level(); InternalKey smallest, largest; AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level], \u0026amp;c-\u0026gt;inputs_[0]); //获取input_[0]所有文件的最大键和最小键  GetRange(c-\u0026gt;inputs_[0], \u0026amp;smallest, \u0026amp;largest); //根据input_[0]中的最大键/最小键查找level n+1层的文件，并分别赋值到input_[1]中  current_-\u0026gt;GetOverlappingInputs(level + 1, \u0026amp;smallest, \u0026amp;largest, \u0026amp;c-\u0026gt;inputs_[1]); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level + 1], \u0026amp;c-\u0026gt;inputs_[1]); InternalKey all_start, all_limit; //继续获取input_[0]和[1]的所有文件的最大键和最小键  GetRange2(c-\u0026gt;inputs_[0], c-\u0026gt;inputs_[1], \u0026amp;all_start, \u0026amp;all_limit); //在不扩大level n+1层的前提下，尝试扩大level n层的文件，并且扩大后的文件总大小不超过50M  if (!c-\u0026gt;inputs_[1].empty()) { std::vector\u0026lt;FileMetaData*\u0026gt; expanded0; current_-\u0026gt;GetOverlappingInputs(level, \u0026amp;all_start, \u0026amp;all_limit, \u0026amp;expanded0); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level], \u0026amp;expanded0); const int64_t inputs0_size = TotalFileSize(c-\u0026gt;inputs_[0]); const int64_t inputs1_size = TotalFileSize(c-\u0026gt;inputs_[1]); const int64_t expanded0_size = TotalFileSize(expanded0); if (expanded0.size() \u0026gt; c-\u0026gt;inputs_[0].size() \u0026amp;\u0026amp; inputs1_size + expanded0_size \u0026lt; ExpandedCompactionByteSizeLimit(options_)) { InternalKey new_start, new_limit; GetRange(expanded0, \u0026amp;new_start, \u0026amp;new_limit); std::vector\u0026lt;FileMetaData*\u0026gt; expanded1; current_-\u0026gt;GetOverlappingInputs(level + 1, \u0026amp;new_start, \u0026amp;new_limit, \u0026amp;expanded1); AddBoundaryInputs(icmp_, current_-\u0026gt;files_[level + 1], \u0026amp;expanded1); if (expanded1.size() == c-\u0026gt;inputs_[1].size()) { Log(options_-\u0026gt;info_log, \u0026#34;Expanding@%d %d+%d (%ld+%ld bytes) to %d+%d (%ld+%ld bytes)\\n\u0026#34;, level, int(c-\u0026gt;inputs_[0].size()), int(c-\u0026gt;inputs_[1].size()), long(inputs0_size), long(inputs1_size), int(expanded0.size()), int(expanded1.size()), long(expanded0_size), long(inputs1_size)); smallest = new_start; largest = new_limit; c-\u0026gt;inputs_[0] = expanded0; c-\u0026gt;inputs_[1] = expanded1; GetRange2(c-\u0026gt;inputs_[0], c-\u0026gt;inputs_[1], \u0026amp;all_start, \u0026amp;all_limit); } } } if (level + 2 \u0026lt; config::kNumLevels) { current_-\u0026gt;GetOverlappingInputs(level + 2, \u0026amp;all_start, \u0026amp;all_limit, \u0026amp;c-\u0026gt;grandparents_); } //将本次Compaction的最大键保存到compact_pointer_中，下次Compaction时根据该值选取level n层文件  compact_pointer_[level] = largest.Encode().ToString(); c-\u0026gt;edit_.SetCompactPointer(level, largest); }   当 Level n 层和 Level n+1 层的文件都已经选定，LevelDB 的实现中有一个优化点，即判断是否可以在不扩大 Level n+1 层文件个数的情况下，将 Level n 层的文件个数扩大，优化逻辑如下：\n1. inputs_ [1] 选取完毕之后，首先计算 inputs_ [0] 和 inputs_ [1] 所有文件的最大/最小键范围，然后通过该范围重新去 Level n 层计算 inputs_ [0]，此时有可能选取到新的文件进入 inputs_ [0]。\r2. 通过新的 inputs_ [0] 的键范围重新选取 inputs_ [1] 中的文件，如果 inputs_ [1] 中的文件个数不变并且扩大范围后所有文件的总大小不超过50MB，则使用新的 inputs_ [0] 进行本次 Compaction ，否则继续使用原来的inputs_ [0]。50 MB 的限制是防止执行一次 Compaction 导致大量的 I/O 操作，从而影响系统性能。\r3. 如果扩大 Level n 层的文件个数之后导致 Level n+1 层的文件个数也进行了扩大，则不能进行此次优化。因为Level 1到 Level 6 的所有文件键范围不能有重叠，如果继续执行该优化，会导致 Compaction 之后 Level n+1 层的文件有键重叠的情况产生。\r 整体流程 其代码实现在 DBImpl 中的 DoCompactionWork，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::DoCompactionWork(CompactionState* compact) { //...  //计算最小序列号，如果Compaction中存在重复写入或者删除的键，则根据序列号判断是否需要删除  //如果没有快照，则将当前版本的last_sequence赋值为最小的序列号  if (snapshots_.empty()) { compact-\u0026gt;smallest_snapshot = versions_-\u0026gt;LastSequence(); } else { //如果有则根据最老的快照获取序列号  compact-\u0026gt;smallest_snapshot = snapshots_.oldest()-\u0026gt;sequence_number(); } //获取一个归并排序迭代器，每次选取最小的键写入文件  Iterator* input = versions_-\u0026gt;MakeInputIterator(compact-\u0026gt;compaction); //...  input-\u0026gt;SeekToFirst(); //...  //遍历迭代器  while (input-\u0026gt;Valid() \u0026amp;\u0026amp; !shutting_down_.load(std::memory_order_acquire)) { //...  //用于标记一个文件是否需要删除  bool drop = false; //...  //如果需要写入新文件，则写入到SSTable中  if (!drop) { if (compact-\u0026gt;builder == nullptr) { status = OpenCompactionOutputFile(compact); if (!status.ok()) { break; } } if (compact-\u0026gt;builder-\u0026gt;NumEntries() == 0) { compact-\u0026gt;current_output()-\u0026gt;smallest.DecodeFrom(key); } compact-\u0026gt;current_output()-\u0026gt;largest.DecodeFrom(key); compact-\u0026gt;builder-\u0026gt;Add(key, input-\u0026gt;value()); if (compact-\u0026gt;builder-\u0026gt;FileSize() \u0026gt;= compact-\u0026gt;compaction-\u0026gt;MaxOutputFileSize()) { status = FinishCompactionOutputFile(compact, input); if (!status.ok()) { break; } } } //继续查找归并排序中下一个最小的键  input-\u0026gt;Next(); } //...  //生成一个SSTable文件并刷新到磁盘  if (status.ok() \u0026amp;\u0026amp; compact-\u0026gt;builder != nullptr) { status = FinishCompactionOutputFile(compact, input); } //...  //调用VersionSet中的LogAndApply生成新的版本  if (status.ok()) { status = InstallCompactionResults(compact); } //... }   DoCompactionWork 的执行步骤如下：\n 计算一个本次 Compaction 的最小序列号值，如果有快照，则取最老的快照的序列号，如果没有快照，则选取当前版本 current_ 的序列号。因为快照存在时需要有一个一致性的读取视图，因此如果一个键的序列号比该值大，则该键不能够删除。 生成一个归并排序的迭代器，该迭代器会遍历 inputs_ [0] 和 inputs_ [1] 中的所有文件，每次选取一个最小的键写入新生成的文件。 选取键之后，判断该键是否可以删除，两种情况下可以删除一个键：  第一种情况为重复写入一个键，因为新键的序列号更大，因此之前被覆盖的键可以删除（当然被删除键的序列号需要小于第一步中计算得到的最小序列号）。 第二种情况为删除了一个键（实际上也是写入该键，不过被标记为删除操作），并且更高层级没有该键，则该键可以彻底删除（前提也是该键的序列号需要小于第 1 步中计算得到的最小序列号），即不需要写入新生成的 SSTable。   如果该键不需要删除，则将其写入新生成的 SSTable，并且当一个 SSTable 大小大于 2 MB 时，将该文件刷新到磁盘并且重新打开一个新的 SSTable。 执行 VersionSet 的 LogAndApply，生成一个新的版本并挂载到 VersionSet 中，并且将新版本赋值为当前版本。  垃圾回收 随着 Compaction 操作的进行，会有新文件生成，生成新文件之后可以进行旧文件清理。每次当一个 MemTable生成 SSTable 并刷新到磁盘之后，该 MemTable 对应的日志也可以进行删除。\nLevelDB 中负责清理文件的是 RemoveObsoleteFiles，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  void DBImpl::RemoveObsoleteFiles() { mutex_.AssertHeld(); if (!bg_error_.ok()) { return; } //将所有正在Compaction和versionset中的版本文件放入live集合  std::set\u0026lt;uint64_t\u0026gt; live = pending_outputs_; versions_-\u0026gt;AddLiveFiles(\u0026amp;live); //将所有数据目录下的文件放入filenames数组中  std::vector\u0026lt;std::string\u0026gt; filenames; env_-\u0026gt;GetChildren(dbname_, \u0026amp;filenames); // Ignoring errors on purpose  uint64_t number; FileType type; std::vector\u0026lt;std::string\u0026gt; files_to_delete; //保存可以删除的文件  //遍历所有文件，判断是否可以删除  for (std::string\u0026amp; filename : filenames) { //解析出每个文件的序列号和类型  if (ParseFileName(filename, \u0026amp;number, \u0026amp;type)) { //keep标记文件是否需要保留  bool keep = true; switch (type) { //删除序列号小于versionset的log_number，且不等于prev_log_number的日志  case kLogFile: keep = ((number \u0026gt;= versions_-\u0026gt;LogNumber()) || (number == versions_-\u0026gt;PrevLogNumber())); break; //删除版本较低的Manifest  case kDescriptorFile: keep = (number \u0026gt;= versions_-\u0026gt;ManifestFileNumber()); break; //删除没有参与Compaction的且不在versionset中的sstable  case kTableFile: keep = (live.find(number) != live.end()); break; //临时文件  case kTempFile: keep = (live.find(number) != live.end()); break; case kCurrentFile: case kDBLockFile: case kInfoLogFile: keep = true; break; } //将keep为false的放入files_to_delete，后续删除。  if (!keep) { files_to_delete.push_back(std::move(filename)); //如果是sstable文件则从缓存中删除  if (type == kTableFile) { table_cache_-\u0026gt;Evict(number); } Log(options_.info_log, \u0026#34;Delete type=%d #%lld\\n\u0026#34;, static_cast\u0026lt;int\u0026gt;(type), static_cast\u0026lt;unsigned long long\u0026gt;(number)); } } } //删除所有files_to_delete中的文件  mutex_.Unlock(); for (const std::string\u0026amp; filename : files_to_delete) { env_-\u0026gt;RemoveFile(dbname_ + \u0026#34;/\u0026#34; + filename); } mutex_.Lock(); }   执行逻辑如下：\n 将正在进行 Compaction 操作的 SSTable 文件和 VersionSet 的所有版本中的 SSTable 文件放入 live 集合中。 通过 filename 找到对应的文件，解析出文件序列号和文件类型。 遍历所有文件，判断是否需要删除，如果需要删除则将 keep 标记为 false，并放入 files_to_delete 中。 遍历 files_to_delete 数组，调用 RemoveObsoleteFiles 删除文件。  ","date":"2022-05-23T23:47:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-compaction%E6%A8%A1%E5%9D%97/","title":"LevelDB Compaction模块"},{"content":"版本管理  为什么 LevelDB 需要版本的概念呢？\n 针对共享的资源，有三种方式：\n 悲观锁：这是最简单的处理方式。加锁保护，读写互斥。效率低。 乐观锁：它假设多用户并发的事物在处理时不会彼此互相影响，各事务能够在不产生锁的的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。 果其他事务有更新的话，正在提交的事务会进行回滚；这样做不会有锁竞争更不会产生死锁， 但如果数据竞争的概率较高，效率也会受影响 。 MVCC：MVCC 是一个数据库常用的概念。Multi Version Concurrency Control 多版本并发控制。每一个执行操作的用户，看到的都是数据库特定时刻的的快照 （Snapshot）， Writer 的任何未完成的修改都不会被其他的用户所看到；当对数据进行更新的时候并是不直接覆盖，而是先进行标记，然后在其他地方添加新的数据（这些变更存储在 VersionEdit），从而形成一个新版本，此时再来读取的 Reader 看到的就是最新的版本了。所以这种处理策略是维护了多个版本的数据的，但只有一个是最新的（VersionSet 中维护着全局最新的 Seqnum）。  LevelDB 通过 Version 以及 VersionSet 来管理元信息，用 Manifest 来保存元信息。\nManifest Manifest 文件专用于记录版本信息。LevelDB 采用了增量式的存储方式，记录每一个版本相较于一个版本的变化情况。\n 变化情况大致包括：\n（1）新增了哪些 SSTable 文件；\n（2）删除了哪些 SSTable 文件（由于Compaction导致）；\n（3）最新的 Journal 日志文件标号等；\n 一个 Manifest 文件中，包含了多条 Session Record。其中第一条 Session Record 记载了当时 LevelDB 的全量版本信息，其余若干条 Session Record 仅记录每次更迭的变化情况。具体结构如下图：\nLevelDB 启动时会先到数据目录寻找一个名为 CURRENT 的文件，该文件中会保存 Manifest 的文件名称，通过读取 Manifest 记录的 Session Record，从初始状态开始不断地应用这些版本改动，即可使得系统的版本信息恢复到最近一次使用的状态。\nVersion Version 表示当前的一个版本，该结构中会保存每个层级拥有的文件信息以及指向前一个和后一个版本的指针等。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85  class Version { public: struct GetStats { FileMetaData* seek_file; int seek_file_level; }; void AddIterators(const ReadOptions\u0026amp;, std::vector\u0026lt;Iterator*\u0026gt;* iters); Status Get(const ReadOptions\u0026amp;, const LookupKey\u0026amp; key, std::string* val, GetStats* stats); bool UpdateStats(const GetStats\u0026amp; stats); bool RecordReadSample(Slice key); void Ref(); void Unref(); void GetOverlappingInputs( int level, const InternalKey* begin, // nullptr means before all keys  const InternalKey* end, // nullptr means after all keys  std::vector\u0026lt;FileMetaData*\u0026gt;* inputs); bool OverlapInLevel(int level, const Slice* smallest_user_key, const Slice* largest_user_key); int PickLevelForMemTableOutput(const Slice\u0026amp; smallest_user_key, const Slice\u0026amp; largest_user_key); int NumFiles(int level) const { return files_[level].size(); } std::string DebugString() const; private: friend class Compaction; friend class VersionSet; class LevelFileNumIterator; explicit Version(VersionSet* vset) : vset_(vset), next_(this), prev_(this), refs_(0), file_to_compact_(nullptr), file_to_compact_level_(-1), compaction_score_(-1), compaction_level_(-1) {} Version(const Version\u0026amp;) = delete; Version\u0026amp; operator=(const Version\u0026amp;) = delete; ~Version(); Iterator* NewConcatenatingIterator(const ReadOptions\u0026amp;, int level) const; void ForEachOverlapping(Slice user_key, Slice internal_key, void* arg, bool (*func)(void*, int, FileMetaData*)); VersionSet* vset_; // VersionSet to which this Version belongs  Version* next_; // Next version in linked list  Version* prev_; // Previous version in linked list  int refs_; // Number of live refs to this version  std::vector\u0026lt;FileMetaData*\u0026gt; files_[config::kNumLevels]; FileMetaData* file_to_compact_; int file_to_compact_level_; double compaction_score_; int compaction_level_; }; struct FileMetaData { FileMetaData() : refs(0), allowed_seeks(1 \u0026lt;\u0026lt; 30), file_size(0) {} int refs;\t// 引用计数  int allowed_seeks; // 用于seek compaction  uint64_t number;\t// 唯一标识一个sstable  uint64_t file_size; // 文件大小  InternalKey smallest; // 最小key  InternalKey largest; // 最大key };    成员变量：  GetStats：键查找时用来保存中间状态的一个结构。 vset_：该版本属于的版本集合。 next_：指向后一个版本的指针。 prev_：指向前一个版本的指针。 refs_：该版本的引用计数。 files_：每个层级所包含的 SSTable 文件，每一个文件以一个 FileMetaData 结构表示。 file_to_compact_：下次需要进行 Compaction 操作的文件。 file_to_compact_level_：下次需要进行 Compaction 操作的文件所属的层级。 compaction_score_：如果 compaction_score_ 大于 1，说明需要进行一次 Compaction 操作。 compaction_level_：表明需要进行 Compaction 操作的层级。    VersionEdit **VersionEdit 是一个版本的中间状态，会保存一次 Compaction 操作后增加的删除文件信息以及其他一些元数据。**当数据库正常/非正常关闭，重新打开时，只需要按顺序把 Manifest 文件中的 VersionEdit 执行一遍，就可以把数据恢复到宕机前的最新版本。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  // https://github.com/google/leveldb/blob/master/db/version_edit.h  class VersionEdit { public: VersionEdit() { Clear(); } ~VersionEdit() = default; void Clear(); void SetComparatorName(const Slice\u0026amp; name) { has_comparator_ = true; comparator_ = name.ToString(); } void SetLogNumber(uint64_t num) { has_log_number_ = true; log_number_ = num; } void SetPrevLogNumber(uint64_t num) { has_prev_log_number_ = true; prev_log_number_ = num; } void SetNextFile(uint64_t num) { has_next_file_number_ = true; next_file_number_ = num; } void SetLastSequence(SequenceNumber seq) { has_last_sequence_ = true; last_sequence_ = seq; } void SetCompactPointer(int level, const InternalKey\u0026amp; key) { compact_pointers_.push_back(std::make_pair(level, key)); } void AddFile(int level, uint64_t file, uint64_t file_size, const InternalKey\u0026amp; smallest, const InternalKey\u0026amp; largest) { FileMetaData f; f.number = file; f.file_size = file_size; f.smallest = smallest; f.largest = largest; new_files_.push_back(std::make_pair(level, f)); } void RemoveFile(int level, uint64_t file) { deleted_files_.insert(std::make_pair(level, file)); } void EncodeTo(std::string* dst) const; Status DecodeFrom(const Slice\u0026amp; src); std::string DebugString() const; private: friend class VersionSet; typedef std::set\u0026lt;std::pair\u0026lt;int, uint64_t\u0026gt;\u0026gt; DeletedFileSet; std::string comparator_; uint64_t log_number_;\t//已经弃用  uint64_t prev_log_number_; uint64_t next_file_number_; SequenceNumber last_sequence_; bool has_comparator_; bool has_log_number_; bool has_prev_log_number_; bool has_next_file_number_; bool has_last_sequence_; std::vector\u0026lt;std::pair\u0026lt;int, InternalKey\u0026gt;\u0026gt; compact_pointers_; DeletedFileSet deleted_files_; std::vector\u0026lt;std::pair\u0026lt;int, FileMetaData\u0026gt;\u0026gt; new_files_; };    成员变量：  comparator_：比较器名称。 log_number_：日志文件序号。 **next_file_number_：**下一个文件序列号。 last_sequence_：下一个写入序列号。 has_xxxxx_：has_comparator_ ，has_log_number_ ，has_prev_log_number_ ，has_last_sequence_ ，has_next_file_number_，布尔型变量，表明相应的成员变量是否已经设置。 compact_pointers_：该变量用来指示 LevelDB 中每个层级下一次进行 Compaction 操作时需要从哪个键开始。 **deleted_files_：**记录每个层级执行 Compaction 操作之后删除掉的文件。 new_files_：记录每个层级执行 Compaction 操作之后新增的文件。新增文件记录为一个个FileMetaData 结构体。    EncodeTo EncodeTo 会将 VersionEdit 各个成员变量的信息编码为一个字符串，编码时会先给每个成员变量定义一个 Tag，Tag的枚举值如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  enum Tag { kComparator = 1,\t//比较器  kLogNumber = 2,\t//日志文件序列号  kNextFileNumber = 3,\t//下一个文件序列号  kLastSequence = 4,\t//下一个写入序列号  kCompactPointer = 5,\t//CompactPointer类型  kDeletedFile = 6,\t//删除的文件  kNewFile = 7,\t//增加的文件  // 8 曾经用于大Value的引用，现以弃用  kPrevLogNumber = 9\t//前一个日志文件序列号 };   接下来看看 EncodeTo 的实现逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  void VersionEdit::EncodeTo(std::string* dst) const { //如果为true，则先将kComparator Tag编码，然后将比较器名称编码写入。  if (has_comparator_) { PutVarint32(dst, kComparator); PutLengthPrefixedSlice(dst, comparator_); } //与上面类似，就不再重复  if (has_log_number_) { PutVarint32(dst, kLogNumber); PutVarint64(dst, log_number_); } if (has_prev_log_number_) { PutVarint32(dst, kPrevLogNumber); PutVarint64(dst, prev_log_number_); } if (has_next_file_number_) { PutVarint32(dst, kNextFileNumber); PutVarint64(dst, next_file_number_); } if (has_last_sequence_) { PutVarint32(dst, kLastSequence); PutVarint64(dst, last_sequence_); } //依次将compact_pointers_中的level和Key编码  for (size_t i = 0; i \u0026lt; compact_pointers_.size(); i++) { PutVarint32(dst, kCompactPointer); PutVarint32(dst, compact_pointers_[i].first); // level  PutLengthPrefixedSlice(dst, compact_pointers_[i].second.Encode()); } //依次将deleted_file_中的level和file number编码  for (const auto\u0026amp; deleted_file_kvp : deleted_files_) { PutVarint32(dst, kDeletedFile); PutVarint32(dst, deleted_file_kvp.first); // level  PutVarint64(dst, deleted_file_kvp.second); // file number  } //依次将new_files_中的level和FileMetaData进行编码  for (size_t i = 0; i \u0026lt; new_files_.size(); i++) { const FileMetaData\u0026amp; f = new_files_[i].second; PutVarint32(dst, kNewFile); PutVarint32(dst, new_files_[i].first); // level  //编码FileMetaData  PutVarint64(dst, f.number); PutVarint64(dst, f.file_size); PutLengthPrefixedSlice(dst, f.smallest.Encode()); PutLengthPrefixedSlice(dst, f.largest.Encode()); } }   EncodeTo 函数会依次以 Tag 开头，将比较器名称、日志序列号、上一个日志序列号、下一个文件序列号、最后一个序列号、CompactPointers、每个层级删除的文件以及增加的文件信息保存到一个字符串中。\nDecodeFrom 解码逻辑与上面的类似，根据不同的 tag 解析对应的成员变量，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102  // https://github.com/google/leveldb/blob/master/db/version_edit.cc  Status VersionEdit::DecodeFrom(const Slice\u0026amp; src) { Clear(); Slice input = src; const char* msg = nullptr; uint32_t tag; // Temporary storage for parsing  int level; uint64_t number; FileMetaData f; Slice str; InternalKey key; while (msg == nullptr \u0026amp;\u0026amp; GetVarint32(\u0026amp;input, \u0026amp;tag)) { //根据不同的tag执行对应的解码逻辑  switch (tag) { case kComparator: if (GetLengthPrefixedSlice(\u0026amp;input, \u0026amp;str)) { comparator_ = str.ToString(); has_comparator_ = true; } else { msg = \u0026#34;comparator name\u0026#34;; } break; case kLogNumber: if (GetVarint64(\u0026amp;input, \u0026amp;log_number_)) { has_log_number_ = true; } else { msg = \u0026#34;log number\u0026#34;; } break; case kPrevLogNumber: if (GetVarint64(\u0026amp;input, \u0026amp;prev_log_number_)) { has_prev_log_number_ = true; } else { msg = \u0026#34;previous log number\u0026#34;; } break; case kNextFileNumber: if (GetVarint64(\u0026amp;input, \u0026amp;next_file_number_)) { has_next_file_number_ = true; } else { msg = \u0026#34;next file number\u0026#34;; } break; case kLastSequence: if (GetVarint64(\u0026amp;input, \u0026amp;last_sequence_)) { has_last_sequence_ = true; } else { msg = \u0026#34;last sequence number\u0026#34;; } break; case kCompactPointer: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;key)) { compact_pointers_.push_back(std::make_pair(level, key)); } else { msg = \u0026#34;compaction pointer\u0026#34;; } break; case kDeletedFile: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;number)) { deleted_files_.insert(std::make_pair(level, number)); } else { msg = \u0026#34;deleted file\u0026#34;; } break; case kNewFile: if (GetLevel(\u0026amp;input, \u0026amp;level) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;f.number) \u0026amp;\u0026amp; GetVarint64(\u0026amp;input, \u0026amp;f.file_size) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;f.smallest) \u0026amp;\u0026amp; GetInternalKey(\u0026amp;input, \u0026amp;f.largest)) { new_files_.push_back(std::make_pair(level, f)); } else { msg = \u0026#34;new-file entry\u0026#34;; } break; default: msg = \u0026#34;unknown tag\u0026#34;; break; } } if (msg == nullptr \u0026amp;\u0026amp; !input.empty()) { msg = \u0026#34;invalid tag\u0026#34;; } Status result; if (msg != nullptr) { result = Status::Corruption(\u0026#34;VersionEdit\u0026#34;, msg); } return result; }   VersionSet LevelDB 为了支持 MVCC，引入了 Version 和 VersionEdit 的概念，那么如何来有效的管理这些 Version 呢？于是引出了 VersionSet，VersionSet 是一个双向链表，而且整个 DB 只会有一个 VersionSet。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107  class VersionSet { public: VersionSet(const std::string\u0026amp; dbname, const Options* options, TableCache* table_cache, const InternalKeyComparator*); VersionSet(const VersionSet\u0026amp;) = delete; VersionSet\u0026amp; operator=(const VersionSet\u0026amp;) = delete; ~VersionSet(); Status LogAndApply(VersionEdit* edit, port::Mutex* mu) EXCLUSIVE_LOCKS_REQUIRED(mu); Status Recover(bool* save_manifest); Version* current() const { return current_; } uint64_t ManifestFileNumber() const { return manifest_file_number_; } uint64_t NewFileNumber() { return next_file_number_++; } void ReuseFileNumber(uint64_t file_number) { if (next_file_number_ == file_number + 1) { next_file_number_ = file_number; } } int NumLevelFiles(int level) const; int64_t NumLevelBytes(int level) const; uint64_t LastSequence() const { return last_sequence_; } void SetLastSequence(uint64_t s) { assert(s \u0026gt;= last_sequence_); last_sequence_ = s; } void MarkFileNumberUsed(uint64_t number); uint64_t LogNumber() const { return log_number_; } uint64_t PrevLogNumber() const { return prev_log_number_; } Compaction* PickCompaction(); Compaction* CompactRange(int level, const InternalKey* begin, const InternalKey* end); int64_t MaxNextLevelOverlappingBytes(); Iterator* MakeInputIterator(Compaction* c); bool NeedsCompaction() const { Version* v = current_; return (v-\u0026gt;compaction_score_ \u0026gt;= 1) || (v-\u0026gt;file_to_compact_ != nullptr); } void AddLiveFiles(std::set\u0026lt;uint64_t\u0026gt;* live); uint64_t ApproximateOffsetOf(Version* v, const InternalKey\u0026amp; key); struct LevelSummaryStorage { char buffer[100]; }; const char* LevelSummary(LevelSummaryStorage* scratch) const; private: class Builder; friend class Compaction; friend class Version; bool ReuseManifest(const std::string\u0026amp; dscname, const std::string\u0026amp; dscbase); void Finalize(Version* v); void GetRange(const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs, InternalKey* smallest, InternalKey* largest); void GetRange2(const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs1, const std::vector\u0026lt;FileMetaData*\u0026gt;\u0026amp; inputs2, InternalKey* smallest, InternalKey* largest); void SetupOtherInputs(Compaction* c); Status WriteSnapshot(log::Writer* log); void AppendVersion(Version* v); Env* const env_; const std::string dbname_; const Options* const options_; TableCache* const table_cache_; const InternalKeyComparator icmp_; uint64_t next_file_number_; uint64_t manifest_file_number_; uint64_t last_sequence_; uint64_t log_number_; uint64_t prev_log_number_; // 0 or backing store for memtable being compacted  WritableFile* descriptor_file_; log::Writer* descriptor_log_; Version dummy_versions_; // Head of circular doubly-linked list of versions.  Version* current_; // == dummy_versions_.prev_  std::string compact_pointer_[config::kNumLevels]; };    关键成员变量：  **next_file_number_**下一个文件序列号。 manifest_file_number_：Manifest 文件的文件序列号。 last_sequence_：当前最大的写入序列号。 log_number_：Log文件的文件序列号。 current_：当前的最新版本。 compact_pointer_：记录每个层级下一次开始进行 Compaction 操作时需要从哪个键开始。    下面就介绍一下其中最核心的 LogAndApply 和 Recover 两个函数。\nLogAndApply 当每次进行完 Compaction 操作后，需要调用并执行 VersionSet 中的 LogAndApply 写入版本变化后并生成一个新的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) { //根据版本变化进行处理  if (edit-\u0026gt;has_log_number_) { assert(edit-\u0026gt;log_number_ \u0026gt;= log_number_); assert(edit-\u0026gt;log_number_ \u0026lt; next_file_number_); } else { edit-\u0026gt;SetLogNumber(log_number_); } if (!edit-\u0026gt;has_prev_log_number_) { edit-\u0026gt;SetPrevLogNumber(prev_log_number_); } edit-\u0026gt;SetNextFile(next_file_number_); edit-\u0026gt;SetLastSequence(last_sequence_); //生成新的版本，其为上一个Version+VersionEdit  Version* v = new Version(this); { Builder builder(this, current_); builder.Apply(edit); builder.SaveTo(v); } //计算这个新version的compact score和compact level，算出最应该被Compact的level  Finalize(v); std::string new_manifest_file; Status s; if (descriptor_log_ == nullptr) { //这里没有必要进行unlock操作，因为只有在第一次调用，也就是打开数据库的时候才会走到这个路径里面来  assert(descriptor_file_ == nullptr); new_manifest_file = DescriptorFileName(dbname_, manifest_file_number_); edit-\u0026gt;SetNextFile(next_file_number_); s = env_-\u0026gt;NewWritableFile(new_manifest_file, \u0026amp;descriptor_file_); //此时重新开启一个新的MANIFEST文件，并将当前状态作为base状态写入快照  if (s.ok()) { descriptor_log_ = new log::Writer(descriptor_file_); //写入当前快照  s = WriteSnapshot(descriptor_log_); } } //在MANIFEST写入磁盘时解锁（此时没必要加锁，由后台线程完成）  { mu-\u0026gt;Unlock(); 后将其写入磁盘 if (s.ok()) { std::string record; edit-\u0026gt;EncodeTo(\u0026amp;record); //将版本变化写入MANIFEST  s = descriptor_log_-\u0026gt;AddRecord(record); if (s.ok()) { //将MANIFEST写入磁盘文件  s = descriptor_file_-\u0026gt;Sync(); } if (!s.ok()) { Log(options_-\u0026gt;info_log, \u0026#34;MANIFEST write: %s\\n\u0026#34;, s.ToString().c_str()); } } //当产生新的Manifest时更新current  if (s.ok() \u0026amp;\u0026amp; !new_manifest_file.empty()) { s = SetCurrentFile(env_, dbname_, manifest_file_number_); } mu-\u0026gt;Lock(); } // 将新生成的版本挂在到VersionSet，并且将当前版本（current_）设置为新生成的版本。  if (s.ok()) { AppendVersion(v); log_number_ = edit-\u0026gt;log_number_; prev_log_number_ = edit-\u0026gt;prev_log_number_; } else { delete v; if (!new_manifest_file.empty()) { delete descriptor_log_; delete descriptor_file_; descriptor_log_ = nullptr; descriptor_file_ = nullptr; env_-\u0026gt;RemoveFile(new_manifest_file); } } return s; }   执行逻辑如下:\n 将当前的版本根据版本变化（VersionEdit）进行处理，然后生成一个新的版本。 如果是第一次调用，则创建一个新的 Manifest 文件，并将当前状态作为 base 写入。 调用 Finalize 算出下一次需要 Compaction 的 level。 将版本变化写入 Manifest，把 Manifest 写入磁盘。 将新生成的版本挂载到 VersionSet 中，并且将 current_ 设置为新生成的版本。  Recover Recover 会根据 Manifest 文件中记录的每次版本变化（调用 VersionEdit 的 DecodeFrom 方法）逐次回放生成一个最新的版本。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135  // https://github.com/google/leveldb/blob/master/db/version_set.cc  Status VersionSet::Recover(bool* save_manifest) { struct LogReporter : public log::Reader::Reporter { Status* status; void Corruption(size_t bytes, const Status\u0026amp; s) override { if (this-\u0026gt;status-\u0026gt;ok()) *this-\u0026gt;status = s; } }; //读取CURRENT文件，找到Manifest文件  std::string current; Status s = ReadFileToString(env_, CurrentFileName(dbname_), \u0026amp;current); if (!s.ok()) { return s; } if (current.empty() || current[current.size() - 1] != \u0026#39;\\n\u0026#39;) { return Status::Corruption(\u0026#34;CURRENT file does not end with newline\u0026#34;); } current.resize(current.size() - 1); std::string dscname = dbname_ + \u0026#34;/\u0026#34; + current; SequentialFile* file; s = env_-\u0026gt;NewSequentialFile(dscname, \u0026amp;file); if (!s.ok()) { if (s.IsNotFound()) { return Status::Corruption(\u0026#34;CURRENT points to a non-existent file\u0026#34;, s.ToString()); } return s; } bool have_log_number = false; bool have_prev_log_number = false; bool have_next_file = false; bool have_last_sequence = false; uint64_t next_file = 0; uint64_t last_sequence = 0; uint64_t log_number = 0; uint64_t prev_log_number = 0; Builder builder(this, current_); int read_records = 0; { LogReporter reporter; reporter.status = \u0026amp;s; log::Reader reader(file, \u0026amp;reporter, true /*checksum*/, 0 /*initial_offset*/); Slice record; std::string scratch; //读取versionedit并获取变更  while (reader.ReadRecord(\u0026amp;record, \u0026amp;scratch) \u0026amp;\u0026amp; s.ok()) { ++read_records; VersionEdit edit; s = edit.DecodeFrom(record); if (s.ok()) { if (edit.has_comparator_ \u0026amp;\u0026amp; edit.comparator_ != icmp_.user_comparator()-\u0026gt;Name()) { s = Status::InvalidArgument( edit.comparator_ + \u0026#34; does not match existing comparator \u0026#34;, icmp_.user_comparator()-\u0026gt;Name()); } } if (s.ok()) { builder.Apply(\u0026amp;edit); } if (edit.has_log_number_) { log_number = edit.log_number_; have_log_number = true; } if (edit.has_prev_log_number_) { prev_log_number = edit.prev_log_number_; have_prev_log_number = true; } if (edit.has_next_file_number_) { next_file = edit.next_file_number_; have_next_file = true; } if (edit.has_last_sequence_) { last_sequence = edit.last_sequence_; have_last_sequence = true; } } } delete file; file = nullptr; if (s.ok()) { if (!have_next_file) { s = Status::Corruption(\u0026#34;no meta-nextfile entry in descriptor\u0026#34;); } else if (!have_log_number) { s = Status::Corruption(\u0026#34;no meta-lognumber entry in descriptor\u0026#34;); } else if (!have_last_sequence) { s = Status::Corruption(\u0026#34;no last-sequence-number entry in descriptor\u0026#34;); } if (!have_prev_log_number) { prev_log_number = 0; } MarkFileNumberUsed(prev_log_number); MarkFileNumberUsed(log_number); } if (s.ok()) { //生成最新版本  Version* v = new Version(this); builder.SaveTo(v); Finalize(v); //加入versionset并设置current指针  AppendVersion(v); manifest_file_number_ = next_file; next_file_number_ = next_file + 1; last_sequence_ = last_sequence; log_number_ = log_number; prev_log_number_ = prev_log_number; //判断是否能够复用已有MANIFEST文件  if (ReuseManifest(dscname, current)) { } else { *save_manifest = true; } } else { std::string error = s.ToString(); Log(options_-\u0026gt;info_log, \u0026#34;Error recovering version set with %d records: %s\u0026#34;, read_records, error.c_str()); } return s; }   执行流程如下：\n 通过 current_ 获取到 Manifest，读取 Manifest 获取 base 状态。 读取 Manifest 文件中的 VersionEdit，并执行变更。 生成最终的版本，并将其加入 VersionSet，更新 current_。 判断是否能够复用已有 MANIFEST 文件。  ","date":"2022-05-23T23:43:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/","title":"LevelDB 版本管理"},{"content":"WAL日志模块 当向 LevelDB 写入数据时，只需要将数据写入内存中的 MemTable，而由于内存是易失性存储，因此 LevelDB 需要一个额外的持久化文件：预写日志（Write-Ahead Log，WAL），又称重做日志。这是一个追加修改、顺序写入磁盘的文件。当宕机或者程序崩溃时 WAL 能够保证写入成功的数据不会丢失。将 MemTable 成功写入 SSTable 后，相应的预写日志就可以删除了。\n结构 Log文件以块为基本单位，一条记录可能全部写到一个块上，也可能跨几个块。记录的格式如下图所示：\n首先我们来看看 Log 中的数据格式，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // https://github.com/google/leveldb/blob/master/db/log_format.h  namespace log { enum RecordType { // Zero is reserved for preallocated files  kZeroType = 0, kFullType = 1, // For fragments  kFirstType = 2, kMiddleType = 3, kLastType = 4 }; static const int kMaxRecordType = kLastType; static const int kBlockSize = 32768; // Header is checksum (4 bytes), length (2 bytes), type (1 byte).  static const int kHeaderSize = 4 + 2 + 1; } // namespace log   结合上面的代码和图片，我们可以看到每一个块大小为 32768 字节，并且每一个块由头部和正文组成。头部由 4 字节校验，2 字节的长度与 1 字节的类型构成，即每一个块的开始 7 字节属于头部。头部中的类型字段有如下 4 种：\n kZeroType：为预分配的文件保留。 kFullType：表示一条记录完整地写到了一个块上。 kFirstType：表示该条记录的第一部分。 kMiddleType：表示该条记录的中间部分。 kLastType：表示该条记录的最后一部分。  通过记录结构可以推测出 Log 文件的读取流程，即首先根据头部的长度字段确定需要读取多少字节，然后根据头部类型字段确定该条记录是否已经完整读取，如果没有完整读取，继续按该流程进行，直到读取到记录的最后一部分，其头部类型为 kLastType。\n读写流程 写入 Log 的读取主要由 Writer 中的 AddRecord 实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  // https://github.com/google/leveldb/blob/master/db/log_writer.h  class Writer { public: explicit Writer(WritableFile* dest); Writer(WritableFile* dest, uint64_t dest_length); Writer(const Writer\u0026amp;) = delete; Writer\u0026amp; operator=(const Writer\u0026amp;) = delete; ~Writer(); Status AddRecord(const Slice\u0026amp; slice); private: Status EmitPhysicalRecord(RecordType type, const char* ptr, size_t length); WritableFile* dest_; int block_offset_; // Current offset in block  uint32_t type_crc_[kMaxRecordType + 1]; }; // https://github.com/google/leveldb/blob/master/db/log_writer.cc  Status Writer::AddRecord(const Slice\u0026amp; slice) { const char* ptr = slice.data(); size_t left = slice.size(); Status s; //begin表明本条记录是第一次写入，即当前块中第一条记录  bool begin = true; do { //当前块剩余空间，用于判断头部能否完整写入  const int leftover = kBlockSize - block_offset_; assert(leftover \u0026gt;= 0); if (leftover \u0026lt; kHeaderSize) { //如果块剩余空间小于七个字节且不等于0，说明当前无法完整写入数据，此时填充\\x00，从下一个块写入  if (leftover \u0026gt; 0) { static_assert(kHeaderSize == 7, \u0026#34;\u0026#34;); dest_-\u0026gt;Append(Slice(\u0026#34;\\x00\\x00\\x00\\x00\\x00\\x00\u0026#34;, leftover)); } //此时块正好写满，将block_offset_置为0，表明开始写入新的块  block_offset_ = 0; } assert(kBlockSize - block_offset_ - kHeaderSize \u0026gt;= 0); //计算块剩余空间  const size_t avail = kBlockSize - block_offset_ - kHeaderSize; //计算当前块能够写入的数据大小（块剩余空间和记录剩余内容中最小的）  const size_t fragment_length = (left \u0026lt; avail) ? left : avail; RecordType type; //end表明该记录是否已经完整写入，即最后一条记录  const bool end = (left == fragment_length); //根据begin与end来确定记录类型  if (begin \u0026amp;\u0026amp; end) { //记录为第一条且同时又是最后一条，说明当前是完整的记录，状态为kFullType  type = kFullType; } else if (begin) { //记录为第一条，状态为kFirstType  type = kFirstType; } else if (end) { //记录为最后一条，标记状态为kLastType  type = kLastType; } else { //记录不为第一条，也并非最后一条，则说明是中间状态，标记为kMiddleType  type = kMiddleType; } //将数据按照格式写入，并刷新到磁盘文件中  s = EmitPhysicalRecord(type, ptr, fragment_length); ptr += fragment_length; left -= fragment_length; begin = false; } while (s.ok() \u0026amp;\u0026amp; left \u0026gt; 0); //循环至数据完全写入或者写入失败时才停止  return s; }   写入流程如下：\n 判断头部能否完整写入，如果不能则将剩余空间用 \\x00 填充，接着从新的块开始写入。 根据 begin 和 end 判断记录类型。 将数据按照格式写入，并刷新到磁盘文件中。 循环至数据完全写入或者写入失败后停止，将结果返回。  读取 Log 的读取主要由 Reader 中的 ReadRecord 实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148  // https://github.com/google/leveldb/blob/master/db/log_reader.h  class Reader { public: // Interface for reporting errors.  class Reporter { public: virtual ~Reporter(); virtual void Corruption(size_t bytes, const Status\u0026amp; status) = 0; }; Reader(SequentialFile* file, Reporter* reporter, bool checksum, uint64_t initial_offset); Reader(const Reader\u0026amp;) = delete; Reader\u0026amp; operator=(const Reader\u0026amp;) = delete; ~Reader(); bool ReadRecord(Slice* record, std::string* scratch); uint64_t LastRecordOffset(); private: enum { kEof = kMaxRecordType + 1, kBadRecord = kMaxRecordType + 2 }; }; // https://github.com/google/leveldb/blob/master/db/log_reader.cc  bool Reader::ReadRecord(Slice* record, std::string* scratch) { if (last_record_offset_ \u0026lt; initial_offset_) { if (!SkipToInitialBlock()) { return false; } } scratch-\u0026gt;clear(); record-\u0026gt;clear(); bool in_fragmented_record = false; uint64_t prospective_record_offset = 0; Slice fragment; while (true) { //ReadPhysicalRecord读取log文件并将记录保存到fragment，同时返回记录的类型  const unsigned int record_type = ReadPhysicalRecord(\u0026amp;fragment); uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size(); if (resyncing_) { if (record_type == kMiddleType) { continue; } else if (record_type == kLastType) { resyncing_ = false; continue; } else { resyncing_ = false; } } //根据记录的类型来判断是否需要将当前记录附加到scratch后并继续读取  switch (record_type) { //类型为kFullType则说明当前是完整的记录，直接赋值给record后返回  case kFullType: if (in_fragmented_record) { if (!scratch-\u0026gt;empty()) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;partial record without end(1)\u0026#34;); } } prospective_record_offset = physical_record_offset; scratch-\u0026gt;clear(); *record = fragment; last_record_offset_ = prospective_record_offset; return true; //类型为kFirstType则说明当前是第一部分，先将记录复制到scratch后继续读取  case kFirstType: if (in_fragmented_record) { if (!scratch-\u0026gt;empty()) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;partial record without end(2)\u0026#34;); } } prospective_record_offset = physical_record_offset; scratch-\u0026gt;assign(fragment.data(), fragment.size()); in_fragmented_record = true; break; //类型为kMiddleType则说明当前是中间部分，先将记录追加到scratch后继续读取  case kMiddleType: //初始读取到的类型为kMiddleType或者kLastType，则需要忽略并且继续偏移  if (!in_fragmented_record) { ReportCorruption(fragment.size(), \u0026#34;missing start of fragmented record(1)\u0026#34;); } else { scratch-\u0026gt;append(fragment.data(), fragment.size()); } break; //类型为kLastType则说明当前为最后，继续追加到scratch，并将scratch赋值给record并返回  case kLastType: if (!in_fragmented_record) { ReportCorruption(fragment.size(), \u0026#34;missing start of fragmented record(2)\u0026#34;); } else { scratch-\u0026gt;append(fragment.data(), fragment.size()); *record = Slice(*scratch); last_record_offset_ = prospective_record_offset; return true; } break; //如果状态为kEof、kBadRecord时说明日志损坏，此时清空scratch并返回false  case kEof: if (in_fragmented_record) { scratch-\u0026gt;clear(); } return false; case kBadRecord: if (in_fragmented_record) { ReportCorruption(scratch-\u0026gt;size(), \u0026#34;error in middle of record\u0026#34;); in_fragmented_record = false; scratch-\u0026gt;clear(); } break; //未定义的类型，输出日志，剩余同上处理  default: { char buf[40]; std::snprintf(buf, sizeof(buf), \u0026#34;unknown record type %u\u0026#34;, record_type); ReportCorruption( (fragment.size() + (in_fragmented_record ? scratch-\u0026gt;size() : 0)), buf); in_fragmented_record = false; scratch-\u0026gt;clear(); break; } } } return false; }   执行流程如下：\n ReadRecord 读取一条记录到 fragment 变量中，并且返回该条记录的类型。 根据记录的类型来判断是否需要将当前记录附加到 scratch 后并继续读取：  kFullType：当前是完整的记录，直接赋值给 record 后返回。 kFirstType：当前是第一部分，先将记录覆盖到 scratch 后继续读取。 kMiddleType：当前是中间部分，先将记录追加到 scratch 后继续读取。 kLastType：当前为最后部分，继续追加到 scratch，并将完整的 scratch 赋值给 record 后返回。 其它/异常：清空 scratch 并返回 false，如果是未定义类型需要输出日志。    这里还有一个需要注意的细节，由于读取 Log 文件时可以从指定偏移量开始，所以如果初始读取到的类型为 kMiddleType 或者 kLastType，则需要忽略并且继续偏移，直到碰见第一个 kFirstType。\n崩溃恢复 当打开一个 LevelDB 的数据文件时，需先检验是否进行崩溃恢复，如果需要，则会从 Log 文件生成一个MemTable，其实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::RecoverLogFile(uint64_t log_number, bool last_log, bool* save_manifest, VersionEdit* edit, SequenceNumber* max_sequence) { struct LogReporter : public log::Reader::Reporter { Env* env; Logger* info_log; const char* fname; Status* status; // null if options_.paranoid_checks==false  void Corruption(size_t bytes, const Status\u0026amp; s) override { Log(info_log, \u0026#34;%s%s: dropping %d bytes; %s\u0026#34;, (this-\u0026gt;status == nullptr ? \u0026#34;(ignoring error) \u0026#34; : \u0026#34;\u0026#34;), fname, static_cast\u0026lt;int\u0026gt;(bytes), s.ToString().c_str()); if (this-\u0026gt;status != nullptr \u0026amp;\u0026amp; this-\u0026gt;status-\u0026gt;ok()) *this-\u0026gt;status = s; } }; mutex_.AssertHeld(); //打开log文件  std::string fname = LogFileName(dbname_, log_number); SequentialFile* file; Status status = env_-\u0026gt;NewSequentialFile(fname, \u0026amp;file); if (!status.ok()) { MaybeIgnoreError(\u0026amp;status); return status; } //创建log reader.  LogReporter reporter; reporter.env = env_; reporter.info_log = options_.info_log; reporter.fname = fname.c_str(); reporter.status = (options_.paranoid_checks ? \u0026amp;status : nullptr); log::Reader reader(file, \u0026amp;reporter, true /*checksum*/, 0 /*initial_offset*/); Log(options_.info_log, \u0026#34;Recovering log #%llu\u0026#34;, (unsigned long long)log_number); //读取所有的records并写入一个memtable  std::string scratch; Slice record; WriteBatch batch; int compactions = 0; MemTable* mem = nullptr; //循环读取日志文件  while (reader.ReadRecord(\u0026amp;record, \u0026amp;scratch) \u0026amp;\u0026amp; status.ok()) { if (record.size() \u0026lt; 12) { reporter.Corruption(record.size(), Status::Corruption(\u0026#34;log record too small\u0026#34;)); continue; } WriteBatchInternal::SetContents(\u0026amp;batch, record); if (mem == nullptr) { mem = new MemTable(internal_comparator_); mem-\u0026gt;Ref(); } //将records写入memtable  status = WriteBatchInternal::InsertInto(\u0026amp;batch, mem); MaybeIgnoreError(\u0026amp;status); if (!status.ok()) { break; } const SequenceNumber last_seq = WriteBatchInternal::Sequence(\u0026amp;batch) + WriteBatchInternal::Count(\u0026amp;batch) - 1; if (last_seq \u0026gt; *max_sequence) { *max_sequence = last_seq; } //如果memtable大于阈值，则将其转换成sstable(默认4MB)\t if (mem-\u0026gt;ApproximateMemoryUsage() \u0026gt; options_.write_buffer_size) { compactions++; *save_manifest = true; status = WriteLevel0Table(mem, edit, nullptr); mem-\u0026gt;Unref(); mem = nullptr; if (!status.ok()) { break; } } } delete file; //判断是否应该继续重复使用最后一个日志文件  if (status.ok() \u0026amp;\u0026amp; options_.reuse_logs \u0026amp;\u0026amp; last_log \u0026amp;\u0026amp; compactions == 0) { assert(logfile_ == nullptr); assert(log_ == nullptr); assert(mem_ == nullptr); uint64_t lfile_size; if (env_-\u0026gt;GetFileSize(fname, \u0026amp;lfile_size).ok() \u0026amp;\u0026amp; env_-\u0026gt;NewAppendableFile(fname, \u0026amp;logfile_).ok()) { Log(options_.info_log, \u0026#34;Reusing old log %s \\n\u0026#34;, fname.c_str()); log_ = new log::Writer(logfile_, lfile_size); logfile_number_ = log_number; if (mem != nullptr) { mem_ = mem; mem = nullptr; } else { mem_ = new MemTable(internal_comparator_); mem_-\u0026gt;Ref(); } } } if (mem != nullptr) { if (status.ok()) { *save_manifest = true; status = WriteLevel0Table(mem, edit, nullptr); } mem-\u0026gt;Unref(); } return status; }   具体的逻辑如下：\n 打开 log，创建 log reader 开始读取数据。 循环读取日志文件，并将其写入 MemTable 中。 如果 MemTable 过大，则将其转换为 SSTable。  ","date":"2022-05-23T23:42:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-wal%E6%97%A5%E5%BF%97%E6%A8%A1%E5%9D%97/","title":"LevelDB WAL日志模块"},{"content":"SSTable模块 SSTable（Sorted Strings Table，有序字符串表），在各种存储引擎中得到了广泛的使用，包括 LevelDB、HBase、Cassandra 等。SSTable 会根据 Key 进行排序后保存一系列的 K-V 对，这种方式不仅方便进行范围查找，而且便于对 K-V 对进行更加有效的压缩。\nSSTable Format SSTable 文件由一个个块组成，块中可以保存数据、数据索引、元数据或者元数据索引。整体的文件格式如下图：\n如上图，SSTable 文件整体分为 4 个部分：\n Data Block（数据区域）：保存具体的键-值对数据。 Meta Block（元数据区域）：保存元数据，例如布隆过滤器。 Index Block（索引区域）：分为数据索引和元数据索引。  数据索引：数据索引块中的键为前一个数据块的最后一个键（即一个数据块中最大的键，因为键是有序排列保存的）与后一个数据块的第一个键（即一个数据块中的最小键）的最短分隔符。 元数据索引：元数据索引块可指示如何查找该布隆过滤器的数据。   File Footer（尾部）：总大小为48个字节。  BlockHandle BlockHandle在SSTable中是经常使用的一个结构，其定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  // https://github.com/google/leveldb/blob/master/table/format.h  class BlockHandle { public: // Maximum encoding length of a BlockHandle  enum { kMaxEncodedLength = 10 + 10 }; BlockHandle(); // The offset of the block in the file.  uint64_t offset() const { return offset_; } void set_offset(uint64_t offset) { offset_ = offset; } // The size of the stored block  uint64_t size() const { return size_; } void set_size(uint64_t size) { size_ = size; } void EncodeTo(std::string* dst) const; Status DecodeFrom(Slice* input); private: uint64_t offset_; uint64_t size_; };   BlockHandler 本质就是封装了 offset 和 size，用于定位某些区域。\nBlock Format SSTable 中一个块默认大小为 4 KB，由 4 部分组成：\n 键-值对数据：即我们保存到 LevelDB 中的多组键-值对。 重启点数据：最后 4 字节为重启点的个数，前边部分为多个重启点，每个重启点实际保存的是偏移量。并且每个重启点固定占据 4 字节的空间。 压缩类型：在 LevelDB 的 SSTable 中有两种压缩类型：  kNoCompression：没有压缩。 kSnappyCompression：Snappy压缩。   校验数据：4 字节的 CRC 校验字段。  Block读写流程 生成Block 块生成主要在 BlockBuilder 中实现，下面先看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // https://github.com/google/leveldb/blob/master/table/block_builder.h  class BlockBuilder { public: explicit BlockBuilder(const Options* options); BlockBuilder(const BlockBuilder\u0026amp;) = delete; BlockBuilder\u0026amp; operator=(const BlockBuilder\u0026amp;) = delete; void Reset(); void Add(const Slice\u0026amp; key, const Slice\u0026amp; value); Slice Finish(); size_t CurrentSizeEstimate() const; bool empty() const { return buffer_.empty(); } private: const Options* options_; std::string buffer_; // Destination buffer  std::vector\u0026lt;uint32_t\u0026gt; restarts_; // Restart points  int counter_; // Number of entries emitted since restart  bool finished_; // Has Finish() been called?  std::string last_key_; };    成员变量  options_：在 BlockBuilder 类构造函数中传入，表示一些配置选项。 buffer_：块的内容，所有的键-值对都保存到 buffer_ 中。 restarts_：每次开启新的重启点后，会将当前 buffer_ 的数据长度保存到 restarts_ 中，当前 buffer_ 中的数据长度即为每个重启点的偏移量。 counter_：开启新的重启点之后加入的键-值对数量，默认保存 16 个键-值对，之后会开启一个新的重启点。 finished_：指明是否已经调用了 Finish 方法，BlockBuilder 中的 Add 方法会将数据保存到各个成员变量中，而 Finish 方法会依据成员变量的值生成一个块。 last_key_：上一个保存的键，当加入新键时，用来计算和上一个键的共同前缀部分。    介绍完了结构，下面来看具体的生成方法。当需要保存一个键-值对时，需要调用 BlockBuilder 类中的 Add 方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  // https://github.com/google/leveldb/blob/master/table/block_builder.cc  void BlockBuilder::Add(const Slice\u0026amp; key, const Slice\u0026amp; value) { //保存上一个加入的key  Slice last_key_piece(last_key_); assert(!finished_); assert(counter_ \u0026lt;= options_-\u0026gt;block_restart_interval); assert(buffer_.empty() // No values yet?  || options_-\u0026gt;comparator-\u0026gt;Compare(key, last_key_piece) \u0026gt; 0); size_t shared = 0; //判断counter_是否大于block_restart_interval  if (counter_ \u0026lt; options_-\u0026gt;block_restart_interval) { const size_t min_length = std::min(last_key_piece.size(), key.size()); //计算相同前缀的长度  while ((shared \u0026lt; min_length) \u0026amp;\u0026amp; (last_key_piece[shared] == key[shared])) { shared++; } } else { //如果键-值对数量超过block_restart_interval，则开启新的重启点，清空计数器  restarts_.push_back(buffer_.size()); counter_ = 0; } const size_t non_shared = key.size() - shared; //将共同前缀长度、非共享部分长度、值长度追加到buffer_中  PutVarint32(\u0026amp;buffer_, shared); PutVarint32(\u0026amp;buffer_, non_shared); PutVarint32(\u0026amp;buffer_, value.size()); //将key的非共享数据追加到buffer_中_  buffer_.append(key.data() + shared, non_shared); //将Value数据追加到buffer_中  buffer_.append(value.data(), value.size()); //更新状态  last_key_.resize(shared); last_key_.append(key.data() + shared, non_shared); assert(Slice(last_key_) == key); counter_++; }   执行流程如下：\n1. 判断 counter_ 是否大于 block_restart_interval，如果大于，则开启新的重启点，清空计数器并保存 buffer_ 中数据长度的值（该值即每个重启点的偏移量）压到 restarts_ 数组中。\r2. 如果 counter_ 未超出配置的每个重启点可以保存的键-值对数值，则计算当前键和上一次保存键的共同前缀，然后将键-值对按格式保存到 buffer_ 中。\r3. 更新状态，将 last_key_ 置为当前保存的 key，并且将 counter_ 加 1。\r 从上面的代码中可以看出，Add 中将所有的键-值对按格式保存到成员变量 buffer_ 中。实际生成 Block 的其实是 Finish 。代码如下：\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/table/block_builder.cc  Slice BlockBuilder::Finish() { //将重启点偏移量写入buffer_中  for (size_t i = 0; i \u0026lt; restarts_.size(); i++) { PutFixed32(\u0026amp;buffer_, restarts_[i]); } PutFixed32(\u0026amp;buffer_, restarts_.size()); finished_ = true; return Slice(buffer_); }   Finish  首先将所有重启点偏移量的值依次以 4 字节大小追加到 buffer_ 字符串，最后将重启点个数继续以 4 字节大小追加到 buffer_ 后部，此时返回的结果就是一个完整的 Block。\n读取Block 读取 Block 由 Block 类实现，其主要依靠 NewIterator 生成一个 Block 迭代器，再借助迭代器的 Seek 来查找对应的 Key。首先来看看 Block 的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // https://github.com/google/leveldb/blob/master/table/block.h  class Block { public: // Initialize the block with the specified contents.  explicit Block(const BlockContents\u0026amp; contents); Block(const Block\u0026amp;) = delete; Block\u0026amp; operator=(const Block\u0026amp;) = delete; ~Block(); size_t size() const { return size_; } Iterator* NewIterator(const Comparator* comparator); private: class Iter; uint32_t NumRestarts() const; const char* data_; size_t size_; uint32_t restart_offset_; // Offset in data_ of restart array  bool owned_; // Block owns data_[] };   读取一个块通过在 NewIterator 中生成一个迭代器来实现，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/table/block.cc  Iterator* Block::NewIterator(const Comparator* comparator) { if (size_ \u0026lt; sizeof(uint32_t)) { return NewErrorIterator(Status::Corruption(\u0026#34;bad block contents\u0026#34;)); } const uint32_t num_restarts = NumRestarts(); if (num_restarts == 0) { return NewEmptyIterator(); } else { return new Iter(comparator, data_, restart_offset_, num_restarts); } }   这里的逻辑比较简单，就不多作介绍了。\n核心的查找逻辑主要是迭代器中的 Seek 下面直接看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/table/block.cc  void Seek(const Slice\u0026amp; target) override { uint32_t left = 0; uint32_t right = num_restarts_ - 1; int current_key_compare = 0; if (Valid()) { current_key_compare = Compare(key_, target); if (current_key_compare \u0026lt; 0) { left = restart_index_; } else if (current_key_compare \u0026gt; 0) { right = restart_index_; } else { return; } } //通过重启点进行二分查找  while (left \u0026lt; right) { uint32_t mid = (left + right + 1) / 2; uint32_t region_offset = GetRestartPoint(mid); uint32_t shared, non_shared, value_length; const char* key_ptr = DecodeEntry(data_ + region_offset, data_ + restarts_, \u0026amp;shared, \u0026amp;non_shared, \u0026amp;value_length); if (key_ptr == nullptr || (shared != 0)) { CorruptionError(); return; } Slice mid_key(key_ptr, non_shared); //如果key小于target，则将left置为mid  if (Compare(mid_key, target) \u0026lt; 0) { left = mid; //如果key大于等于target，则将right置为mid-1  } else { right = mid - 1; } } assert(current_key_compare == 0 || Valid()); //在块中线性查找，依次遍历每一个K-V对，将key与target对比，直到找到第一个大于等于target的后返  bool skip_seek = left == restart_index_ \u0026amp;\u0026amp; current_key_compare \u0026lt; 0; if (!skip_seek) { SeekToRestartPoint(left); } while (true) { if (!ParseNextKey()) { return; } if (Compare(key_, target) \u0026gt;= 0) { return; } } }   查找逻辑如下：\n1. 对重启点数组进行二分查找，找到可能包含数据的重启点。\r2. 在块中线性查找，依次遍历每一个 K-V 对，将 key 与 target 对比。\r3. 找到第一个 key 大于等于 target 的后将该 K-V 对存储后返回。\r SSTable读写 生成SSTable SSTable 的生成主要在 TableBuilder 中实现，下面先看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  // https://github.com/google/leveldb/blob/master/include/leveldb/table_builder.h  class LEVELDB_EXPORT TableBuilder { public: TableBuilder(const Options\u0026amp; options, WritableFile* file); TableBuilder(const TableBuilder\u0026amp;) = delete; TableBuilder\u0026amp; operator=(const TableBuilder\u0026amp;) = delete; ~TableBuilder(); Status ChangeOptions(const Options\u0026amp; options); void Add(const Slice\u0026amp; key, const Slice\u0026amp; value); void Flush(); Status status() const; Status Finish(); void Abandon(); uint64_t NumEntries() const; uint64_t FileSize() const; private: bool ok() const { return status().ok(); } void WriteBlock(BlockBuilder* block, BlockHandle* handle); void WriteRawBlock(const Slice\u0026amp; data, CompressionType, BlockHandle* handle); struct Rep; Rep* rep_; };   在介绍该 TableBuilder 的核心逻辑之前，首先我们要看看里面的一个结构体 Rep。其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  struct TableBuilder::Rep { Rep(const Options\u0026amp; opt, WritableFile* f) : options(opt), index_block_options(opt), file(f), offset(0), data_block(\u0026amp;options), index_block(\u0026amp;index_block_options), num_entries(0), closed(false), filter_block(opt.filter_policy == nullptr ? nullptr : new FilterBlockBuilder(opt.filter_policy)), pending_index_entry(false) { index_block_options.block_restart_interval = 1; } Options options; Options index_block_options; WritableFile* file; uint64_t offset; Status status; BlockBuilder data_block; BlockBuilder index_block; std::string last_key; int64_t num_entries; bool closed; // Either Finish() or Abandon() has been called.  FilterBlockBuilder* filter_block; bool pending_index_entry; BlockHandle pending_handle; // Handle to add to index block  std::string compressed_output; };   我们需要注意的关键变量如下：\n file：SSTable 生成的文件。 data_block：用于生成 SSTable 的数据区域。 index_block：用于生成 SSTable 的索引区域。 pending_index_entry：决定是否需要写数据索引。 **pending_handle：**写数据索引的方法。SSTable中每次完整写入一个块后需要生成该块的索引，索引中的键是当前块最大键与即将插入的键的最短分隔符，例如一个块中最大键为 abceg，即将插入的键为 abcqddh，则二者之间的最小分隔符为 abcf。  了解完结构后，接着就看看生成 SSTable 的核心函数 Add 和 Finish。\n首先来看 Add，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  void TableBuilder::Add(const Slice\u0026amp; key, const Slice\u0026amp; value) { Rep* r = rep_; assert(!r-\u0026gt;closed); if (!ok()) return; if (r-\u0026gt;num_entries \u0026gt; 0) { assert(r-\u0026gt;options.comparator-\u0026gt;Compare(key, Slice(r-\u0026gt;last_key)) \u0026gt; 0); } //判断是否需要写入数据索引块中  if (r-\u0026gt;pending_index_entry) { assert(r-\u0026gt;data_block.empty()); //找到最短分隔符，即大于等于上一个块最大的键，小于下一个块最小的键  r-\u0026gt;options.comparator-\u0026gt;FindShortestSeparator(\u0026amp;r-\u0026gt;last_key, key); std::string handle_encoding; r-\u0026gt;pending_handle.EncodeTo(\u0026amp;handle_encoding); //在数据索引块中写入key和BlockHandle  r-\u0026gt;index_block.Add(r-\u0026gt;last_key, Slice(handle_encoding)); r-\u0026gt;pending_index_entry = false; } //写入元数据块中  if (r-\u0026gt;filter_block != nullptr) { r-\u0026gt;filter_block-\u0026gt;AddKey(key); } //修改last_key为当前要插入的key  r-\u0026gt;last_key.assign(key.data(), key.size()); r-\u0026gt;num_entries++; //写入数据块中  r-\u0026gt;data_block.Add(key, value); //判断数据块大小是否大于配置的块大小，如果大于则调用Flush写入SSTable文件并刷新到硬盘  const size_t estimated_block_size = r-\u0026gt;data_block.CurrentSizeEstimate(); if (estimated_block_size \u0026gt;= r-\u0026gt;options.block_size) { Flush(); } }   Add 主要就是调用生成数据块与数据索引块的方法 BlockBuilder::Add 以及生成元数据块的方法 FilterBlockBuilder::Add 依次将键值对加入数据索引块、元数据块以及数据块。\n可以看到，最后会判断数据块大小是否大于配置的块大小，如果大于则调用 Flush 写入 SSTable 文件并刷新到硬盘中。我们接着来看看 Flush 的执行逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  void TableBuilder::Flush() { Rep* r = rep_; assert(!r-\u0026gt;closed); if (!ok()) return; if (r-\u0026gt;data_block.empty()) return; assert(!r-\u0026gt;pending_index_entry); //写入数据块  WriteBlock(\u0026amp;r-\u0026gt;data_block, \u0026amp;r-\u0026gt;pending_handle); if (ok()) { //将pending_index_entry修改为true，表明下一次将写入数据索引块。  r-\u0026gt;pending_index_entry = true; //将文件刷新到磁盘\t r-\u0026gt;status = r-\u0026gt;file-\u0026gt;Flush(); } if (r-\u0026gt;filter_block != nullptr) { r-\u0026gt;filter_block-\u0026gt;StartBlock(r-\u0026gt;offset); } }   执行逻辑如下：\n1. 将数据写入数据块中。\r2. 将 pending_index_entry 修改为 true，表明下一次调用 `Add` 时将写入数据索引块。\r3. 将文件刷新到磁盘中。\r 介绍完了 Add，下面来看看 Finish。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  // https://github.com/google/leveldb/blob/master/table/table_builder.cc  Status TableBuilder::Finish() { Rep* r = rep_; //写入SSTable文件并刷新到硬盘中  Flush(); assert(!r-\u0026gt;closed); r-\u0026gt;closed = true; BlockHandle filter_block_handle, metaindex_block_handle, index_block_handle; //写入元数据块  if (ok() \u0026amp;\u0026amp; r-\u0026gt;filter_block != nullptr) { WriteRawBlock(r-\u0026gt;filter_block-\u0026gt;Finish(), kNoCompression, \u0026amp;filter_block_handle); } //写入元数据块索引  if (ok()) { BlockBuilder meta_index_block(\u0026amp;r-\u0026gt;options); if (r-\u0026gt;filter_block != nullptr) { std::string key = \u0026#34;filter.\u0026#34;; key.append(r-\u0026gt;options.filter_policy-\u0026gt;Name()); std::string handle_encoding; filter_block_handle.EncodeTo(\u0026amp;handle_encoding); meta_index_block.Add(key, handle_encoding); } WriteBlock(\u0026amp;meta_index_block, \u0026amp;metaindex_block_handle); } //写入数据块索引  if (ok()) { if (r-\u0026gt;pending_index_entry) { r-\u0026gt;options.comparator-\u0026gt;FindShortSuccessor(\u0026amp;r-\u0026gt;last_key); std::string handle_encoding; r-\u0026gt;pending_handle.EncodeTo(\u0026amp;handle_encoding); r-\u0026gt;index_block.Add(r-\u0026gt;last_key, Slice(handle_encoding)); r-\u0026gt;pending_index_entry = false; } WriteBlock(\u0026amp;r-\u0026gt;index_block, \u0026amp;index_block_handle); } //写入尾部  if (ok()) { Footer footer; footer.set_metaindex_handle(metaindex_block_handle); footer.set_index_handle(index_block_handle); std::string footer_encoding; footer.EncodeTo(\u0026amp;footer_encoding); r-\u0026gt;status = r-\u0026gt;file-\u0026gt;Append(footer_encoding); if (r-\u0026gt;status.ok()) { r-\u0026gt;offset += footer_encoding.size(); } } return r-\u0026gt;status; }   Finish 会按照我们一开始给出的 SSTable 的格式，将数据分别写入数据块、数据块索引、元数据块、元数据块索引、尾部。最后生成 SSTable。\n读取SSTable 介绍完了写入，我们再来看看读取。SSTable 读取的逻辑与 Block 类似，都是借助于迭代器实现的。主要实现代码在 Table 类中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // https://github.com/google/leveldb/blob/master/include/leveldb/table.h  class LEVELDB_EXPORT Table { public: static Status Open(const Options\u0026amp; options, RandomAccessFile* file, uint64_t file_size, Table** table); Table(const Table\u0026amp;) = delete; Table\u0026amp; operator=(const Table\u0026amp;) = delete; ~Table(); Iterator* NewIterator(const ReadOptions\u0026amp;) const; uint64_t ApproximateOffsetOf(const Slice\u0026amp; key) const; private: friend class TableCache; struct Rep; static Iterator* BlockReader(void*, const ReadOptions\u0026amp;, const Slice\u0026amp;); explicit Table(Rep* rep) : rep_(rep) {} Status InternalGet(const ReadOptions\u0026amp;, const Slice\u0026amp; key, void* arg, void (*handle_result)(void* arg, const Slice\u0026amp; k, const Slice\u0026amp; v)); void ReadMeta(const Footer\u0026amp; footer); void ReadFilter(const Slice\u0026amp; filter_handle_value); Rep* const rep_; };   这里我们主要关注生成迭代器的 NewIterator 和 第二层生成迭代器的 BlockReader。\n首先看 NewIterator 的代码：\n1 2 3 4 5 6 7  // https://github.com/google/leveldb/blob/master/table/table.cc  Iterator* Table::NewIterator(const ReadOptions\u0026amp; options) const { return NewTwoLevelIterator( rep_-\u0026gt;index_block-\u0026gt;NewIterator(rep_-\u0026gt;options.comparator), \u0026amp;Table::BlockReader, const_cast\u0026lt;Table*\u0026gt;(this), options); }   这里返回了一个双层迭代器 NewTwoLevelIterator。第一层为数据索引块的迭代器，即 rep_-\u0026gt;index_block-\u0026gt;NewIterator。通过第一层的数据索引块迭代器查找一个键应该属于的块，然后通过第二层迭代器去读取这个块并查找该键。\n接着看生成第二层块迭代器的 BlockReader：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  // https://github.com/google/leveldb/blob/master/table/table.cc  Iterator* Table::BlockReader(void* arg, const ReadOptions\u0026amp; options, const Slice\u0026amp; index_value) { Table* table = reinterpret_cast\u0026lt;Table*\u0026gt;(arg); Cache* block_cache = table-\u0026gt;rep_-\u0026gt;options.block_cache; Block* block = nullptr; Cache::Handle* cache_handle = nullptr; BlockHandle handle; Slice input = index_value; Status s = handle.DecodeFrom(\u0026amp;input); if (s.ok()) { //contents保存一个块的内容  BlockContents contents; if (block_cache != nullptr) { char cache_key_buffer[16]; EncodeFixed64(cache_key_buffer, table-\u0026gt;rep_-\u0026gt;cache_id); EncodeFixed64(cache_key_buffer + 8, handle.offset()); Slice key(cache_key_buffer, sizeof(cache_key_buffer)); cache_handle = block_cache-\u0026gt;Lookup(key); if (cache_handle != nullptr) { block = reinterpret_cast\u0026lt;Block*\u0026gt;(block_cache-\u0026gt;Value(cache_handle)); } else { //通过BlockHandle存储的偏移量和大小读取一个块的数据到contents  s = ReadBlock(table-\u0026gt;rep_-\u0026gt;file, options, handle, \u0026amp;contents); if (s.ok()) { //根据contents生成一个Block结构  block = new Block(contents); if (contents.cachable \u0026amp;\u0026amp; options.fill_cache) { cache_handle = block_cache-\u0026gt;Insert(key, block, block-\u0026gt;size(), \u0026amp;DeleteCachedBlock); } } } } else { s = ReadBlock(table-\u0026gt;rep_-\u0026gt;file, options, handle, \u0026amp;contents); if (s.ok()) { block = new Block(contents); } } } Iterator* iter; if (block != nullptr) { //生成块迭代器，通过该迭代器读取数据  iter = block-\u0026gt;NewIterator(table-\u0026gt;rep_-\u0026gt;options.comparator); if (cache_handle == nullptr) { iter-\u0026gt;RegisterCleanup(\u0026amp;DeleteBlock, block, nullptr); } else { iter-\u0026gt;RegisterCleanup(\u0026amp;ReleaseBlock, block_cache, cache_handle); } } else { iter = NewErrorIterator(s); } return iter; }   SSTable 的读取先通过第一层迭代器（即数据索引）获取到一个键需要查找的块位置，读取该块的内容并且构造第二层迭代器遍历该块，通过两层迭代器即可在 SSTable 中进行键的查找。\n布隆过滤器 如果在 LevelDB 中查找某个不存在的键，必须先检查内存表 MemTable，然后逐层查找，为了优化这种读取，LevelDB 中会使用布隆过滤器。当布隆过滤器判定键不存在时，可以直接返回，无须继续查找。\n这里就不过多介绍原理，如果想了解可以看看我的往期博客 海量数据处理（一） ：位图与布隆过滤器的概念以及实现\n实现 布隆过滤器继承了 LevelDB 中抽象出的过滤器纯虚类 FilterPolicy，其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/include/leveldb/filter_policy.h  class LEVELDB_EXPORT FilterPolicy { public: virtual ~FilterPolicy(); virtual const char* Name() const = 0; virtual void CreateFilter(const Slice* keys, int n, std::string* dst) const = 0; virtual bool KeyMayMatch(const Slice\u0026amp; key, const Slice\u0026amp; filter) const = 0; };   FilterPolicy 定义了几个接口：\n Name：返回过滤器名称。 CreateFilter：将一个字符串加入过滤器中。 KeyMayMatch：通过过滤器内容判断一个元素是否存在。  接下来我们看看 BloomFilterPolicy 是如何实现这些接口的，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  // https://github.com/google/leveldb/blob/master/util/bloom.cc  class BloomFilterPolicy : public FilterPolicy { public: explicit BloomFilterPolicy(int bits_per_key) : bits_per_key_(bits_per_key) { k_ = static_cast\u0026lt;size_t\u0026gt;(bits_per_key * 0.69); // 0.69 =~ ln(2)  if (k_ \u0026lt; 1) k_ = 1; if (k_ \u0026gt; 30) k_ = 30; } const char* Name() const override { return \u0026#34;leveldb.BuiltinBloomFilter2\u0026#34;; } void CreateFilter(const Slice* keys, int n, std::string* dst) const override { size_t bits = n * bits_per_key_; if (bits \u0026lt; 64) bits = 64; size_t bytes = (bits + 7) / 8; bits = bytes * 8; const size_t init_size = dst-\u0026gt;size(); dst-\u0026gt;resize(init_size + bytes, 0); dst-\u0026gt;push_back(static_cast\u0026lt;char\u0026gt;(k_)); // Remember # of probes in filter  char* array = \u0026amp;(*dst)[init_size]; //依次处理每一个Key  for (int i = 0; i \u0026lt; n; i++) { uint32_t h = BloomHash(keys[i]); //通过位运算获取delta  const uint32_t delta = (h \u0026gt;\u0026gt; 17) | (h \u0026lt;\u0026lt; 15); // Rotate right 17 bits  //计算哈希值，对每一个key计算k次哈希值（这里采用对每次计算出的的哈希值增加delta来模拟）  for (size_t j = 0; j \u0026lt; k_; j++) { const uint32_t bitpos = h % bits; array[bitpos / 8] |= (1 \u0026lt;\u0026lt; (bitpos % 8)); h += delta; } } } bool KeyMayMatch(const Slice\u0026amp; key, const Slice\u0026amp; bloom_filter) const override { const size_t len = bloom_filter.size(); if (len \u0026lt; 2) return false; const char* array = bloom_filter.data(); const size_t bits = (len - 1) * 8; const size_t k = array[len - 1]; if (k \u0026gt; 30) { return true; } //计算出key的哈希  uint32_t h = BloomHash(key); const uint32_t delta = (h \u0026gt;\u0026gt; 17) | (h \u0026lt;\u0026lt; 15); // Rotate right 17 bits  //判断对应位置是否全为1，如果有任何一个为0说明数据不可能存在布隆过滤器中，返回false，否则true  for (size_t j = 0; j \u0026lt; k; j++) { const uint32_t bitpos = h % bits; if ((array[bitpos / 8] \u0026amp; (1 \u0026lt;\u0026lt; (bitpos % 8))) == 0) return false; h += delta; } return true; } private: size_t bits_per_key_; size_t k_; };   具体逻辑都标识在了注释中，就不多说了，这里提一下这里用到的一个哈希小技巧：\n 为了避免哈希冲突，大部分布隆过滤器中都会采用多种哈希算法来计算。LevelDB 为了简化规则，使用位运算 计算出 delta，对每轮计算出的哈希值累加上 delta，模拟多轮哈希计算。  应用 LevelDB 中具体使用布隆过滤器时又封装了两个类，分别为 FilterBlockBuilder 和 FilterBlockReader。\nFilterBlockBuilder 的主要功能是通过调用 BloomFilterPolicy 的 CreateFilter 方法生成布隆过滤器，并且将布隆过滤器的内容写入 SSTable 的元数据块。其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  // https://github.com/google/leveldb/blob/master/table/filter_block.h  class FilterBlockBuilder { public: explicit FilterBlockBuilder(const FilterPolicy*); FilterBlockBuilder(const FilterBlockBuilder\u0026amp;) = delete; FilterBlockBuilder\u0026amp; operator=(const FilterBlockBuilder\u0026amp;) = delete; void StartBlock(uint64_t block_offset); void AddKey(const Slice\u0026amp; key); Slice Finish(); private: void GenerateFilter(); const FilterPolicy* policy_; std::string keys_; // Flattened key contents  std::vector\u0026lt;size_t\u0026gt; start_; // Starting index in keys_ of each key  std::string result_; // Filter data computed so far  std::vector\u0026lt;Slice\u0026gt; tmp_keys_; // policy_-\u0026gt;CreateFilter() argument  std::vector\u0026lt;uint32_t\u0026gt; filter_offsets_; };    成员变量  policy_：实现了 FilterPolicy 接口的类，在布隆过滤器中为 BloomFilterPolicy。 keys_：生成布隆过滤器的键。 start_：数组类型，保存 keys_ 参数中每一个键的开始索引。 result_：保存生成的布隆过滤器内容。 tmp_keys_：生成布隆过滤器时，会通过 keys_ 和 start_ 拆分出每一个键，将拆分出的每一个键保存到 tmp_keys_ 数组中。 filter_offsets_：过滤器偏移量，即每一个过滤器在元数据块中的偏移量。    写入的核心逻辑主要在 AddKey 和 Finish 中，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  // https://github.com/google/leveldb/blob/master/table/filter_block.cc  void FilterBlockBuilder::AddKey(const Slice\u0026amp; key) { Slice k = key; //记录key的索引位置  start_.push_back(keys_.size()); //将key追加到该字符串中  keys_.append(k.data(), k.size()); } Slice FilterBlockBuilder::Finish() { //写入内容  if (!start_.empty()) { GenerateFilter(); } //写入偏移量  const uint32_t array_offset = result_.size(); for (size_t i = 0; i \u0026lt; filter_offsets_.size(); i++) { PutFixed32(\u0026amp;result_, filter_offsets_[i]); } //写入总大小  PutFixed32(\u0026amp;result_, array_offset); //写入基数  result_.push_back(kFilterBaseLg); // Save encoding parameter in result  return Slice(result_); }   首先调用 AddKey 记录 Key 的索引位置，并将 Key 追加到 Keys_中。接着，调用 Finish 生成一个元数据块，并分别写入布隆过滤器的内容、偏移量、内容总大小和基数。\nFilterBlockReader 用于查找一个元素是否在一个布隆过滤器中，其结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/table/filter_block.h  class FilterBlockReader { public: // REQUIRES: \u0026#34;contents\u0026#34; and *policy must stay live while *this is live.  FilterBlockReader(const FilterPolicy* policy, const Slice\u0026amp; contents); bool KeyMayMatch(uint64_t block_offset, const Slice\u0026amp; key); private: const FilterPolicy* policy_; const char* data_; // Pointer to filter data (at block-start)  const char* offset_; // Pointer to beginning of offset array (at block-end)  size_t num_; // Number of entries in offset array  size_t base_lg_; // Encoding parameter (see kFilterBaseLg in .cc file) };    成员变量  policy_：实现了 FilterPolicy 接口的类，在布隆过滤器中为 BloomFilterPolicy。 data_：指向元数据块的开始位置。 offset_：指向元数据块中过滤器偏移量的开始位置。 num_：过滤器偏移量的个数。 base_lg_：过滤器基数。    FilterBlockReader 中的 KeyMayMatch 根据数据块偏移量找到对应的过滤器内容，然后调用 BloomFilterPolicy 中的 KeyMayMatch 方法判断一个元素是否在该过滤器之中。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/table/filter_block.cc  bool FilterBlockReader::KeyMayMatch(uint64_t block_offset, const Slice\u0026amp; key) { uint64_t index = block_offset \u0026gt;\u0026gt; base_lg_; if (index \u0026lt; num_) { uint32_t start = DecodeFixed32(offset_ + index * 4); uint32_t limit = DecodeFixed32(offset_ + index * 4 + 4); if (start \u0026lt;= limit \u0026amp;\u0026amp; limit \u0026lt;= static_cast\u0026lt;size_t\u0026gt;(offset_ - data_)) { Slice filter = Slice(data_ + start, limit - start); return policy_-\u0026gt;KeyMayMatch(key, filter); } else if (start == limit) { // Empty filters do not match any keys  return false; } }   LRU Cache 我们希望经常使用的 SSTable 内容尽量保存在内存中，但如果磁盘中的 SSTable 文件的总大小大于服务器内存大小，或者需要控制 LevelDB 的内存总占用量时，就需要使用 LRU（least recentlyused）Cache 来管理内存。LRU 是一种缓存置换策略，根据该策略不仅可以管理内存的占用量，还可以将热数据尽量保存到内存中，以加快读取速度。\n内存是有限并且昂贵的资源，因此 LevelDB 通过 LRU 策略管理读取到内存的数据。LRU 基于这样一种假设：如果一个资源最近没有或者很少被使用到，那么将来也会很少甚至不被使用。因此如果内存不足，需要淘汰数据时，可以根据 LRU 策略来执行。\n这里就不过多介绍原理，如果想了解可以看看我的往期博客 高级数据结构与算法 | LRU缓存机制（Least Recently Used）\n结构 为了方便拓展其他的缓存置换算法（目前仅实现了 LRU），LevelDB 抽象出了一个 Cache 纯虚类，结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  // https://github.com/google/leveldb/blob/master/include/leveldb/cache.h  class LEVELDB_EXPORT Cache { public: Cache() = default; Cache(const Cache\u0026amp;) = delete; Cache\u0026amp; operator=(const Cache\u0026amp;) = delete; virtual ~Cache(); struct Handle {}; virtual Handle* Insert(const Slice\u0026amp; key, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)) = 0; virtual Handle* Lookup(const Slice\u0026amp; key) = 0; virtual void Release(Handle* handle) = 0; virtual void* Value(Handle* handle) = 0; virtual void Erase(const Slice\u0026amp; key) = 0; virtual uint64_t NewId() = 0; virtual void Prune() {} virtual size_t TotalCharge() const = 0; private: void LRU_Remove(Handle* e); void LRU_Append(Handle* e); void Unref(Handle* e); struct Rep; Rep* rep_; };   LevelDB 中 LRU 由 LRUCache 实现，并且 LRU 节点由 LRUHandle 实现。首先我们来看看 LRUHandle：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/util/cache.cc  struct LRUHandle { void* value; void (*deleter)(const Slice\u0026amp;, void* value); LRUHandle* next_hash; LRUHandle* next;\tLRUHandle* prev; size_t charge; // TODO(opt): Only allow uint32_t?  size_t key_length; bool in_cache; // Whether entry is in the cache.  uint32_t refs; // References, including cache reference, if present.  uint32_t hash; // Hash of key(); used for fast sharding and comparisons  char key_data[1]; // Beginning of key  Slice key() const { assert(next != this); return Slice(key_data, key_length); } };    成员变量  value：具体的值，指针类型。 deleter：自定义回收节点的回调函数。 next_hash：用于 hashtable 冲突时，下一个节点。 next：代表 LRU 中双向链表中下一个节点。 prev：代表 LRU 中双向链表中上一个节点。 charge：记录当前 value 所占用的内存大小，用于后面超出容量后需要进行 lru。 key_length：数据 key 的长度。 in_cache：表示是否在缓存中。 refs：引用计数，因为当前节点可能会被多个组件使用，不能简单的删除。 hash：记录当前 key 的 hash 值。    这个节点设计的非常巧妙，既可以用来当做 LRU 节点，也可以用来当作 hashtable 的节点。\n接着我们来看看 LRUCache 的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  // https://github.com/google/leveldb/blob/master/util/cache.cc  class LRUCache { public: LRUCache(); ~LRUCache(); void SetCapacity(size_t capacity) { capacity_ = capacity; } Cache::Handle* Insert(const Slice\u0026amp; key, uint32_t hash, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)); Cache::Handle* Lookup(const Slice\u0026amp; key, uint32_t hash); void Release(Cache::Handle* handle); void Erase(const Slice\u0026amp; key, uint32_t hash); void Prune(); size_t TotalCharge() const { MutexLock l(\u0026amp;mutex_); return usage_; } private: void LRU_Remove(LRUHandle* e); void LRU_Append(LRUHandle* list, LRUHandle* e); void Ref(LRUHandle* e); void Unref(LRUHandle* e); bool FinishErase(LRUHandle* e) EXCLUSIVE_LOCKS_REQUIRED(mutex_); size_t capacity_; mutable port::Mutex mutex_; size_t usage_ GUARDED_BY(mutex_); LRUHandle lru_ GUARDED_BY(mutex_); LRUHandle in_use_ GUARDED_BY(mutex_); HandleTable table_ GUARDED_BY(mutex_); };   实现 LRU Cache 的实现有如下两处细节需要注意：\n 减小锁的粒度：LRUCache 的并发操作不安全，因此操作时需要加锁。为了减小锁的粒度，LevelDB 中通过哈希将键分为 16 个段，可以理解为有 16 个相同的 LRU Cache 结构，每次进行 Cache 操作时需要先去查找键属于的段。 缓存淘汰：每个LRU Cache结构中有两个成员变量：lru_ 和 in_use_ 。lru_ 双向链表中的节点是可以进行淘汰的，而 in_use_ 双向链表中的节点表示正在使用，因此不可以进行淘汰。  减小锁的粒度 LevelDB 为了减小锁的粒度，封装了一个 ShardedLRUCache，其中有一个大小为 16（默认值） 的 shard_ 成员，shard_ 成员的每个元素均为一个 LRUCache 结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  // ShardedLRUCache class ShardedLRUCache : public Cache { private: LRUCache shard_[kNumShards]; port::Mutex id_mutex_; uint64_t last_id_; static inline uint32_t HashSlice(const Slice\u0026amp; s) { return Hash(s.data(), s.size(), 0); } static uint32_t Shard(uint32_t hash) { return hash \u0026gt;\u0026gt; (32 - kNumShardBits); } public: explicit ShardedLRUCache(size_t capacity) : last_id_(0) { const size_t per_shard = (capacity + (kNumShards - 1)) / kNumShards; for (int s = 0; s \u0026lt; kNumShards; s++) { shard_[s].SetCapacity(per_shard); } } ~ShardedLRUCache() override {} Handle* Insert(const Slice\u0026amp; key, void* value, size_t charge, void (*deleter)(const Slice\u0026amp; key, void* value)) override { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } Handle* Lookup(const Slice\u0026amp; key) override { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } void Release(Handle* handle) override { LRUHandle* h = reinterpret_cast\u0026lt;LRUHandle*\u0026gt;(handle); shard_[Shard(h-\u0026gt;hash)].Release(handle); } void Erase(const Slice\u0026amp; key) override { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } void* Value(Handle* handle) override { return reinterpret_cast\u0026lt;LRUHandle*\u0026gt;(handle)-\u0026gt;value; } uint64_t NewId() override { MutexLock l(\u0026amp;id_mutex_); return ++(last_id_); } void Prune() override { for (int s = 0; s \u0026lt; kNumShards; s++) { shard_[s].Prune(); } } size_t TotalCharge() const override { size_t total = 0; for (int s = 0; s \u0026lt; kNumShards; s++) { total += shard_[s].TotalCharge(); } return total; } };   为了能使哈希值刚好能够映射 shard_ 数组，其通过位操作再次对哈希值进行处理，如下代码：\n1 2 3  // https://github.com/google/leveldb/blob/master/util/cache.cc  static uint32_t Shard(uint32_t hash) { return hash \u0026gt;\u0026gt; (32 - kNumShardBits); }   其将哈希值右移 28 位，只取最高的 4 位，这样就能够保证处理过的哈希值刚好小于 16。\n缓存淘汰 每个LRU Cache结构中有两个成员变量：lru_ 和 in_use_ 。lru_ 双向链表中的节点是可以进行淘汰的，而 in_use_ 双向链表中的节点表示正在使用，因此不可以进行淘汰。如果内存超出限制需要淘汰一个节点时，LevelDB 会将 lru_ 链表中的节点逐个淘汰。\n那我们如何决定节点放置的规则呢？其采用如下规则：如果一个缓存节点的 in_cache 为 true，并且 refs 等于 1，则放置到 lru_ 中；如果 in_cache 为 true，并且 refs 大于等于 2，则放置到 in_use_ 中。\n 对于 in_cache，其会在以下情况变为 false：  删除该节点后。 调用 LRUCache 的析构函数时，会将所有节点的 in_cache 置为 false。 插入一个节点时，如果已经存在一个键值相同的节点，则旧节点的 in_cache 会置为 false。   对于 refs，其变化规则如下:  每次调用 Ref 函数，会将 refs 变量加 1，调用 Unref 函数，会将 refs 变量减 1。 插入一个节点时，该节点会放到 in_use_ 链表中，并且初始的引用计数为 2，不再使用该节点时将引用计数减 1，如果此时节点也不再被其他地方引用，那么引用计数为 1，将其放到 lru_ 链表中。 查找一个节点时，如果查找成功，则调用 Ref，将该节点的引用计数加 1，如果引用计数大于等于 2，会将节点放到 in_use_ 链表中，同理，不再使用该节点时将引用计数减 1，如果此时节点也不再被其他地方引用，那么引用计数为 1，将其放到 lru_ 链表中。    Ref 和 Unref 代码如下：\n1 2 3 4 5 6 7 8 9  // https://github.com/google/leveldb/blob/master/util/cache.cc  void LRUCache::Ref(LRUHandle* e) { if (e-\u0026gt;refs == 1 \u0026amp;\u0026amp; e-\u0026gt;in_cache) { // If on lru_ list, move to in_use_ list.  LRU_Remove(e); LRU_Append(\u0026amp;in_use_, e); } e-\u0026gt;refs++; }   如果缓存节点在 lru_ 链表中（refs 为 1，in_cache 为 true），则首先从 lru_ 链表中删除该节点，然后将节点放到 in_use_ 链表中。最后将节点的 refs 变量加 1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/util/cache.cc  void LRUCache::Unref(LRUHandle* e) { assert(e-\u0026gt;refs \u0026gt; 0); e-\u0026gt;refs--; if (e-\u0026gt;refs == 0) { // Deallocate.  assert(!e-\u0026gt;in_cache); (*e-\u0026gt;deleter)(e-\u0026gt;key(), e-\u0026gt;value); free(e); } else if (e-\u0026gt;in_cache \u0026amp;\u0026amp; e-\u0026gt;refs == 1) { // No longer in use; move to lru_ list.  LRU_Remove(e); LRU_Append(\u0026amp;lru_, e); } }   Unref 首先将节点的 refs 变量减 1，然后判断如果 refs 已经等于 0，则删除并释放该节点，否则，如果 refs 变量等于 1 并且 in_cache 为 true，则将该节点从 in_use_ 链表中删除并且移动到 lru_ 链表中。如果内存超出限制需要淘汰一个节点时，LevelDB 会将 lru_ 链表中的节点逐个淘汰。\n应用 LevelDB 中 Cache 缓存的主要是 SSTable，即缓存节点的键为 8 字节的文件序号，值为一个包含了 SSTable 实例的结构。其主要逻辑由 TableCache 实现，首先我们先来看看它的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  // https://github.com/google/leveldb/blob/master/db/table_cache.h  class TableCache { public: TableCache(const std::string\u0026amp; dbname, const Options\u0026amp; options, int entries); ~TableCache(); Iterator* NewIterator(const ReadOptions\u0026amp; options, uint64_t file_number, uint64_t file_size, Table** tableptr = nullptr); Status Get(const ReadOptions\u0026amp; options, uint64_t file_number, uint64_t file_size, const Slice\u0026amp; k, void* arg, void (*handle_result)(void*, const Slice\u0026amp;, const Slice\u0026amp;)); void Evict(uint64_t file_number); private: Status FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle**); Env* const env_; const std::string dbname_; const Options\u0026amp; options_; Cache* cache_; };     成员变量\n env_：用于读取 SSTable 文件。 dbname_：SSTable 名字。 options_：Cache 参数配置。 cache_：缓存基类句柄。    这里的核心逻辑 FindTable 方法会使用文件序号（file_number）作为键，并且在 cache_ 中查找是否存在该键，如果不存在，则需要打开一个 SSTable，经过处理之后作为值插入 cache_ 中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  Status TableCache::FindTable(uint64_t file_number, uint64_t file_size, Cache::Handle** handle) { Status s; char buf[sizeof(file_number)]; EncodeFixed64(buf, file_number); Slice key(buf, sizeof(buf)); //在缓存中查找key是否存在  *handle = cache_-\u0026gt;Lookup(key); //如果不存在，则打开一个SSTable文件  if (*handle == nullptr) { //生成fname和RandomAccessFile实例  std::string fname = TableFileName(dbname_, file_number); RandomAccessFile* file = nullptr; Table* table = nullptr; s = env_-\u0026gt;NewRandomAccessFile(fname, \u0026amp;file); if (!s.ok()) { std::string old_fname = SSTTableFileName(dbname_, file_number); if (env_-\u0026gt;NewRandomAccessFile(old_fname, \u0026amp;file).ok()) { s = Status::OK(); } } //打开SSTable文件并且生成一个Table实例，并保存到table变量中。  if (s.ok()) { s = Table::Open(options_, file, file_size, \u0026amp;table); } if (!s.ok()) { assert(table == nullptr); delete file; } else { TableAndFile* tf = new TableAndFile; tf-\u0026gt;file = file; tf-\u0026gt;table = table; //以文件序号为key，TableAndFile为Value，插入缓存中。  *handle = cache_-\u0026gt;Insert(key, tf, 1, \u0026amp;DeleteEntry); } } return s; }   SSTable 在 Cache 中缓存时的键为文件序列号，值为一个 TableAndFile 实例，该实例中包括两个成员变量，分别为 file 和 table，查找时通过保存在 table 变量中的 Table 实例迭代器进行查找。\n","date":"2022-05-23T23:41:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-sstable%E6%A8%A1%E5%9D%97/","title":"LevelDB SSTable模块"},{"content":"MemTable模块 MemTable 在 LevelDB 中，MemTable 是底层数据结构 SkipList 的封装。\n结构 首先我们来看看它的结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  // https://github.com/google/leveldb/blob/master/db/memtable.h  class MemTable { public: explicit MemTable(const InternalKeyComparator\u0026amp; comparator); MemTable(const MemTable\u0026amp;) = delete; MemTable\u0026amp; operator=(const MemTable\u0026amp;) = delete; void Ref() { ++refs_; } void Unref() { --refs_; assert(refs_ \u0026gt;= 0); if (refs_ \u0026lt;= 0) { delete this; } } size_t ApproximateMemoryUsage(); Iterator* NewIterator(); void Add(SequenceNumber seq, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value); bool Get(const LookupKey\u0026amp; key, std::string* value, Status* s); private: friend class MemTableIterator; friend class MemTableBackwardIterator; struct KeyComparator { const InternalKeyComparator comparator; explicit KeyComparator(const InternalKeyComparator\u0026amp; c) : comparator(c) {} int operator()(const char* a, const char* b) const; }; typedef SkipList\u0026lt;const char*, KeyComparator\u0026gt; Table; ~MemTable(); // Private since only Unref() should be used to delete it  KeyComparator comparator_; int refs_; Arena arena_; Table table_; };   其组成如下：\n 成员变量  comparator_：比较器，用于决定 key 的顺序。 refs__：引用计数器，当计数为 0 时释放资源。 arena_：内存池，负责管理内存。 table_：底层存储的 SkipList。   成员函数  Ref ：引用计数增加。 Unref：引用计数减少。 ApproximateMemoryUsage：统计内存使用量。 NewIterator：返回首部迭代器。 Add：插入数据。 Get：查找数据。    接着我们来看看最为核心的插入与查找。\n插入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  // https://github.com/google/leveldb/blob/master/db/memtable.cc  void MemTable::Add(SequenceNumber s, ValueType type, const Slice\u0026amp; key, const Slice\u0026amp; value) { //计算需要的内存大小  size_t key_size = key.size(); size_t val_size = value.size(); size_t internal_key_size = key_size + 8; const size_t encoded_len = VarintLength(internal_key_size) + internal_key_size + VarintLength(val_size) + val_size; //分配内存，并按照 len key sequencelValueType value_len value顺序将数据写入缓冲区  char* buf = arena_.Allocate(encoded_len); char* p = EncodeVarint32(buf, internal_key_size); std::memcpy(p, key.data(), key_size); p += key_size; EncodeFixed64(p, (s \u0026lt;\u0026lt; 8) | type); p += 8; p = EncodeVarint32(p, val_size); std::memcpy(p, value.data(), val_size); assert(p + val_size == buf + encoded_len); //调用底层SkipList的Insert将数据插入  table_.Insert(buf) }   插入的逻辑主要分为三步：\n 计算需要的内存大小。 分配内存，并按照 len key sequencelValueType value_len value顺序将数据写入缓冲区。 调用底层SkipList的Insert将数据插入。  查找 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  // https://github.com/google/leveldb/blob/master/db/memtable.cc  bool MemTable::Get(const LookupKey\u0026amp; key, std::string* value, Status* s) { Slice memkey = key.memtable_key(); //获取迭代器  Table::Iterator iter(\u0026amp;table_);\t//通过迭代器的Seek查找数据  iter.Seek(memkey.data()); if (iter.Valid()) { // entry format is:  // klength varint32  // userkey char[klength]  // tag uint64  // vlength varint32  // value char[vlength]  const char* entry = iter.key(); uint32_t key_length; const char* key_ptr = GetVarint32Ptr(entry, entry + 5, \u0026amp;key_length); //判断查找是否成功  if (comparator_.comparator.user_comparator()-\u0026gt;Compare( Slice(key_ptr, key_length - 8), key.user_key()) == 0) { const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8); //判断查找到的类型  switch (static_cast\u0026lt;ValueType\u0026gt;(tag \u0026amp; 0xff)) { //如果数据存在，则将数据保存后返回true  case kTypeValue: { Slice v = GetLengthPrefixedSlice(key_ptr + key_length); value-\u0026gt;assign(v.data(), v.size()); return true; } //如果数据删除，则将状态标记为未找到并返回true  case kTypeDeletion: *s = Status::NotFound(Slice()); return true; } } } //没有找到则返回false  return false; }   查找流程如下：\n 获取迭代器，通过 Seek 查找数据。 判断查找结果：  查找成功，数据存在，保存数据后返回 true。 查找成功，数据已删除（曾经存在），标记状态为 NotFound 后返回 true。 查找失败，数据不存在，返回 false。    了解完了 MemTable，接下来再看看它底层使用的 SkipList 是如何实现的。\nSkipList SkipList 是一个多层有序链表结构，通过在每个节点中保存多个指向其他节点的指针，将有序链表平均的复杂度O（N） 降低到 O（logN）。SkipList 因具有实现简单、性能优良等特点得到了广泛应用，例如 Redis 中的 ZSet，以及 LevelDB 的 MemTable。\n具体信息可以看看我之前写的博客 高级数据结构与算法 | 跳跃表（Skip List）。这里就不多作介绍，直接看代码。\n结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; class SkipList { private: struct Node; public: explicit SkipList(Comparator cmp, Arena* arena); SkipList(const SkipList\u0026amp;) = delete; SkipList\u0026amp; operator=(const SkipList\u0026amp;) = delete; void Insert(const Key\u0026amp; key); bool Contains(const Key\u0026amp; key) const; class Iterator { public: explicit Iterator(const SkipList* list); bool Valid() const; const Key\u0026amp; key() const; void Next(); void Prev(); void Seek(const Key\u0026amp; target); void SeekToFirst(); void SeekToLast(); private: const SkipList* list_; Node* node_; }; private: enum { kMaxHeight = 12 }; inline int GetMaxHeight() const { return max_height_.load(std::memory_order_relaxed); } Node* NewNode(const Key\u0026amp; key, int height); int RandomHeight(); bool Equal(const Key\u0026amp; a, const Key\u0026amp; b) const { return (compare_(a, b) == 0); } bool KeyIsAfterNode(const Key\u0026amp; key, Node* n) const; Node* FindGreaterOrEqual(const Key\u0026amp; key, Node** prev) const; Node* FindLessThan(const Key\u0026amp; key) const; Node* FindLast() const; Comparator const compare_; Arena* const arena_; Node* const head_; std::atomic\u0026lt;int\u0026gt; max_height_; Random rnd_; };   这里也是只介绍几个重要的函数——晋升、插入、查找。\n晋升 在 LevelDB 中，每次插入节点的层高由 RandomHeight 决定，比起 Redis 的 1/2 来说，LevelDB 中的晋升逻辑更加复杂，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; int SkipList\u0026lt;Key, Comparator\u0026gt;::RandomHeight() { static const unsigned int kBranching = 4; int height = 1; while (height \u0026lt; kMaxHeight \u0026amp;\u0026amp; ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u0026gt; 0); assert(height \u0026lt;= kMaxHeight); return height; }   上述代码中 rnd_.Next() 的作用是生成一个随机数，将该随机数对 4 取余，如果余数等于 0 并且层高小于规定的最大层高 12，则将层高加 1。因为对 4 取余数结果只有 0、1、2、3 这 4 种可能，因此可以推导得出每个节点层高为 1 的概率是 3/4，层高为 2 的概率是 1/4。依此类推，层高为 3 的概率是 3/16（ 1/4 × 3/4 ），层高为4的概率是3/64（ 1/4 × 1/4 × 3/4 ），即层级越高，概率越小。\n查找 SkipList 的查找主要是借助迭代器的 Seek 来实现的，而在 Seek 中又调用了 FindGreaterOrEqual，下面看看它们的实现逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; inline void SkipList\u0026lt;Key, Comparator\u0026gt;::Iterator::Seek(const Key\u0026amp; target) { node_ = list_-\u0026gt;FindGreaterOrEqual(target, nullptr); } SkipList\u0026lt;Key, Comparator\u0026gt;::FindGreaterOrEqual(const Key\u0026amp; key, Node** prev) const { //从顶层开始查询  Node* x = head_; int level = GetMaxHeight() - 1; while (true) { Node* next = x-\u0026gt;Next(level); //如果当前节点的值小于要查询的值，则在该层继续查找  if (KeyIsAfterNode(key, next)) { x = next; } else { //如果大于等于，则说明不可能在该层，前往下一层查找。  if (prev != nullptr) prev[level] = x; //如果查询到底就直接返回，此时有两种情况1.查询成功，返回底层结果 2.查询失败，返回对应最底层位置  if (level == 0) { return next; } else { // Switch to next list  level--; } } } }   查找逻辑与常规 SkipList 实现一样：\n 从顶层开始查询。 对比当前阶段的值是否小于查询的值：  小于：沿着当前层继续查找。 大于等于：前往下一层查找。   判断当前层数是否到底，没到就继续往下走。 到底了返回数据，如果返回的数据与 key 相同则说明查询成功，否则失败。  插入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  // https://github.com/google/leveldb/blob/master/db/skiplist.h  template \u0026lt;typename Key, class Comparator\u0026gt; void SkipList\u0026lt;Key, Comparator\u0026gt;::Insert(const Key\u0026amp; key) { //记录每一层级查找到的位置  Node* prev[kMaxHeight]; //查找适合插入的位置  Node* x = FindGreaterOrEqual(key, prev); assert(x == nullptr || !Equal(key, x-\u0026gt;key)); //获取本次插入的最高层数  int height = RandomHeight(); //如果本次插入的最高层数大于目前最高层数，则将多出的几层指向新插入节点  if (height \u0026gt; GetMaxHeight()) { for (int i = GetMaxHeight(); i \u0026lt; height; i++) { prev[i] = head_; } max_height_.store(height, std::memory_order_relaxed); } x = NewNode(key, height); //将需要更新的节点依次更新  for (int i = 0; i \u0026lt; height; i++) { x-\u0026gt;NoBarrier_SetNext(i, prev[i]-\u0026gt;NoBarrier_Next(i)); prev[i]-\u0026gt;SetNext(i, x); } }   具体逻辑如下：\n 首先用一个数组存储每一层所查找到的位置。 使用 FindGreaterOrEqual 获取适合插入的位置。 通过 RandomHeight 获取本次插入的最高层数：  如果本次插入的最高层数大于目前最高层数，则将多出的几层指向新插入节点。 如果小于等于，则无需更新。   遍历 prev，将需要更新的节点依次更新。  MemTable写入SSTable 在 LSM 树的实现中，会先将数据写入 MemTable，当 MemTable 大于配置的阈值时，将其作为 SSTable 写入磁盘。\n这里我们只看核心逻辑，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  // https://github.com/google/leveldb/blob/master/db/db_impl.cc  Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { //...  //生成一个MemTable迭代器  Iterator* iter = mem-\u0026gt;NewIterator(); Log(options_.info_log, \u0026#34;Level-0 table #%llu: started\u0026#34;, (unsigned long long)meta.number); Status s; { mutex_.Unlock(); //调用BuildTable，将MemTable迭代器作为参数传入，生成一个SSTable  s = BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026amp;meta); mutex_.Lock(); } //...  return s; }   在这里首先会生成一个 MemTable 迭代器，调用 BuildTable ，将 MemTable 迭代器作为参数传入，生成一个 SSTable。\n我们接着来看 BuildTable 的逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  // https://github.com/google/leveldb/blob/master/db/builder.cc  Status BuildTable(const std::string\u0026amp; dbname, Env* env, const Options\u0026amp; options, TableCache* table_cache, Iterator* iter, FileMetaData* meta) { Status s; meta-\u0026gt;file_size = 0; //将迭代器移动到首部  iter-\u0026gt;SeekToFirst(); //生成SSTable文件名  std::string fname = TableFileName(dbname, meta-\u0026gt;number); if (iter-\u0026gt;Valid()) { WritableFile* file; s = env-\u0026gt;NewWritableFile(fname, \u0026amp;file); if (!s.ok()) { return s; } //创建一个TableBuilder  TableBuilder* builder = new TableBuilder(options, file); meta-\u0026gt;smallest.DecodeFrom(iter-\u0026gt;key()); Slice key; //遍历迭代器，将MemTable中的每一对K-V写入TableBuilder中  for (; iter-\u0026gt;Valid(); iter-\u0026gt;Next()) { key = iter-\u0026gt;key(); builder-\u0026gt;Add(key, iter-\u0026gt;value()); } if (!key.empty()) { meta-\u0026gt;largest.DecodeFrom(key); } //调用TableBuilder的Finish函数生成SSTable文件  s = builder-\u0026gt;Finish(); if (s.ok()) { meta-\u0026gt;file_size = builder-\u0026gt;FileSize(); assert(meta-\u0026gt;file_size \u0026gt; 0); } delete builder; //调用Sync将文件刷新到磁盘中\t if (s.ok()) { s = file-\u0026gt;Sync(); } //关闭文件  if (s.ok()) { s = file-\u0026gt;Close(); } //...  return s; }   核心逻辑如下：\n 将 MemTable 迭代器移动到首部。 生成 SSTable 文件名。 创建一个 TableBuilder。 遍历迭代器，将 MemTable 中的每一对 K-V 写入 TableBuilder 中。 调用 TableBuilder 的 Finish 函数生成 SSTable 文件。 调用 Sync 将文件刷新到磁盘中。 关闭文件。  ","date":"2022-05-23T23:40:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-memtable%E6%A8%A1%E5%9D%97/","title":"LevelDB MemTable模块"},{"content":"公共基础类 内存管理 Arena 内存频繁创建/释放的地方就会有内存池的出现，LevelDB 也不例外。在 Memtable 组件中，会有大量内存创建（数据持续 put）和释放（dump 到磁盘后内存结束），于是 LevelDB 通过 Arena 来管理内存。\n 它有什么好处呢？\n  提升性能：内存申请本身就需要占用一定的资源，消耗空间与时间。而 Arena 内存池的基本思路就是预先申请一大块内存，然后多次分配给不同的对象，从而减少 malloc 或 new 的调用次数。 提高内存利用率：频繁进行内存的申请与释放易造成内存碎片。即内存余量虽然够用，但由于缺乏足够大的连续空闲空间，从而造成申请一段较大的内存不成功的情况。而 Arena 具有整块内存的控制权，用户可以任意操作这段内存，从而避免内存碎片的产生。  结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  // https://github.com/google/leveldb/blob/master/util/arena.h  class Arena { public: Arena(); Arena(const Arena\u0026amp;) = delete; Arena\u0026amp; operator=(const Arena\u0026amp;) = delete; ~Arena(); char* Allocate(size_t bytes); malloc. char* AllocateAligned(size_t bytes); size_t MemoryUsage() const { return memory_usage_.load(std::memory_order_relaxed); } private: char* AllocateFallback(size_t bytes); char* AllocateNewBlock(size_t block_bytes); char* alloc_ptr_; size_t alloc_bytes_remaining_; std::vector\u0026lt;char*\u0026gt; blocks_; std::atomic\u0026lt;size_t\u0026gt; memory_usage_; };   其组成如下：\n 成员变量 alloc_ptr_ ：当前已使用内存的指针  blocks_ ：实际分配的内存池_ alloc_bytes_remaining_ ：剩余内存字节数 memory_usage_ ：记录内存的使用情况 kBlockSize ：一个块大小（默认4k）   成员函数  AllocateFallback ：按需分配内存，可能会有内存浪费。 AllocateAligned ：分配偶数大小的内存，主要是 skiplist 节点时，目的是加快访问。 MemoryUsage：统计内存使用量。    内存分配 Arena 中与内存分配有关的两个接口函数：Allocate 与 AllocateAligned。\n首先我们来看看 Allocate 与它依赖的两个函数 AllocateFallback 与 AllocateNewBlock。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12  // https://github.com/google/leveldb/blob/master/util/arena.h  inline char* Arena::Allocate(size_t bytes) { assert(bytes \u0026gt; 0); if (bytes \u0026lt;= alloc_bytes_remaining_) { char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } return AllocateFallback(bytes); }   首先，对于 Allocate 来说，它存在两种情况\n 需要分配的字节数小于等于 alloc_bytes_remaining_：Allocate 直接返回 alloc_ptr_ 指向的地址空间，然后对 alloc_ptr_ 与 alloc_bytes_remaining_ 进行更新。 需要分配的字节数大于 alloc_bytes_remaining_：调用 AllocateFallback 方法进行扩容。  AllocateFallback 用于申请一个新 Block 内存空间，然后分配需要的内存并返回。因此当前 Block 剩余空闲内存就不可避免地浪费了。接着看看 AllocateFallback 的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateFallback(size_t bytes) { if (bytes \u0026gt; kBlockSize / 4) { char* result = AllocateNewBlock(bytes); return result; } alloc_ptr_ = AllocateNewBlock(kBlockSize); alloc_bytes_remaining_ = kBlockSize; char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; }   AllocateFallback 的使用也包括两种情况：\n 需要分配的空间大于 kBlockSize 的 1/4（即1024字节）：直接申请需要分配空间大小的 Block，从而避免剩余内存空间的浪费。 需要分配的空间小于等于 kBlockSize 的 1/4 ：申请一个大小为 kBlockSize 的新 Block 空间，然后在新的Block上分配需要的内存并返回其首地址。  对于 Block 的分配，这里又调用了 AllocateNewBlock，其逻辑如下：\n1 2 3 4 5 6 7 8 9  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); memory_usage_.fetch_add(block_bytes + sizeof(char*), std::memory_order_relaxed); return result; }   这里的逻辑非常简单，其通过 new，申请了一段大小为 block_bytes 的内存空间，并将这块空间的地址存储到 blocks_ 中，之后更新当前可用的总空间大小后将空间首地址返回。\n讲解完了 Allocate ，我们接着再来看看 AllocateAligned。虽然它也用于内存分配，但不同点在于它考虑了内存分配时的内存对齐问题（进行内存分配所返回的起始地址应为 b / 8 的倍数，在这里 b 代表操作系统平台的位数）。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateAligned(size_t bytes) { const int align = (sizeof(void*) \u0026gt; 8) ? sizeof(void*) : 8; static_assert((align \u0026amp; (align - 1)) == 0, \u0026#34;Pointer size should be a power of 2\u0026#34;); size_t current_mod = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(alloc_ptr_) \u0026amp; (align - 1); size_t slop = (current_mod == 0 ? 0 : align - current_mod); size_t needed = bytes + slop; char* result; if (needed \u0026lt;= alloc_bytes_remaining_) { result = alloc_ptr_ + slop; alloc_ptr_ += needed; alloc_bytes_remaining_ -= needed; } else { // AllocateFallback always returned aligned memory  result = AllocateFallback(bytes); } assert((reinterpret_cast\u0026lt;uintptr_t\u0026gt;(result) \u0026amp; (align - 1)) == 0); return result; }   由于主流的服务器平台采用 64 位的操作系统，64位 操作系统的指针同样为 64 位（即 8 个字节），因此这里的对齐就需要使分配的内存起始地址必然为 8 的倍数。要满足这一条件，采用的主要办法就是判断当前空闲内存的起始地址是否为 8 的倍数：如果是，则直接返回；如果不是，则对 8 求模，然后向后寻找最近的 8 的倍数的内存地址并返回。\n由于计算机进行乘除或求模运算的速度远慢于位操作运算，因此这里巧妙的用了位运算来进行优化。\n 用位运算判断某个数值是否为2的正整数幂：   static_assert((align \u0026amp; (align - 1)) == 0, \u0026quot;Pointer size should be a power of 2\u0026quot;);\n用位运算进行求模：  size_t current_mod = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(alloc_ptr_) \u0026amp; (align - 1);\n位运算的细节这里就i不提了，大家可以自行百度了解。\n内存使用率统计 memory_usage_ 用于存储当前 Arena 所申请的总共的内存空间大小，为了保证线程安全，其为 atomic\u0026lt;size_t\u0026gt; 变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/util/arena.h  size_t MemoryUsage() const { return memory_usage_.load(std::memory_order_relaxed); } // https://github.com/google/leveldb/blob/master/util/arena.cc  char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); memory_usage_.fetch_add(block_bytes + sizeof(char*), std::memory_order_relaxed); return result; }   在 AllocateNewBock 申请内存的过程中，其会通过以下的公式来更新当前总大小\nmemory_usage_ = memory_usage_ + block_bytes + sizeof(char*) \n 为什么这里还需要加上一个sizeof（char*）呢？\n 因为在申请完 Block 后，Block 的首地址需要存储在一个 vector\u0026lt;char*\u0026gt; 的动态数组 blocks_ 中，因而需要额外占用一个指针的空间。\nTCMalloc LevelDB 中针对一些需要调用 new 或 malloc 方法进行堆内存操作的情况（即非内存池的内存分配），其使用 TCMalloc 进行优化。\nTCMalloc（Thread-Caching Malloc）是 google-perftool 中一个管理堆内存的内存分配器工具，可以降低内存频繁分配与释放所造成的性能损失，并有效控制内存碎片。默认 C/C++ 在编译器中主要采用 glibc 的内存分配器 ptmalloc2。同样的 malloc 操作，TCMalloc 比 ptmalloc2 更具性能优势。\nTCMalloc 的详细介绍可以参见http://goog-perftools.sourceforge.net/doc/tcmalloc.html。\nEnv家族 Env 是一个抽象接口类，用纯虚函数的形式定义了一些与平台操作的相关接口，如文件系统、多线程、时间操作等。接口定义如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT Env { public: Env(); Env(const Env\u0026amp;) = delete; Env\u0026amp; operator=(const Env\u0026amp;) = delete; virtual ~Env(); static Env* Default(); virtual Status NewSequentialFile(const std::string\u0026amp; fname, SequentialFile** result) = 0; virtual Status NewRandomAccessFile(const std::string\u0026amp; fname, RandomAccessFile** result) = 0; virtual Status NewWritableFile(const std::string\u0026amp; fname, WritableFile** result) = 0; virtual Status NewAppendableFile(const std::string\u0026amp; fname, WritableFile** result); virtual bool FileExists(const std::string\u0026amp; fname) = 0; virtual Status GetChildren(const std::string\u0026amp; dir, std::vector\u0026lt;std::string\u0026gt;* result) = 0; virtual Status RemoveFile(const std::string\u0026amp; fname); virtual Status DeleteFile(const std::string\u0026amp; fname); virtual Status CreateDir(const std::string\u0026amp; dirname) = 0; virtual Status RemoveDir(const std::string\u0026amp; dirname); virtual Status DeleteDir(const std::string\u0026amp; dirname); virtual Status GetFileSize(const std::string\u0026amp; fname, uint64_t* file_size) = 0; virtual Status RenameFile(const std::string\u0026amp; src, const std::string\u0026amp; target) = 0; virtual Status LockFile(const std::string\u0026amp; fname, FileLock** lock) = 0; virtual Status UnlockFile(FileLock* lock) = 0; virtual void Schedule(void (*function)(void* arg), void* arg) = 0; virtual void StartThread(void (*function)(void* arg), void* arg) = 0; virtual Status GetTestDirectory(std::string* path) = 0; virtual Status NewLogger(const std::string\u0026amp; fname, Logger** result) = 0; virtual uint64_t NowMicros() = 0; virtual void SleepForMicroseconds(int micros) = 0; };   Env 作为抽象类，有 3 个派生子类：PosixEnv、EnvWrapper 与 InMemoryEnv。\n这里就不过多的介绍它们的实现原理，只是大概的描述一下概念，方便理解。\nPosixEnv PosixEnv 是 LevelDB 中默认的 Env 实例对象。从字面意思上看，PosixEnv 就是针对 POSIX 平台的 Env 接口实现。\nEnvWrapper EnvWrapper 也是 Env 的一个派生类，与 PosixEnv 不同的是，EnvWrapper 中并没有定义众多纯虚函数接口的具体实现，而是定义了一个私有成员变量 Env* target_，并在构造函数中通过传递预定义的 Env 实例对象，从而实现对 target_ 的初始化操作。基于 EnvWrapper 的派生类，易于实现用户在某一个 Env 派生类的基础上改写其中一部分接口的需求。\nInMemoryEnv InMemoryEnv 就是 EnvWrapper 的一个子类，主要对 Env 中有关文件的接口进行了重写。InMemoryEnv 主要是将所有的操作都置于内存中，从而提升文件I/O的读取速度。\n文件操作 在 LevelDB 中，主要有三种文件 I/O 操作：\n SequentialFile：顺序读，如日志文件的读取、Manifest文件的读取。 WritableFile：顺序写，用于日志文件、SSTable文件、Manifest文件的写入。 RandomAccessFile：随机读，如SSTable文件的读取。  SequentialFile SequentialFile 定义了文件顺序读抽象接口，其接口定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT SequentialFile { public: SequentialFile() = default; SequentialFile(const SequentialFile\u0026amp;) = delete; SequentialFile\u0026amp; operator=(const SequentialFile\u0026amp;) = delete; virtual ~SequentialFile(); virtual Status Read(size_t n, Slice* result, char* scratch) = 0; virtual Status Skip(uint64_t n) = 0; };   其主要有两个接口方法，即 Read 与 Skip：\n Read：用于从文件当前位置顺序读取指定的字节数。 Skip：用于从当前位置，顺序向后忽略指定的字节数。   无论是Read方法还是Skip方法，对于多线程环境而言均不是线程安全的访问方法，需要开发者在调用过程中采用外部手段进行线程同步操作。\n PosixSequentialFile，是在符合POSIX标准的文件系统上对顺序读的实现。这里就不过多介绍，感兴趣的可以自己去了解。\nWritableFile WritableFile定义了文件顺序写抽象接口，其定义下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT WritableFile { public: WritableFile() = default; WritableFile(const WritableFile\u0026amp;) = delete; WritableFile\u0026amp; operator=(const WritableFile\u0026amp;) = delete; virtual ~WritableFile(); virtual Status Append(const Slice\u0026amp; data) = 0; virtual Status Close() = 0; virtual Status Flush() = 0; virtual Status Sync() = 0; };   WritableFile 主要有4个纯虚函数接口：Append、Close、Flush 与 Sync：\n Append：用于以追加的方式对文件顺序写入。 Close：用于关闭文件。 Flush：用于将 Append 操作写入到缓冲区的数据强制刷新到内核缓冲区。 Sync：用于将内存缓冲区的数据强制保存到磁盘。  PosixWritableFile 是对符合 POSIX 标准平台的 WritableFile 的派生实现。\nRandomAccessFile 随机读就是指可以定位到文件任意某个位置进行读取。RandomAccessFile 是文件随机读的抽象接口，其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  // https://github.com/google/leveldb/blob/master/include/leveldb/env.h  class LEVELDB_EXPORT RandomAccessFile { public: RandomAccessFile() = default; RandomAccessFile(const RandomAccessFile\u0026amp;) = delete; RandomAccessFile\u0026amp; operator=(const RandomAccessFile\u0026amp;) = delete; virtual ~RandomAccessFile(); virtual Status Read(uint64_t offset, size_t n, Slice* result, char* scratch) const = 0; };   与顺序读接口相比，随机读没有 Skip 操作，只有一个 Read 方法：\n Read：可从文件指定的任意位置读取一段指定长度数据。  在 LevelDB 中，RandomAccessFile 有两个派生类：PosixRandomAccessFile 与 PosixMmapReadableFile。这两个派生类是两种对随机文件操作的实现形式：一种是基于 pread() 方法的随机访问；另一种是基于 mmap() 方法的随机访问。\n数值编码 LevelDB 是一个嵌入式的存储库，其存储的内容可以是字符，也可以是数值。LevelDB 为了减少数值型内容对内存空间的占用，分别针对不同的需求定义了两种编码方式：一种是定长编码，另一种是变长编码。\n字节序 在讲编码之前，先聊聊字节序。字节序是处理器架构的特性，比如一个16 位的整数，他是由 2 个字节组成。内存中存储 2 个字节有两种方法：\n 将低序字节存储在起始地址，称为小端。 将高序字节存储在起始地址，称为大端。  LevelDB 为了便于操作，编码的数据统一采用小端模式，并存放到对应的字符串中，即数据低位存在内存低地址，数据高位存在内存高地址。\n具体的关于大小端的信息可以参考往期博客 大端小端存储解析以及判断方法。\n定长编码 定长的数值编码比较简单，主要是将原有的 uint64 或 uint32 的数据直接存储在对应的 8 字节或 4 字节中。而在直接存储的过程中，会基于大小端的不同，采取不同的执行逻辑（新版代码简化逻辑，统一按照大端逻辑处理）：\n 大端：在复制过程中调换字节顺序，以小端模式进行编码保存。 小端：直接利用 memcpy 进行内存间的复制。  具体实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  // https://github.com/google/leveldb/blob/master/util/coding.h  //32位数据定长编码 void EncodeFixed32(char* buf, uint32_t value) { if (port::kLittleEndian) { memcpy(buf, \u0026amp;value, sizeof(value)); } else { buf[0] = value \u0026amp; 0xff; buf[1] = (value \u0026gt;\u0026gt; 8) \u0026amp; 0xff; buf[2] = (value \u0026gt;\u0026gt; 16) \u0026amp; 0xff; buf[3] = (value \u0026gt;\u0026gt; 24) \u0026amp; 0xff; } } //64位数据定长编码 void EncodeFixed64(char* buf, uint64_t value) { if (port::kLittleEndian) { memcpy(buf, \u0026amp;value, sizeof(value)); } else { buf[0] = value \u0026amp; 0xff; buf[1] = (value \u0026gt;\u0026gt; 8) \u0026amp; 0xff; buf[2] = (value \u0026gt;\u0026gt; 16) \u0026amp; 0xff; buf[3] = (value \u0026gt;\u0026gt; 24) \u0026amp; 0xff; buf[4] = (value \u0026gt;\u0026gt; 32) \u0026amp; 0xff; buf[5] = (value \u0026gt;\u0026gt; 40) \u0026amp; 0xff; buf[6] = (value \u0026gt;\u0026gt; 48) \u0026amp; 0xff; buf[7] = (value \u0026gt;\u0026gt; 56) \u0026amp; 0xff; } } //32位数据定长解码 inline uint32_t DecodeFixed32(const char* ptr) { if (port::kLittleEndian) { // Load the raw bytes  uint32_t result; memcpy(\u0026amp;result, ptr, sizeof(result)); // gcc optimizes this to a plain load  return result; } else { return ((static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[0]))) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[1])) \u0026lt;\u0026lt; 8) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[2])) \u0026lt;\u0026lt; 16) | (static_cast\u0026lt;uint32_t\u0026gt;(static_cast\u0026lt;unsigned char\u0026gt;(ptr[3])) \u0026lt;\u0026lt; 24)); } } //64位数据定长解码 inline uint64_t DecodeFixed64(const char* ptr) { if (port::kLittleEndian) { // Load the raw bytes  uint64_t result; memcpy(\u0026amp;result, ptr, sizeof(result)); // gcc optimizes this to a plain load  return result; } else { uint64_t lo = DecodeFixed32(ptr); uint64_t hi = DecodeFixed32(ptr + 4); return (hi \u0026lt;\u0026lt; 32) | lo; } }   变长编码 对于 4 字节或 8 字节表示的无符号整型数据而言，数值较小的整数的高位空间基本为 0，如 uint32 类型的数据 128，高位的 3 个字节都是 0。为了更好的利用这些高位空间，如果能基于某种机制，将为 0 的高位数据忽略，有效地保留其有效位，从而减少所需字节数、节约存储空间。于是 LevelDB 就采用了 Google 的另一个开源项目 Protobuf 中提出的 varint 变长编码。\n**varint 将实际的一个字节分成了两个部分，最高位定义为 MSB（mostsignificant bit），后续低 7 位表示实际数据。**MSB 是一个标志位，用于表示某一数值的字节是否还有后续的字节，如果为 1 表示该数值后续还有字节，如果为 0 表示该数值所编码的字节至此完毕。每一个字节中的第 1 到第 7 位表示的是实际的数据，由于有 7 位，则只能表示大小为 0～127 的数值。\n下图给出了 127、300、2^28 - 1 的编码结果\n具体实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  // https://github.com/google/leveldb/blob/master/util/coding.cc  //32位数据变长编码 char* EncodeVarint32(char* dst, uint32_t v) { // Operate on characters as unsigneds  uint8_t* ptr = reinterpret_cast\u0026lt;uint8_t*\u0026gt;(dst); static const int B = 128; if (v \u0026lt; (1 \u0026lt;\u0026lt; 7)) { *(ptr++) = v; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 14)) { *(ptr++) = v | B; *(ptr++) = v \u0026gt;\u0026gt; 7; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 21)) { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = v \u0026gt;\u0026gt; 14; } else if (v \u0026lt; (1 \u0026lt;\u0026lt; 28)) { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = (v \u0026gt;\u0026gt; 14) | B; *(ptr++) = v \u0026gt;\u0026gt; 21; } else { *(ptr++) = v | B; *(ptr++) = (v \u0026gt;\u0026gt; 7) | B; *(ptr++) = (v \u0026gt;\u0026gt; 14) | B; *(ptr++) = (v \u0026gt;\u0026gt; 21) | B; *(ptr++) = v \u0026gt;\u0026gt; 28; } return reinterpret_cast\u0026lt;char*\u0026gt;(ptr); } //32位数据变长解码 const char* GetVarint32PtrFallback(const char* p, const char* limit, uint32_t* value) { uint32_t result = 0; for (uint32_t shift = 0; shift \u0026lt;= 28 \u0026amp;\u0026amp; p \u0026lt; limit; shift += 7) { uint32_t byte = *(reinterpret_cast\u0026lt;const uint8_t*\u0026gt;(p)); p++; if (byte \u0026amp; 128) { // More bytes are present  result |= ((byte \u0026amp; 127) \u0026lt;\u0026lt; shift); } else { result |= (byte \u0026lt;\u0026lt; shift); *value = result; return reinterpret_cast\u0026lt;const char*\u0026gt;(p); } } return nullptr; }   ","date":"2022-05-23T23:29:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%85%AC%E5%85%B1%E5%9F%BA%E7%A1%80%E7%B1%BB/","title":"LevelDB 公共基础类"},{"content":"LevelDB 架构 源码结构 LevelDB 的源码托管在 GitHub 上（https://github.com/google/leveldb），其中与程序实现源码相关的主要有以下几项：\n db：包含数据库的一些基本接口操作与内部实现。 table：为排序的字符串表 SSTable（Sorted String Table）的主体实现。 helpers：定义了 LevelDB 底层数据部分完全运行于内存环境的实现方法，主要用于相关的测试或某些全内存的运行场景。 util：包含一些通用的基础类函数，如内存管理、布隆过滤器、编码、CRC 等相关函数。 include：包含 LevelDB 库函数、可供外部访问的接口、基本数据结构等。 port：定义了一个通用的底层文件，以及多个进程操作接口，还有基于操作系统移植性实现的各平台的具体接口。  具体的环境搭建流程在前面的实战章节就讲过，这里就不过多介绍了。\n整体架构 LevelDB 总体模块架构主要包括接口 API（DB API 与 POSIX API）、Utility 公用基础类、LSM 树（Log、MemTable、SSTable）3个部分，如下图：\n 接口 API：接口 API 主要包括客户端调用的 DB API 以及针对操作系统底层的统一接口 POSIX API：  DB API：主要用于封装一些供客户端应用进行调用的接口，即头文件中的相关 API 函数接口，客户端应用可以通过这些接口实现数据引擎的各种操作。 POSIX API：实现了对操作系统底层相关操作的接口封装，主要用于保证 LevelDB 的可移植性，从而实现 LevelDB 在各 POSIX 操作系统的跨平台特性   Utility 公用基础类：主要用于实现主体功能所依赖的各种对象功能，例如内存管理 Arena、布隆过滤器、缓存、CRC 校验、哈希表、测试框架等。 LSM 树：LSM 树是 LevelDB 最重要的组件，也是实现其他功能的核心。一般而言，在常规的物理硬盘存储介质上，顺序写比随机写速度要快，而 LSM 树正是充分利用了这一物理特性，从而实现对频繁、大量数据写操作的支持。  LSM 架构如下：\n MemTable：内存数据结构，具体实现是 SkipList。接受用户的读写请求，新的数据修改会首先在这里写入。 Immutable MemTable：当 MemTable 的大小达到设定的阈值时，会变成 Immutable MemTable，只接受读操作，不再接受写操作，后续由后台线程 Flush 到磁盘上。 SSTable：Sorted String Table Files，磁盘数据存储文件。分为 Level0 到 LevelN 多层，每一层包含多个 SST 文件，文件内数据有序。Level0 直接由 Immutable Memtable Flush 得到，其它每一层的数据由上一层进行 Compaction 得到。 Manifest ：Manifest 文件中记录 SST 文件在不同 Level 的分布，单个 SST 文件的最大、最小 key，以及其他一些 LevelDB 需要的元信息。由于 LevelDB 支持 snapshot，需要维护多版本，因此可能同时存在多个 Manifest 文件。 Current ：由于 Manifest 文件可能存在多个，Current 记录的是当前的 Manifest 文件名。 Log ：用于防止 MemTable 丢数据的日志文件。  基本组件 Slice Slice 是 LevelDB 中的一种基本的、以字节为基础的数据存储单元，既可以存储 key，也可以存储 data。Slice 对数据字节的大小没有限制，其内部采用一个const char* 的常量指针存储数据，具有两个接口 data() 和size()，分别返回其表示的数据及数据的长度。\n此外，为了方便使用，其内部封装了 compare 函数，以及重载了用于比较的运算符 ==、!=，同时其可以与标准库中的 string 相互转换。\n 为什么 C++ 中已经有 string 了，还需要实现一个 Slice 呢？\n  string 默认语意为拷贝，传参和返回时会损失性能（在可预期的条件下，指针传递即可）。 Slice 不以 \u0026rsquo; \\0\u0026rsquo; 作为字符的终止符，可以存储值为 \u0026lsquo;\\0\u0026rsquo; 的数据。 string 不支持 remove_prefix 和 starts_with 等前缀操作函数，使用不方便。  具体实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  // https://github.com/google/leveldb/blob/master/include/leveldb/slice.h  class LEVELDB_EXPORT Slice { public: Slice() : data_(\u0026#34;\u0026#34;), size_(0) {} Slice(const char* d, size_t n) : data_(d), size_(n) {} Slice(const std::string\u0026amp; s) : data_(s.data()), size_(s.size()) {} Slice(const char* s) : data_(s), size_(strlen(s)) {} Slice(const Slice\u0026amp;) = default; Slice\u0026amp; operator=(const Slice\u0026amp;) = default; const char* data() const { return data_; } size_t size() const { return size_; } bool empty() const { return size_ == 0; } char operator[](size_t n) const { assert(n \u0026lt; size()); return data_[n]; } void clear() { data_ = \u0026#34;\u0026#34;; size_ = 0; } void remove_prefix(size_t n) { assert(n \u0026lt;= size()); data_ += n; size_ -= n; } std::string ToString() const { return std::string(data_, size_); } int compare(const Slice\u0026amp; b) const; bool starts_with(const Slice\u0026amp; x) const { return ((size_ \u0026gt;= x.size_) \u0026amp;\u0026amp; (memcmp(data_, x.data_, x.size_) == 0)); } private: const char* data_; size_t size_; }; inline bool operator==(const Slice\u0026amp; x, const Slice\u0026amp; y) { return ((x.size() == y.size()) \u0026amp;\u0026amp; (memcmp(x.data(), y.data(), x.size()) == 0)); } inline bool operator!=(const Slice\u0026amp; x, const Slice\u0026amp; y) { return !(x == y); } inline int Slice::compare(const Slice\u0026amp; b) const { const size_t min_len = (size_ \u0026lt; b.size_) ? size_ : b.size_; int r = memcmp(data_, b.data_, min_len); if (r == 0) { if (size_ \u0026lt; b.size_) r = -1; else if (size_ \u0026gt; b.size_) r = +1; } return r; }   Status 在 LevelDB 中，为了便于抛出异常，定义了一个 Status 类。Status 主要用于记录 LevelDB 中的状态信息，保存错误码和对应的字符串错误信息。（写死的，不支持拓展）\n对于错误代码，LevelDB 中定义了 6 种状态码在一个枚举类型 Code 中，如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // https://github.com/google/leveldb/blob/master/include/leveldb/status.h  class LEVELDB_EXPORT Status { private: enum Code { kOk = 0, kNotFound = 1, kCorruption = 2, kNotSupported = 3, kInvalidArgument = 4, kIOError = 5 }; }   其中 KoK 代表正常；kNotFound 代表未找到 ；kCorruption 代表数据异常崩溃；kNotSupported 代表不支持；kInvalidArgument 代表参数非法；kIOError 代表 I/O 错误；\n1 2 3 4 5 6 7 8 9 10 11  // https://github.com/google/leveldb/blob/master/include/leveldb/status.h  class LEVELDB_EXPORT Status { private: // OK status has a null state_. Otherwise, state_ is a new[] array  // of the following form:  // state_[0..3] == length of message  // state_[4] == code  // state_[5..] == message  const char* state_; }   在 Status 类中，所有状态信息，包括状态码与具体描述，都保存在一个私有的成员变量 const char*state_ 中。其具体表示方法如下。\n 当状态为 OK 时，state_ 为 NULL，说明当前操作一切正常。 否则，state_ 为一个 char 数组，即 state[0...3] 为 msg 的长度，state[4] 为状态码 code，state[5...] 为具体的 msg。  如下图：具体的处理逻辑如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // https://github.com/google/leveldb/blob/master/util/status.cc Status::Status(Code code, const Slice\u0026amp; msg, const Slice\u0026amp; msg2) { assert(code != kOk); const uint32_t len1 = static_cast\u0026lt;uint32_t\u0026gt;(msg.size()); const uint32_t len2 = static_cast\u0026lt;uint32_t\u0026gt;(msg2.size()); const uint32_t size = len1 + (len2 ? (2 + len2) : 0); char* result = new char[size + 5]; std::memcpy(result, \u0026amp;size, sizeof(size)); result[4] = static_cast\u0026lt;char\u0026gt;(code); std::memcpy(result + 5, msg.data(), len1); if (len2) { result[5 + len1] = \u0026#39;:\u0026#39;; result[6 + len1] = \u0026#39; \u0026#39;; std::memcpy(result + 7 + len1, msg2.data(), len2); } state_ = result; }   Comparator LevelDB 是按 key 排序后进行存储，因无论是插入还是删除，都必然少不了对 key 的比较。于是乎 LevelDB 抽象出了一个纯虚类 Comparator。\n具体定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // https://github.com/google/leveldb/blob/master/include/leveldb/comparator.h  class LEVELDB_EXPORT Comparator { public: virtual ~Comparator(); virtual int Compare(const Slice\u0026amp; a, const Slice\u0026amp; b) const = 0; virtual const char* Name() const = 0; virtual void FindShortestSeparator(std::string* start, const Slice\u0026amp; limit) const = 0; virtual void FindShortSuccessor(std::string* key) const = 0; };   在 LevelDB 中，有两个实现 Comparator 的类：一个是 BytewiseComparatorImpl，另一个是InternalKeyComparator。\n BytewiseComparatorImpl：默认比较器，主要采用字典序对两个字符串进行比较。 InternalKeyComparator：内部调用的也是 BytewiseComparatorImpl。  Iterate LevelDB 具有迭代器遍历功能，设计人员只需要调用相应的接口，就可以实现对容器数据类型的遍历访问。\n迭代器定义了一个纯虚类的接口，LevelDB 中的任何集合类型对象的迭代器均基于这一纯虚类进行实现。具体代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  // https://github.com/google/leveldb/blob/master/include/leveldb/iterator.h  class LEVELDB_EXPORT Iterator { public: Iterator(); Iterator(const Iterator\u0026amp;) = delete; Iterator\u0026amp; operator=(const Iterator\u0026amp;) = delete; virtual ~Iterator(); virtual bool Valid() const = 0; virtual void SeekToFirst() = 0; virtual void SeekToLast() = 0; virtual void Seek(const Slice\u0026amp; target) = 0; virtual void Next() = 0; virtual void Prev() = 0; virtual Slice key() const = 0; virtual Slice value() const = 0; virtual Status status() const = 0; using CleanupFunction = void (*)(void* arg1, void* arg2); void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2); private: struct CleanupNode { bool IsEmpty() const { return function == nullptr; } void Run() { assert(function != nullptr); (*function)(arg1, arg2); } CleanupFunction function; void* arg1; void* arg2; CleanupNode* next; }; CleanupNode cleanup_head_; };   从上面的接口可以看出，LevelDB 实现的是双向迭代器，并且支持 Seek 定位功能。同时还嵌套了一个子结构 CleanupNode 用于释放链表，其中的 CleanupFunction 即用户自定义的清除函数。当用户需要释放资源时，就会通过遍历 CleanupNode 中的 next 链表，并对每一个节点调用一次 CleanupFunction 将资源释放。\nOption 头文件 options.h 中定义了一系列与数据库操作相关的选项参数类型，例如与数据库操作相关的 Options，与读操作相关的 ReadOptions，与写操作相关的 WriteOptions。这几个类型均为结构体，在进行数据库的初始化、数据库的读写等操作时，这些参数直接决定了数据库相关的性能指标。\n Options（DB 参数）  Comparator：被用来表中 key 比较，默认是字典序。 create_if_missing：打开数据库，如果数据库不存在，是否创建新的。默认为 false。 error_if_exists：打开数据库，如果数据库存在，是否抛出错误。默认为 false。 paranoid_checks：默认为 false。如果为 true，则实现将对其正在处理的数据进行积极检查，如果检测到任何错误，则会提前停止。 这可能会产生不可预见的后果：例如，一个数据库条目的损坏可能导致大量条目变得不可读或整个数据库变得无法打开。 env：环境变量，封装了平台相关接口。 info_log：db 日志句柄。 write_buffer_size：memtable 的大小(默认4mb)  值大有利于性能提升 但是内存可能会存在两份，太大需要注意oom 过大刷盘之后，不利于数据恢复   max_open_files：允许打开的最大文件数。 block_cache：block 的缓存。 block_size：每个 block 的数据包大小(未压缩)，默认是4k。 block_restart_interval：block 中记录完整 key 的间隔。 max_file_size：生成新文件的阈值(对于性能较好的文件系统可以调大该阈值，但会增加数据恢复的时间)，默认 2k compression：数据压缩类型，默认是 kSnappyCompression，压缩速度快  kSnappyCompression 在 Intel(R) Core(TM)2 2.4GHz 上的典型速度：  ~200-500MB/s 压缩 ~400-800MB/s 解压     reuse_logs：是否复用之前的 MANIFES 和 log files。 filter_policy：block 块中的过滤策略，支持布隆过滤器。   ReadOptions（读操作参数）  verify_checksums：是否对从磁盘读取的数据进行校验。 fill_cache：读取到 block 数据，是否加入到 cache 中。 snapshot：记录的是当前的快照。   WriteOptions（写操作参数）  sync：是否同步刷盘，也就是调用完 write 之后是否需要显式 fsync。    ","date":"2022-05-23T23:22:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E6%9E%B6%E6%9E%84/","title":"LevelDB 架构"},{"content":"基本操作 环境搭建 1 2 3 4 5 6 7 8 9 10 11 12 13 14  # 下载源码 git clone https://github.com.cnpmjs.org/google/leveldb.git # 下载依赖第三方库（benchmark、googletest） git submodule update --init # 执行编译 cd leveldb/ mkdir -p build \u0026amp;\u0026amp; cd build cmake -DCMAKE_BUILD_TYPE=Debug .. \u0026amp;\u0026amp; cmake --build . # 头文件加入系统目录 cp -r ./include/leveldb /usr/include/ cp build/libleveldb.a /usr/local/lib/   实战使用 创建、关闭数据库 创建数据库或打开数据库，均通过 Open 函数实现。Open 为一个静态成员函数，其函数声明如下所示：\n1 2  static Status Open(const Options\u0026amp; options, const std::string\u0026amp; name, DB** dbptr);    const Options \u0026amp; options：用于指定数据库创建或打开后的基本行为。 const std::string \u0026amp; name：用于指定数据库的名称。 DB** dbptr：定义了一个DB类型的指针的指针，该指针作为Open函数操作后传给调用者使用的DB类型的实际指针。 Status：当操作成功时，函数返回 status.ok() 的值为 True，*dbptr 分配了不为 NULL 的实际指针地址；若其中的操作存在错误，则 status.ok() 的值为 False，并且对应的 *dbptr 为 NULL。  关闭数据库非常简单，只需要使用 delete 释放 db 即可。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026lt;string\u0026gt;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //关闭数据库  delete db; return 0; }   数据读、写、删除 数据库中的读、写与删除操作分别用 Get 、 Put、Delete 这3个接口函数实现。接口定义如下：\n1 2 3 4 5  Status Get(const ReadOptions\u0026amp; options, const Slice\u0026amp; key, std::string* value); Status Put(const WriteOptions\u0026amp;, const Slice\u0026amp; key, const Slice\u0026amp; value); Status Delete(const WriteOptions\u0026amp;, const Slice\u0026amp; key);    ReadOptions\u0026amp; options：代表实际读操作传入的行为参数。 WriteOptions\u0026amp; options：代表实际写操作传入的行为参数。  这里需要注意的是 Delete 并不会直接删除数据，而是在对应位置插入一个 key 的删除标志，然后在后续的Compaction 过程中才最终去除这条 key-value 记录。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //写入数据  leveldb::Slice key(\u0026#34;hello\u0026#34;); string value(\u0026#34;world\u0026#34;); status = db-\u0026gt;Put(leveldb::WriteOptions(), key, value); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; \u0026#34; 写入成功。\u0026#34; \u0026lt;\u0026lt; endl; } //查找数据  status = db-\u0026gt;Get(leveldb::ReadOptions(), key, \u0026amp;value); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; \u0026#34; 查找成功。\u0026#34; \u0026lt;\u0026lt; endl; } //删除数据  status = db-\u0026gt;Delete(leveldb::WriteOptions(), key); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; key.ToString() \u0026lt;\u0026lt; \u0026#34; 删除成功。\u0026#34; \u0026lt;\u0026lt; endl; } //关闭数据库  delete db; return 0; }   批量处理 针对大量的操作，LevelDB 不具有传统数据库所具备的事务操作机制，然而它提供了一种批量操作的方法。这种批量操作方法主要具有两个作用：一是提供了一种原子性的批量操作方法；二是提高了整体的数据操作速度。\nLevelDB 针对批量操作定义了 WriteBatch 的类型。WriteBatch 有 3 个非常重要的接口：数据写（Put）、数据删 除（Delete）以及清空批量写入缓存（Clear），具体定义如下所示：\n1 2 3 4 5 6 7 8 9 10  class LEVELDB_EXPORT WriteBatch { public:\t//...  void Put(const Slice\u0026amp; key, const Slice\u0026amp; value); void Delete(const Slice\u0026amp; key); void Clear(); //... };   当我们想将 WriteBatch 中的数据写入 DB 时，只需要调用 Write 接口，其主要用于处理之前保存在 WriteBatch 对象中的所有批量操作，其详细接口定义如下所示：\n1  Status Write(const WriteOptions\u0026amp; options, WriteBatch* updates);    注意：一旦我们写入完成后，就会调用 updates 中的 Clear 来清空之前保存的批量操作。  代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026#34;leveldb/write_batch.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } //批量写入  leveldb::Slice key; string value; leveldb::WriteBatch batch; for(int i = 0; i \u0026lt; 10; i++) { value = (\u0026#39;0\u0026#39; + i); key = \u0026#34;k\u0026#34; + value; batch.Put(key, value); } status = db-\u0026gt;Write(leveldb::WriteOptions(), \u0026amp;batch); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;批量写入成功\u0026#34; \u0026lt;\u0026lt; endl; } //批量删除  for(int i = 0; i \u0026lt; 10; i++) { key = \u0026#34;k\u0026#34; + (\u0026#39;0\u0026#39; + i); batch.Delete(key); } status = db-\u0026gt;Write(leveldb::WriteOptions(), \u0026amp;batch); if(status.ok()) { cout \u0026lt;\u0026lt; \u0026#34;批量删除成功\u0026#34; \u0026lt;\u0026lt; endl; } //关闭数据库  delete db; return 0; }   迭代器遍历 针对 DB 中所有的数据记录，LevelDB 不仅支持前向的遍历，也支持反向的遍历。在 DB 对象类型中，通过调用NewIterator 创建一个新的迭代器对象，该接口具体定义如下：\n1  Iterator* NewIterator(const ReadOptions\u0026amp;);    ReadOptions\u0026amp; option：用于指定在遍历访问过程中的相关设置。  这里有一个需要注意的地方，这里返回的迭代器不能直接使用，而是需要先使用对应的 Seek 操作偏移到指定位置后才能进行对应的迭代操作。\n代码示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51  #include\u0026#34;leveldb/db.h\u0026#34;#include\u0026#34;leveldb/slice.h\u0026#34;#include\u0026#34;leveldb/iterator.h\u0026#34;#include\u0026lt;iostream\u0026gt; using namespace std; int main() { leveldb::DB* db; leveldb::Options op; op.create_if_missing = true; //打开数据库  leveldb::Status status = leveldb::DB::Open(op, \u0026#34;/tmp/testdb\u0026#34;, \u0026amp;db); if(!status.ok()) { cout \u0026lt;\u0026lt; status.ToString() \u0026lt;\u0026lt; endl; exit(1); } leveldb::Iterator* it = db-\u0026gt;NewIterator(leveldb::ReadOptions()); //正向遍历  cout \u0026lt;\u0026lt; \u0026#34;开始正向遍历:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;SeekToFirst(); it-\u0026gt;Valid(); it-\u0026gt;Next()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } //逆向遍历  cout \u0026lt;\u0026lt; \u0026#34;开始逆向遍历:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;SeekToLast(); it-\u0026gt;Valid(); it-\u0026gt;Prev()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } //范围查询  cout \u0026lt;\u0026lt; \u0026#34;开始范围查询:\u0026#34; \u0026lt;\u0026lt; endl; for(it-\u0026gt;Seek(\u0026#34;k4\u0026#34;); it-\u0026gt;Valid() \u0026amp;\u0026amp; it-\u0026gt;key().ToString() \u0026lt; \u0026#34;k8\u0026#34;; it-\u0026gt;Next()) { cout \u0026lt;\u0026lt; \u0026#34;key: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;key().ToString() \u0026lt;\u0026lt; \u0026#34; value: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;value().ToString() \u0026lt;\u0026lt; endl; } delete it; //关闭数据库  delete db; return 0; }   常用优化方案 压缩 当每一个块写入存储设备中时，可以选择是否对块进行压缩后再存储，以及 Options 参数的 compression 成员变量决定是否开启压缩。默认情况下，压缩是开启的，且压缩的速度很快，基本对整体的性能没有太大影响\n用户在调用时，可以用 kNoCompression 或 kSnappyCompression 对 compression 参数进行设定，从而确定块在实际存储过程中是否进行压缩。\n1 2 3  leveldb::Options op; op.compression = leveldb::kNoCompression;\t//不启用压缩 op.compression = leveldb::kSnappyCompression;\t//Snappy压缩   缓存 Cache的作用是充分利用内存空间，减少磁盘的 I/O 操作，从而提升整体运行性能。LevelDB 默认的 Cache 采用的是 LRU 算法，即近期最少使用的数据优先从 Cache 中淘汰，而经常使用的数据驻留在内存，从而实现对需要频繁读取的数据的快速访问。\nLevelDB 中定义了一个全局函数 NewLRUCache 用于创建一个 LRUCache。\n1 2 3  leveldb::Options op; op.block_cache = leveldb::NewLRUCache(10 * 1024 * 1024); //参数主要用于指定LevelDB的块的Cache空间，如果为NULL则默认为8MB   过滤器 由于 LevelDB 中所有的数据均保存在磁盘中，因而一次 Get 的调用，有可能导致多次的磁盘 I/O 操作。为了尽可能减少读过程时磁盘 I/O 的操作次数，LevelDB 采用了 FilterPolicy 机制。LevelDB 中 Options 对象类型的filter_policy 参数，主要用于确定运行过程中 Get 所遵循的 FilterPolicy 机制。\n用户可以通过调用 NewBloomFilterPolicy 接口函数以创建布隆过滤器，并将其赋值给对应的 filter_policy 参数。\n1 2  leveldb::Options op; op.filter_policy = leveldb::NewBloomFilterPolicy(10);   命名 LevelDB 中磁盘数据读取与缓存均以块为单位，并且实际存储中所有的数据记录均以 key 进行顺序存储。根据排序结果，相邻的 key 所对应的数据记录一般均会存储在同一个块中。正是由于这一特性，用户针对自身的应用场景需要充分考虑如何优化 key 的命名设计，从而最大限度地提升整体的读取性能。\n为了提升性能，命名规则是：**针对需要经常同时访问的数据，其 key 在命名时，可以通过将这些 key 设置相同的前缀保证这些数据的 key 是相邻近的，从而使这些数据可存储在同一个块内。**基于此，那些不常用的数据记录自然会放置在其他块内。\n","date":"2022-05-23T23:12:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C/","title":"LevelDB 基础操作"},{"content":"LevelDB 基本概念 LevelDB 是一个由 Google 公司所研发的 K-V 存储嵌入式数据库管理系统编程库，以开源的 BSD 许可证发布。其作为 LSM Tree 的经典实现，具有很高的随机写，顺序读/写性能，但是随机读的性能很一般，也就是说，LevelDB很适合应用在查询较少，而写很多的场景。\n为什么需求K-V数据库？ K-V 数据库主要用于存取、管理关联性的数组。关联性数组又称映射、字典，是一种抽象的数据结构，其数据对象为一个个的 key-value 数据对，且在整个数据库中每个 key 均是唯一的。\n随着近年来互联网的兴起，云计算、大数据应用越来越广泛，对于数据库来说也出现了一些需要面对的新情况：\n 数据量呈指数级增长，存储也开始实现分布式。 查询响应时间要求越来越快，需在1秒内完成查询操作。 应用一般需要 7 × 24 小时连续运行，因此对稳定性要求越来越高，通常要求数据库支持热备份，并实现故障下快速无缝切换。 在某些应用中，写数据比读数据更加频繁，对数据写的速度要求也越来越高。 在实际应用中，并不是所有环境下的数据都是完整的结构化数据，非结构化数据普遍存在，因此如何实现对灵活多变的非结构化数据的支持是需要考虑的一个问题。  正是在上述情况的催生下，2010年开始兴起了一场 NoSQL 运动，而 K-V 数据库作为 NoSQL 中一种重要的数据库也日益繁荣，因此催生出了许多成功的商业化产品，并得到了广泛应用。\nBigTable与LevelDB 早在 2004 年，Google 开始研发一种结构化的分布式存储系统，它被设计用来处理海量数据，通常是分布在数千台普通服务器上的 PB 级的数据——这一系统就是风靡全球的 Bigtable。\n2006 年，Google 发表了一篇论文——《Bigtable: A Distributed Storage System for StructuredData》。这篇论文公布了 Bigtable 的具体实现方法，揭开了 Bigtable 的技术面纱。Bigtable 虽然也有行、列、表的概念，但不同于传统的关系数据库，从本质上讲，它是一个稀疏的、分布式的、持久化的、多维的排序键-值映射。\n虽然 Google 公布了 Bigtable 的实现论文，但 Bigtable 依赖于 Google 其他项目所开发的未开源的库，Google 一直没有将 Bigtable 的代码开源。然而这一切在 2011 年迎来了转机。Sanjay Ghemawat 和 Jeff Dean 这两位来自 Google 的重量级工程师，为了能将 Bigtable 的实现原理与技术细节分享给大众开发者，于2011年基于 Bigtable 的基本原理，采用 C++ 开发了一个高性能的 K-V 数据库——LevelDB。由于没有历史的产品包袱，LevelDB 结构简单，不依赖于任何第三方库，具有很好的独立性，虽然其有针对性地对 Bigtable 进行了一定程度的简化，然而Bigtable的主要技术思想与数据结构均在 LevelDB 予以体现了。因此 LevelDB 可看作 Bigtable 的简化版或单机版。\n特点 优点： key 与 value 采用字符串形式，且长度没有限制。 数据能持久化存储，同时也能将数据缓存到内存，实现快速读取。 基于 key 按序存放数据，并且 key 的排序比较函数可以根据用户需求进行定制。 支持简易的操作接口 API，如 Put、Get、Delete，并支持批量写入。 可以针对数据创建数据内存快照。 支持前向、后向的迭代器。 采用 Google 的 Snappy 压缩算法对数据进行压缩，以减少存储空间。 基本不依赖其他第三方模块，可非常容易地移植到 Windows、Linux、UNIX、Android、iOS。  缺点： 不是传统的关系数据库，不支持SQL查询与索引。 只支持单进程，不支持多进程。 不支持多种数据类型。 不支持 C/S 的访问模式。用户在应用时，需要自己进行网络服务的封装。  应用场景 LevelDB主要应用于查少写多的场景，如：\n  常见的 Web 场景，可以存储用户的个人信息资料、对文章或博客的评论、邮件等。\n  具体到电子商务领域，可以存储购物车中的商品、产品类别、产品评论。\n  存储整个网页，其将网页的 URL 作为 key，网页的内容作为 value。\n  构建更为复杂的存储系统，如日志系统、缓存、消息队列等。\n  ……\n  RocksDB RocksDB 是基于 LevelDB 开发的，并保留、继承了 LevelDB 原有的基本功能，也是一个嵌入式的 K-V 数据存储库。RocksDB 设计之初，正值 SSD 硬盘兴起。然而在当时，无论是传统的关系数据库如 MySQL，还是分布式数据库如 HDFS、HBase，均没有充分发挥 SSD 硬盘的数据读写性能。因而 Facebook 当时的目标就是开发一款针对 SSD 硬盘的数据存储产品，从而有了后面的 RocksDB。RocksDB 采用嵌入式的库模式，充分发挥了 SSD 的性能。\n 为什么要基于LevelDB实现RocksDB？\n一般而言，数据库产品有两种访问模式可供选择。一种是直接访问本地挂载的硬盘，即嵌入式的库模式；另一种是客户端通过网络访问数据服务器，并获取数据。假设 SSD 硬盘的读写约 100 μs，机械硬盘的读写约 10 ms，两台 PC 间的网络传输延迟为 50 μs。可以分析得知，如果在机械硬盘时代，采用 C/S 的数据服务模式，客户端进行一次数据查询约为 10.05 ms，可见网络延迟对于数据查询速度的影响微乎其微；而在 SSD 硬盘时代，客户端进行一次数据查询约为 150 μs，但与直接访问 SSD 硬盘相比，整体速度慢了 50%，因而直接影响了整体性能。正是在这样的背景下，Facebook的工程师们选择了 LevelDB 来实现 RocksDB 的原型。\n RocksDB 使用了一个插件式的架构，这使得它能够通过简单的扩展适用于各种不同的场景。插件式架构主要体现在以下几个方面：\n 不同的压缩模块插件：例如 Snappy、Zlib、BZip 等（LevelDB 只使用 Snappy）。 Compaction 过滤器：一个应用能够定义自己的 Compaction 过滤器，从而可以在 Compaction 操作时处理键。例如，可以定义一个过滤器处理键过期，从而使 RocksDB 有了类似过期时间的概念。 MemTable 插件：LevelDB 中的 MemTable 是一个 SkipList，适用于写入和范围扫描，但并不是所有场景都同时需要良好的写入和范围扫描功能，此时用 SkipList 就有点大材小用了。因此 RocksDB 的 MemTable 定义为一个插件式结构，可以是 SkipList，也可以是一个数组，还可以是一个前缀哈希表（以字符串前缀进行哈希，哈希之后寻找桶，桶内的数据可以是一个 B 树）。因为数组是无序的，所以大批量写入比使用 SkipList 具有更好的性能，但不适用于范围查找，并且当内存中的数组需要生成为 SSTable 时，需要进行再排序后写入Level 0。前缀哈希表适用于读取、写入和在同一前缀下的范围查找。因此可以根据场景使用不同的MemTable。 SSTable 插件：SSTable 也可以自定义格式，使之更适用于 SSD 的读取和写入。除了插件式架构，RocksDB 还进行了一些写入以及 Compaction 操作方面的优化，主要有以下几个方面：  线程池：可以定义一个线程池进行 Level 0～Level 5 的 Compaction 操作，另一个线程池进行将MemTable 生成为 SSTable 的操作。如果 Level 0～Level 5 的 Compaction 操作没有重叠的文件，可以并行操作，以加快 Compaction 操作的执行。 多个 Immutable MemTable：当 MemTable 写满之后，会将其赋值给一个 ImmutableMemTable，然后由后台线程生成一个 SSTable。但如果此时有大量的写入，MemTable 会迅速再次写满，此时如果Immutable MemTable 还未执行完 Compaction 操作就会阻塞写入。因此 RocksDB 使用一个队列将Immutable MemTable 放入，依次由后台线程处理，实现同时存在多个 ImmutableMemTable。以此优化写入，避免写放大，当使用慢速存储时也能够加大写吞吐量。    ","date":"2022-05-23T23:11:13+08:00","permalink":"https://blog.orekilee.top/p/leveldb-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"LevelDB 基本概念"},{"content":"分布式原理 分布式存储 路由 当索引一个文档的时候，Elasticsearch会通过哈希来决定将文档存储到哪一个主分片中，路由计算公式如下：\n1 2 3 4  shard = hash(routing) % number_of_primary_shards //routing：默认为文档id,也可以自定义。 //number_of_primary_shards：主分片的数量     查询时指定routing：可以直接根据routing信息定位到某个分片查询，不需要查询所有的分配，经\n过协调节点排序。\n  查询时不指定routing：因为不知道要查询的数据具体在哪个分片上，所以整个过程分为 2 个步骤\n 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上。 聚合：协调节点搜集到每个分片上查询结果，在将查询的结果进行排序，之后给用户返回结果。    从上面的这个公式我们也可以看到一个问题，路由的逻辑与当前主分片的数量强关联，也就是说如果分片数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。这也就是为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量。\n 分片数量固定是否意味着会使索引难以进行扩容？\n 答案是否定的，Elasticsearch还提供了其他的一些方案来让我们轻松的实现扩容，如：\n 分片预分配：一个分片存在于单个节点，但一个节点可以持有多个分片。因此我们可以根据未来的数据的扩张状况来预先分配一定数量的分片到各个节点中。（注意⚠️：预先分配过多的分片会导致性能的下降以及影响搜索结果的相关度） 新建索引：分片数不够时，可以考虑新建索引，搜索1个有着50个分片的索引与搜索50个每个都有1个分片的索引完全等价。  更多关于水平拓展的内容可以参考官方文档扩容设计。\n新增、索引和删除文档 我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。 在下面的例子中，将所有的请求发送到 Node 1 ，我们将其称为协调节点(coordinating node) 。\n 当发送请求的时候， 为了扩展负载，更好的做法是轮询集群中所有的节点。\n 新建、索引和删除请求都是写操作， 必须在主分片上面完成之后才能被复制到相关的副本分片。\n流程如下：\n 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。  取回文档 由于取回文档为读操作，我们可以从主分片或者从其它任意副本分片检索文档。\n流程如下：\n 客户端向 Node 1 发送获取请求。 节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。 Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。  在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。\n并发控制 在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失：\n 悲观并发控制：这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。 乐观并发控制：Elasticsearch中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。 然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。 例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。  Elasticsearch是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许会乱序。所以Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。\n在Elasticsearch中，其通过版本号机制来实现乐观并发控制。即每一个文档中都会有一个_version版本号字段，当文档被修改时版本号递增。 Elasticsearch使用_version来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。\n我们可以利用_version号来确保应用中相互冲突的变更不会导致数据丢失。我们通过指定想要修改文档的 version 号来达到这个目的。 如果该版本不是当前版本号，我们的请求将会失败。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  // 例如我们想更新文档的内容，并指定版本号为1 PUT /website/blog/1?version=1 { \u0026#34;title\u0026#34;: \u0026#34;My first blog entry\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Starting to get the hang of this...\u0026#34; } // 当文档的版本号为1时，次请求成功，同时响应体告诉我们版本号递增到2 { \u0026#34;_index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;blog\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2 \u0026#34;created\u0026#34;: false } // 此时我们再次尝试更新文档的内容，仍然指定版本号为1，由于版本号不符合，此时返回409 Conflict HTTP 响应码 { \u0026#34;error\u0026#34;: { \u0026#34;root_cause\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[blog][1]: version conflict, current [2], provided [1]\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;shard\u0026#34;: \u0026#34;3\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[blog][1]: version conflict, current [2], provided [1]\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;website\u0026#34;, \u0026#34;shard\u0026#34;: \u0026#34;3\u0026#34; }, \u0026#34;status\u0026#34;: 409 }   分布式搜索 搜索需要一种更加复杂的执行模型，因为我们不知道查询会命中哪些文档，这些文档有可能在集群的任何分片上。 一个搜索请求必须询问我们关注的索引的所有分片的某个副本来确定它们是否含有任何匹配的文档。\n但是找到所有的匹配文档仅仅完成事情的一半。 在 search 接口返回一个 page 结果之前，多分片中的结果必须组合成单个排序列表。 为此，搜索被执行成一个两阶段过程，我们称之为query then fetch（查询后取回）。\n查询阶段 在查询阶段时， 查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的优先队列。\n查询阶段包含以下三个步骤\n 客户端发送一个 search 请求到 Node 3 ，此时Node 3成为协调节点，由它来负责本次的查询。 Node 3 将查询请求广播到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的ID和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。至此查询过程结束。   一个索引可以由一个或几个主分片组成， 所以一个针对单个索引的搜索请求需要能够把来自多个分片的结果组合起来。 针对 multiple 或者 all 索引的搜索工作方式也是完全一致的——仅仅是包含了更多的分片而已。\n 取回阶段 在查询阶段中，我们标识了哪些文档满足搜索请求，而接下来我们就需要取回这些文档。\n取回阶段由以下步骤构成\n 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。例如，如果我们的查询指定了 { \u0026quot;from\u0026quot;: 90, \u0026quot;size\u0026quot;: 10 } ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。 每个分片加载并丰富文档（如_source字段和高亮参数），接着返回文档给协调节点。 协调节点等待所有文档被取回，将结果返回给客户端。  集群内部原理 集群与节点 一个运行中的Elasticsearch实例称为一个节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n由于Elasticsearch采用了主从模式，所以当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 因为主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。\n作为用户，我们可以将请求发送到集群中的任何节点（这个处理请求的节点也叫做协调节点）。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。\n分片 在分布式系统中，单机无法存储规模巨大的数据，要依靠大规模集群处理和存储这些数据，一般通过增加机器数量来提高系统水平扩展能力。因此，需要将数据分成若干小块分配到各个机器上。然后通过某种路由策略找到某个数据块所在的位置。\n分片（shard）是底层的基本读写单元，分片的目的是分割巨大索引，让读写可以并行操作，由多台机器共同完成。读写请求最终落到某个分片上，分片可以独立执行读写工作。Elasticsearch利用分片将数据分发到集群内各处。分片是数据的容器，文档保存在分片内，不会跨分片存储。分片又被分配到集群内的各个节点里。当集群规模扩大或缩小时，Elasticsearch会自动在各节点中迁移分片，使数据仍然均匀分布在集群里。\n为了应对并发更新问题，Elasticsearch将分片分为两部分，即主分片（primary shard）和副本分片（replica shard）。主数据作为权威数据，写过程中先写主分片，成功后再写副分片，恢复阶段以主分片为准。\n一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。\n 那索引与分片之间又有什么关系呢？\n 一个Elasticsearch索引包含了很多个分片，每个分片又是一个Lucene的索引，它本身就是一个完整的搜索引擎，可以独立执行建立索引和搜索任务。Lucene索引又由很多分段组成，每个分段都是一个倒排索引。Elasticsearch每次refresh都会生成一个新的分段，其中包含若干文档的数据。在每个分段内部，文档的不同字段被单独建立索引。每个字段的值由若干词（Term）组成，Term是原文本内容经过分词器处理和语言处理后的最终结果。\n选举 在主节点选举算法的选择上，基本原则是不重复造轮子。最好实现一个众所周知的算法，这样的好处是其中的优点和缺陷是已知的。Elasticsearch的选举算法的选择上主要考虑下面两种。\n Bully算法：Leader选举的基本算法之一。它假定所有节点都有一个唯一的ID，使用该ID对节点进行排序。任何时候的当前Leader都是参与集群的最高ID节点。该算法的优点是易于实现。但是，当拥有最大ID的节点处于不稳定状态的场景下会有问题。例如，Master负载过重而假死，集群拥有第二大ID的节点被选为新主，这时原来的Master恢复，再次被选为新主，然后又假死…… Paxos算法：Paxos非常强大，尤其在什么时机，以及如何进行选举方面的灵活性比简单的Bully算法有很大的优势，因为在现实生活中，存在比网络连接异常更多的故障模式。但Paxos实现起来非常复杂。  Elasticsearch的选主算法是基于Bully算法的改进，主要思路是对节点ID排序，取ID值最大的节点作为Master，每个节点都运行这个流程。同时，为了解决Bully算法的缺陷，其通过推迟选举，直到当前的Master失效来解决上述问题，只要当前主节点不挂掉，就不重新选主。但是容易产生脑裂（双主），为此，再通过法定得票人数过半解决脑裂问题。\nElasticsearch对Bully附加的三个约定条件\n 参选人数需要过半。当达到多数时就选出临时主节点，为什么是临时的？每个节点运行排序取最大值的算法，结果不一定相同。举个例子，集群有5台主机，节点ID分别是1、2、3、4、5。当产生网络分区或节点启动速度差异较大时，节点1看到的节点列表是1、2、3、4，选出4；节点2看到的节点列表是2、3、4、5，选出5。结果就不一致了，由此产生下面的第二条限制。 得票数需要过半。某节点被选为主节点，必须判断加入它的节点数达到半数以上，才确认Master身份（推迟选举）。 当探测到节点离开事件时，必须判断当前节点数是否过半。如果达不到半数以上，则放弃Master身份，重新加入集群。如果不这么做，则设想以下情况：假设5台机器组成的集群产生网络分区，2台一组，3台一组，产生分区前，Master位于2台中的一个，此时3台一组的节点会重新并成功选取Master，产生双主，俗称脑裂。（节点失效检测）  流程如下图\n节点失效检测会监控节点是否离线，然后处理其中的异常。失效检测是选主流程之后不可或缺的步骤，不执行失效检测可能会产生脑裂（双主或多主）。在此我们需要启动两种失效探测器：\n 在Master节点，启动NodesFaultDetection，简称NodesFD。定期探测加入集群的节点是否活跃。 非Master节点启动MasterFaultDetection，简称MasterFD。定期探测Master节点是否活跃。  分片内部原理 索引不变性 早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。\n倒排索引被写入磁盘后是不可改变的，索引的不变性具有以下好处：\n 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。 一旦索引被读入内核的文件系统缓存，便会留在哪里。由于其不变性，只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。 缓存(像过滤器缓存)在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。  当然，一个不变的索引也有不好的地方。最大的缺点就是它是不可变的，我们无法对其进行修改。如果我们需要让一个新的文档可被搜索，就需要重建整个索引。这不仅对一个索引所能包含的数据量造成了巨大的限制，而且对索引可被更新的频率同样造成了影响。\n动态更新索引  那么我们如何能在保留不变性的前提下实现倒排索引的动态更新呢？\n 答案就是使用更多的索引，即新增内容并写到一个新的倒排索引中，查询时，每个倒排索引都被轮流查询，查询完再对结果进行合并。\nElasticsearch基于Lucene引入了按段写入的概念——每次内存缓冲的数据被写入文件时，会产生一个新的Lucene段，每个段都是一个倒排索引。同时，在提交点中描述了当前Lucene索引都含有哪些分段。\n按段写入的流程如下：\n 新文档被收集到内存的索引中缓存 当缓存堆积到一定规模时，就会进行提交  一个新的段（倒排索引）被写入磁盘。 一个新的提交点被写入磁盘。 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。   新的段被开启，让它包含的文档可见以被搜索 内存缓存被清空，等待接收新的文档  当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。\n 那插入和更新又如何实现呢？\n 段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。\n  当一个文档被删除时，它实际上只是在 .del 文件中被标记删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。\n  当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。\n  近实时搜索 Elasticsearch和磁盘之间是文件系统缓存，在执行写操作时，为了降低从索引到可被搜索的延迟，一般新段会被先写入到文件系统缓存，再将这些数据写入硬盘（磁盘I/O是性能瓶颈）。\n在写操作中，一般会先在内存中缓冲一段数据，再将这些数据写入硬盘，每次写入硬盘的这批数据称为一个分段。如同任何写操作一样，通过操作系统的write接口写到磁盘的数据会先到达系统缓存（内存）。write函数返回成功时，数据未必被刷到磁盘。通过手工调用flush，或者操作系统通过一定策略将文件系统缓存刷到磁盘。\n这种策略大幅提升了写入效率。从write函数返回成功开始，无论数据有没有被刷到磁盘，只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\nLucene允许新段被写入和打开——使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\nElasticsearch中将写入和打开一个新段的过程叫做refresh(刷新) 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说Elasticsearch是近实时搜索——文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n事务日志 由于系统先缓冲一段数据才写，且新段不会立即刷入磁盘，这两个过程中如果出现某些意外情况（如主机断电），则会存在丢失数据的风险。\n为了解决这个问题，Elasticsearch增加了一个translog（事务日志），在每一次对Elasticsearch进行操作时均进行了日志记录，当Elasticsearch启动的时候，重放translog中所有在最后一次提交后发生的变更操作。\n其执行流程如下：\n 一个文档被索引之后，就会被添加到内存缓冲区，并且追加到了translog  新的文档被添加到内存缓冲区并且被追加到了事务日志，如下图    分片会每秒自动执行一次刷新，这些内存缓冲区的文档被写入新的段中并打开以便搜索，同时清空内存缓冲区。  刷新完成后, 缓存被清空但是事务日志不会，同时新段写入文件系统缓冲区    这个进程继续工作，更多的文档被添加到内存缓冲区和追加到translog  事务日志不断积累文档    当translog足够大时，就会执行全量提交，对文件系统缓存执行flush，将其内容全部写入硬盘中，并清空事务日志。  在刷新（flush）之后，段被全量提交，并且事务日志被清空     除此之外，translog还有下面这些功能\n translog提供所有还没有被刷到磁盘的操作的一个持久化纪录。当Elasticsearch启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放translog中所有在最后一次提交后发生的变更操作。 translog也被用来提供实时CRUD 。当你试着通过ID查询、更新、删除一个文档，在从相应的段中检索之前， 首先检查translog任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。  段合并 由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段，所以段越多，搜索也就越慢。\nElasticsearch通过在后台进行段合并来解决这个问题，其会选择大小相似的分段进行合并。在合并过程中，标记为删除（更新）的数据不会写入新分段，当合并过程结束，旧的分段数据被删除，标记删除的数据才从磁盘删除。\n流程如下图\n合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。\n整体写入流程如下图\n ","date":"2022-05-23T22:20:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86/","title":"ElasticSearch 分布式原理"},{"content":"索引原理 倒排索引 例如，假设我们有两个文档，每个文档的 content 域包含如下内容：\n The quick brown fox jumped over the lazy dog Quick brown foxes leap over lazy dogs in summer  为了创建倒排索引，首先我们需要借助分词器，将每个文档的 content 域拆分成单独的词（我们称它为 词条 或 tokens、term ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  Term Doc_1 Doc_2 ------------------------- Quick | | X The | X | brown | X | X dog | X | dogs | | X fox | X | foxes | | X in | | X jumped | X | lazy | X | X leap | | X over | X | X quick | X | summer | | X the | X | ------------------------   如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档：\n1 2 3 4 5 6  Term Doc_1 Doc_2 ------------------------- brown | X | X quick | X | ------------------------ Total | 2 | 1   这里我们匹配到了两个文档（为了节省空间，这里返回的只是文档ID，最后再通过文档id去查询到具体文档。）。对于搜索引擎来说，用户总希望能够先看到相关度更高的结果，因此实际使用时我们通过一些算法来进行权重计算，将查询的结果按照权重降序返回。\nTerm dictionary与Term index Elasticsearch为了能够快速的在倒排索引中找到某个term，他会按照字典序对所有的term进行排序，再通过二分查找来找到term，这就是Term Dictionary，但即使有了Term Dictionary，O(logN)的磁盘读写仍然是影响性能的一大问题。\nB-Tree通过减少磁盘寻道次数来提高查询性能，Elasticsearch也是采用同样的思路，直接通过内存查找term，不读磁盘，但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index，从下图可以看出，Term Index其实就是一个Trie树（前缀树）\n这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。\n 为什么Elasticsearch/Lucene检索比mysql快呢？\n Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。检索一个term需要若干次的random access的磁盘操作。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。\n列式存储——Doc Values Doc values的存在是因为倒排索引只对某些操作是高效的。 倒排索引的优势在于查找包含某个项的文档，而对于从另外一个方向的相反操作并不高效，即：确定哪些项是否存在单个文档里，聚合需要这种次级的访问模式。\n以排序来举例——虽然倒排索引的检索性能非常快，但是在字段值排序时却不是理想的结构。\n 在搜索的时候，我们能通过搜索关键词快速得到结果集。 当排序的时候，我们需要倒排索引里面某个字段值的集合。换句话说，我们需要转置倒排索引。  转置 结构经常被称作 列式存储 。它将所有单字段的值存储在单数据列中，这使得对其进行操作是十分高效的，例如排序、聚合等操作。\n在Elasticsearch中，Doc Values就是一种列式存储结构，在索引时与倒排索引同时生成。也就是说Doc Values和倒排索引一样，基于 Segement生成并且是不可变的。同时Doc Values和倒排索引一样序列化到磁盘。\nDoc Values常被应用到以下场景：\n 对一个字段进行排序 对一个字段进行聚合 某些过滤，比如地理位置过滤 某些与字段相关的脚本计算  下面举一个例子，来讲讲它是如何运作的\n 假设存在以下倒排索引\n1 2 3 4 5 6  Term Doc_1 Doc_2 Doc_3 ------------------------------------ brown | X | X | dog | X | | X dogs | | X | X ------------------------------------   那么其生成的DocValues如下（实际存储时不会存储doc_id，值所在的顺位即为doc_id）\n1 2 3 4 5 6 7 8 9  Doc_id Values ------------------ Doc_1 | brown | Doc_1 | dog | Doc_2 | brown | Doc_2 | dogs | Doc_3 | dog | Doc_3 | dogs | ------------------   假设我们需要计算出brown出现的次数\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  GET /my_index/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;:{ \u0026#34;body\u0026#34;:\u0026#34;brown\u0026#34; } }, \u0026#34;aggs\u0026#34;:{ \u0026#34;popular_terms\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;body\u0026#34; } } }, \u0026#34;size\u0026#34; : 0 }   下面来分析上述请求在ES中是如何来进行查询的：\n 定位数据范围。通过倒排索引，来找到所有包含brown的doc_id。 进行聚合计算。借助doc_id在doc_values中定位到为brown的字段，此时进行聚合累加得到计算结果。browm的count=2。   但是doc_values仍存在一些问题，其不支持analyzed类型的字段，因为这些字段在进行文本分析时可能会被分词处理，从而导致doc_values将其存储为多行记录。但是在我们实际使用时，为什么仍然能对analyzed的字段进行聚合操作呢？这时就需要介绍一下Fielddata\n Fielddata doc values不生成分析的字符串，那为什么这些字段仍然可以使用聚合呢？是因为使用了fielddata的数据结构。与doc values不同，fielddata构建和管理100%在内存中，常驻于JVM内存堆。\n 从历史上看，fielddata 是所有字段的默认设置。但是Elasticsearch已迁移到doc values以减少 OOM 的几率。分析的字符串是仍然使用fielddata的最后一块阵地。 最终目标是建立一个序列化的数据结构类似于doc values ，可以处理高维度的分析字符串，逐步淘汰 fielddata。\n 它的一些特性如下\n 延迟加载。如果你从来没有聚合一个分析字符串，就不会加载fielddata到内存中，其是在查询时候构建的。 基于字段加载。 只有很活跃地使用字段才会增加fielddata的负担。 会加载索引中（针对该特定字段的） 所有的文档，而不管查询是否命中。逻辑是这样：如果查询会访问文档 X、Y 和 Z，那很有可能会在下一个查询中访问其他文档。 如果空间不足，使用最久未使用（LRU）算法移除fielddata。  因此，在聚合字符串字段之前，请评估情况：\n 这是一个not_analyzed字段吗？如果是，可以通过doc values节省内存 。 否则，这是一个analyzed字段，它将使用fielddata并加载到内存中。这个字段因为N-grams有一个非常大的基数？如果是，这对于内存来说极度不友好。  索引压缩 FOR编码 在Elasticsearch中，为了能够更方便的计算交集和并集，它要求倒排索引是有序的，而这个特点同时也带来了一个额外的好处，我们可以使用增量编码来压缩倒排索引，也就是FOR（Frame of Reference）编码\n 增量编码压缩，将大数变小数，按字节存储\n FOR编码分为三个步骤\n 增量编码 增量分区 位压缩  如下图所示，如果我们的倒排索引中存储的文档id为[73, 300, 302, 332, 343, 372]，那么经过增量编码后的结果则为[73, 227, 2, 30, 11, 29]。这种压缩的好处在哪里呢？我们通过增量将原本的大数变成了小数，使得所有的增量都在0～255之间，因此每一个值就只需要使用一个字节就可以存储，而不会使用int或者bigint，大大的节约了空间。\n接着，第二步我们将这些增量分到不同的区块中（Lucene底层用了256个区块，下面为了方便展示之用了两个）。\n第三步，我们计算出每组数据中最大的那个数所占用的bit位数，例如下图中区块1最大的为227，所以只占用8个bit位，所以三个数总共占用3 * 8bits即3字节。而区块2最大为29，只占用5个bit位，因此这三个数总共占用3 * 5bits即差不多2字节。通过这种方法，将原本6个整数从24字节压缩到了5字节，效果十分出色。\nRoaring Bitmaps FOR编码对于倒排索引来说效果很好，但对于需要存储在内存中的过滤器缓存等不太合适，两者之间有很多不同之处：\n 由于我们仅仅缓存那些经常使用的过滤器，因此它的压缩率并不需要像倒排索引那么高（倒排索引需要对每个词都进行编码）。 缓存过滤器的目的就是为了加速处理效率，因此它必须要比重新执行过滤器要快，因此使用一个好的数据结构和算法非常重要。 缓存的过滤器存储在内存之中，而倒排索引通常存储在磁盘中。  基于以上的不同，对于缓存来说FOR编码并不适用，因此我们还需要考虑其他的一些选择。\n 整数数组：数组可能是我们马上能想到的最简单的实现方式，我们将文档id存储在数组中，这样就使得我们的迭代变得非常简单，但是这种方法的内存利用率又十分低下，因为每个文档都需要4个字节。 Bitmaps：在数据分布密集的下，位图是一个很好的选择。它本质上就是一个数组，其中每一个文档id占据一个位，用0和1来标记文档是否存在。这种方法大大节约了内存，将一个文档从4字节降低到了一个位，但是一旦数据分布稀疏，此时的位图性能将大打折扣，因为无论数据量多少，位图的大小都是由数据的上下区间来决定的。 Roaring Bitmaps：Roaring Bitmaps即是对位图的一种优化，它会根据16位最高位将倒排索引划分为多块，如第一个块将对0到65535之间的值进行编码，第二个块将在65536和131071之间进行编码。在每一个块中，我们再对低16位进行编码，如果它的值小于4096在使用数组，否则就使用位图。由于我们编码的时候只会对低16位进行编码，因此在这里数组每个元素只需要2个字节   为什么要使用4096作为数组和位图选取的阈值呢？\n 下面是官方给出的数据报告，在一个块中只有文档数量超过4096，位图的内存优势才会凸显出来\n这就是Roaring Bitmaps高效率的原因，它基于两种完全不同的方案来进行编码，并根据内存效率来动态决定使用哪一种方案。\n官方也给出了几种方案的性能测试\n从上述对比可以看出，没有一种方法是完美的，但是以下两种方法的巨大劣势使得它们不会被选择\n 数组：性能很好，但是内存占用巨大。 Bitmaps：数据稀疏分布的时候内存和性能都会大打折扣。  因此在综合考量下，Elasticsearch还是选择使用Roaring Bitmaps，并且在很多大家了解的开源大数据框架中，也都使用了这一结构，如Hive、Spark、Kylin、Druid等。\n联合索引 如果多个字段索引的联合查询，倒排索引如何满足快速查询的要求呢？\n 跳表：同时遍历多个字段的倒排索引，互相skip。 位图：对多个过滤器分别求出位图，对这几个位图做AND操作。  Elasticsearch支持以上两种的联合索引方式，如果查询的过滤器缓存到了内存中（以位图的形式），那么合并就是两个位图的AND。如果查询的过滤器没有缓存，那么就用跳表的方式去遍历两个硬盘中的倒排索引。\n假设有下面三个倒排索引需要联合索引：\n 如果使用跳表，则对最短的倒排索引中的每一个id，逐个在另外两个倒排索引中查看是否存在，来判断是否存在交集。 如果使用位图，则直接将几个位图按位与运算，最终得到的结果就是最后的交集。  ","date":"2022-05-23T22:18:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/","title":"ElasticSearch 索引原理"},{"content":"权重计算原理 在Elasticsearch中，每个文档都有相关性评分，用一个正浮点数字段 _score 来表示 。 _score 的评分越高，相关性越高。为了保证搜索到的结果相关度更高，在默认情况下返回结果会按照相关度降序排序。\nElasticsearch使用布尔模型查找匹配文档，并用一个名为实用评分函数的公式来计算相关度。这个公式借鉴了词频/逆向文档频率和向量空间模型，同时也加入了一些现代的新特性，如协调因子，字段长度归一化，以及词或查询语句权重提升。\n布尔模型 布尔模型（Boolean Model） 适用于在查询中使用 AND 、 OR 和 NOT （与、或、非）这样的条件来查找匹配的文档，如以下查询：\n1  fullANDtextANDsearchAND(elasticsearchORlucene)  会将所有包括词 full 、 text 和 search ，以及 elasticsearch 或 lucene 的文档作为结果集。\n这个过程简单且快速，它将所有可能不匹配的文档排除在外。\n词频/逆向文档频率（TF/IDF） Elasticsearch的相似度算法被定义为检索词频率/反向文档频率（TF/IDF），主要依赖以下内容\n  检索词频率\n  检索词在该字段出现的频率。出现频率越高，相关性也越高。5次提到同一词的字段比只提到1次的更相关。\n  词频的计算方式如下：\n1  tf(t in d) = √frequency   词 t 在文档 d 的词频（ tf ）是该词在文档中出现次数的平方根。\n    逆向文档频率\n  每个检索词在索引中出现的频率。频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。\n  逆向文档频率的计算公式如下：\n1  idf(t) = 1 + log ( numDocs / (docFreq + 1))   词 t 的逆向文档频率（ idf ）是：索引中文档数量除以所有包含该词的文档数，然后求其对数。\n    字段长度归一值\n  字段的长度。长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。\n  字段长度的归一值公式如下：\n1  norm(d) = 1 / √numTerms   字段长度归一值（ norm ）是字段中词数平方根的倒数。\n    以上三个因素是在索引时计算并存储的。最后将它们结合在一起计算单个词在特定文档中的权重 。\n向量空间模型 当然，查询通常不止一个词，所以需要一种合并多词权重的方式——向量空间模型（vector space model）。\n向量空间模型（vector space model）提供一种比较多词查询的方式，单个评分代表文档与查询的匹配程度，为了做到这点，这个模型将文档和查询都以 向量（vectors） 的形式表示，而向量实际上就是包含多个数的一维数组，例如：\n1  [1,2,5,22,3,8]   在向量空间模型里，向量空间模型里的每个数字都代表一个词的权重 ，与词频/逆向文档频率计算方式类似，下面举一个例子。\n设想如果查询 “happy hippopotamus” ，常见词 happy 的权重较低，不常见词 hippopotamus 权重较高，假设 happy 的权重是 2 ， hippopotamus 的权重是 5 ，可以将这个二维向量—— [2,5] ——在坐标系下作条直线，线的起点是 (0,0) 终点是 (2,5) ，如下图\n现在，设想我们有三个文档：\n I am happy in summer 。 After Christmas I’m a hippopotamus 。 The happy hippopotamus helped Harry 。  三篇文档的命中词如下\n 文档 1： (happy,____________) —— [2,0] 文档 2： ( ___ ,hippopotamus) —— [0,5] 文档 3： (happy,hippopotamus) —— [2,5]  可以为每个文档都创建包括每个查询词—— happy 和 hippopotamus ——权重的向量，然后将这些向量置入同一个坐标系中，如下图\n向量之间是可以比较的，只要测量查询向量和文档向量之间的角度就可以得到每个文档的相关度，文档 1 与查询之间的角度最大，所以相关度低；文档 2 与查询间的角度较小，所以更相关；文档 3 与查询的角度正好吻合，完全匹配。\n 在实际中，只有二维向量（两个词的查询）可以在平面上表示，幸运的是， 线性代数为我们提供了计算两个多维向量间角度工具，这意味着可以使用如上同样的方式来解释多个词的查询。\n ","date":"2022-05-23T22:17:18+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E6%9D%83%E9%87%8D%E8%AE%A1%E7%AE%97/","title":"ElasticSearch 权重计算"},{"content":"基本操作 环境搭建 搭建Elasticsearch环境 下载docker镜像\n1  docker pull elasticsearch:7.4.2   映射配置文件\n1 2 3 4 5 6 7 8 9  # 配置映射文件夹 mkdir -p /mydata/elasticsearch/config mkdir -p /mydata/elasticsearch/data # 设置文件夹权限任何用户可读可写 chmod 777 /mydata/elasticsearch -R # 配置 http.host echo \u0026#34;http.host: 0.0.0.0\u0026#34; \u0026gt;\u0026gt; /mydata/elasticsearch/config/elasticsearch.yml   启动容器\n1 2 3 4 5 6 7  docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\ -e \u0026#34;discovery.type\u0026#34;=\u0026#34;single-node\u0026#34; \\\t # 设置为单节点 -e ES_JAVA_OPTS=\u0026#34;-Xms64m -Xmx128m\u0026#34; \\ # 设置启动时ES的初始内存以及最大内存 -v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\ -v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\ -v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\ -d elasticsearch:7.4.2   访问ES服务，http://82.157.127.173:9200/\n得到相应体如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  { \u0026#34;name\u0026#34; : \u0026#34;de85ed684243\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;UeIP1PrXT2OFd7FlEEl3hQ\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.4.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;2f90bbf7b93631e52bafb59b3b049cb44ec25e96\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2019-10-28T20:40:44.881551Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.2.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; }   可以通过/_cat来获取节点信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  # 访问http://82.157.127.173:9200/_cat # 属性列表 /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates   搭建Kibana环境 下载docker镜像\n1  docker pull kibana:7.4.2   启动容器\n1  docker run --name kibana -e ELASTICSEARCH_HOSTS=http://192.168.0.128:9200 -p 5601:5601 -d kibana:7.4.2   访问Kibana服务，http://192.168.0.128:5601/\nRESTful  一种软件架构风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。\n ES的基本REST命令：\n   Method Url 描述     PUT localhost:9200/索引名称/类型名称/文档id 创建文档（指定文档id）   GET localhost:9200/索引名称/类型名称/文档id 通过文档id查询文档   POST localhost:9200/索引名称/类型名称 创建文档（随机文档id）   POST localhost:9200/索引名称/类型名称/文档id/_update 修改文档   POST localhost:9200/索引名称/类型名称/_search 查询所有数据   DELETE localhost:9200/索引名称/类型名称/文档id 删除文档    CRUD 创建索引 在创建索引时，我们可以声明字段与数据类型的映射\n请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13  PUT /test0 { \u0026#34;mappings\u0026#34;:{ \u0026#34;properties\u0026#34;:{ \u0026#34;name\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; }, \u0026#34;author\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;text\u0026#34; } } } }   响应:\n1 2 3 4 5  { \u0026#34;acknowledged\u0026#34;: true, \u0026#34;shards_acknowledged\u0026#34;: true, \u0026#34;index\u0026#34;: \u0026#34;test0\u0026#34; }   即使如果我们没有配置类型，ES也会根据字段的内容来自行推导。\n 注意⚠️：由于索引具有不变性，我们只能进行追加而不能更改已经存在的映射字段，必须创建新的索引后进行数据迁移。\n 1 2 3 4 5 6 7 8 9  POST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;test0\u0026#34; }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;test1\u0026#34; } }   插入文档 PUT和POST都可以插入文档：\n POST：如果不指定 id，自动生成 id。如果指定 id，则修改这条记录，并新增版本号。 PUT：必须指定 id，如果没有这条记录，则新增，如果有，则更新。   示例：在 test1 索引下的books类型中保存标识为 1 的文档。\n 请求：\n1 2 3 4 5  PUT /test1/books/1 { \u0026#34;name\u0026#34;:\u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34; : \u0026#34;Daniel Defoe\u0026#34; }   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test\u0026#34;,\t//索引  \u0026#34;_type\u0026#34;: \u0026#34;book\u0026#34;,\t//类型  \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;,\t//id  \u0026#34;_version\u0026#34;: 1,\t//版本号  \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;,\t//操作类型  \u0026#34;_shards\u0026#34;: {\t//分片  \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1,\t//并发控制字段，每次更新就会+1，用来做乐观锁  \u0026#34;_primary_term\u0026#34;: 1 //同上，主分片重新分配，如重启，就会变化 }   查询文档  示例：查询test1索引下的books类型中保存标识为 1 的文档的内容。\n 请求：\n1  GET /test1/books/1   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;_seq_no\u0026#34;: 0, \u0026#34;_primary_term\u0026#34;: 1, \u0026#34;found\u0026#34;: true, \u0026#34;_source\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Daniel Defoe\u0026#34; } }   更新文档 使用POST命令，在ID后面加_update，并把需要修改的内容放入doc属性中\n 示例：更新test1 索引下的books类型中保存标识为 1 的文档的内容。\n 请求：\n1 2 3 4 5 6 7 8  POST /test1/books/1/_update { \u0026#34;doc\u0026#34; : { \u0026#34;name\u0026#34;:\u0026#34;three days to see\u0026#34;, \u0026#34;author\u0026#34; : \u0026#34;Daniel Defoe\u0026#34;, \u0026#34;country\u0026#34; : \u0026#34;England\u0026#34; } }   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 2, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 1, \u0026#34;_primary_term\u0026#34;: 1 }   删除文档和索引 删除使用DELETE命令\n 示例：删除文档/test1/books/1\n 请求：\n1  DELETE /test1/books/1   响应:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  { \u0026#34;_index\u0026#34;: \u0026#34;test1\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;books\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;_version\u0026#34;: 3, \u0026#34;result\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 2, \u0026#34;_primary_term\u0026#34;: 1 }    示例：删除索引/test1\n 1  DELETE /test1   响应:\n1 2 3  { \u0026#34;acknowledged\u0026#34;: true }   Search 为了方便测试，可以从官网导入测试数据https://download.elastic.co/demos/kibana/gettingstarted/accounts.zip\n1 2 3 4 5 6  POST /test_data/account/_bulk {\u0026#34;index\u0026#34;:{\u0026#34;_id\u0026#34;:\u0026#34;1\u0026#34;}} {\u0026#34;account_number\u0026#34;:1,\u0026#34;balance\u0026#34;:39225,\u0026#34;firstname\u0026#34;:\u0026#34;Amber\u0026#34;,\u0026#34;lastname\u0026#34;:\u0026#34;Duke\u0026#34;,\u0026#34;age\u0026#34;:32,\u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;880 Holmes Lane\u0026#34;,\u0026#34;employer\u0026#34;:\u0026#34;Pyrami\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;amberduke@pyrami.com\u0026#34;,\u0026#34;city\u0026#34;:\u0026#34;Brogan\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;IL\u0026#34;} {\u0026#34;index\u0026#34;:{\u0026#34;_id\u0026#34;:\u0026#34;6\u0026#34;}} {\u0026#34;account_number\u0026#34;:6,\u0026#34;balance\u0026#34;:5686,\u0026#34;firstname\u0026#34;:\u0026#34;Hattie\u0026#34;,\u0026#34;lastname\u0026#34;:\u0026#34;Bond\u0026#34;,\u0026#34;age\u0026#34;:36,\u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;address\u0026#34;:\u0026#34;671 Bristol Street\u0026#34;,\u0026#34;employer\u0026#34;:\u0026#34;Netagy\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;hattiebond@netagy.com\u0026#34;,\u0026#34;city\u0026#34;:\u0026#34;Dante\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;TN\u0026#34;} ....................   查询方式 ES支持两种查询方式，一种是直接在URL后加上参数，另一种是在URL后加上JSON格式的请求体。\n 示例：查找到收入最高的十条记录\n URL + 参数 常用的参数如下\n q：用于指定搜索的关键词。 from \u0026amp; size：类似于SQL中的offset和limit。 sort：对结果排序，默认为降序。 _source：指定想要返回的属性。  1  GET /test_data/_search?q=*\u0026amp;sort=balance:desc\u0026amp;from=0\u0026amp;size=10   URL + QueryDSL 1 2 3 4 5 6 7 8 9 10 11  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34; : {} }, \u0026#34;sort\u0026#34; : [{ \u0026#34;balance\u0026#34; : \u0026#34;desc\u0026#34; }], \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 10 }   虽然URL+参数的写法非常简洁，但是随着逻辑的复杂化，其可读性也越来越差，所以通常都会使用URL + QueryDSL的格式。\nmatch 匹配 match 匹配查询 无论你在任何字段上进行的是全文搜索还是精确查询，match 查询是你可用的标准查询。\n对于not_analyzed的字段，match能做到精确查询，而对于analyzed的字段，match能做到匹配查询（全文搜索）。\n 示例：查找所有年龄为25岁的记录（精确查询）\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34; : { \u0026#34;age\u0026#34;: 25 } } }    示例：查询所有地址与976 Lawrence Street相关的记录（全文搜索）\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34; : { \u0026#34;address\u0026#34;: \u0026#34;976 Lawrence Street\u0026#34; } } }   match_all 全部匹配 match_all 用于查询所有文档。在没有指定查询方式时，它是默认。\n 示例：查询年龄最小的十条记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34; : {} }, \u0026#34;sort\u0026#34; : [{ \u0026#34;age\u0026#34; : \u0026#34;asc\u0026#34; }], \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 10 }   match_phase 短语匹配 match_phase用于进行短语的匹配，它查询时并不是像term一样不进行分词直接查询，而是借助分析器返回的查询词的相对顺序以及偏移量来做判断——满足所有查询词且顺序完全相同的记录才会被匹配上。\n 示例：地址包含502 Baycliff Terrace的记录\n 请求：\n1 2 3 4 5 6 7 8  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_phase\u0026#34; : { \u0026#34;address\u0026#34;: \u0026#34;502 Baycliff Terrace\u0026#34; } } }   multi_match 多字段匹配 multi_match 可以在多个字段上执行相同的 match 查询。\n 示例：查找city或address字段中包含Dixie或Street的记录。\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;multi_match\u0026#34;:{ \u0026#34;query\u0026#34;:\u0026#34;Dixie Street\u0026#34;, \u0026#34;fields\u0026#34;:[ \u0026#34;city\u0026#34;, \u0026#34;address\u0026#34; ] } } }   term 精确查询 term即直接在倒排索引中查询，也就是精确查找，不进行分词器分析，文档中必须包含整个搜索的词汇。\nterm和match的区别:\n match是经过分析处理的，查询词先被文本分析器处理后再进行查询。所以根据不同的文本分析器，分析出  的结果也会不同。\n term是不经过分析处理的，直接去倒排索引查找精确的值。   由于text字段会被文本分析器处理，所以通常全文检索字段用match，其他非text字段（not_analyzed）匹配用term。\n 1 2 3 4 5 6 7 8 9 10  GET /test_data/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;Street\u0026#34; } } } // 虽然文档中存在”702 Quentin Street“，但是由于文本分析器默认会转为小写，无法搜到任何数据   布尔查询（复合查询） 借助布尔查询可以实现如SQL中（and、or、!=）等逻辑条件的判断，并且可以合并任何其他查询语句，包括复合语句。复合语句之间可以相互嵌套，可以表达复杂的逻辑。\n  must（and）：文档必须匹配这些条件才能被包含进来。（影响相关性得分）\n  must_not（not）：文档必须不匹配这些条件才能被包含进来。（不影响相关性得分）\n  should（or）：如果满足这些语句中的任意语句，将增加得分 。（用于修正相关性得分）\n   示例：查找年龄不等于18的地址包含Street的男性，且优先展示居住在30岁以上的的记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;must\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;address\u0026#34;:\u0026#34;Street\u0026#34; } }, { \u0026#34;match\u0026#34;:{ \u0026#34;gender\u0026#34;:\u0026#34;M\u0026#34; } } ], \u0026#34;must_not\u0026#34;:[ { \u0026#34;match\u0026#34;:{ \u0026#34;age\u0026#34;:\u0026#34;18\u0026#34; } } ], \u0026#34;should\u0026#34;:[ { \u0026#34;range\u0026#34;:{ \u0026#34;age\u0026#34;:{ \u0026#34;gt\u0026#34;:30 } } } ] } } }   Filter 过滤器 Filter通常搭配布尔查询一起使用，用于过滤出所有满足Filter的记录，不影响相关性得分。\n 示例：查找年龄在30～60之间的记录\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;bool\u0026#34;:{ \u0026#34;filter\u0026#34;:[ { \u0026#34;range\u0026#34;:{ \u0026#34;age\u0026#34;:{ \u0026#34;gte\u0026#34;:30, \u0026#34;lte\u0026#34;:60 } } } ] } } }   Aggregations 聚合 要掌握聚合，你只需要明白两个主要的概念：\n 桶（Buckets）：满足特定条件的文档的集合 指标（Metrics）：对桶内的文档进行统计计算  翻译成SQL的形式来理解的话：\n1 2 3 4 5  SELECTCOUNT(1),MAX(balance)FROMtableGROUPBYgender;  桶在概念上类似于 SQL 的分组（GROUP BY，如上面的GROUP BY gender），而指标则类似于 COUNT() 、 SUM() 、 MAX() 等统计方法，如MAX(balance)。\n聚合的语法如下：\n1 2 3 4 5 6 7 8 9 10  \u0026#34;aggregations\u0026#34; : { \u0026#34;\u0026lt;聚合名称 1\u0026gt;\u0026#34; : { \u0026#34;\u0026lt;聚合类型\u0026gt;\u0026#34; : { \u0026lt;聚合体内容\u0026gt; } [,\u0026#34;元数据\u0026#34; : { [\u0026lt;meta_data_body\u0026gt;] }]? [,\u0026#34;aggregations\u0026#34; : { [\u0026lt;sub_aggregation\u0026gt;]+ }]? } [\u0026#34;聚合名称 2\u0026gt;\u0026#34; : { ... }]* }    示例：按照性别进行分组，计算平均年龄和最高收入\n 请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  GET /test_data/_search { \u0026#34;query\u0026#34;:{ \u0026#34;match_all\u0026#34;: {} }, \u0026#34;aggs\u0026#34;:{ \u0026#34;group_gender\u0026#34;:{ \u0026#34;terms\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;gender\u0026#34; }, \u0026#34;aggs\u0026#34;:{ \u0026#34;avg_age\u0026#34;:{ \u0026#34;avg\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;age\u0026#34; } }, \u0026#34;max_balance\u0026#34;:{ \u0026#34;max\u0026#34;:{ \u0026#34;field\u0026#34;:\u0026#34;balance\u0026#34; } } } } }, \u0026#34;size\u0026#34;:0 }   ","date":"2022-05-23T22:17:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"ElasticSearch 基本操作"},{"content":"ElasticSearch 基本概念 Elasticsearch是一个分布式的免费开源搜索和分析引擎，适用于包括文本、数字、地理空间、结构化和非结构化数据等在内的所有类型的数据。\nElasticsearch在Apache Lucene的基础上开发而成。然而，Elasticsearch不仅仅是Lucene，并且也不仅仅只是一个全文搜索引擎。 它可以被下面这样准确的形容：\n 一个分布式的实时文档存储，每个字段都可以被索引与搜索 一个分布式实时分析搜索引擎 能胜任上百个服务节点的扩展，并支持PB级别的结构化或者非结构化数据  Lucene Lucene是一套用于全文索引和搜索的开源程式库，由Apache软件基金会支持和提供。Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜索。但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎（英文与德文两种西方语言）。\nLucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。\n在Java开发环境里Lucene是一个成熟的免费开源工具。就其本身而言，Lucene是当前以及最近几年最受欢迎的免费Java信息检索程序库。人们经常提到信息检索程序库，虽然与搜索引擎有关，但不应该将信息检索程序库与搜索引擎相混淆。\nELK Elastic Stack是一套适用于数据采集、扩充、存储、分析和可视化的免费开源工具。人们通常将Elastic Stack称为ELK Stack（代指 Elasticsearch、Logstash 和 Kibana）。\n Logstash是什么？\n Logstash 是 Elastic Stack 的核心产品之一，可用来对数据进行聚合和处理，并将数据发送到 Elasticsearch。Logstash 是一个开源的服务器端数据处理管道，允许您在将数据索引到 Elasticsearch 之前同时从多个来源采集数据，并对数据进行充实和转换。\n Kibana是什么？\n Kibana 是一款适用于Elasticsearch的数据可视化和管理工具，可以提供实时的直方图、线形图、饼状图和地图。Kibana同时还包括诸如 Canvas和Elastic Maps等高级应用程序\n Canvas允许用户基于自身数据创建定制的动态信息图表， Elastic Maps用来对地理空间数据进行可视化。  Elasticsearch的特点   Elasticsearch 很快。 由于Elasticsearch是在Lucene基础上构建而成的，所以在全文本搜索方面表现十分出色。Elasticsearch同时还是一个近实时的搜索平台，这意味着从文档索引操作到文档变为可搜索状态之间的延时很短，一般只有一秒。因此，Elasticsearch 非常适用于对时间有严苛要求的用例，例如安全分析和基础设施监测。\n  Elasticsearch 具有分布式的本质特征。 Elasticsearch中存储的文档分布在不同的容器中，这些容器称为分片，可以进行复制以提供数据冗余副本，以防发生硬件故障。Elasticsearch的分布式特性使得它可以扩展至数百台（甚至数千台）服务器，并处理 PB 量级的数据。\n  Elasticsearch 包含一系列广泛的功能。 除了速度、可扩展性和弹性等优势以外，Elasticsearch 还有大量强大的内置功能（例如数据汇总和索引生命周期管理），可以方便用户更加高效地存储和搜索数据。\n  Elastic Stack 简化了数据采集、可视化和报告过程。 通过与 Beats 和 Logstash 进行集成，用户能够在向 Elasticsearch 中索引数据之前轻松地处理数据。同时，Kibana 不仅可针对 Elasticsearch 数据提供实时可视化，同时还提供 UI 以便用户快速访问应用程序性能监测 (APM)、日志和基础设施指标等数据。\n  应用场景 Elasticsearch在速度和可扩展性方面都表现出色，而且还能够索引多种类型的内容，这意味着其可用于多种用例：\n 应用程序搜索 网站搜索 企业搜索 日志处理和分析 基础设施指标和容器监测 应用程序性能监测 地理空间数据分析和可视化 安全分析 业务分析  架构设计  Gateway：Elasticsearch用来存储索引的文件系统，支持多种类型。ElasticSearch默认先把索引存储在内存中，然后当内存满的时候，再持久化到Gateway里。当ES集群关闭或重启的时候，它就会从Gateway里去读取索引数据。比如LocalFileSystem和HDFS、AS3等。 DistributedLucene Directory：Lucene里的一些列索引文件组成的目录。它负责管理这些索引文件。包括数据的读取、写入，以及索引的添加和合并等。 Mapping：映射解析模块。 Search Moudle：搜索模块。 Index Moudle：索引模块。 Disvcovery：节点发现模块。不同机器上的节点要组成集群需要进行消息通信，集群内部需要选举master节点，这些工作都是由Discovery模块完成。支持多种发现机制，如 Zen 、EC2、gce、Azure。 Scripting：Scripting用来支持在查询语句中插入javascript、python等脚本语言。 3rd plugins：第三方插件。 Transport：传输模块，支持多种传输协议，如 Thrift、memecached、http，默认使用http。 JMX：JMX是java的管理框架，用来管理ES应用。 Java(Netty)：java的通信框架。 RESTful Style API：提供给用户的接口，通过RESTful方式来实现API编程。  基本概念 Elasticsearch是一个文档型数据库，为了方便理解，下面给出其与传统的关系型数据库的对比\n   Relational DB Elasticsearch     数据库 Database 索引 Index   表 Table 类型 Type   行 Rows 文档 Document   列 columns 字段 fields   表结构 schema 映射 mapping    文档 Elasticsearch是面向文档的，索引和搜索数据的最小单位是文档，并且使用JSON来作为文档的序列化格式。每个文档可以由一个或多个字段组成，类似于关系型数据库中的一行记录，在Elasticsearch中，文档有几个重要属性\n 自我包含：一篇文档同时包含字段和对应的值。 层次型：文档中还可以包含新的文档，一个字段的取值也可以包含其他字段和取值。 结构灵活：文档不依赖预先定义的模式。在关系型数据库中，要提前定义字段才能使用，在Elasticsearch中，对于字段是非常灵活的，有时候，我们可以忽略该字段，或者动态的添加一个新的字段。  尽管我们可以随意的新增或者忽略某个字段，但是，每个字段的类型非常重要，比如一个年龄字段类型，可以是字 串也可以是整形。因为Elasticsearch会保存字段和类型之间的映射及其他的设置。这种映射具体到每个映射的每种类型，这也是为什么在Elasticsearch中，类型有时候也称为映射类型。\n类型 类型是文档的逻辑容器，类似于表格是行的容器。在不同的类型中，最好放入不同结构的文档。（有点类似于关系型数据库中的表的概念）。\n每个类型中对于字段的定义称为映射。比如name字段映射为string类型。 我们说文档是无模式的，它们不需要拥有映射中所定义的所有字段，例如我们可能会新增一个映射中不存在的字段，那么Elasticsearch是怎么做的呢?\nElasticsearch会自动的将新字段加入映射，由于不确定这个字段是什么类型，Elasticsearch就会根据字段的值来推导它的类型，如果这个值是10，那么Elasticsearch会认为它是整形。 但是这种推导也可能会存在问题，如果这里的10实际上是字符串“10”，如果后续在索引字符串“hello world”，就会因为类型不一致而导致索引失败。 所以最安全的方式就是在索引数据之前，就定义好所需要的映射，这点跟关系型数据库殊途同归了，先定义好字段，然后再使用。\n映射类型只是将文档进行逻辑划分。从物理角度来看，同一索引的文档都是写入磁盘，而不考虑它们所属的映射类型。\n 注意⚠️：为什么ElasticSearch要在7.X版本去掉type?\n 在Elasticsearch设计初期，是直接查考了关系型数据库的设计模式，存在了类型（表）的概念。 但是，其搜索引擎是基于Lucene的，这种基因决定了类型是多余的。Lucene的全文检索功能之所以快，是因为倒序索引的存在。 而这种倒序索引的生成是基于索引的，而并非 类型。多个类型反而会减慢搜索的速度——两个不同type下的两个user_name，在ES同一个索引下其实被认为是同一个filed，你必须在两个不同的type中定义相同的filed映射。否则，不同type中的相同字段名称就会在处理中出现冲突的情况，导致Lucene处理效率下降。\n索引 在Elasticsearch中，索引有两个含义：\n  名词\n 索引是映射类型的容器，Elasticsearch中的索引是一个非常大的文档集合，非常类似关系型数据库中库的概念。索引存储了所有映射类型的字段。    动词\n 索引一个文档就是存储一个文档到一个索引 （名词）中以便被检索和查询。这非常类似于SQL中的INSERT关键词，不同的是文档已存在时，新文档会替换旧文档（UPSERT）。    在Elasticsearch中，索引使用了一种称为倒排索引（即通过查询词索引到文档）的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。\n","date":"2022-05-23T22:16:13+08:00","permalink":"https://blog.orekilee.top/p/elasticsearch-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ElasticSearch 基本概念"},{"content":"ZAB协议 ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的原子广播协议。 在ZooKeeper中，主要依赖ZAB协议来实现分布式数据一致性，基于该协议，ZooKeeper实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。\nZAB协议包括了两种基本的模式，分别是崩溃恢复和消息广播。\n消息广播 为了保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致，ZAB协议中引入了消息广播模式。\n在上面我们提到了，ZooKeeper集群中只有Leader服务器能够执行写操作，为了保证集群的数据一致性，我们需要将Leader节点更新的数据同步到Follower与Observer服务器中，所以当Leader服务器接收到客户端发送的写请求后，会自动生成对应的提案并发起一轮消息广播。\n消息广播的执行流程如下：\n 接受到客户端发送的事务请求，Leader服务器为其生成对应的事务提议。 Leader为每一个Follower和Observer都准备了一个FIFO的队列，并把提议发送到队列上。 当Follower接收到事务提议后，都会先将其以事务日志的形式写入本地磁盘中，然后再写入成功后反馈给Leader服务器一个ACK。 当Leader接收到半数以上Follower节点的ACK，它就会认为大部分节点都同意议题，准备开始提交。 Leader向所有节点发送提交事务的Commit请求，完成事务。  为了防止因为网络等原因导致的Follower、Observer节点处理请求的顺序不同而导致的数据不一致问题，保证消息广播过程中消息接收与发送的顺序性，消息广播中引入了FIFO队列和事务ID来解决这个问题。\n 在消息广播的过程中，Leader服务器会为每一个Follower、Observer服务器都各自分配一个单独的队列，然后将需要广播的事务提议放到这些队列中，并根据FIFO策略进行消息发送。由于ZAB由于协议是通过TCP协议来进行网络通信的，这样不仅保证了消息的发送顺序性，也保证了接受顺序性。 在广播事务提议之前，Leader服务器会先给这个提议分配一个全局单调递增的唯一事务ID（ZXID）。为了保证每一个消息严格的因果关系，必须将每一个事务提议按照其ZXID的先后顺序来进行排序与处理。  如果你了解过二阶段提交（2PC）协议，你会发现其实消息广播的过程实际上就是一个简化版本的二阶段提交过程，他将二阶段提交中的中断逻辑删除，Leader服务器不需要等待集群中的全部Follower服务器都响应反馈，只需要得到过半Follower的ACK就开始执行事务的提交。这种简化版的2PC虽然提高了效率，但是无法处理Leader服务器崩溃退出而导致的数据不一致问题，因此ZooKeeper中又添加了崩溃恢复模式来解决这个问题。\n崩溃恢复 当Leader服务器出现崩溃退出或机器重启，亦或是集群中不存在半数以上的服务器与Leader服务器保持正常通信时，在重新开始新的一轮原子广播事务操作之前，此时所有节点都会使用崩溃恢复协议来使彼此达到一个一致的状态。\n 崩溃恢复过程需要确保那些已经在Leader服务器上提交的事务最终被所有的事务提交。\n 假设一个事务中Leader服务器（server2）上被提交了，并且已经得到了过半Follower服务器的ACK反馈，但是在它将Commit消息发送给所有的Follower机器之前，Leader服务器就挂掉了，如下图：\n从上图可以看到，部分的节点收到了commit请求并进行了提交，而有一部分Leader还没来得及发送就已经崩溃了。针对这种情况，崩溃恢复必须要确保该事务最终能够在所有的服务器上都被提交成功，否则将会出现数据不一致的情况。所以在重新选举的时候，必定会选取ZXID最大的节点来确保其保留了最新的事件。\n 崩溃恢复过程需要确保丢弃那些只在Leader服务器上被提出的事务。\n 如果Leader服务器在提交了一个事务之后，还没来得及广播发送commit就已经崩溃推出了，从而导致集群中的其他服务器都没有收到这个事务提议。当原先的Leader节点故障恢复后，再次以Follower的角色加入集群后，此时就因为只有它完成了事务提交，而产生了数据不一致的情况，如下图：\n针对这种情况，我们需要让server2在故障恢复后能够丢弃这些只在它这个节点上提出的事务，来确保数据一致。\n为了能够满足上述的两个要求，所以ZooKeeper让Leader选举算法保证新选举出来的Leader服务器拥有集群中所有机器最高的事务编号（ZXID最大），那么这就肯定能够保证新选举出来的Leader一定具有所有已经提交的提案，此时新的Leader就会将事务日志中尚未提交的消息同步到各个服务器中。\n","date":"2022-05-23T21:14:13+08:00","permalink":"https://blog.orekilee.top/p/zab-%E5%8D%8F%E8%AE%AE/","title":"ZAB 协议"},{"content":"集群 集群角色 通常在分布式系统中，构成一个集群的每一台机器都有自己的角色，最常见的集群模式就是Master/Slave模式（主从模式），在这种模式中，通常Maste服务器作为主服务器提供写服务，其他的Slave服务器从服务器通过异步复制的方式获取Master服务器最新的数据提供读服务。\n但是ZooKeeper并没有沿用传统的主从模式，而是引入了Leader、Follower和Observer三种角色，如下图所示：\nZooKeeper集群中的所有机器通过一个Leader 选举过程来选定一台称为Leader的机器，Leader既可以为客户端提供写服务又能提供读服务。除了Leader外，Follower和Observer都只能提供读服务。Follower和Observer唯一的区别在于Observer机器不参与Leader的选举过程，也不参与写操作的“过半写成功”策略，因此Observer机器可以在不影响写性能的情况下提升集群的读性能。\n Leader：为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。 Follower：为客户端提供读服务，如果是写服务则转发给Leader。在选举过程中参与投票。 Observer：与Follower工作原理基本一致，唯一的区别是Observer不参与任何形式的投票。Observer主要用来在不影响写性能的情况下提升集群的读性能，是ZooKeeper3.3中新增的角色。  ZooKeeper用以下四种状态来表示各个节点\n LOOKING：竞选状态。 LEADING：Leader状态。 FOLLOWING：Follower状态。 OBSERVING：Observer状态。  选举 Leader选举是ZooKeeper中最重要的技术之一，也是保证分布式数据一致性的关键所在。\n当ZooKeeper集群中的一台服务器出现以下两种情况之一时，就会开始进入Leader选举\n 服务器初始化启动 服务器运行期间无法和Leader保持连接  在介绍选举之前，首先介绍三个重要参数\n 服务器ID（myid）：编号越大在选举算法中权重越大。 事务ID（zxid）：值越大说明数据越新，权重越大。 逻辑时钟（epoch-logicalclock）：同一轮投票过程中的逻辑时钟值是相同的，每投完一次值会增加。  服务器启动时期的选举 ZooKeeper集群初始化时，当满足以下两个条件时，就会开始进入Leader选举流程：\n 集群规模至少为2台机器 集群内的机器能够互相通信  如上图，选举流程如下：\n 每个服务器会发出一个投票：由于是初始化状态，每一个节点都会将自己作为Leader来进行投票，每次投票的内容包含：推举服务器的myid（在集群中的机器序号）和ZXID（事务ID），以(myid, ZXID)的形式表示。初始化阶段每个节点都会将票投给自己，然后将这个投票发给集群总其他所有机器。 接受来自各个服务器的投票：每个服务器都会接收来自其他服务器的投票，在接收投票后首先会判断该投票的有效性，如是否为本轮投票（逻辑时钟）、是否来自LOOING状态的服务器。 处理投票：在接收到来自其他服务器的投票后，针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下  优先检查ZXID，ZXID较大的优先作为Leader。 如果Leader相同则比较myid，myid较大的作为Leader。    统计投票：每次投票后，服务器都会统计所有投票，判断是否已经有过半的机器接收到相同的投票信息。当票数达到半数以上时，此时就认为已经选出了Leader。 改变服务器状态：一旦确认了Leader，则每个服务器开始改变自己的状态。如果是Follower则更改成FOLLOWING；如果是Observer则更改成OBSERVING；如果是Leader则更改成LEADING；  服务器运行时期的选举 在ZooKeeper集群正常运行中，一旦选出一个Leader，那么所有服务器的集群角色一般不会再发生变化——也就是说Leader服务器将一直作为集群的Leader，即使集群中有非Leader机器挂了或者是有机器加入集群也不会影响Leader。但是一旦Leader所在的机器挂掉，那么此时整个集群将暂时无法对外服务，马上进入新一轮的Leader选举。\n服务器运行期间的Leader选举和启动时期的Leader选举基本是一致的，只增加了一个变更状态的步骤，流程如下：\n 变更状态：当Leader挂了之后，剩下的所有非Observer服务器都会将自己的服务器状态变更为LOOKING，然后开始进入Leader选举流程。 每个服务器会发出一个投票 接受来自各个服务器的投票 处理投票 统计投票 改变服务器状态  集群的机器数量  为什么ZooKeeper提倡集群的机器数量最好要做奇数台呢？\n ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n/2。先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。\n综上，何必增加那一个不必要的 ZooKeeper 呢？\n","date":"2022-05-23T21:13:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E9%9B%86%E7%BE%A4/","title":"ZooKeeper 集群"},{"content":"系统模型 数据模型 ZooKeeper数据存储的结构与标准的Unix文件系统非常类似，但是并没有引入传统文件系统中目录和文件等概念，而是使用了其特有的数据节点的概念，我们称之为ZNode。Znode是zookeeper中的最小数据单元，每个Znode上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。\n如上下图，Znode的节点路径标识方式和Unix文件系统路径非常相似，都是由一系列使用斜杠（/）进行分割的路径表示，最上层的根结点以/代表，并且每个Znode都有一个唯一的路径标识。\n ⚠️注意：ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的上限是每个结点的数据大小最大是 1M。\n 节点特性 节点类型 在ZooKeeper中，Znode节点类型分为持久节点（PERSISTENT）、临时节点（EPHEMERAL）、**顺序节点（SEQUENTIAL）**三大类，通过组合使用，可以生成以下四种组合型节点类型：\n 持久节点：该数据节点被创建后，就会一直存在于ZooKeeper的服务器上，直到有删除操作来主动清除这个节点。 持久顺序节点：基本特性与持久节点一致，额外的特性是顺序性，即一个父节点可以为其子节点维护一个创建的先后顺序 ，这个顺序体现在节点名称上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数，上限是整型最大值。 临时节点：临时节点的生命周期与客户端的会话绑定在一起，如果客户端会话失效，则这个节点就会自动被清理掉。同时，临时节点不能创建子节点，它只能作为叶子节点使用。 临时顺序节点：基本特性与临时节点一致，额外的特性是顺序性。  节点状态 每个数据节点除了存储数据内容之外，还存储了数据节点本身的一些状态信息（如子节点数量、事务id、版本信息等），这些状态信息由Stat这个类来维护。\n czxid：Created ZXID，该数据节点被创建时的事务ID。 mzxid：Modified ZXID，节点最后一次被更新时的事务ID。 ctime：Created Time，该节点被创建的时间。 mtime： Modified Time，该节点最后一次被修改的时间。 version：节点的版本号。 cversion：子节点的版本号。 aversion：节点的 ACL 版本号。 ephemeralOwner：创建该节点的会话的sessionID ，如果该节点为持久节点，该值为0。 dataLength：节点数据内容的长度。 numChildre：该节点的子节点个数，如果为临时节点为0。 pzxid：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的列表 ，不是内容。  版本 ZooKeeper中为节点引入了版本的概念，每个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化。\n dataVersion：当前Znode节点数据内容版本号 cversion：当前Znode节点的子节点版本号。 aclVersion：当前Znode的ACL变更版本号。  为了解决那些数据更新竞争激烈的场景，ZooKeeper借助版本号机制来实现乐观并发控制。\nWatcher Watcher（事件监听器），是ZooKeeper中的一个很重要的特性。ZooKeeper允许客户端在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。\n从上图中我们可以看到，ZooKeeper的Watcher机制主要包括客户端线程、客户端WatchManager、ZooKeeper服务器三个部分。工作流程如下：\n 客户端向ZooKeeper服务器注册Watcher对象 客户端将Watcher对象存储在客户端的WatchManager中。 当ZooKeeper服务器触发Watcher事件后，会向客户端发送通知。 客户端从WatchManager中取出对应的Watcher对象来执行回调逻辑。  ACL ZooKeeper提供了一套完善的ACL（Access Control Lists）权限控制机制来保障数据的安全，类似于Unix/Linux中的UGO权限控制机制。\nACL机制由以下三部分组成，我们通常使用scheme:id:permission来标识一个有效的ACL信息。\n 权限模式（Scheme） 授权对象（ID） 权限（Permission）  权限模式：Scheme 权限模式是用来确定权限验证过程中使用的校验策略。在ZooKeeper中最常用的就是以下四种权限模式\n IP：IP模式通过IP地址粒度来进行权限控制。 Digest：Digest是最常用的权限控制模式，其以类似于username:password形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。当我们配置了权限标识后，为了保证安全，ZooKeeper会分别使用SHA-1算法和BASE64编码进行处理，将其混淆为无法辨识的字符串。 World：World是最开放的权限控制模式，是一种特殊的Digest模式。在该模式下数据节点的访问权限对所有用户开放，即所有用户都可以在不进行任何数据校验的情况下操作ZooKeeper上的数据。权限标识为world:anyone Super：Super模式即超级用户模式，是一种特殊的Digest模式。在该模式下超级用户可以对任意ZooKeeper上的数据节点进行任何操作。  权限对象：ID 授权对象指的是权限赋予的用户或一个指定实体，例如IP地址或是机器等。在不同的权限关系下，授权对象是不同的，对应关系如下图：\n IP：通常是一个IP地址或者一个IP网段，如192.168.0.110或192.168.0.1/24 Digest：Digest是最常用的权限控制模式，其以类似于username:password形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制。当我们配置了权限标识后，为了保证安全，ZooKeeper会分别使用SHA-1算法和BASE64编码进行处理，将其混淆为无法辨识的字符串。 World：只有一个ID：anyone Super：与Digest模式一致。  权限：Permission 权限就是指那些通过权限检查后可以被允许执行的操作。在ZooKeeper中，所有对数据的操作权限分为以下五大类：\n CREATE：数据节点的创建权限，允许授权对象在该数据节点下创建子节点。 DELETE：数据节点的删除权限，允许授权对象删除该数据节点的子节点。 READ：数据节点的读取权限，允许授权对象访问该数据节点并读取其数据内容或子节点列表等。 WRITE：数据节点的更新权限，允许授权对象对该数据节点进行更新操作。 ADMIN：数据节点的管理权限，允许授权对象在该数据节点进行ACL相关的设置操作。  Session Session（会话） 可以看作是ZooKeeper服务器与客户端的之间的一个TCP长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watcher事件通知。\nSession有一个属性叫做：sessionTimeout ，sessionTimeout 代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。\n另外在每次客户端向服务端发起会话创建请求时，服务端都会为其分配一个SessionID，SessionID用来唯一标识一个会话，因此ZooKeeper必须保证SessionID的全局唯一性。\n","date":"2022-05-23T21:11:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E7%B3%BB%E7%BB%9F%E6%A8%A1%E5%9E%8B/","title":"ZooKeeper 系统模型"},{"content":"ZooKeeper 基本概念 ZooKeeper是一个开源的分布式协调服务，由知名互联网公司雅虎创建，是Google Chubby的开源实现。它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。\n 原语： 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。\n 特点 ZooKeeper可以保证如下分布式一致性特性：\n 顺序一致性：从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一视图：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性：一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 实时性：ZooKeeper保证在一定时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。  设计目标 ZooKeeper致力于实现一个高性能、高可用，具有严格顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务。高性能使得ZooKeeper能够应用于那些对系统吞吐有明确要求的大型分布式系统，高可用使得分布式的单点问题得到了很好的解决，而严格的顺序访问控制使得客户端能够基于ZooKeeper实现一些复杂的同步原语。\n针对以上需求，ZooKeeper的四个设计目标如下：\n  简单的数据模型：ZooKeeper使用一个共享的、树形结构的命名空间来协调分布式程序。其数据模型类似一个文件系统，不过与传统的文件系统不同，ZooKeeper将全量数据存储在内存中，以此来实现提高服务器吞吐、减少延迟的目的。\n  可以构建集群：一个ZooKeeper集群通常由一组机器组成，组成ZooKeeper集群的每台机器都会在内存中维护当前服务器状态，并且每台机器之间都互相保持通信。只要集群中存在半数以上的机器能够正常工作，整个集群就可以正常对外提供服务。\n  顺序访问：对于来自客户端的每个更新请求，ZooKeeper都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序，可以根据这个特性来实现更高层次的同步原语。\n  高性能：由于ZooKeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景。\n  应用场景 ZooKeeper是一个典型的分布式数据一致性的解决方案，通常用于以下这些场景：\n 数据发布/订阅 负载均衡 命名服务 分布式协调/通知 集群管理 Master选举 分布式锁 分布式队列  ","date":"2022-05-23T21:10:13+08:00","permalink":"https://blog.orekilee.top/p/zookeeper-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ZooKeeper 基本概念"},{"content":"分布式文件系统 基本概念 分布式文件系统（Distributed File System）是网络文件系统的延伸，其关键点在于存储端可以灵活地横向扩展。也就是可以通过增加设备（主要是服务器）数量的方法来扩充存储系统的容量和性能。同时，分布式文件系统还要对客户端提供统一的视图。也就是说，虽然分布式文件系统服务由多个节点构成，但客户端并不感知。在客户端来看就好像只有一个节点提供服务，而且是一个统一的分布式文件系统。\n分布式文件系统 VS 网络文件系统 从本质上来说，分布式文件系统其实也是网络文件系统的一种，其与网络文件系统的差异在于服务端包含多个节点，也就是服务端是可以横向扩展的，其可以通过增加节点的方式增加文件系统的容量，提升性能。\n由于其数据被存储在多个节点上，因此还有其他特点：\n 支持按照既定策略在多个节点上放置数据。 可以保证在出现硬件故障时，仍然可以访问数据。 可以保证在出现硬件故障时，不丢失数据。 可以在硬件故障恢复时，保证数据的同步。 可以保证多个节点访问的数据一致性。  横向拓展结构 对于存储集群端主要有两种类型的架构模式：一种是以有中心控制节点的分布式架构，另一种是对等的分布式架构，也就是没有中心控制节点的架构。\n中心架构 中心架构是指在存储集群中有一个或多个中心节点，中心节点维护整个文件系统的元数据，为客户端提供统一的命名空间。 在实际生产环境中，中心节点通常是多于一个的，其主要目的是保证系统的可用性和可靠性。\n在中心架构中，集群节点的角色分为两种：\n 控制节点：这种类型的节点会存储文件系统的元数据信息，并对请求进行协调与处理，根据元数据将请求转发只对应的节点上。 数据节点：这种类型的节点用于存储文件系统的用户数据。  架构示意图如下：\n当客户端需要对一个文件进行读/写时，首先会访问控制节点，控制节点通过对一些元数据进行处理（鉴权、文件锁、位置计算等），并将文件所在的数据节点的位置响应给客户端。此时客户端再与数据节点交互，完成数据的访问。\n对等架构 对等架构是没有中心节点的架构，集群中并没有一个特定的节点负责文件系统元数据的管理。在集群中所有节点既是元数据节点，也是数据节点。 在实际实现中，其实并不进行角色的划分，只是作为一个普通的存储节点。\n由于在对等架构中没有中心节点，因此主要需要解决两个问题：\n 客户端需要一种位置计算算法来计算数据应该存储的位置。 需要将元数据存储在各个存储节点，在某些情况下需要客户端来汇总。  关键原理 分布式文件系统本身也是文件系统，因此它与本地文件系统和网络文件系统等具备一些公共技术。除此之外，鉴于其分布式的特点，还涉及一些分布式的技术。\n数据布局 分布式文件系统的数据布局与本地文件系统不同，其关注的不是数据在磁盘的布局，而是数据在存储集群各个节点的放置问题。\n在分布式文件系统中，数据布局解决的主要问题是性能和负载均衡的问题。其解决方案就是通过多个节点来均摊客户端的负载，也就是实现存储集群的横向扩展。因此数据布局的核心，就是要保证数据量均衡与负载均衡。\n基于动态监测的数据布局 基于动态监测的数据布局是指通过监测存储集群各个节点的负载、存储容量和网络带宽等系统信息来决定新数据放置的位置。 另外，集群节点之间还要有一些心跳信息，这样当有数据节点故障的情况下，控制节点可以及时发现，保证在决策时剔除。\n由于需要汇总各个节点的信息进行决策，因此基于动态监测的数据布局通常需要一个中心节点。中心节点负责汇总各种信息并进行决策，并且会记录数据的位置信息等元数据信息。当客户端需要写入数据时，客户端首先与控制节点交互；控制节点根据汇总的信息计算出新数据的位置，然后反馈给客户端；客户端根据位置信息，直接与对应的数据节点交互。\n基于计算位置的数据布局 基于计算位置的数据布局是一种固定的数据分配方式。在该架构中通过一个算法来计算文件或数据存储的具体位置。 当客户端要访问某个文件时，请求在客户端或经过的某个代理节点计算出数据的具体位置，然后将请求路由到该节点进行处理。\n当客户端访问集群数据时，首先计算出数据的位置（根据请求的特征来计算数据具体应该放到哪个节点，例如一致性哈希算法），然后与该节点交互。\n数据可靠性 分布式数据的可靠性是指在出现组件故障的情况下依然能够能提供正常服务的能力。\n复制（Replication） 复制技术是通过将数据复制到多个节点的方式来实现系统的高可靠。 由于同一份数据会被复制到多个节点，这样同一个数据就存在多个副本，因此也称为多副本技术，这样当出现节点故障时就不会影响数据的完整性和可访问性。\n复制技术有两种不同的模式：\n  主从节点复制：即在副本节点中有一个节点是主节点，所有的数据请求先经过主节点。对于一个写数据请求，客户端将请求发送到主节点，主节点将数据复制到从节点，再给客户端应答。\n  无主节点复制：即在集群端并没有一个主节点，副本逻辑在客户端或代理层完成。 当客户端发送一个写数据请求时，客户端会根据策略自行（或者通过代理层）找到副本服务器，并将多个副本发送到副本服务器上。\n  纠删码（Erasure Code） 副本技术的本质就是冗余存储，因此需要消耗很多额外的存储空间。以 3 个副本为例，需要额外消耗 2 倍的存储空间来保证数据的可靠性。虽然副本技术在性能和可靠性方面优势明显，但成本明显比较高。为了降低存储的成本，很多公司采用纠删码技术来保证数据的可靠性。\n纠删码是一种通过校验数据来保证数据可靠性的技术，也就是该技术通过保存额外的一个或多个校验块来提供数据冗余。与副本技术不同，这种数据冗余技术不能通过简单复制来恢复数据，而是经过计算来得到丢失的数据。\n纠删码的基本原理是采用矩阵运算，将 n 个数据转换为 n+m 个数据进行存储。其基本流程如下：\n 校验数据生成：找到一个生成矩阵，通过该矩阵与原始数据的运算可以得到最终要存储的校验数据。 数据恢复：由于生成矩阵是可逆的，因此可利用生成矩阵和剩余可用数据来计算出原始数据。  数据一致性 在分布式文件系统中，由于同一个数据块被放置在不同的节点上，我们无法保证多个节点的数据时时刻刻是相同的，因此会出现一致性的问题。这里的一致性包括两个方面：一个方面是各个节点数据的一致性问题；另一个方面是从客户端访问角度一致性的问题。\n通常来说，我们是无法保证各个节点上数据是完全一致的（故障、宕机、延迟、网络分区等原因），只能保证客户端访问的一致性。为了保证客户端访问数据的一致性，通常需要对存储系统进行特殊的设计，从而在系统层面保证数据的一致性。通常提供的一致性保证有如下两种：\n 强一致性：当数据的写入操作反馈给客户端后，任何对该数据的读操作都会读到刚刚写入的数据。 最终一致性：在执行一个写入操作后，如果没有新的写入操作的情况下， 该写入的数据会最终同步到所有副本节点上，但中间会有时间窗口。  故障与容错 在分布式文件系统中必须要解决设备故障的问题。这是因为在大规模分布式文件系统中设备的总量达到数万个甚至数十万个，设备发生故障就会成为常态。\n设备的故障分为两种类型：\n 临时故障：指短时间可以恢复的故障，如服务器重启、网线松动或交换机掉电等。 永久故障：指设备下线，且永远不会恢复，如硬盘损坏等。  为了应对系统随时出现的故障，分布式文件系统在设计时必须要考虑容错处理。容错主要包括以下几方面内容：\n 故障预测：在故障发生前，预知设备故障，然后有计划地将该设备下线，避免突然下线导致的性能等问题。 故障检测：在故障发生时，及时发现故障原因，方便进行问题的修复。如检测磁盘、通信链路或服务的故障等。 故障恢复：在故障发生后，快速进行响应，保证系统仍然能够对外提供无损的服务。如通过部件冗余、主备链路等。当系统发生故障时，可以通过切换链路，或者通过冗余节点来提供服务。  ","date":"2022-05-23T18:55:13+08:00","permalink":"https://blog.orekilee.top/p/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"分布式文件系统"},{"content":"网络文件系统 基本概念 网络文件系统（Network File System）是基于 TCP/IP 协议（整个协议可能会跨层）的文件系统，它可以将远端服务器文件系统的目录挂载到本地文件系统的目录上，允许用户或者应用程序像访问本地文件系统的目录结构一样，访问远端服务器文件系统的目录结构，而无需理会远端服务器文件系统和本地文件系统的具体类型，非常方便地实现了目录和文件在不同机器上进行共享。\n网络文件系统通常分为客户端和服务端，其中客户端类似本地文件系统，差异是在读/写数据时不是访问磁盘等设备，而是通过网络将请求传输到服务端。而服务端则是对数据进行管理的系统，将数据存储到磁盘等存储介质上。\n网络文件系统 VS 本地文件系统 网络文件系统与本地文件系统主要存在以下差异：\n 数据的访问过程。 本地文件系统的数据是持久化存储到磁盘上的，而网络文件系统则需要将数据传输到服务端进行持久化处理。 是否需要格式化。 本地文件系统需要进行格式化处理才可以使用。而网络文件系统则不需要客户端进行格式化操作，通常只需要挂载到客户端就可以直接使用。当然在服务端通常是要做一些配置工作的，包括格式化操作。  网络文件系统最主要的特性是实现了数据的共享。 基于数据共享的特性，使得网络文件系统有很多优势，如增大存储空间的利用效率（降低成本）、方便组织之间共享数据和易于实现系统的高可用等。\n关键原理 由于网络文件系统基于网络实现，其分为本地的客户端与远端的服务端。因此其除了之前提到的本地文件系统相关的技术，还引入文件系统协议、RPC，网络文件锁等机制。\n文件系统协议 网络文件系统本质上是一个基于 C/S（客户端/服务端）架构的应用，其大部分功能是通过客户端与服务端交互来实现的。因此，对于网络文件系统来说，其核心之一是客户端与服务端的交互语言——文件系统协议。\n网络文件系统的协议的定义类似函数调用，包含 ID（可以理解为函数名称），参数和返回值。这里以 NFS v3 协议举例：\n从上图可以看出，协议语义与文件系统操作的语义基本上一一对应，因此客户端对网络文件系统的访问都可以通过协议传输到服务端进行相应的处理。\n远程过程调用（RPC） 由于在客户端与服务端都要实现对协议数据的封装和解析，因此实现起来比较复杂。为了降低复杂性，通常会在文件系统业务层与 TCP/IP 层之间实现一层交互层，这就是 RPC 协议。\nRPC（Remote Procedure Call，远程过程调用） 是 TCP/IP 模型中应用层的网络协议（OSI 模型中会话层的协议）。RPC 协议通过一种类似函数调用的方式实现了客户端对服务端功能的访问，简化了客户端访问服务端功能的复杂度。\nRPC 协议通常架构如下：\n 客户端：文件系统。 服务端：文件系统服务。 存根：定义的函数集。以网络文件系统为例，函数集包括创建文件、删除文件、写数据和读数据等。函数集通常需要分别在客户端和服务端定义一套接口。 RPC 运行时库：实现了 RPC 协议的公共功能，如请求的封装与解析、消息收发和网络层面的错误处理等。  在客户端调用 RPC 函数时，会调用 RPC 库的接口将该函数调用转化为一个网络消息转发到服务端，而服务端的 RPC 库则对网络数据包进行反向解析，调用服务端注册的函数集（存根）中的函数实现功能，最后将执行的结果反馈给客户端。\n对于客户端的应用，这个函数调用与本地函数调用并没有明显的差异。\n网络文件锁 本地文件系统可以在文件系统内实现文件锁。但由于网络文件系统会有多个不同的客户端文件系统访问同一个服务端的文件系统，文件锁是无法在客户端的文件系统中实现的，只能在服务端实现。这样就需要一个协议将客户端的加锁、解锁等请求传输到服务端，并且在服务端维护文件锁的状态。\n在 NFS 协议族中，其通过 NLM（Network Lock Manager） 协议实现了一个网络文件锁服务。由于网络文件锁需要考虑到网络分区、丢包、服务端/客户端宕机等多方面因素，实现要比本地文件锁服务复杂得多，但其核心原理还是与本地文件锁一样：维护一个数据结构，负责记录每个文件的加锁情况。当客户端传来新的加锁请求时查找该结构，判断是否存在锁冲突，来考虑是允许加锁还是报错返回。\n","date":"2022-05-23T18:54:13+08:00","permalink":"https://blog.orekilee.top/p/%E7%BD%91%E7%BB%9C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"网络文件系统"},{"content":"本地文件系统 关键技术 虚拟文件系统 虚拟文件系统（Virtual File System，VFS） 是具体文件系统（如 Ext2、Ext4 和 XFS 等）与应用程序之间的一个接口层，它对 Linux 的每个文件系统的所有细节进行抽象，使得不同的文件系统在 Linux 核心以及系统中运行的其他进程看来都是相同的。以此达到使操作系统能够适配多种文件系统的目的。\nVFS 提供了一个文件系统框架，本地文件系统可以基于 VFS 实现，其主要做了如下几方面的工作：\n 作为抽象层为应用层提供了统一的接口（ read、write 和 chmod 等）。 实现了一些公共的功能，如 Inode 缓存（Inode Cache）和页缓存（Page Cache）等。 规范了具体文件系统应该实现的接口。  基于上述设定，其他具体的文件系统只需要按照 VFS 的约定实现相应的接口及内部逻辑，并注册在系统中，就可以完成文件系统的功能了。当用户调用操作系统提供的文件系统 API 时，会通过软中断的方式调用内核 VFS 实现的函数。\n磁盘空间布局 文件系统的核心功能是实现对磁盘空间的管理。对磁盘空间的管理是指要知道哪些空间被使用了，哪些空间没有被使用。这样，在用户层需要使用磁盘空间时，文件系统就可以从未使用的区域分配磁盘空间。\n为了对磁盘空间进行管理，文件系统往往将磁盘空间通常被划分为元数据区与数据区两个区域。数据区就是存储数据的地方，用户在文件中的数据都会存储在该区域；而元数据区则是对数据区进行管理的地方。\n基于固定功能区的磁盘空间布局 基于固定功能区的磁盘空间布局是指将磁盘的空间按照功能划分为不同的子空间，每种子空间有具体的功能。 以 Linux 中的 Ext 文件系统为例，其空间被划分为数据区和元数据区，而元数据区又被划分为数据块位图、inode 位图和 inode 表等区域。如下图：\n基于功能分区的磁盘空间布局空间职能清晰，便于手动进行丢失数据的恢复。但是由于元数据功能区大小固定，因此容易出现资源不足的情况==（位图长度有限）==。比如，在海量小文件的应用场景下，有可能会出现磁盘剩余空间充足，但 inode 不够用的情况\n基于非固定功能区的磁盘空间布局 在磁盘空间管理中有一种非固定功能区的磁盘空间管理方法。这种方法也分为元数据和数据，但是元数据和数据的区域并非固定的，而是随着文件系统对资源的需求而动态分配的。\n例如比较经典的实现 XFS，其它不是通过固定的位图区域来管理磁盘空间的，而是通过 B+ 树管理磁盘空间。这样做就可以动态的去分配元数据空间，而不是像位图一样具有限制。但是这也带来了新的问题，就是 B+ 树无法根据 inode 的偏移量来确定编号，因此 XFS 又引入复杂的编号机制来解决这个问题。\n基于数据追加的磁盘空间布局 前文介绍的磁盘空间布局方式对于数据的变化都是原地修改的，也就是对于已经分配的逻辑块，当对应的文件数据改动时都是在该逻辑块进行修改的。在文件随机 I/O 比较多的情况下，不太适合使用 SSD 设备，这主要由 SSD 设备的修改和擦写特性所决定。\n有一种基于数据追加的磁盘空间布局方式，也被称为基于日志（Log structured）的磁盘空间布局方式。这种磁盘空间布局方式对数据的变更并非在原地修改，而是以追加写的方式写到后面的剩余空间。这样，所有的随机写都转化为顺序写，非常适合用于 SSD 设备。\n文件数据管理 对于文件系统来说，无论文件是什么格式，存储的是什么内容，它都不关心。文件就是一个线性空间，类似一个大数组。而且文件的空间被文件系统划分为与文件系统块一样大小的若干个逻辑块。文件系统要做的事情就是将文件的逻辑块与磁盘的物理块建立关系。这样当应用访问文件的数据时，文件系统可以找到具体的位置，进行相应的操作。\n基于连续区域的文件数据管理 基于连续区域的文件数据管理方式是一次性为文件分配其所需要的空间，且空间在磁盘上是连续的。 由于文件数据在磁盘上是连续存储的，因此只要知道文件的起始位置所对应的磁盘位置和文件的长度就可以知道文件数据在磁盘上是如何存储的。如下图：\n这种文件数据管理方式的最大缺点是不够灵活，特别是对文件进行追加写操作非常困难。如果该文件后面没有剩余磁盘空间，那么需要先将该文件移动到新的位置，然后才能追加写操作。如果整个磁盘的可用空间没有能够满足要求的空间，那么会导致写入失败。\n除了追加写操作不够灵活，该文件数据管理方式还有另一个缺点就是容易形成碎片空间。由于文件需要占用连续的空间，因此很多小的可用空间就可能无法被使用，从而降低磁盘空间利用率。\n基于链表的文件数据管理 基于链表的文件数据管理方式将磁盘空间划分为大小相等的逻辑块。 在目录项中包含文件名、数据的起始位置和终止位置。在每个数据块的后面用一个指针来指向下一个数据块。如下图：\n这种方式可以有效地解决连续区域的碎片问题，但是对文件的随机读/写却无能为力。这主要是因为在文件的元数据中没有足够的信息描述每块数据的位置。为了实现随机读写，还需要在实现时附加一些额外的机制。\n基于索引的文件数据管理 索引方式的数据管理是指通过索引项来实现对文件内数据的管理。 如下图所示，与文件名称对应的是索引块在磁盘的位置，索引块中存储的并非用户数据，而是索引列表。当读/写数据时，根据文件名可以找到索引块的位置，然后根据索引块中记录的索引项可以找到数据块的位置，并访问数据。\n常见的索引方式有以下两种：\n 基于间接块的文件数据管理：在索引方式中，最为直观、简单的就是对文件的每个逻辑块都有一个对应的索引项，并将索引项用一个数组进行管理。当想要访问文件某个位置的数据时，就可以根据该文件逻辑偏移计算出数组的索引值，然后根据数组的索引值找到索引项，进而找到磁盘上的数据。    基于 Extent 的文件数据管理：在 Extent 文件数据管理方式中，每一个索引项记录的值不是一个数据块的地址，而是数据块的起始地址和长度。\n  缓存 文件系统的缓存（Cache）的作用主要用来解决磁盘访问速度慢的问题。缓存技术是指在内存中存储文件系统的部分数据和元数据而提升文件系统性能的技术。由于内存的访问延时是机械硬盘访问延时的十万分之一，因此采用缓存技术可以大幅提升文件系统的性能。\n文件系统缓存的原理主要还是基于数据访问的时间局部性和空间局部性特性。时间局部性就是如果一块数据之前被访问过，那么最近很有可能会被再次访问。空间局部性则是指在访问某一个区域之后，通常会访问临近的区域。\n缓存置换算法 由于内存的容量要比磁盘的容量小得多，当用户持续写入数据时就会面临缓存不足的情况，此时就涉及如何将缓存数据刷写到磁盘，然后存储新数据的问题。而将缓存数据落盘，并置换新数据的这一过程被称为缓存置换。\n在文件系统中通常会采取以下这些缓存置换算法：\n LRU（Least Recently Used）：LRU 基于时间局部性原理。将最久没有使用的数据淘汰出缓存。因此，当空间满时，最久没有使用的数据即被淘汰。 LFU（Least Frequently Used）：LFU 基于缓存命中频次作为置换依据。如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，将命中频次最少的数据淘汰掉。  预读算法 预读算法是针对读数据的一种缓存算法。预读算法通过识别 I/O 模式方式来提前将数据从磁盘读到缓存中。这样，应用读取数据时就可以直接从缓存读取数据，从而极大地提高读数据的性能。\n通常有两种情况会触发预读操作：\n 当有多个地址连续地读请求时会触发预读操作。 应用访问到有预读标记的缓存时会触发预读操作。这里，预读标记的缓存是在预读操作完成时在缓存页做的标记，当应用读到有该标记的缓存时会触发下一次的预读，从而省略对 I/O 模式的识别。  快照与克隆 快照（Snapshot）和克隆（Clone）技术可以应用于整个文件系统或单个文件。快照技术可以实现文件的可读备份，而克隆技术则可以实现文件的可写备份。针对文件，用的更多的是克隆技术。\n快照 目前快照技术有两种实现方式：\n 写时拷贝（Copy-On-Write，COW）：刚开始创建快照时原始文件和快照文件指向相同的存储区域。当原始文件被修改时，就需要将该位置的原始数据拷贝到新的位置，并且更新快照文件中的地址信息。 写时重定向（Redirect-On-Write，ROW）：当原始文件写数据时并不在原始位置写入数据，而是分配一个新的位置写入。在读取时，创建快照前的数据从源卷读，创建快照后产生的数据，从快照卷读。  可以看出 COW 在写入时需要拷贝一份数据到快照卷的动作，而 ROW 是直接重定向到快照卷写入，所以 ROW 适合写密集型；相反，由于 ROW 不断进行指针重定向，读性能会有较大影响，而 COW 不会，所以 COW 适合读密集型。对文件系统而言，用得最多的是 ROW 方式。\n克隆 克隆技术的原理与快照技术的原理类似，其相同点在于其实现方式依然是 ROW 或 COW，而差异点则主要表现在两个方面：一个方面，克隆生成的克隆文件是可以写的；另一个方面，克隆的数据最终会与原始文件的数据完全隔离。\n日志 在文件系统中，一个简单的写操作在底层可能会分解为多个步骤的修改。如果不能保证操作的原子性，那么倘若出现宕机、断电的情况，此时就会导致数据的丢失、不一致，甚至是文件系统的不可用。为了解决这个问题，在文件系统中引入了日志机制。\n日志通常是一块独立的空间，其采用环形缓冲区的结构，当进行文件修改操作时，相关数据块会被打包成一组操作写入日志空间，再更新实际数据。这里的一组操作被称为一个事务。\n由于实际数据的更新在写入日志之后，如果在数据更新过程中出现了系统崩溃，那么通过读取日志来更新。这样就能保证数据是我们所期望的数据。还有一种异常场景是日志数据刷写过程中。由于此时日志完成标记还没有置位，而且实际数据还没有更新，那么只需要放弃该条日志即可。\n权限管理 在多用户操作系统中，由于多个用户的存在，就必须实现用户之间资源访问的隔离。这种管理用户可访问资源的特性就是权限管理。\nRWX 权限管理 Linux最常用的权限管理就是 RWX-UGO 权限管理。其中，RWX 是 Read、Write 和 eXecute 的缩写。而 UGO 则是 User、Group 和 Other 的缩写。通过该机制建立用户和组实现对文件的不同的访问权限。\n如下图所示：\n当我们对文件进行操作时，首先会调用 inode_permission() 函数来判断执行用户的访问权限，如果权限不足则阻止对应操作的执行。\nACL 权限管理 ACL（Access Control List，访问控制列表），是一个针对文件/目录的访问控制列表。它在 RWX-UGO 权限管理的基础上为文件系统提供一个额外的、更灵活的权限管理机制。ACL 允许给任何用户或用户组设置任何文件/目录的访问权限，这样就能形成网状的交叉访问关系。\n根据是否继承父目录的 ACL 属性，ACL 分为以下两类：\n access ACL：每一个对象（文件/目录）都可以关联一个 ACL 来控制其访问权限，这样的 ACL 被称为 access ACL。 default ACL：目录关联的一种 ACL。当目录具备该属性时，在该目录中创建的对象（文件或子目录）默认具有相同的 ACL。  ACL 在操作系统内部是通过文件的扩展属性实现的。 当用户为文件添加一个 ACL 规则时，其实就是为该文件添加一个扩展属性。如下图：\n需要注意的是，ACL 的数据与文件的普通扩展属性数据存储在相同的位置，只不过通过特殊的标记进行了区分，这样就可以屏蔽普通用户对 ACL 的访问。\nSELinux 权限管理 SELinux（Security-Enhanced Linux）是一个在内核中实现的强制存取控制（MAC）安全性机制。SELinux 与 RWX、ACL 最大的区别是基于访问者（应用程序）与资源的规则，而不是用户与资源的规则，因此其安全性更高。这里基于应用程序与资源的规则是指规定了哪些应用程序可以访问哪些资源，而与运行该应用程序的用户无关。\nSELinux 通过将权限从用户收缩到应用，即使应用被攻破，黑客也只能访问应用程序所限定的资源，而不会扩散到其他地方。\n其原理图如下：\n当访问者访问被访问者（资源）时，需要调用内核的接口，此时会经过 SELinux 内核的判断逻辑，该判断逻辑根据策略数据库的内容确定访问者是否有权访问被访问者，如果允许访问则放行，否则拒绝该请求并记录审计日志\n配额管理 在多用户环境中，不仅要防止用户对其他用户数据的非法访问，还要确保某些用户所使用的存储空间不能太多。在这种情况下，就需要一种配额 （Quota）管理技术。\n配额管理是一种对使用空间进行限制的技术，其主要包括针对用户（或组）的限制和针对目录的限制两种方式。针对用户（或组）的限制是指某一个用户（或组）对该文件系统的空间使用不能超过设置的上限，如果超过上限则无法写入新的数据。针对目录的限制是指该目录中的内容总量不能超过设置的上限。\n在配额管理中，通常涉及三个基本概念：\n 软上限（Soft Limit）：指数据总量可以超过该上限。如果超过该上限则会有一个告警信息。 硬上限（Hard Limit）：指数据总量不可以超过该上限，如果超过该上限则无法写入新的数据。 宽限期（Grace Period）：宽限期通常是针对软上限而言的。如果设置了该值（如 3 天），则在 3 天内允许数据量超过软上限，当超过3天后，无法写入新的数据。  其实现的原理非常简单，当有新的写入请求时，配额模块就会去分析该请求。如果需要计入配额管理中，则进行配额上限的检查，在小于上限的情况下会更新配额管理数据，否则将阻止新的数据写入，并发出告警信息。\n文件锁 文件锁的基本作用就是保证多个进程对某个文件并发访问时数据的一致性。如果没有文件锁，就有可能出现多个进程同时访问文件相同位置数据的问题，从而导致文件数据的不一致性。\n文件锁分为以下两种类型：\n 劝告锁（Advisory Lock）：劝告锁是一种建议性的锁，通过该锁告诉访问者现在该文件被其他进程访问，但不强制阻止访问。 强制锁（Mandatory Lock）：强制锁则是在有进程对文件锁定的情况下，其他进程是无法访问该文件的。  为了提高性能，减少等待锁时间，以上两种锁都引入了共享锁、排他锁机制。\n 共享锁（Shared Lock）：在任意时间内，一个文件的共享锁可以被多个进程拥有，共享锁不会导致系统进程的阻塞。 排他锁（Exclusive Lock）：在任意时间内，一个文件的排他锁只能被一个进程拥有。  当有新的加锁请求到来时，其会与已有的锁信息逐一比对，判断是否存在锁冲突。如果存在，则将该进程置为休眠状态（阻塞，如果为非阻塞则返回直接错误码）。如果不存在，则将锁信息记录下来后允许用户访问。\n","date":"2022-05-23T18:53:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","title":"本地文件系统"},{"content":"文件系统 基础 什么是文件系统？  文件系统是一种存储和组织计算机数据的方法，它使得对其访问和查找变得容易，文件系统使用文件和树形目录的抽象逻辑概念代替了硬盘和光盘等物理设备使用数据块的概念，用户使用文件系统来保存数据不必关心数据实际保存在硬盘（或者光盘）的地址为多少的数据块上，只需要记住这个文件的所属目录和文件名。在写入新数据之前，用户不必关心硬盘上的那个块地址没有被使用，硬盘上的存储空间管理（分配和释放）功能由文件系统自动完成，用户只需要记住数据被写入到了哪个文件中。\n 上文是维基百科对于文件系统的描述，我们将其精炼一下，得到以下结论：\n 文件系统是一套实现了数据的存储、分级组织、访问和获取等操作的抽象数据类型（Abstract data type）。其可以构建在磁盘、内存、甚至是网络上。  用于解决什么问题？  我们为什么要使用文件系统呢？换句话说，文件系统它解决了什么问题呢？\n 文件系统解决的是对磁盘空间使用的问题。 如果我们不抽象出一个文件系统，而是每个应用程序都独立去访问磁盘空间，那会出现什么样的情况呢？\n 磁盘空间的访问会存在冲突。 如果没有一个第三方去统一管理磁盘空间，那么此时应用程序的数据访问并不会进行隔离，而是会互相影响，产生冲突。 磁盘空间的管理会非常复杂。 由于文件有不同格式、大小，存储的设备与位置也有所不同（本地磁盘、内存、甚至远端机器）。直接管理磁盘空间将会非常的复杂。  文件系统正是为了解决这两个问题，而在应用程序与磁盘空间之间抽象出的一层。文件系统对下实现了对磁盘空间的管理，对上为应用程序呈现层级数据组织形式和统一的访问接口。\n基于文件系统，应用程序只需要创建、删除或读取文件即可，他们并不需要关注磁盘空间的细节，所有磁盘空间管理相关的动作交由文件系统来处理。\n相关概念  目录：在文件系统中目录是一种容器，它可以容纳子目录和普通文件。但同时目录本身也是一种文件。只不过目录中存储的数据是特殊的数据，这些数据就是关于文件名称等元数据（管理数据的数据）的信息。 文件：在文件系统中，最基本的概念是文件，文件是存储数据的实体。从用户的角度来看，文件是文件系统中最小粒度的单元。为了便于用户对文件进行识别和管理，文件系统为每个文件都分配了一个名称，称为文件名。同时为了方便用户辨别文件的类型，每个文件都会有一个拓展名来标识其类型。（但在文件系统中都以字节流存储） 链接：Linux 中的链接分为软链接（Soft Link）和硬链接（Hard Link）两种。其中，软链接又被称为符号链接（Symbolic Link），它是文件的另外一种形态，其内容指向另外一个文件路径（相对路径或绝对路径）。硬链接则不同，它是一个已经存在文件的附加名称，也就是同一个文件的第 2 个或第 N 个名称。  基本原理  文件系统的主要核心就是对下实现了对磁盘空间的管理，对上为应用程序呈现层级数据组织形式和统一的访问接口。\n 在文件系统中，为了简化用户对数据的访问，向用户屏蔽数据的存储方式，其会对磁盘空间进行规划、组织和编号处理。\n以 Ext4 文件系统为例，它会将磁盘空间进行划分。 首先将磁盘空间划分为若干个子空间，这些子空间称为块组。然后将每个子空间划分为等份的逻辑块，逻辑块是最小的管理单元。\n为了管理这些逻辑块，需要一些区域来记录哪些逻辑块已经被使用了，哪些还没有被使用。记录这些数据的数据通常在磁盘的特殊区域，我们称这些数据为文件系统的元数据（Metadata）。通过元数据，文件系统实现了对磁盘空间的管理，最终为用户提供了简单易用的接口。\n当我们调用这些接口时，用户对文件的操作就转化为文件系统对磁盘空间的操作。 比如，当用户向某个文件写入数据时，文件系统会将该请求转换为对磁盘的操作，包括分配磁盘空间、写入数据等。而对文件的读操作则转换为定位到磁盘的某个位置、从磁盘读取数据等。\n分类 目前，常见的文件系统有几十种。虽然文件系统的具体实现形式纷繁复杂，具体特性也各不相同，但是有一定规律可循。下面将介绍一下常见的文件系统都有哪些。\n本地文件系统 本地文件系统是对磁盘空间进行管理的文件系统，也是最常见的文件系统形态。从呈现形态上来看，本地文件系统就是一个树形的目录结构。本地文件系统本质上就是实现对磁盘空间的管理，实现磁盘线性空间与目录层级结构的转换。\n从普通用户的角度来说，本地文件系统主要方便了对磁盘空间的使用，降低了使用难度，提高了利用效率。常见的本地文件系统有 ExtX、Btrfs、XFS 和 ZFS 等。\n伪文件系统 伪文件系统是 Linux 中的概念，它是对传统文件系统的延伸。伪文件系统存在于内存，不占用硬盘。它以文件系统的形态实现用户与内核数据交互的接口，其通过文件的形式向用户提供一些系统信息，用户通过读写这些文件就可以读取、修改系统的一些信息。 常见的伪文件系统有 proc、sysfs 和 configfs 等。\n例如我们在 Linux 中通常用来检测性能的一些工具（如 iostat、top 等），其本质上就是通过访问 /proc/cpuinfo、/proc/diskstats、/proc/meminfo 等文件来获取内核信息。\n网络文件系统 网络文件系统（Network File System）是基于 TCP/IP 协议（整个协议可能会跨层）的文件系统，它可以将远端服务器文件系统的目录挂载到本地文件系统的目录上，允许用户或者应用程序像访问本地文件系统的目录结构一样，访问远端服务器文件系统的目录结构，而无需理会远端服务器文件系统和本地文件系统的具体类型，非常方便地实现了目录和文件在不同机器上进行共享。\n网络文件系统通常分为客户端和服务端，其中客户端类似本地文件系统，而服务端则是对数据进行管理的系统。网络文件系统的使用与本地文件系统的使用没有任何差别，只需要执行 mount 命令挂载远端文件系统即可。常见的网络文件系统如 NFS、SMB 等。\n集群文件系统 集群文件系统（Clustered File System）是指运行在多台计算机之上，之间通过某种方式相互通信从而将集群内所有存储空间资源整合、虚拟化并对外提供文件访问服务的文件系统。\n其本质上还是一种本地文件系统，但与 NTFS、EXT 等本地文件系统的目的不同，它通常构建在基于网络的 区域存储网络（SAN） 设备上，且在多个节点中共享 SAN 磁盘。前者是为了扩展性，后者运行在单机环境，纯粹管理块和文件之间的映射以及文件属性。\n集群文件系统最大的特点是可以实现客户端节点对磁盘介质的共同访问，且视图具有一致性。其访问模式如下图：\n其最大的特点是多个节点可以同时为应用层提供文件系统服务，特别适合用于业务多活的场景，通过集群文件系统提供高可用集群机制，避免因为宕机造成服务失效。\n分布式文件系统 分布式文件系统（Distributed File System）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点（可简单的理解为一台计算机）相连；或是若干不同的逻辑磁盘分区或卷标组合在一起而形成的完整的有层次的文件系统。\n从本质上来说，分布式文件系统其实也是网络文件系统的一种，其与网络文件系统的差异在于服务端包含多个节点，也就是服务端是可以横向扩展的。\n","date":"2022-05-23T18:52:13+08:00","permalink":"https://blog.orekilee.top/p/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-%E5%9F%BA%E7%A1%80/","title":"文件系统 基础"},{"content":"Git 分支 什么是分支？ Git 保存的不是文件的变化或者差异，而是一系列不同时刻的文件快照。 在进行提交操作时，Git 会保存一个提交对象，该提交对象会包含一个指向暂存内容快照的指针，以及作者的姓名和邮箱、提交时输入的信息以及指向它的父对象的指针。\n 首次提交产生的提交对象没有父对象，普通提交操作产生的提交对象有一个父对象，而由多个分支合并产生的提交对象有多个父对象，\n 假设现在有一个工作目录，里面包含了三个将要被暂存和提交的文件。暂存操作会为每一个文件计算校验和，然后会把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 对象来保存它们），最终将校验和加入到暂存区域等待提交。\n当进行提交操作时，Git 会先计算每一个子目录的校验和，然后将 Git 仓库中这些校验和保存为树对象。 随后 Git 便会创建一个提交对象，它不仅包含上面提到的那些信息，还包含指向这个树对象（项目根目录）的指针。如此一来，Git 就可以在需要的时候重现此次保存的快照。\n如下图，此时Git 仓库中有五个对象：三个 blob 对象（保存着文件快照）、一个树对象（记录着目录结构和 blob 对象索引）以及一个提交对象（包含着指向前述树对象的指针和所有提交信息）。\n做些修改后再次提交，那么这次产生的提交对象会包含一个指向上次提交对象（父对象）的指针。\nGit 的分支，其实本质上仅仅是指向提交对象的可变指针。 Git 的默认分支名字是 master。 在多次提交操作之后， master 分支会在每次的提交操作中自动向前移动，并指向最后一个提交对象。\n分支管理 查看分支 当我们使用 git branch 命令，并不加任何参数时，就可以得到当前所有分支的列表：\n1 2 3  $ git branch master * test   ps：* 代表当前所在的分支，即当前 HEAD 指针所指向的分支。\n我们可以在后面加上这两个参数，来获取到这个列表中已经合并或尚未合并到当前分支的分支。\n1 2  git branch --merged # 查看哪些分支已经合并到当前分支 git branch --no-merged # 查看所有包含未合并工作的分支   我们还可以通过 -v 选项，查看每一个分支的最后一次提交：\n1 2 3  $ git branch -v master f876558 [ahead 1, behind 1] test * test da8f5f4 commit   创建分支 当我们要创建一个分支时，可以使用以下命令：\n1  git branch [branch_name]   例如我们创建一个 testing 分支，这会在当前所在的提交对象上创建一个指针。\n 那么，Git 又是怎么知道当前在哪一个分支上呢？ 也很简单，它有一个名为 HEAD 的特殊指针。\n 此时指针关系如下图：\n想要新建一个分支并同时切换到那个分支上，你可以运行一个带有 -b 参数的 git checkout 命令：\n1 2 3 4  git checkout -b [branch_name] 等价于 git branch [branch_name] git checkout [branch_name]   切换分支 当我们要切换到一个已经存在的分支上时，可以使用下面这个命令将 HEAD 指向该分支：\n1  git checkout [branch_name]   例如我们切换到 testing 分支上，此时指向关系如下图：\n如果我们在这个新分支上做了修改，并将修改提交后，此时 testing 分支就会向前移动，如下图：\n此时由于两个分支的版本不同，当我们再次切换回 master 分支时，此时会执行两个操作：\n 使 HEAD 指向 master。 将工作目录恢复成 master 分支所指向的快照内容。  如果我们再次在 master 分支上进行修改后提交，此时两个分支就会完全分叉，走上不同的路线，如下图：\n此时我们就需要通过 merge 来对分支进行合并，使其重新回到统一的一个版本。\n合并分支 当我们在特性分支上完成开发后，此时就需要将特性分支合并入 master 分支。此时我们就需要先切换到主分支，在主分支上执行 git merge：\n1 2  git checkout master git merge [branch]   Git 会自行决定选取哪一个提交作为最优的共同祖先，并以此作为合并的基础。接着它会把两个分支的最新快照以及二者最近的共同祖先进行三方合并，合并的结果是生成一个新的快照（并提交）。\n但事实上，只有在这些分支没有任何冲突时才能这么顺利的进行合并，如果存在冲突，我们必须要将冲突的内容解决后，才能进行合并。\n合并冲突解决 如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法合并它们。此时 Git 会暂停下来，等待你去解决合并产生的冲突。\n1 2 3 4 5  $ git merge test Removing test.md Auto-merging README.md CONFLICT (content): Merge conflict in README.md Automatic merge failed; fix conflicts and then commit the result.   此时我们可以使用 git status 命令来查看那些因包含合并冲突而处于未合并状态的文件，并查看它的内容：\n1 2 3 4 5  \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD Hello world123123:` ======= HELLO WORLD !!! \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; test   此时 Git 用 ======= 分割了两个分支的冲突的内容，\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; 后面显示了 HEAD 所指向的版本，\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 后面展示了 test 指向的版本。此时我们就需要进行权衡，选择应该怎么样去解决冲突。\n当我们将 =======、\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;、\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 删除后，并对冲突内容进行解决。此时我们可以对每个文件使用 git add 命令来将其标记为冲突已解决。 一旦暂存这些原本有冲突的文件，Git 就会将它们标记为冲突已解决。此时我们再进行 git commit，就会自动完成 merge 操作。\n删除分支 当我们想要删除某个分支的时候，可以使用下面这个命令：\n1  git branch -d [branch name]   如果该分支还未被 merge，则此时会报错\n1 2 3  $ git branch -d testing error: The branch \u0026#39;testing\u0026#39; is not fully merged. If you are sure you want to delete it, run \u0026#39;git branch -D testing\u0026#39;.   如果仍然需要删除，则可以使用 -D 选项强制删除分支。\n分支开发的工作流程 长期分支 在项目的开发过程中，为了维护不同层次的稳定性，通常会拥有几个长期存在的分支。大多数人采用下面这种开发模式，只在 master 分支上保留完全稳定的代码，而在 develop、next 等分支上进行后续的开发，当这些分支通过测试，达到稳定状态时就可以将它们并入到 master 分支中。\n如上图，稳定分支的指针总是在提交历史中落后一大截，而前沿分支的指针往往比较靠前。\n特性分支 特性分支是一种短期分支，它被用来实现单一特性或其相关工作。例如我们需要快速实现一个新功能，或者修复一些遗留的问题，就可以在特性分支上进行开发，然后将其并入主分支后再将其删除。\n由于工作被分散到不同的流水线中，在不同的流水线中每个分支都仅与其目标特性相关，因此，在做代码审查之类的工作的时候就能更加容易地看出你做了哪些改动。 我们可以把做出的改动在特性分支中保留几分钟、几天甚至几个月，等它们成熟之后再合并，而不用在乎它们建立的顺序或工作进度。对于不想要的分支，也可以直接将其抛弃，并且不会造成任何影响。\n远程分支 远程引用是对远程仓库的引用（指针），包括分支、标签等等。 可以通过 git ls-remote (remote) 来显式地获得远程引用的完整列表，或者通过 git remote show (remote) 获得远程分支的更多信息。 然而，一个更常见的做法是利用远程跟踪分支。\n远程跟踪分支是远程分支状态的引用。 它们是你不能移动的本地引用，当你做任何网络通信操作时，它们会自动移动。 远程跟踪分支像是你上次连接到远程仓库时，那些分支所处状态的书签。它们以 (remote)/(branch) 形式命名。\n如上图，例如我们从 Git 服务器上 git clone 一个仓库时，会为自动将其该仓库命名为 origin，拉取它的所有数据，创建一个指向它的 master 分支的指针，并且在本地将其命名为 origin/master。 Git 也会给你一个与 origin 的 master 分支在指向同一个地方的本地 master 分支，这样我们就可以在本地进行开发工作。\n推送 当你想要公开分享一个分支时，需要将其推送到有写入权限的远程仓库上，此时可以使用以下命令：\n1 2 3  git push [remote] [branch] # 如果想要修改远程仓库的分支名，可以使用下面这个命令 git push [remote] [local_branch_name]:[remote_branch_name]   跟踪 从一个远程跟踪分支检出一个本地分支会自动创建一个跟踪分支。 跟踪分支是与远程分支有直接关系的本地分支。 如果在一个跟踪分支上输入 git pull，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。（例如当克隆一个仓库时，自动创建的跟踪 origin/master 的 master 分支）。\n如果我们想要设置一个新的跟踪分支，可以使用下面这个命令：\n1  git checkout -b [branch] [remote]/[branch]   如果我们想要让已有的本地分支跟踪一个远程分支，或者是修改已有分支的跟踪目标，就可以使用下面这个命令：\n1 2 3  git branch -u [remote]/[branch] 或者 git branch --set-upstream-to [remote]/[branch]   如果我们想查看设置的所有跟踪分支，可以使用下面这个命令：\n1  git branch -vv   这些数字的值来自于你从每个服务器上最后一次抓取的数据。 这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。\n1 2  git fetch --all git branch -vv   拉取 如果我们想要同步远程分支上的数据时，可以使用下面两种方式抓取本地没有的数据，并且更新本地数据库，移动 remote/branch 指针指向新的、更新后的位置。\n  git fetch ：该命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。 它只会获取数据然后让你自己使用 git merge 进行合并。\n1 2 3 4 5 6 7 8  # 下载远端分支到本地 git fetch [remote] [remote_branch]:[local_branch] # 比较差异 git diff local_branch # 合并分支 git merge local_branch # 旧删除分支 git branch -d local_branch     git pull：它其实相当于 git fetch + git merge，它会查找当前分支所跟踪的服务器与分支，从服务器上抓取数据然后尝试合并入那个远程分支。\n1  git pull [remote] [remote_branch]:[local_branch]     虽然 git pull 简化了流程，但由于 git pull 会对本地工作目录造成修改，所以通常都会使用 git fetch 来拉取分支。\n删除 当我们在某个分支上已经完成了开发，并将其合并到了远程仓库的 master 分支后，此时该分支就失去了作用，可以将其从服务器上删除。此时我们可以使用带有 --delete 选项的 git push 命令来删除一个远程分支：\n1  git push [remote] --delete [branch]   这个命令做的只是从服务器上移除这个指针。 Git 服务器通常会保留数据一段时间直到垃圾回收运行，所以如果不小心删除掉了，通常是很容易恢复的。\nRebase 在 Git 中整合不同分支的方法除了 merge 以外，还有 rebase。\n什么是 Rebase？ 例如下图这个场景，此时出现了两个不同的分支，并且各自又提交了更新，导致出现了分叉：\n如果我们要整合这两个分支，最简单的方法就是使用 merge，它会两个分支的最新快照（C3 和 C4）以及二者最近的共同祖先（C2）进行三方合并，合并的结果是生成一个新的快照（并提交）。\n但除此之外，还有另一种方法，即提取在 C4 中引入的补丁和修改，然后在 C3 的基础上应用一次。在 Git 中，这种操作就叫做 rebase（变基）。 你可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上。\n1 2 3  git rebase [topicbranch] # 此时basebranch为当前HEAD指向的分支 or git rebase [basebranch] [topicbranch]   rebase 的原理是首先找到这两个分支的最近共同祖先，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底，最后以此将之前另存为临时文件的修改依序应用。\n例如执行下面这个命令：\n1 2 3 4  git checkout experiment git rebase master 或者 git rebase master experiment # 等价于前两句，将master变基到experiment上，避免切换分支   此时就会将 C4 中的修改变基到 C3 上，如下图：\n此时 experiment 与 master 处于同一条之间中，我们回到 master 分支上执行一次快进合并，即可再次回到一条直线：\n1 2  git checkout master git merge experiment   此时就已经完成了分支的合并，不仅和 merge 的效果相同，而且还是得提交历史更加整洁，是一条没有分叉的直线。\n因此当我们需要确保在向远程分支推送时能保持提交历史的整洁时就可以使用 rebase，我们在自己的分支开发完毕后将代码 rebase 到 master 分支上，master 在进行快进合并即可整合分支。\n 除了直接 rebase 一个分支，rebase 还支持更细粒度的操作\n 1 2 3  git rebase --onto [branch] [from] [to] # from 待合并片段的起始commitId（不包含） # to 待合并片段的结束commitId（包含）   即取出 to 分支，找出处于 from 分支和 to 分支的共同祖先之后的修改，然后把它们在 base 分支上重放一遍。 以下图为例，执行 git rebase --onto master server client，此时会将 client 的 c8 和 c9 在 master 上重放：\nRebase 的风险 如果要使用 rebase，就必须记住这条原则：只对尚未推送或分享给别人的本地修改执行变基操作清理历史，从不对已推送至别处的提交执行变基操作。\n假设这样一个场景，我们与项目的其他成员在同一个远程仓库中进行协作开发，此时我们拉取下来项目，并在其基础上进行开发，如图：\n此时用户 A 也在该仓库进行开发，它首先提交了修改 c4 和 c5，并将它们 merge 得到 c6。此时我们使用 pull 将这些修改拉取到本地，此时提交历史如下图：\n此时用户 A 对 merge 不满意，他想通过 rebase 来使提交记录更加清晰，于是将 merge 回滚后改为使用 rebase，并使用 git push --force 强制覆盖了提交历史。此时提交记录如下：\n如果我们再次使用 pull 拉取数据，此时提交历史如下：\n此时就出现了意料之外的情况：\n 对我们而言，我们将相同的内容又合并了一次，生成了一个新的提交，并且用户 A 使用了 rebase，因此这两次提交的 log 中作者、日期、日志等信息是一模一样的。 对于用户 A 而言，如果我们此时将修改 push 到远程仓库中，又再次将用户 A 丢弃的 c4 和 c6 提交记录找回，而这又违背了用户 A 简化提交历史的初衷。   那在这种情况下，有什么方法能解决这个问题呢？\n 可以使用 git pull --rebase 命令而不是直接 git pull。 又或者可以自己手动完成这个过程，先 git fetch，再 git rebase teamone/master。但这种方法需要每一个人都执行该命令，因此我们通常把变基命令当作是在推送前清理提交使之整洁的工具，并且只在从未推送至共用仓库的提交上执行变基命令，以避免这种场景的出现。\n","date":"2022-05-23T18:39:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%88%86%E6%94%AF/","title":"Git 分支"},{"content":"Git 基础 Git 的安装与配置 这里以 CentOS 8 举例。\n首先使用 yum 安装 Git\n1  sudo yum install git   检查是否安装成功并查看版本号\n1  git --version   接着开始配置用户信息\n1 2 3 4 5  # 配置用户名 git config --global user.name \u0026#34;yourname\u0026#34; # 配置邮箱 git config --global user.email \u0026#34;yourname@email.com\u0026#34;   接着列出所有配置，查看是否配置成功\n1  git config --list   获取仓库 获取 Git 项目仓库的方法有两种：\n 在现有项目或目录下导入所有文件到 Git 中； 从一个服务器克隆一个现有的 Git 仓库。  初始化新仓库 如果打算使用当前已有的项目来初始化一个新仓库，你只需要进入该项目目录并输入：\n1  git init   该命令将创建一个名为 .git 的子目录，这个子目录含有你初始化的 Git 仓库中所有的必须文件。接下来使用 git add 来跟踪已有的文件，再使用 git commit 来进行提交，就可以完成当前仓库的构建。\n克隆已有仓库 如果想获得一份已经存在了的 Git 仓库的拷贝，可以使用下面这个命令：\n1  git clone [url] (可选，本地仓库名称)   其会在当前目录下创建一个与项目同名（默认）的目录，并在这个目录下初始化一个 .git 文件夹，从远程仓库拉取下所有数据放入 .git 文件夹，然后从中读取最新版本的文件的拷贝。此时所有的项目文件已经存放在本地仓库中，准备就绪等待后续的开发和使用。\n Git 克隆的是该 Git 仓库服务器上的几乎所有数据，而不是仅仅复制完成你的工作所需要文件。 当你执行 git clone 命令的时候，默认配置下远程 Git 仓库中的每一个文件的每一个版本都将被拉取下来。\n 记录更新至仓库 查看文件状态 当我们要查看文件的状态时，可以使用 git status 命令。例如：\n1 2 3 4 5 6 7 8 9  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) test2.cpp   我们可以看到，其说明了我们当前所处的分支，以及展示了当前未被跟踪的文件，以及暂存区中待提交的文件。\n我们还可以在后面加上 -s 或者 --short 参数来使输出更加简洁\n1 2 3 4  $ git status -s M README.md A test.cpp ?? test2.cpp   左边的状态码的含义如下；\n  新添加的未跟踪文件前面有 ?? 标记。\n  新添加到暂存区中的文件前面有 A 标记。\n  修改过的文件前面有 M 标记。出现在右边的 M 表示该文件被修改了但是还没放入暂存区，出现在靠左边的 M 表示该文件被修改了并放入了暂存区。\n  跟踪新文件  git add 是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。\n 我们可以使用命令 git add 来跟踪一个文件，例如：\n1  git add test.cpp    git add 命令使用文件或目录的路径作为参数；如果参数是目录的路径，该命令将递归地跟踪该目录下的所有文件。\n 此时用 git status 就可以看到文件已经被跟踪，并放入暂存区中：\n1 2 3 4 5  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp   暂存已修改文件 如果我们修改了一个已经被跟踪的文件，此时它的状态如下：\n1 2 3 4 5 6 7 8 9 10  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: test.cpp Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: README.md   此时文件内容发生了变化，但是还没有被放入暂存区中，此时就需要我们使用 git add 将其添加到暂存区\n1 2 3 4 5 6 7  $ git add README.md $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp   此时文件已被加入到暂存区中，在下次使用 git commit 提交时就会被添加到仓库。但是，如果我们此时修改一个暂存区中的文件，会怎么样呢？\n1 2 3 4 5 6 7 8 9 10 11  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: README.md   此时看起来像是文件同时出现在了暂存区和非暂存区，但事实是这样吗？答案是否定的，git 中有版本的概念，此时存在暂存区中的是我们上一次提交的那个版本，而此时在非暂存区中的即是最新的版本。\n此时，我们就需要再次使用 git add 来将最新的版本暂存起来\n1 2 3 4 5 6 7  $ git add README.md $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: README.md new file: test.cpp   忽略文件 文件 .gitignore 的格式规范如下：\n 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。   glob 模式是指 shell 所使用的简化了的正则表达式。\n 星号（*）匹配零个或多个任意字符； [abc] 匹配任何一个列在方括号中的字符； 问号（?）只匹配一个任意字符； 如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（*) 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。   例如一个 C++ 的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # Prerequisites *.d # Compiled Object files *.slo *.lo *.o *.obj # Precompiled Headers *.gch *.pch # Compiled Dynamic libraries *.so *.dylib *.dll # Fortran module files *.mod *.smod # Compiled Static libraries *.lai *.la *.a *.lib # Executables *.exe *.out *.app    GitHub 有一个十分详细的针对数十种项目及语言的 .gitignore 文件列表：https://github.com/github/gitignore 。\n 差异比较 git diff 用于比较已写入暂存区和已经被修改但尚未写入暂存区文件的区别。\n例如此时我们修改了一个已写入暂存区的文件，此时结果如下：\n1 2 3 4 5 6 7 8  $ git diff diff --git a/README.md b/README.md index d0e7d60..a261b14 100644 --- a/README.md +++ b/README.md @@ -1 +1 @@ -HELLO WORLD +HELLO WORLD !!!   如果要查看已暂存的将要添加到下次提交里的内容，可以用下面这些命令：\n1 2 3  git diff --cached\t// 老版本 或者 git diff --staged // 新版本   请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。\n提交更新 如果暂存区中的内容已经准备就绪了，此时就可以使用 git commit 来提交更新，例如：\n1 2  $ git commit -m \u0026#34;commit\u0026#34; # -m 用于将提交信息与命令放在同一行   提交完成后，它会告诉你当前是在哪个分支提交的，本次提交的完整 SHA-1 校验和是什么，以及在本次提交中，有多少文件修订过，多少行添加和删改过。\n1 2 3  [test da8f5f4] commit 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 test.md   跳过使用暂存区域 如果觉得每次提交前都要将已跟踪的文件存入暂存区过于麻烦，可以为 git commit 加上一个 -a 参数，例如：\n1 2 3 4 5 6 7  $ git commit -a -m README.md [test fa38827] README.md 1 file changed, 1 insertion(+) $ git status On branch test nothing to commit, working tree clean   此时 Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。\n删除文件 如果我们想要从 Git 中删除一个文件，就必须要将其从已跟踪文件清单移除，然后 commit 提交。\n我们可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。\n1 2 3 4 5 6 7 8  $ git rm test.md rm \u0026#39;test.md\u0026#39; $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) deleted: test.md    如果我们直接删除文件，会怎么样呢？\n 1 2 3 4 5 6 7 8 9  $ rm test.md $ git status On branch test Changes not staged for commit: (use \u0026#34;git add/rm \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) deleted: test.md no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;)   此时这个操作会被加入未暂存清单中，我们仍然需要使用 git rm 来进行删除。\n 如果只是想从仓库中删除文件（或者暂存区），而不想在本地删除，那该怎么做呢？\n 此时可以使用 –cached 参数：\n1  git rm --cached [file]    如果在删除前，这个文件就已经在暂存区中，那该怎么办呢？\n 1 2 3 4 5  $ git status On branch test Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: test.md   此时文件已经存储在暂存区了，我们尝试使用 git rm 删除它：\n1 2 3 4  $ git rm test.md error: the following file has changes staged in the index: test.md (use --cached to keep the file, or -f to force removal)   此时我们发现其无法直接被删除，原因是 Git 为了防止误删还没有添加到快照中的数据（这种数据无法被 Git 恢复），为其添加了安全策略，需要使用强制删除的选项 -f 才行。\n1  git rm -rf test.md   移动/重命名文件 当我们想要将一个文件移动到其他目录时，可以使用下面命令：\n1  git mv [源文件] [新路径]   同样，我们也可以使用 git mv 来完成文件的重命名，例如：\n1  git mv [原文件名] [新文件名]   那它是如何做到既能移动，又能够重命名呢？其实运行 git mv 就相当于运行了下面三条命令：\n1 2 3  mv [原文件名] [新文件名] git rm [原文件名] git add [新文件名]   此操作必须要在暂存区或者文件commit之后才能进行\n查看提交记录 当我们想要查看提交记录时，可以使用 git log 命令，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  $ git log commit af1248beeb9de2b58d339441c5fccc2cbd652374 (HEAD -\u0026gt; master, origin/master, origin/HEAD) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 11 14:43:39 2022 +0800 update commit 1801c1003b9250e7854eec43dd53c4974b5b04b2 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Sat Apr 9 18:13:42 2022 +0800 C++ 20 done commit 11ed22aab5099608427492711c560a2c967ba7d3 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Fri Apr 8 21:34:39 2022 +0800 Cpp 17 done 20 doing commit 9d519fa0ada5991fc8791012d98c736704ee3b68 Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Thu Apr 7 21:49:42 2022 +0800   我们可以看到，其默认以提交时间降序排序，同时列出了每个提交的 SHA-1 校验和、作者和邮箱、提交时间以及提交说明。\n其常用选项如下：\n   选项 说明     -p 按补丁格式显示每个更新之间的差异。   \u0026ndash;stat 显示每次更新的文件修改统计信息。   \u0026ndash;shortstat 只显示 \u0026ndash;stat 中最后的行数修改添加移除统计。   \u0026ndash;name-only 仅在提交信息后显示已修改的文件清单。   \u0026ndash;name-status 显示新增、修改、删除的文件清单。   \u0026ndash;abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。   \u0026ndash;relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。   \u0026ndash;graph 显示 ASCII 图形表示的分支合并历史。   \u0026ndash;pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。    其常用的限制输出的参数如下：\n   选项 说明     -(n) 仅显示最近的 n 条提交   \u0026ndash;since, \u0026ndash;after 仅显示指定时间之后的提交。   \u0026ndash;until, \u0026ndash;before 仅显示指定时间之前的提交。   \u0026ndash;author 仅显示指定作者相关的提交。   \u0026ndash;committer 仅显示指定提交者相关的提交。   \u0026ndash;grep 仅显示含指定关键字的提交   -S 仅显示添加或移除了某个关键字的提交    撤销操作 补充文件/信息 如果我们已经 git commit 提交之后，才发现可能有几个文件忘记提交，又或者是提交的信息写错了。这时可以使用下列命令来尝试重新提交\n1  git commit --amend   这个命令会将暂存区中的文件提交。 如果自上次提交以来你还未做任何修改，那么快照会保持不变，而你所修改的只是提交信息。而如果进行了修改，则第二次提交将代替第一次提交的结果。\n撤销暂存的文件 如果我们不小心使用 git add 将不必要的文件加入暂存区，如何将这些文件撤销呢？这时候就要用到下述命令：\n1 2 3  git reset HEAD \u0026lt;file\u0026gt;...\t// 老版本 或者 git restore --staged \u0026lt;file\u0026gt;... // 新版本   在调用时加上 \u0026ndash;hard 选项会令 git reset 成为一个危险的命令，即可能导致工作目录中所有当前进度丢失。\n撤销对文件的修改 当我们对一个文件进行修改后，如果不满意刚才的修改，像将其回滚到上次提交的状态时，可以使用如下命令：\n1 2 3  git checkout -- \u0026lt;file\u0026gt;...\t// 老版本 或者 git restore \u0026lt;file\u0026gt;... // 新版本   git checkout -- [file] 是一个危险的命令。 你对那个文件做的任何修改都会消失， 除非确定不想要那个文件，否则不要使用这个命令。\n远程仓库 查看远程仓库 如果想查看你已经配置的远程仓库服务器，可以运行 git remote 命令。 它会列出你指定的每一个远程服务器的简写。也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。例如：\n1 2 3  $ git remote -v origin https://gitee.com/HONGYU-LEE/git-tes.git (fetch) origin https://gitee.com/HONGYU-LEE/git-tes.git (push)   如果还想要查看某一个远程仓库的更多信息，可以使用 git remote show [remote-name] 命令。例如：\n1 2 3 4 5 6 7 8 9 10 11  $ git remote show origin * remote origin Fetch URL: https://gitee.com/HONGYU-LEE/git-tes.git Push URL: https://gitee.com/HONGYU-LEE/git-tes.git HEAD branch: master Remote branch: master tracked Local branch configured for \u0026#39;git pull\u0026#39;: master merges with remote master Local ref configured for \u0026#39;git push\u0026#39;: master pushes to master (local out of date)   它同样会列出远程仓库的 URL 与跟踪分支的信息，并且告诉你正处于 master 分支，并且如果运行 git pull，就会抓取所有的远程引用，然后将远程 master 分支合并到本地 master 分支。 它也会列出拉取到的所有远程引用。\n添加远程仓库并拉取数据 可以通过以下命令来添加一个新的远程 Git 仓库：\n1  git remote add \u0026lt;shortname\u0026gt; \u0026lt;url\u0026gt;    如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写。\n 从远程仓库中拉取数据 使用下面这个命令，就可以从远端仓库中获得数据：\n1  git fetch [remote-name]   这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。\n虽然 git fetch 命令会将数据拉取到你的本地仓库，但是它并不会自动合并或修改你当前的工作，所以在准备好时必须手动将其合并入你的工作。\n如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。\n推送到远程仓库 如果我们想分享项目时，必须将其推送到上游。此时可以使用下面这个命令：\n1  git push [remote-name] [branch-name]   只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。\n远端仓库移除/重命名 如果想要修改一个远程仓库的简写名，可以使用以下命令：\n1  git remote rename [old_name] [new_name]   值得注意的是这同样也会修改你的远程分支名字。 例如那些过去引用 old_name/master 的现在会引用 new_name/master。\n如果因为一些原因想要移除一个远程仓库，可以使用以下命令：\n1  git remote rm [remote-name]   标签（Tag） 与其他版本控制系统一样，Git 可以给历史中的某一个提交打上标签，以示重要。 Git 使用两种主要类型的标签：轻量标签（lightweight）与附注标签（annotated）。\n查看标签 在 Git 中列出已有的标签是非常简单直观的， 只需要输入 git tag，就会以字典序排列出所有的标签：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  $ git tag 1.21 1.22 1.23 v1.10 v1.11 v1.12 v1.13 v1.14 v1.15 v1.16 v1.17 v1.18 v1.19 v1.20 v1.3 v1.4 v1.5 v1.6 v1.7 v1.8 v1.9   也可以使用特定的模式查找标签，例如想查找 v1.10 ~ v1.13 版本的标签，则可以使用正则表达式：\n1 2 3 4 5  $ git tag -l \u0026#39;v1.1[0-3]\u0026#39; v1.10 v1.11 v1.12 v1.13   附注标签 附注标签是存储在 Git 数据库中的一个完整对象。 它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。\n 通常建议创建附注标签，这样你可以拥有以上所有信息；但是如果你只是想用一个临时的标签，或者因为某些原因不想要保存那些信息，轻量标签也是可用的。\n 它的创建也非常简单，只需要加上参数 -a 即可：\n1  $ git tag -a v1.0 -m \u0026#39;version 1.0\u0026#39;   如果是想要对过去的提交打上标签，只需要在末尾指定已提交的校验和（部分校验和也可以，会自动识别）即可：\n1  $ git tag -a v0.8 fa38827eb9d27a2559e8178aa67189105d882fca   此时查看 git tag，发现已经给那个位置追加上了标签\n1 2 3 4  $ git tag v0.8 v1.0 v1.0-test   通过使用 git show 命令可以看到标签信息与对应的提交信息：\n1 2 3 4 5 6 7 8 9 10  $ git show commit da8f5f463cad799940543c7304df4d436a9efb2d (HEAD -\u0026gt; test, tag: v1.0) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 18 17:35:08 2022 +0800 commit diff --git a/test.md b/test.md deleted file mode 100644 index e69de29..0000000   输出显示了打标签者的信息、打标签的日期时间、附注信息，然后显示具体的提交信息。\n轻量标签 轻量标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。 本质上是将提交校验和存储到一个文件中（没有保存任何其他信息）。\n创建一个轻量标签时只需要使用 git tag，而不需要加别的参数，例如：\n1 2 3 4 5 6 7 8 9 10 11  $ git tag v1.0-test $ git show commit da8f5f463cad799940543c7304df4d436a9efb2d (HEAD -\u0026gt; test, tag: v1.0-test, tag: v1.0) Author: HONGYU-LEE \u0026lt;756687451@qq.com\u0026gt; Date: Mon Apr 18 17:35:08 2022 +0800 commit diff --git a/test.md b/test.md deleted file mode 100644 index e69de29..0000000   此时不会看到额外的标签信息， 命令只会显示出提交信息。\n发布标签 默认情况下，git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。\n1  git push origin [tagname]   如果想要一次性推送很多标签，也可以使用带有 --tags 选项的 git push 命令。 这将会把所有不在远程仓库服务器上的标签全部传送到那里。\n1  git push origin --tags   Tag 与 Commit Git 的标签虽然是版本库的快照，但其实它就是指向某个 commit 的指针，但是它与 commit 又有些不同（分支可以移动，标签不能移动）。\n那么为什么有了 commit 后，还要引入 tag 呢？\n \u0026ldquo;请把上周一的那个版本打包发布，commit号是6a5819e…\u0026rdquo;\n\u0026ldquo;一串乱七八糟的数字不好找！\u0026rdquo;\n如果换一个办法：\n\u0026ldquo;请把上周一的那个版本打包发布，版本号是v1.2\u0026rdquo;\n\u0026ldquo;好的，按照tag v1.2查找commit就行！\u0026rdquo;\n所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。\n ","date":"2022-05-23T18:38:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","title":"Git 基本操作"},{"content":"Git 介绍 Git 是什么？ Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。也是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。\n与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。\n版本控制 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。\n本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。 这么做唯一的好处就是简单，但是特别容易犯错。有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。\n为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。\n这时人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 为了解决这个问题，集中化的版本控制系统（Centralized Version Control Systems，CVCS）应运而生。\n集中化版本控制系统 这类系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 多年以来，这已成为版本控制系统的标准做法。\n但此时人们发现，集中化的版本控制系统存在单点问题，如果中央服务器宕机，则在宕机的这段时间所有人都无法提交更新，也就无法协同工作。而如果数据库发生损坏，而有没有及时备份时，所有的数据都将会丢失，只剩下人们在各自机器上保留的单独快照。\n为了解决这个问题，分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。\n分布式版本控制系统 在这类系统中，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。\n更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。 你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。\nGit 的特性   直接记录快照，而非差异比较：Git 把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。\n  近乎所有操作都是本地执行：在 Git 中的绝大多数操作都只需要访问本地文件和资源，一般不需要来自网络上其它计算机的信息。\n  完整性保证：Git 中所有数据在存储前都计算校验和（SHA-1），然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。\n  一般只添加数据：你执行的 Git 操作，几乎只往 Git 数据库中增加数据。 很难让 Git 执行任何不可逆操作，或者让它以任何方式清除数据。\n  Git 状态与执行流程 文件状态 Git 中文件有四种状态：\n 未跟踪（Untracked）：标识未被纳入版本控制的文件，它们既不存在于上次快照的记录中，也没有放入暂存区。除此状态之外的所有状态都是已跟踪。 未修改（Unmodified）：表示数据已经安全的提交，并且从上次提交到现在都没有被修改过。 已修改（Modified）：表示自上次提交后修改了文件，但还没保存到数据库中。 已暂存（Staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。  状态转移图如下：\n工作区域 Git 中存在三个工作区域：Git 仓库、工作目录以及暂存区域。\n  Repository（Git 仓库）：是 Git 用来保存项目的元数据和对象数据库的地方。 这是 Git 中最重要的部分，从其它计算机克隆仓库时，拷贝的就是这里的数据。\n  Working Directory（工作目录）：是对项目的某个版本独立提取出来的内容。 这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。\n  Staging Area（暂存区）：是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。 有时候也被称作`‘索引’\u0026rsquo;，不过一般说法还是叫暂存区域。\n  工作流程 对应上面提到的状态，我们来根据一个 Git 文件的工作流程，来分析一个 Git 文件的状态变化以及所处区域：\n 当我们在工作目录下创建一个新文件，此时文件的状态是未跟踪。 我们使用 git add 将文件加入跟踪列表，此时文件的状态变为已暂存，同时被放入暂存区中。 此时我们使用 git commit 将文件进行提交，此时文件被保存到 Git 仓库中，状态变化为未修改。 此时我们再次修改文件，文件状态变为已修改，同时处于未暂存区（工作目录）。  Git VS SVN 我们常用 SVN 与其进行对比，那么它们有什么区别呢？\n Git 是分布式的，SVN 不是：这是 Git 和其它非分布式的版本控制系统，例如 SVN，CVS 等，最核心的区别。 Git 把内容按元数据方式存储，而 SVN 是按文件：所有的资源控制系统都是把文件的元信息隐藏在一个类似 .svn、.cvs 等的文件夹里。 Git 分支和 SVN 的分支不同：分支在 SVN 中一点都不特别，其实它就是版本库中的另外一个目录。 Git 没有一个全局的版本号，而 SVN 有：目前为止这是跟 SVN 相比 Git 缺少的最大的一个特征。 Git 的内容完整性要优于 SVN：Git 的内容存储使用的是 SHA-1 哈希算法。这能确保代码内容的完整性，确保在遇到磁盘故障和网络问题时降低对版本库的破坏。  ","date":"2022-05-23T18:34:13+08:00","permalink":"https://blog.orekilee.top/p/git-%E5%9F%BA%E7%A1%80/","title":"Git 基础"},{"content":"Distributed 引擎 Distributed 表引擎是分布式表的代名词，它自身不存储任何数据，而是作为数据分片的透明代理，能够自动路由数据至集群中的各个节点，所以 Distributed 表引擎需要和其他数据表引擎一起协同工作。\nClickHouse 并不像其他分布式系统那样，拥有高度自动化的分片功能。ClickHouse 提供了**本地表（Local Table）与分布式表（Distributed Table）**的概念\n 本地表：通常以 _local 为后缀进行命名。本地表是承接数据的载体，可以使用非 Distributed 的任意表引擎，一张本地表对应了一个数据分片。 分布式表：通常以 _all 为后缀进行命名。分布式表只能使用 Distributed 表引擎，它与本地表形成一对多的映射关系，日后将通过分布式表代理操作多张本地表。  分布式写入流程 在向集群内的分片写入数据时，通常有两种思路\n  借助外部计算系统，事先将数据均匀分片，再借由计算系统直接将数据写入 ClickHouse 集群的各个本地表。\n  通过 Distributed 表引擎代理写入分片数据。\n  第一种方案通常拥有更好的写入性能，因为分片数据是被并行点对点写入的。但是这种方案的实现主要依赖于外部系统，而不在于 ClickHouse 自身，所以这里主要会介绍第二种思路。为了便于理解整个过程，这里会将分片写入、副本复制拆分成两个部分进行讲解。\n数据写入分片   在第一个分片节点写入本地分片数据：首先在 CH5 节点，对分布式表 test_shard_2_all 执行 INSERT，尝试写入 10、30、200 和 55 四行数据。执行之后分布式表主要会做两件事情：\n  根据分片规则划分数据\n  将属于当前分片的数据直接写入本地表 test_shard_2_local。\n    第一个分片建立远端连接，准备发送远端分片数据：将归至远端分片的数据以分区为单位，分别写入 /test_shard_2_all存储目录下的临时 bin 文件，接着，会尝试与远端分片节点建立连接。\n  第一个分片向远端分片发送数据：此时，会有另一组监听任务负责监听 /test_shard_2_all 目录下的文件变化，这些任务负责将目录数据发送至远端分片，其中，每份目录将会由独立的线程负责发送，数据在传输之前会被压缩。\n  第二个分片接收数据并写入本地：CH6 分片节点确认建立与 CH5 的连接，在接收到来自 CH5 发送的数据后，将它们写入本地表。\n  由第一个分片确认完成写入：最后，还是由 CH5 分片确认所有的数据发送完毕。\n  可以看到，在整个过程中，Distributed 表负责所有分片的写入工作。本着谁执行谁负责的原则，在这个示例中，由 CH5 节点的分布式表负责切分数据，并向所有其他分片节点发送数据。\n在由 Distributed 表负责向远端分片发送数据时，有异步写和同步写两种模式：如果是异步写，则在 Distributed 表写完本地分片之后，INSERT 查询就会返回成功写入的信息；如果是同步写，则在执行 INSERT 查询之后，会等待所有分片完成写入。\n副本复制数据 如果在集群的配置中包含了副本，那么除了刚才的分片写入流程之外，还会触发副本数据的复制流程。数据在多个副本之间，有两种复制实现方式：\n  Distributed 表引擎：副本数据的写入流程与分片逻辑相同，所以 Distributed 会同时负责分片和副本的数据写入工作。但在这种实现方案下，它很有可能会成为写入的单点瓶颈，所以就有了接下来将要说明的第二种方案。\n  ReplicatedMergeTree 表引擎：如果使用 ReplicatedMergeTree 作为本地表的引擎，则在该分片内，多个副本之间的数据复制会交由 ReplicatedMergeTree 自己处理，不再由 Distributed 负责，从而为其减负。\n  分布式查询流程 与数据写入有所不同，在面向集群查询数据的时候，只能通过 Distributed 表引擎实现。当 Distributed 表接收到SELECT查询的时候，它会依次查询每个分片的数据，再合并汇总返回，流程如下：\n多副本的路由规则 在查询数据的时候，如果集群中的某一个分片有多个副本，此时 Distributed 引擎就会通过负载均衡算法从众多的副本中选取一个，负载均衡算法有以下四种。\n在 ClickHouse 的服务节点中，拥有一个全局计数器errors_count，当服务发生任何异常时，该计数累积加1。\n random（默认）：random 算法会选择errors_count 错误数量最少的副本，如果多个副本的errors_count计数相同，则在它们之中随机选择一个。 nearest_hostname：nearest_hostname 可以看作 random 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个 replica 的errors_count计数相同，则选择集群配置中 host 名称与当前 host 最相似的一个。而相似的规则是以当前 host 名称为基准按字节逐位比较，找出不同字节数最少的一个。 in_order：in_order 同样可以看作 random 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个副本的errors_count计数相同，则按照集群配置中 replica 的定义顺序逐个选择。 first_or_random：first_or_random 可以看作 in_order 算法的变种，首先它会选择errors_count错误数量最少的副本，如果多个副本的errors_count计数相同，它首先会选择集群配置中第一个定义的副本，如果该副本不可用，则进一步随机选择一个其他的副本。  多分片查询的流程 分布式查询与分布式写入类似，同样本着谁执行谁负责的原则，它会由接收SELECT查询的 Distributed 表，并负责串联起整个过程。首先它会将针对分布式表的 SQL 语句，按照分片数量将查询拆分成若干个针对本地表的子查询，然后向各个分片发起查询，最后再汇总各个分片的返回结果。\n1 2 3 4 5  --查询分布式表 SELECT*FROMdistributor_table--转换为查询本地表，并将该命令推送到各个分片节点上执行 SELECT*FROMlocal_table  如下图\n 查询各个分片数据：One 和 Remote 步骤是并行执行的，它们分别负责了本地和远端分片的查询动作。 合并返回结果：多个分片数据均查询返回后，在执行节点将所有数据union合并  使用 Global 优化分布式子查询 如果现在有一项查询需求，例如要求找到同时拥有两个仓库的用户，应该如何实现？对于这类交集查询的需求，可以使用IN子查询，此时你会面临两难的选择：IN查询的子句应该使用本地表还是分布式表？（使用JOIN面临的情形与IN类似）。\n使用本地表的问题（可能查询不到结果） 如果在IN查询中使用本地表时，如下列语句\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidIN(SELECTidFROMlocal_tableWHERErepo=200)  在实际执行时，分布式表在接收到查询后会将上述 SQL 替换成本地表的形式，再发送到每个分片进行执行，此时，每个分片上实际执行的是以下语句\n1  SELECTuniq(id)FROMlocal_tableWHERErepo=100ANDidIN(SELECTidFROMlocal_tableWHERErepo=200)  那么此时查询的最终结果就有可能是错误的，因为在单个分片上只保存了部分的数据，这就导致该 SQL 语句可能没有匹配到任何数据，如下图\n使用分布式表的问题（查询请求被放大 N^2 倍，N 为节点数量） 如果在 IN 查询中使用本地表时，如下列语句\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidIN(SELECTidFROMdistributed_tableWHERErepo=200)  对于此次查询，每个分片节点不仅需要查询本地表，还需要再次向其他的分片节点再次发起远端查询，如下图\n因此可以得出结论，在 IN 查询子句使用分布式表的时候，虽然查询的结果得到了保证，但是查询请求会被放大 N 的平方倍，其中 N 等于集群内分片节点的数量，假如集群内有 10 个分片节点，则在一次查询的过程中，会最终导致 100 次的查询请求，这显然是不可接受的。\n使用 GLOBAL 优化查询 为了解决查询放大的问题，我们可以使用 GLOBAL IN 或 GLOBAL JOIN 进行优化，下面就简单介绍一下 GLOBAL 的执行流程\n1  SELECTuniq(id)FROMdistributed_tableWHERErepo=100ANDidGLOBALIN(SELECTidFROMdistributed_tableWHERErepo=200)  如上图，主要有以下五个步骤\n 将IN子句单独提出，发起了一次分布式查询。 将分布式表转 local 本地表后，分别在本地和远端分片执行查询。 将IN子句查询的结果进行汇总，并放入一张临时的内存表进行保存。 将内存表发送到远端分片节点。 将分布式表转为本地表后，开始执行完整的 SQL 语句，IN 子句直接使用临时内存表的数据。  在使用 GLOBAL 修饰符之后，ClickHouse 使用内存表临时保存了 IN 子句查询到的数据，并将其发送到远端分片节点，以此到达了数据共享的目的，从而避免了查询放大的问题。由于数据会在网络间分发，所以需要特别注意临时表的大小，IN 或者 JOIN 子句返回的数据不宜过大。如果表内存在重复数据，也可以事先在子句 SQL 中增加 DISTINCT 以实现去重。\n","date":"2022-05-23T18:27:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86distributed%E5%BC%95%E6%93%8E/","title":"ClickHouse 分布式原理：Distributed引擎"},{"content":"ReplicatedMergeTree引擎 ReplicatedMergeTree是MergeTree的派生引擎，它在MergeTree的基础上加入了分布式协同的能力，只有使用了ReplicatedMergeTree复制表系列引擎，才能应用副本的能力。或者用一种更为直接的方式理解，即使用ReplicatedMergeTree的数据表就是副本。\n在MergeTree中，一个数据分区由开始创建到全部完成，会历经两类存储区域。\n 内存：数据首先会被写入内存缓冲区。 本地磁盘：数据接着会被写入tmp临时目录分区，待全部完成后再将临时目录重命名为正式分区。  ReplicatedMergeTree在上述基础之上增加了ZooKeeper的部分，它会进一步在ZooKeeper内创建一系列的监听节点，并以此实现多个实例之间的通信。在整个通信过程中，ZooKeeper并不会涉及表数据的传输。\n特点 作为数据副本的主要实现载体，ReplicatedMergeTree在设计上有一些显著特点。\n  依赖ZooKeeper：在执行INSERT和ALTER查询的时候，ReplicatedMergeTree需要借助ZooKeeper的分布式协同能力，以实现多个副本之间的同步。但是在查询副本的时候，并不需要使用ZooKeeper。\n  表级别的副本：副本是在表级别定义的，所以每张表的副本配置都可以按照它的实际需求进行个性化定义，包括副本的数量，以及副本在集群内的分布位置等。\n  多主架构（Multi Master）：可以在任意一个副本上执行INSERT和ALTER查询，它们的效果是相同的。这些操作会借助ZooKeeper的协同能力被分发至每个副本以本地形式执行。\n  Block数据块：在执行INSERT命令写入数据时，会依据max_insert_block_size的大小（默认1048576行）将数据切分成若干个Block数据块。所以Block数据块是数据写入的基本单元，并且具有写入的原子性和唯一性。\n  原子性：在数据写入时，一个Block块内的数据要么全部写入成功，要么全部失败。\n  唯一性：在写入一个Block数据块的时候，会按照当前Block数据块的数据顺序、数据行和数据大小等指标，计算Hash信息摘要并记录在案。在此之后，如果某个待写入的Block数据块与先前已被写入的Block数据块拥有相同的Hash摘要（Block数据块内数据顺序、数据大小和数据行均相同），则该Block数据块会被忽略。\n  数据结构 ZooKeeper内的节点结构 ReplicatedMergeTree需要依靠ZooKeeper的事件监听机制以实现各个副本之间的协同。所以，在每张ReplicatedMergeTree表的创建过程中，它会以zk_path为根路径，在Zoo-Keeper中为这张表创建一组监听节点。按照作用的不同，监听节点可以大致分成如下几类：\n 元数据  /metadata：保存元数据信息，包括主键、分区键、采样表达式等。 /columns：保存列字段信息，包括列名称和数据类型。 /replicas：保存副本名称，对应设置参数中的replica_name。   判断标识  /leader_election：用于主副本的选举工作，主副本会主导MERGE和MUTATION操作（ALTER DELETE和ALTER UPDATE）。这些任务在主副本完成之后再借助ZooKeeper将消息事件分发至其他副本。 /blocks：记录Block数据块的Hash信息摘要，以及对应的partition_id。通过Hash摘要能够判断Block数据块是否重复；通过partition_id，则能够找到需要同步的数据分区。 /block_numbers：按照分区的写入顺序，以相同的顺序记录partition_id。各个副本在本地进行MERGE时，都会依照相同的block_numbers顺序进行。 /quorum：记录quorum的数量，当至少有quorum数量的副本写入成功后，整个写操作才算成功。quorum的数量由insert_quorum参数控制，默认值为0。   操作日志  /log：常规操作日志节点（INSERT、MERGE和DROP PARTITION），它是整个工作机制中最为重要的一环，保存了副本需要执行的任务指令。log使用了ZooKeeper的持久顺序型节点，每条指令的名称以log-为前缀递增，例如log-0000000000、log-0000000001等。每一个副本实例都会监听/log节点，当有新的指令加入时，它们会把指令加入副本各自的任务队列，并执行任务。 /mutations：MUTATION操作日志节点，作用与log日志类似，当执行ALERT DELETE和ALERTUPDATE查询时，操作指令会被添加到这个节点。mutations同样使用了ZooKeeper的持久顺序型节点，但是它的命名没有前缀，每条指令直接以递增数字的形式保存，例如0000000000、0000000001等。 /replicas/{replica_name}/*：每个副本各自的节点下的一组监听节点，用于指导副本在本地执行具体的任务指令，其中较为重要的节点有如下几个：  /queue：任务队列节点，用于执行具体的操作任务。当副本从/log或/mutations节点监听到操作指令时，会将执行任务添加至该节点下，并基于队列执行。 /log_pointer：log日志指针节点，记录了最后一次执行的log日志下标信息。 /mutation_pointer：mutations日志指针节点，记录了最后一次执行的mutations日志名称。      Entry日志对象的数据结构 ReplicatedMergeTree在ZooKeeper中有两组非常重要的父节点，那就是/log和/mutations。它们的作用犹如一座通信塔，是分发操作指令的信息通道，而发送指令的方式，则是为这些父节点添加子节点。所有的副本实例，都会监听父节点的变化，当有子节点被添加时，它们能实时感知。这些被添加的子节点在ClickHouse中被统一抽象为Entry对象，而具体实现则由LogEntry和MutationEntry对象承载，分别对应/log和/mutations节点\n LogEntry  source replica：发送这条Log指令的副本来源，对应replica_name。 type：操作指令类型，主要有get、merge和mutate三种，分别对应从远程副本下载分区、合并分区和MUTATION操作。 block_id：当前分区的BlockID，对应/blocks路径下子节点的名称。 partition_name：当前分区目录的名称。   MutationEntry  source replica：发送这条MUTATION指令的副本来源，对应replica_name。 commands：操作指令，主要有ALTER DELETE和ALTER UPDATE。 mutation_id：MUTATION操作的版本号。 partition_id：当前分区目录的ID。    副本协同的核心流程 副本协同的核心流程主要有INSERT、MERGE、MUTATION和ALTER四种，分别对应了数据写入、分区合并、数据修改和元数据修改。INSERT和ALTER是分布式执行的，借助ZooKeeper的事件通知机制，多个副本之间会自动进行有效协同，但是它们不会使用ZooKeeper存储任何分区数据。而其他操作并不支持分布式执行，包括SELECT、CREATE、DROP、RENAME和ATTACH。\n在下列例子中，使用ReplicatedMergeTree实现一张拥有1分片、1副本的数据表来分别执行INSERT、MERGE、MUTATION和ALTER操作，演示执行流程。\nINSERT 当需要在ReplicatedMergeTree中执行INSERT查询以写入数据时，即会进入INSERT核心流程，它的核心流程如下图所示\n 向副本A写入数据 由副本A推送Log日志 各个副本拉取Log日志 各个副本向远端副本发起下载请求  选择一个远端的其他副本作为数据的下载来源。远端副本的选择算法大致是这样的：  从/replicas节点拿到所有的副本节点。 遍历这些副本，选取其中一个。选取的副本需要拥有最大的log_pointer下标，并且/queue子节点数量最少。log_pointer下标最大，意味着该副本执行的日志最多，数据应该更加完整；而/queue最小，则意味着该副本目前的任务执行负担较小。     远端副本响应其它副本的数据下载 各个副本下载数据并完成本地写入  在INSERT的写入过程中，ZooKeeper不会进行任何实质性的数据传输。本着谁执行谁负责的原则，在这个案例中由CH5首先在本地写入了分区数据。之后，也由这个副本负责发送Log日志，通知其他副本下载数据。如果设置了insert_quorum并且insert_quorum\u0026gt;=2，则还会由该副本监控完成写入的副本数量。其他副本在接收到Log日志之后，会选择一个最合适的远端副本，点对点地下载分区数据。\nMERGE 当ReplicatedMergeTree触发分区合并动作时，即会进入这个部分的流程，它的核心流程如下图所示\n无论MERGE操作从哪个副本发起，其合并计划都会交由主副本来制定。\n 创建远程连接，尝试与主副本通信 主副本接收通信 由主副本制定MERGE计划并推送Log日志 各个副本分别拉取Log日志 各个副本分别在本地执行MERGE  可以看到，在MERGE的合并过程中，ZooKeeper也不会进行任何实质性的数据传输，所有的合并操作，最终都是由各个副本在本地完成的。而无论合并动作在哪个副本被触发，都会首先被转交至主副本，再由主副本负责合并计划的制定、消息日志的推送以及对日志接收情况的监控。\nMUTATION 当对ReplicatedMergeTree执行ALTER DELETE或者ALTER UPDATE操作的时候（ClickHouse把delete和update操作也加入到了alter table的范畴中，它并不支持裸的delete或者update操作），即会进入MUTATION部分的逻辑\n与MERGE类似，无论MUTATION操作从哪个副本发起，首先都会由主副本进行响应。\n 推送MUTATION日志 所有副本实例各自监听MUTATION日志 由主副本实例响应MUTATION日志并推送Log日志 各个副本实例分别拉取Log日志 各个副本实例分别在本地执行MUTATION  在MUTATION的整个执行过程中，ZooKeeper同样不会进行任何实质性的数据传输。所有的MUTATION操作，最终都是由各个副本在本地完成的。而MUTATION操作是经过/mutations节点实现分发的。CH6负责了消息的推送。但是无论MUTATION动作从哪个副本被触发，之后都会被转交至主副本，再由主副本负责推送Log日志，以通知各个副本执行最终的MUTATION逻辑。同时也由主副本对日志接收的情况实行监控。\nALTER 当对ReplicatedMergeTree执行ALTER操作进行元数据修改的时候，即会进入ALTER部分的逻辑，例如增加、删除表字段等，核心流程如下图\nALTER的流程与前几个相比简单很多，其执行过程中并不会涉及/log日志的分发，整个流程大致分成3个步骤\n 修改共享元数据 监听共享元数据变更并各自执行本地修改 确认所有副本完成修改  在ALTER整个的执行过程中，ZooKeeper不会进行任何实质性的数据传输。所有的ALTER操作，最终都是由各个副本在本地完成的。本着谁执行谁负责的原则，在这个案例中由CH6负责对共享元数据的修改以及对各个副本修改进度的监控。\n","date":"2022-05-23T18:19:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%89%AF%E6%9C%AC%E5%8D%8F%E5%90%8C%E5%8E%9F%E7%90%86replicatedmergetree%E5%BC%95%E6%93%8E/","title":"ClickHouse 副本协同原理：ReplicatedMergeTree引擎"},{"content":"MergeTree 引擎 存储结构  partition：分区目录，余下各类数据文件（primary.idx、[Column].mrk、[Column]. bin等）都是以分区目录的形式被组织存放的，属于相同分区的数据，最终会被合并到同一个分区目录，而不同分区的数据，永远不会被合并在一起。 checksums：校验文件，使用二进制格式存储。它保存了余下各类文件(primary. idx、count.txt等)的size大小及size的哈希值，用于快速校验文件的完整性和正确性。 columns.txt：列信息文件，使用明文格式存储，用于保存此数据分区下的列字段信息。 count.txt：计数文件，使用明文格式存储，用于记录当前数据分区目录下数据的总行数。 primary.idx：一级索引文件，使用二进制格式存储。用于存放稀疏索引，一张MergeTree表只能声明一次一级索引。借助稀疏索引，在数据查询的时能够排除主键条件范围之外的数据文件，从而有效减少数据扫描范围，加速查询速度。 [Column].bin：数据文件，使用压缩格式存储，用于存储某一列的数据。由于MergeTree采用列式存储，所以每一个列字段都拥有独立的.bin数据文件，并以列字段名称命名。 [Column].mrk：使用二进制格式存储。标记文件中保存了.bin文件中数据的偏移量信息。标记文件与稀疏索引对齐，又与.bin文件一一对应，所以MergeTree通过标记文件建立了primary.idx稀疏索引与.bin数据文件之间的映射关系。即首先通过稀疏索引（primary.idx）找到对应数据的偏移量信息（.mrk），再通过偏移量直接从.bin文件中读取数据。由于.mrk标记文件与.bin文件一一对应，所以MergeTree中的每个列字段都会拥有与其对应的.mrk标记文件 [Column].mrk2：如果使用了自适应大小的索引间隔，则标记文件会以．mrk2命名。它的工作原理和作用与．mrk标记文件相同。 partition.dat与minmax_[Column].idx：如果使用了分区键，例如PARTITION BY EventTime，则会额外生成partition.dat与minmax索引文件，它们均使用二进制格式存储。partition.dat用于保存当前分区下分区表达式最终生成的值；而minmax索引用于记录当前分区下分区字段对应原始数据的最小和最大值。 skp_idx_[Column].idx与skp_idx_[Column].mrk：如果在建表语句中声明了二级索引，则会额外生成相应的二级索引与标记文件，它们同样也使用二进制存储。二级索引在ClickHouse中又称跳数索引。  一级索引 稀疏索引 当我们定义主键之后，MergeTree会依据index_granularity间隔（默认8192行），为数据表生成一级索引并保存至primary.idx文件内，索引数据按照主键排序。相比使用主键定义，更为常见的简化形式是通过ORDER BY指代主键。在此种情形下，主键与ORDER BY定义相同，所以索引（primary.idx）和数据（.bin）会按照完全相同的规则排序。\n一级索引底层采用了稀疏索引来实现，从下图我们可以看出它和稠密索引的区别。\n对于稠密索引而言，每一行索引标记都会对应到具体的一行记录上。而在稀疏索引中，每一行索引标记对应的一大段数据，而不是具体的一行（他们之间的区别就有点类似mysql中innodb的聚集索引与非聚集索引）。\n稀疏索引的优势是显而易见的，它只需要使用少量的索引标记就能够记录大量数据的区间位置信息，并且数据量越大优势愈发明显。例如我们使用默认的索引粒度（8192）时，MergeTree只需要12208行索引标记就能为1亿行数据记录提供索引。由于稀疏索引占用空间小，所以primary.idx内的索引数据能够常驻内存，取用速度自然极快。\n索引粒度index_granularity 索引粒度就如同标尺一般，会丈量整个数据的长度，并依照刻度对数据进行标注，最终将数据标记成多个间隔的小段。数据以index_granularity的粒度(老版本默认8192，新版本实现了自适应粒度)被标记成多个小的区间，其中每个区间最多8192行数据，MergeTree使用MarkRange表示一个具体的区间，并通过start和end表示其具体的范围。\n如下图所示。\nindex_granularity的命名虽然取了索引二字，但它不单只作用于一级索引(.idx)，同时也会影响数据标记(.mrk)和数据文件(.bin)。因为仅有一级索引自身是无法完成查询工作的，它需要借助数据标记才能定位数据，所以一级索引和数据标记的间隔粒度相同(同为index_granularity行)，彼此对齐。而数据文件也会依照index_granularity的间隔粒度生成压缩数据块。\n索引的查询过程 索引查询其实就是两个数值区间的交集判断。其中，一个区间是由基于主键的查询条件转换而来的条件区间；而另一个区间是刚才所讲述的与MarkRange对应的数值区间。\n整个索引的查询过程可以分为三大步骤\n  生成查询条件区间： 将查询条件转换为条件区间。即便是单个值的查询条件，也会被转换成区间的形式。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  --举例-- WHEREID=\u0026#39;A000\u0026#39;=[\u0026#39;A000\u0026#39;,\u0026#39;A000\u0026#39;]WHEREID\u0026gt;\u0026#39;A000\u0026#39;=(\u0026#39;A000\u0026#39;,\u0026#39;+inf\u0026#39;)WHEREID\u0026lt;\u0026#39;A000\u0026#39;=(\u0026#39;-inf\u0026#39;,\u0026#39;A000\u0026#39;)WHEREIDLIKE\u0026#39;A000%\u0026#39;=[\u0026#39;A000\u0026#39;,\u0026#39;A001\u0026#39;)      递归交集判断： 以递归的形式，依次对MarkRange的数值区间与条件区间做交集判断。从最大的区间[A000 , +inf)开始。\n 如果不存在交集，则直接通过剪枝算法优化此整段MarkRange 如果存在交集，且MarkRange步长大于N，则将这个区间进一步拆分为N个子区间，并重复此规则，继续做递归交集判断（N由merge_tree_coarse_index_granularity指定，默认值为8） 如果存在交集，且MarkRange不可再分解，则记录MarkRange并返回    合并MarkRange区间： 将最终匹配的MarkRange聚在一起，合并它们的范围。\n  MergeTree通过递归的形式持续向下拆分区间，最终将MarkRange定位到最细的粒度，以帮助在后续读取数据的时候，能够最小化扫描数据的范围。\n联合主键 当我们以需要以多个字段为主键时，此时数据的查询和存储就涉及到另外一种规则。\n例如以 (CounterID, Date) 以主键，片段中数据首先按 CounterID 排序，具有相同 CounterID 的部分按 Date 排序。排序好的索引的图示会是下面这样：\n1 2 3 4 5 6  全部数据:[-------------------------------------------------------------------------] CounterID:[aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll]Date:[1111111222222233331233211111222222333211111112122222223111112223311122333]标记:|||||||||||a,1a,2a,3b,3e,2e,3g,1h,2i,1i,3l,3标记号:012345678910  如果指定查询如下：\n CounterID in ('a', 'h')，服务器会读取标记号在 [0, 3) 和 [6, 8) 区间中的数据。 CounterID IN ('a', 'h') AND Date = 3，服务器会读取标记号在 [1, 3) 和 [7, 8) 区间中的数据。 Date = 3，服务器会读取标记号在 [1, 10] 区间中的数据。  上面例子可以看出使用索引通常会比全表描述要高效。\n  稀疏索引会引起额外的数据读取。当读取主键单个区间范围的数据时，每个数据块中最多会多读 index_granularity * 2 行额外的数据。\n  稀疏索引使得你可以处理极大量的行，因为大多数情况下，这些索引常驻与内存（RAM）中。\n  从上面可以看出，ClickHouse的联合主键在某种程度上与我们熟知的最左前缀规则有点类似，通常在以下几种场景下我们才会考虑使用联合索引\n 查询会使用 b 列作为条件 很长的数据范围（ index_granularity 的数倍）里 a 都是相同的值，并且这样的情况很普遍。换言之，就是加入另一列后，可以让你的查询略过很长的数据范围。 数据量大，需要改善数据压缩（以主键排序片段数据，数据的一致性越高，压缩越好）  长的主键会对插入性能和内存消耗有负面影响，但主键中额外的列并不影响 SELECT 查询的性能。\n二级索引 除了一级索引之外，MergeTree同样支持二级索引。二级索引又称跳数索引，由数据的聚合信息构建而成。根据索引类型的不同，其聚合信息的内容也不同。跳数索引的目的与一级索引一样，也是帮助查询时减少数据扫描的范围。\n==（二级索引目前还处于测试阶段，官方不建议大量使用）==\n跳数索引 目前，MergeTree共支持4种跳数索引，分别是minmax（最值）、set（集合行数）、ngrambf_v1（N-Gram布隆过滤器）和tokenbf_v1（Token布隆过滤器）。一张数据表支持同时声明多个跳数索引。\n  minmax（最值索引）：minmax索引记录了一段数据内的最小和最大极值，其索引的作用类似分区目录的minmax索引，能够快速跳过无用的数据区间。\n  1  示例：INDEX[index_name][column]TYPEminmaxGRANULARITY[GRANULARITYSIZE]      set（集合行数索引）：set索引直接记录了声明字段或表达式的不重复值，用于检测数据块是否满足WHERE条件。\n  1 2 3  示例：INDEX[index_name][column]TYPEset(max_rows)GRANULARITY[index_granularity]-- max_rows是一个阈值，表示在一个index_granularity内，索引最多记录的数据行数。(如果max_rows=0，则表示无限制)       ngrambf_v1（N-Gram布隆过滤器）：ngrambf_v1索引记录的是指定长度的数据短语的布隆表过滤器，只支持String和FixedString数据类型，同时只能够提升in、notIn、like、equals和notEquals查询的性能。\n  1 2 3 4 5 6 7 8  示例：INDEX[index_name][column]TYPEngrambf_v1(n,size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed)GRANULARITY[index_granularity]/* n:token长度，依据n的长度将数据切割为token短语。 size_of_bloom_filter_in_bytes：布隆过滤器的大小。 number_of_hash_functions：布隆过滤器中使用Hash函数的个数。 random_seed: Hash函数的随机种子。 */    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  布隆过滤器可能会包含不符合条件的匹配，所以ngrambf_v1,tokenbf_v1和bloom_filter索引不能用于负向的函数，例如：--可以用来优化的场景 sLIKE\u0026#39;%test%\u0026#39;NOTsNOTLIKE\u0026#39;%test%\u0026#39;s=1NOTs!=1startsWith(s,\u0026#39;test\u0026#39;)i--不能用来优化的场景 NOTsLIKE\u0026#39;%test%\u0026#39;sNOTLIKE\u0026#39;%test%\u0026#39;NOTs=1s!=1NOTstartsWith(s,\u0026#39;test\u0026#39;)      tokenbf_v1（Token布隆过滤器）：tokenbf_v1索引是ngrambf_v1的变种，同样也是一种布隆过滤器索引。tokenbf_v1除了短语token的处理方法外，其他与ngrambf_v1是完全一样的。tokenbf_v1会自动按照非字符的、数字的字符串分割token。\n  1  示例：INDEXdIDTYPEtokenbf_v1(size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed)      granularity 对于跳数索引而言，index_granularity定义了数据的粒度，而granularity定义了聚合信息汇总的粒度。换言之，granularity定义了一行跳数索引能够跳过多少个index_granularity区间的数据。\n作用规则如下：首先，按照index_granularity粒度间隔将数据划分成n段，总共有[0 , n-1]个区间（n = total_rows /index_granularity，向上取整）。接着，根据索引定义时声明的表达式，从0区间开始，依次按index_granularity粒度从数据中获取聚合信息，每次向前移动1步(n+1)，聚合信息逐步累加。最后，当移动granularity次区间时，则汇总并生成一行跳数索引数据。\n以minmax索引为例，假设index_granularity=8192且granularity=3，则数据会按照index_granularity划分为n等份，MergeTree从第0段分区开始，依次获取聚合信息。当获取到第3个分区时（granularity=3），则汇总并会生成第一行minmax索引（前3段minmax极值汇总后取值为[1 , 9]），如下图\n数据标记 如果把MergeTree比作一本书，primary.idx一级索引好比这本书的一级章节目录，.bin文件中的数据好比这本书中的文字，那么数据标记(.mrk)就好比书签一样，会为一级章节目录和具体的文字之间建立关联。\n对于数据标记而言，它记录了两点重要信息：\n 一级章节对应的页码信息。 一段文字在某一页中的起始位置信息。  这样一来，通过数据标记就能够很快地从一本书中立即翻到关注内容所在的那一页，并知道从第几行开始阅读。\n标记数据与一级索引数据不同，它并不能常驻内存，而是使用LRU（最近最少使用）缓存策略加快其取用速度。\n生成规则 从上图可以看出，数据标记和索引区间是对齐的，均按照index_granularity的粒度间隔。如此一来，只需简单通过索引区间的下标编号就可以直接找到对应的数据标记。\n为了能够与数据衔接，数据标记文件也与.bin文件一一对应。即每一个列字段[Column].bin文件都有一个与之对应的[Column].mrk数据标记文件，用于记录数据在.bin文件中的偏移量信息。同时，.mrk包含了.bin压缩和解压缩这两种不同状态的偏移量，如下图\n工作方式 MergeTree在读取数据时，必须通过标记数据的位置信息才能够找到所需要的数据。整个查找过程大致可以分为读取压缩数据块和读取数据两个步骤。\n对于下图来说，表的index_granularity粒度为8192，所以一个索引片段的数据大小恰好是8192B。按照压缩数据块的生成规则，如果单个批次数据小于64KB，则继续获取下一批数据，直至累积到size\u0026gt;=64KB时，生成下一个压缩数据块。因此在JavaEnable的标记文件中，每8行标记数据对应1个压缩数据块（1B * 8192 = 8192B, 64KB = 65536B, 65536 / 8192 =8）。\n从图能够看到，其左侧的标记数据中，8行数据的压缩文件偏移量都是相同的，因为这8行标记都指向了同一个压缩数据块。而在这8行的标记数据中，它们的解压缩数据块中的偏移量，则依次按照8192B（每行数据1B，每一个批次8192行数据）累加，当累加达到65536(64KB)时则置0。因为根据规则，此时会生成下一个压缩数据块。\n  读取压缩数据块： 在查询某一列数据时，MergeTree无须一次性加载整个.bin文件，而是可以根据需要，只加载特定的压缩数据块。而这项特性需要借助标记文件中所保存的压缩文件中的偏移量。\n  读取数据： 在读取解压后的数据时，MergeTree并不需要一次性扫描整段解压数据，它可以根据需要，以index_granularity的粒度加载特定的一小段。为了实现这项特性，需要借助标记文件中保存的解压数据块中的偏移量。\n  数据标记与压缩数据块的对应关系 由于压缩数据块的划分，与一个间隔index_granularity内的数据大小相关，每个压缩数据块的体积都被严格控制在64KB～1MB。而一个间隔index_granularity的数据，又只会产生一行数据标记。那么根据一个间隔内数据的实际字节大小，数据标记和压缩数据块之间会产生三种不同的对应关系。\n 一对一  一个数据标记对应一个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size大于等于64KB且小于等于1MB时，会出现这种对应关系。    一对多  一个数据标记对应多个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size直接大于1MB时，会出现这种对应关系。    多对一  多个数据标记对应一个压缩数据块，当一个间隔index_granularity内的数据未压缩大小size小于64KB时，会出现这种对应关系。     工作流程 存储流程 数据的存储流程主要有以下几个步骤\n  首先生成分区目录，伴随着每一批数据的写入，都会生成一个新的分区目录。\n  在后续的某一时刻，属于相同分区的目录会依照规则合并到一起\n  接着，按照index_granularity索引粒度，会分别生成primary.idx一级索引（如果声明了二级索引，还会创建二级索引文件）、每一个列字段的．mrk数据标记和．bin压缩数据文件。\n  查询流程 数据查询的本质，可以看作一个不断减小数据范围的过程。在最理想的情况下，MergeTree首先可以依次借助分区索引、一级索引和二级索引，将数据扫描范围缩至最小。然后再借助数据标记，将需要解压与计算的数据范围缩至最小。\n如果一条查询语句没有指定任何WHERE条件，或是指定了WHERE条件，但条件没有匹配到任何索引（分区索引、一级索引和二级索引），那么MergeTree就不能预先减小数据范围。在后续进行数据查询时，它会扫描所有分区目录，以及目录内索引段的最大区间。虽然不能减少数据范围，但是MergeTree仍然能够借助数据标记，以多线程的形式同时读取多个压缩数据块，以提升性能。\n","date":"2022-05-23T18:18:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86mergetree%E5%BC%95%E6%93%8E/","title":"ClickHouse 数据存储原理：MergeTree引擎"},{"content":"ClickHouse 基本概念 ClickHouse 是一个用于联机分析（OLAP）的列式数据库管理系统（DBMS）。\nOLAP 什么是 OLAP? OLAP 名为联机分析，又可以称为多维分析，是由关系型数据库之父埃德加·科德（EdgarFrank Codd）于 1993 年提出的概念。顾名思义，它指的是通过多种不同的维度审视数据，进行深层次分析。\n维度可以看作观察数据的一种视角，例如人类能看到的世界是三维的，它包含长、宽、高三个维度。直接一点理解，维度就好比是一张数据表的字段，而多维分析则是基于这些字段进行聚合查询。\n如上图，多维分析包含以下几种操作：\n 下钻： 从高层次向低层次明细数据穿透，例如从省下钻到市。 上卷： 和下钻相反，从低层次向高层次汇聚，例如从市汇聚成省。 切片： 观察立方体的一层，将一个或多个维度设为单个固定值，然后观察剩余的维度，例如将商品维度固定为足球。 切块： 与切片类似，只是将单个固定值变成多个值。例如将商品维度固定成足球、篮球。 旋转： 旋转立方体的一面，如果要将数据映射到一张二维表，那么就要进行旋转，这就等同于行列置换。  OLAP 与 OLTP OLTP（on-line transaction processing）翻译为联机事务处理， OLAP（On-Line Analytical Processing）翻译为联机分析处理。\n  从字面上来看 OLTP 是做事务处理，OLAP 是做分析处理。\n  从对数据库操作来看，OLTP 主要是对数据的增删改，OLAP 是对数据的查询。\n  因为 OLTP 所产生的业务数据分散在不同的业务系统中，而 OLAP 往往需要将不同的业务数据集中到一起进行统一综合的分析，这时候就需要根据业务分析需求做对应的数据清洗后存储在数据仓库中，然后由数据仓库来统一提供 OLAP 分析。\n  OLTP 是数据库的应用，OLAP 是数据仓库的应用\n  下面用一张图来简要对比。\n列式存储 列式存储与行式存储 在传统的行式数据库系统中，处于同一行中的数据总是被物理的存储在一起，存储方式如下图：\n在列式数据库系统中，来自不同列的值被单独存储，来自同一列的数据被存储在一起，数据按如下的顺序存储：\n不同的数据存储方式适用不同的业务场景，而对于 OLAP 来说，列式存储是最适合的选择。\n列式存储与 OLAP 为什么列式数据库更适合于 OLAP 场景呢？下面这两张图就可以给你答案\n 行式数据库     列式数据库      下面分别从两个 I/O 和 CPU 两个角度来分析为什么他们有如此之大的差别\n I/O  针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。 由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了 I/O 的体积。 由于 I/O 的降低，这将帮助更多的数据被系统缓存,进一步降低了数据传输的成本。   CPU  由于执行一个查询需要处理大量的行，因此在整个向量上执行所有操作将比在每一行上执行所有操作更加高效。同时这将有助于实现一个几乎没有调用成本的查询引擎。如果你不这样做，使用任何一个机械硬盘，查询引擎都不可避免的停止 CPU 进行等待。所以，在数据按列存储并且按列执行是很有意义的。    列式存储与数据压缩 如果你想让查询变得更快，最简单且有效的方法是减少数据扫描范围和数据传输时的大小，而列式存储和数据压缩就可以帮助我们实现上述两点。\n列式存储和数据压缩通常是伴生的。数据按列存储。而具体到每个列字段，数据也是独立存储的，每个列字段都拥有一个与之对应的 .bin 数据文件，相同类型的数据放在同一个文件中，对压缩更加友好。数据默认使用 LZ4 算法压缩，在 Yandex.Metrica 的生产环境中，数据总体的压缩比可以达到 8:1（未压缩前 17PB，压缩后 2PB）。\n核心特点 完备的 DBMS 功能 ClickHouse 拥有完备的管理功能，所以它称得上是一个 DBMS（Database Management System，数据库管理系统），而不仅是一个数据库。作为一个 DBMS，它具备了一些基本功能，\n如下所示。\n DDL（数据定义语言）：可以动态地创建、修改或删除数据库、表和视图，而无须重启服务。 DML（数据操作语言）：可以动态查询、插入、修改或删除数据。 权限控制：可以按照用户粒度设置数据库或者表的操作权限，保障数据的安全性。 数据备份与恢复：提供了数据备份导出与导入恢复机制，满足生产环境的要求。 分布式管理：提供集群模式，能够自动管理多个数据库节点。  关系模型与 SQL 查询 相比 HBase 和 Redis 这类 NoSQL 数据库，ClickHouse 使用关系模型描述数据并提供了传统数据库的概念（数据库、表、视图和函数等）。与此同时，ClickHouse 完全使用 SQL 作为查询语言（支持 GROUP BY、ORDER BY、JOIN、IN 等大部分标准 SQL），这使得它平易近人，容易理解和学习。\n向量化表引擎 向量化执行，可以简单地看作从硬件的角度上消除程序中循环的优化。\n为了实现向量化执行，需要利用 CPU 的 SIMD 指令。SIMD 的全称是 Single Instruction MultipleData，即用单条指令操作多条数据。现代计算机系统概念中，它是通过数据并行以提高性能的一种实现方式，它的原理是在 CPU 寄存器层面实现数据的并行操作。例如有 8 个 32 位整形数据都需要进行移位运行，则由一条对 32 位整形数据进行移位的指令重复执行 8 次完成。SIMD 引入了一组大容量的寄存器，一个寄存器包含 8 * 32 位，可以将这 8 个数据按次序同时放到一个寄存器。同时，CPU 新增了处理这种 8 * 32 位寄存器的指令，可以在一个指令周期内完成 8 个数据的位移运算。（本质就是将每次处理的数据从一条变为一批）\n多样化的表引擎 与 MySQL 类似，ClickHouse 也将存储部分进行了抽象，把存储引擎作为一层独立的接口。ClickHouse 共拥有合并树、内存、文件、接口和其他 6 大类 20 多种表引擎。其中每一种表引擎都有着各自的特点，用户可以根据实际业务场景的要求，选择合适的表引擎使用。\n多主架构 ClickHouse 则采用 Multi-Master多主架构，集群中的每个节点角色对等，客户端访问任意一个节点都能得到相同的效果。这种多主的架构有许多优势，例如对等的角色使系统架构变得更加简单，不用再区分主控节点、数据节点和计算节点，集群中的所有节点功能相同。所以它天然规避了单点故障的问题，非常适合用于多数据中心、异地多活的场景。\n多线程与分布式 在各服务器之间，通过网络传输数据的成本是高昂的，所以相比移动数据，更为聪明的做法是预先将数据分布到各台服务器，将数据的计算查询直接下推到数据所在的服务器。ClickHouse 在数据存取方面，既支持分区（纵向扩展，利用多线程原理），也支持分片（横向扩展，利用分布式原理），可以说是将多线程和分布式的技术应用到了极致。\n分片与分布式查询 数据分片是将数据进行横向切分，这是一种在面对海量数据的场景下，解决存储和查询瓶颈的有效手段，是一种分治思想的体现。ClickHouse 支持分片，而分片则依赖集群。每个集群由 1 到多个分片组成，而每个分片则对应了 ClickHouse 的 1 个服务节点。分片的数量上限取决于节点数量（1 个分片只能对应 1 个服务节点）。\nClickHouse 并不像其他分布式系统那样，拥有高度自动化的分片功能。ClickHouse 提供了**本地表（Local Table）与分布式表（Distributed Table）**的概念。一张本地表等同于一份数据的分片，而分布式表本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。借助分布式表，能够代理访问多个数据分片，从而实现分布式查询。\n应用场景 擅长的场景  绝大多数是读请求 数据以相当大的批次（\u0026gt; 1000 行）更新，而不是单行更新;或者根本没有更新。 已添加到数据库的数据不能修改。 对于读取，从数据库中提取相当多的行，但只提取列的一小部分。 宽表，即每个表包含着大量的列。 查询相对较少（通常每台服务器每秒查询数百次或更少）。 对于简单查询，允许延迟大约 50 毫秒。 列中的数据相对较小：数字和短字符串（例如，每个 URL 60 个字节）。 处理单个查询时需要高吞吐量（每台服务器每秒可达数十亿行）。 事务不是必须的。 对数据一致性要求低。 每个查询有一个大表。除了他以外，其他的都很小。 查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的 RAM 中。  不擅长的场景  OLTP 事务性操作（不支持事务，不支持真正的更新/删除） 不擅长根据主键按行粒度进行查询（如 select * from table where user_id in (xxx, xxx, xxx, ...)） 不擅长存储和查询 blob 或者大量文本类数据（按列存储） 不擅长执行有大量 join 的查询（Distributed 引擎局限） 不支持高并发，官方建议 QPS \u0026lt;= 100  Clickhouse 为什么会这么快？ 首先亮出官方的测试报告：Clickhouse 性能对比报告\n所有用于对比的数据库都使用了相同配置的服务器，在单个节点的情况下，对一张拥有 133 个字段的数据表分别在 1000 万、1 亿和 10 亿这三种数据体量下执行基准测试，基准测试的范围涵盖 43 项 SQL 查询。\n市面上有很多与 Clickhouse 采用了同样技术(如列式存储、向量化引擎等)的数据库，但为什么 ClickHouse 的性能能够将其他数据库远远甩在身后呢？这主要依赖于下面几个方面\n 着眼硬件，先想后做  ClickHouse 会在内存中进行 GROUP BY，并且使用 HashTable 装载数据。 ClickHouse 非常在意 CPU L3 级别的缓存，因为一次 L3 的缓存失效会带来 70 ～ 100ns 的延迟。这意味着在单核CPU上，它会浪费 4000 万次/秒的运算；而在一个 32 线程的 CPU 上，则可能会浪费 5 亿次/秒的运算。   算法在前，抽象在后  对于常量，使用 Volnitsky 算法； 对于非常量，使用 CPU 的向量化执行 SIMD（用于文本转换、数据过滤、数据解压和 JSON 转换等），暴力优化； 正则匹配使用 re2 和 hyperscan 算法。性能是算法选择的首要考量指标。   勇于尝鲜，不行就换  除了字符串之外，其余的场景也与它类似，ClickHouse 会使用最合适、最快的算法。如果世面上出现了号称性能强大的新算法，ClickHouse 团队会立即将其纳入并进行验证。如果效果不错，就保留使用；如果性能不尽人意，就将其抛弃。   特定场景，特殊优化  针对同一个场景的不同状况，选择使用不同的实现方式，尽可能将性能最大化。 例如去重计数 uniqCombined 函数，会根据数据量的不同选择不同的算法：当数据量较小的时候，会选择 Array 保存；当数据量中等的时候，会选择 HashSet；而当数据量很大的时候，则使用 HyperLogLog 算法。 针对不同的场景，Clickhouse 提供了 MergeTree 引擎家族，如 MergeTree、ReplacingMergeTree、SummingMergeTree、AggregatingMergeTree、CollapsingMergeTree和VersionedCollapsingMergeTree 等。   持续测试，持续改进  由于 Yandex 的天然优势，ClickHouse 经常会使用真实的数据进行测试，这一点很好地保证了测试场景的真实性。 ClickHouse 差不多每个月都能发布一个版本，正因为拥有这样的发版频率，ClickHouse 才能够快速迭代、快速改进。    ClickHouse 的架构 目前 ClickHouse 公开的资料相对匮乏，比如在架构设计层面就很难找到完整的资料，甚至连一张整体的架构图都没有，根据官网提供的信息，我们能够得出一个大概的架构，如下图\n  Parser： Parser 分析器可以将一条 SQL 语句以递归下降的方法解析成 AST 语法树的形式。不同的 SQL 语句，会经由不同的 Parser 实现类解析。\n  Interpreter：Interpreter 解释器的作用就像 Service 服务层一样，起到串联整个查询过程的作用，它会根据解释器的类型，聚合它所需要的资源。首先它会解析AST对象；然后执行“业务逻辑”（例如分支判断、设置参数、调用接口等）；最终返回IBlock对象，以线程的形式建立起一个查询执行管道。\n  Tables：Tables由 IStorage 接口表示。该接口的不同实现对应不同的表引擎。比如 StorageMergeTree、StorageMemory 等。这些类的实例就是表。\n IStorage 接口定义了DDL（如 ALTER、RENAME、OPTIMIZE 和 DROP 等）、read 和 write 方法，它们分别负责数据的定义、查询与写入。在数据查询时，IStorage 负责根据 AST 查询语句的指示要求，返回指定列的原始数据。 后续对数据的进一步加工、计算和过滤，则会统一交由Interpreter解释器对象处理。对Table发起的一次操作通常都会经历这样的过程，接收AST查询语句，根据AST返回指定列的数据，之后再将数据交由Interpreter做进一步处理。    Block与Block Streams：ClickHouse 内部的数据操作是面向 Block 对象进行的，并且采用了流的形式。\n Block：虽然 Column 和 Filed 组成了数据的基本映射单元，但对应到实际操作，它们还缺少了一些必要的信息，比如数据的类型及列的名称。于是 ClickHouse 设计了 Block对象，Block 对象可以看作数据表的子集。Block 对象的本质是由数据对象、数据类型和列名称组成的三元组，即 Column、DataType 及列名称字符串。Column 提供了数据的读取能力，而DataType知道如何正反序列化，所以 Block 在这些对象的基础之上实现了进一步的抽象和封装，从而简化了整个使用的过程，仅通过Block对象就能完成一系列的数据操作。在具体的实现过程中，Block 并没有直接聚合Column和DataType对象，而是通过ColumnWithTypeAndName对象进行间接引用。 Block Streams：Block Streams 用于处理数据。我们可以使用 Block Streams 从某个地方读取数据，执行数据转换，或将数据写到某个地方。IBlockInputStream 具有 read 方法，其能够在数据可用时获取下一个块。IBlockOutputStream 具有 write 方法，其能够将块写到某处。    Functions：ClickHouse 主要提供两类函数——普通函数和聚合函数。\n Function：普通函数由IFunction接口定义，其是没有状态的，函数效果作用于每行数据之上。当然，在函数具体执行的过程中，并不会一行一行地运算，而是采用向量化的方式直接作用于一整列数据。 AggregateFunction：聚合函数由IAggregateFunction接口定义，相比无状态的普通函数，聚合函数是有状态的，并且聚合函数的状态支持序列化与反序列化，所以能够在分布式节点之间进行传输，以实现增量计算。    DataType：数据的序列化和反序列化工作由 DataType 负责。根据不同的数据类型，IDataType 接口会有不同的实现类。DataType 虽然会对数据进行正反序列化，但是它不会直接和内存或者磁盘做交互，而是转交给 Column 和 Filed 处理。\n  Column 与 Field：Column 和 Field 是 ClickHouse 数据最基础的映射单元。\n Column：内存中的一列数据由一个 Column 对象表示。Column 对象分为接口和实现两个部分，在IColumn接口对象中，定义了对数据进行各种关系运算的方法，例如插入数据的insertRangeFrom和insertFrom方法、用于分页的cut，以及用于过滤的filter方法等。而这些方法的具体实现对象则根据数据类型的不同，由相应的对象实现。 Field：在大多数场合，ClickHouse 都会以整列的方式操作数据，但凡事也有例外。如果需要操作单个具体的数值（也就是单列中的一行数据），则需要使用 Field 对象，Field 对象代表一个单值。与 Column 对象的泛化设计思路不同，Field 对象使用了聚合的设计模式。在 Field 对象内部聚合了 Null、UInt64、String 和 Array 等 13 种数据类型及相应的处理逻辑。    ","date":"2022-05-23T18:16:13+08:00","permalink":"https://blog.orekilee.top/p/clickhouse-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"ClickHouse 基本概念"}]